\documentclass[a4paper,10pt]{apuntes}

\newcommand{\definition}[1]{\paragraph{Definicion: #1\\}}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{exmath}


\title{Análisis Matemático}
\author{Víctor de Juan Sanz}
\date{13/14 C1}

\pdfinfo{%
  /Title    ()
  /Author   (Víctor de Juan)
  /Creator  ()
  /Producer ()
  /Subject  ()
  /Keywords ()
}

\begin{document}
\maketitle
\newpage
\tableofcontents
\newpage
Datos de interés:\\
Jesus Garcia Azorero\\
Despacho: 17-608\\
Correo: jesus.azorero@uam.es


\section{Contenido de la asignatura}
\subsection{Preliminares}
Repaso de contenidos de Cálculo II como conjuntos abiertos y cerrados, gradiente \dots
\subsection{Teorema funcion inversa, implicita y rango}
Aplicación a funciones no lineales de los teoremas fundamentales de cálculo II
\subsection{Mínimos y máximos condicionados}
Multiplicadores de Lagrange
\subsection{Subvariedades diferenciales}
Objetos de dimensión n en espacios de dimensión m ($n<m$).
\subsection{Integración en subvariedades diferenciales}
\subsection{Teorema de Stokes}
Demostración del teorema con lenguaje de las formas diferenciales.
\newpage


\section{Preliminares del análisis matemático}
A lo largo del curso vamos a trabajar en $\real^n={(x_1,\dotsc, x_n) \  x_j \in \real, j=1,...,N}$
\subsection{Producto escalar, norma y distancia}
Durante todo el año denotaremos al vector $(x_1,x_2,\dots,x_n)$ como $\gx$ por comodidad.

\begin{defn}[Producto escalar euclídeo]

\[ \pesc{x,y} = \sum_{i=1}^{N} x_iy_i \]

Propiedades:
\begin{itemize}
 \item $\pesc{\lambda \gx, \gy} = \lambda \pesc{\gx, \gy}$
 \item $\pesc{\overline{x} + \overline{y}, \overline{z}}= \pesc{\gx+\gz}+\pesc{\gx,\gy}$
 \item $\pesc{gx, \gy} = \pesc{\gy,\gx}$
 \item $\pesc{\gx, \gx} ≥ 0$
 \item $\pesc{\gx,\gx} = 0 \implies \gx = \gor{0}$
\end{itemize}

Las tres primeras son la consecuencia de que el producto escalar tiene que ser bilineal.
\end{defn}

\begin{defn}[Norma\IS euclídea]

\[ \md{\gx} = \left(\pesc{\gx,\gx}\right)^{\frac{1}{2}} = \dotsb = \left(\sum_{i=1}^{N}x_i^2\right)^{\frac{1}{2}} \]
Propiedades:
\begin{itemize}
 \item  $\md{\gx} = 0 \dimplies \gx = 0$
 \item	Homogeneidad: $\md{\lambda\gx} = \lambda\md{\gx}$
 \item	$\md{\gx+\gy} \leq \md{\gx}+\md{\gx}$
\end{itemize}

\end{defn}


\begin{defn}[Norma]
Cualquier operación que cumpla las 3 propiedades anteriores es una \textbf{norma}.
\end{defn}

\begin{lemma}
$\md{\gx} = (\gx * \gx) ^ \frac{1}{2}$ para cualquier producto escalar $*$.
\end{lemma}

\begin{defn}[Norma\IS $p$] Las normas p $\md{\cdot}_p$ se definen todas de la misma forma:

\[ \md{\gx}_p = \left(\sum_{i=1}^N |x_i|^p \right)^\frac{1}{p} \]

\end{defn}

Hay dos casos particulares, la norma uno \index{Norma!uno}
\[ \md{\gx}_1 = |x_1| + |x_2| + ... + |x_n| \]

y la norma infinito \index{Norma!infinito}
\[\md{\gx}_{\infty} = \max\left\{|x_1|,|x_2|,\dots,|x_n|\right\} \]

 Vamos a demostrar que la norma $p$ cumple las 3 propiedades de una norma. Para ello, nos apoyaremos en dos teoremas previos:

\begin{theorem}[Desigualdad\IS de Young]
 Sea $p > 1$ y tomamos $p'$ tal que $\frac{1}{p}+\frac{1}{p'} = 1$ (exponente conjugado). Entonces:

\[ |ab| \leq \frac{1}{p} \cdot |a|^p +\frac{1}{p'} |b| ^ {p'} \]
\end{theorem}
 
\begin{proof}
Se utiliza la idea de la función logaritmo, que es cóncava\footnote{Geométricamente, cóncava significa que si uno 2 puntos de la gráfica, la recta que los une queda debajo de la gráfica.} y creciente.  Tomando 2 puntos $A$ y $B$ tenemos la condición de concavidad 

\[ t \log A + (1-t) \log B \leq \log (tA + (1-t) \cdot B)\]

. Utilizando la derivada hallamos la ecuación de la recta que pasa por $A$ y por $B$ y tomamos un punto que dista $t$ de $A$ y $(1-t)$ de $B$. Como la función es cóncava sabemos que ese valor será menor que el valor del logaritmo en un punto $t$ entre $A$ y $B$.

Tomando $A=|a| ^ p$, $B = |b| ^ p$ y $t = \frac{1}{p} \rightarrow 1-t = \frac{1}{p'}$ tenemos que

\begin{align*}
\frac{1}{p}\cdot log |a|^p + \frac{1}{p'} \cdot log|b|^{p'} &\leq log\left(\frac{1}{p}|a|^p + \frac{1}{p'} |b|^{p'}\right) \\
\log \abs{a} + \log \abs{b} &\leq log\left(\frac{1}{p}|a|^p + \frac{1}{p'} |b|^{p'}\right) \\
\log \abs{ab} &\leq log\left(\frac{1}{p}|a|^p + \frac{1}{p'} |b|^{p'}\right) \\
\abs{ab} &\leq \frac{1}{p}|a|^p + \frac{1}{p'} |b|^{p'} 
\end{align*}

\end{proof} 

\begin{theorem}[Desigualdad\IS de Hölder] Se trata de una generalización de la desigualdad de Cauchy-Schwarz, que ocurre en el caso $p=2$
\[ \sum_{i=1} ^ N \abs{x_i y_i} \leq \md{\gx}_p \md{y_i}_p \]

\label{thmHolder}
\end{theorem}

\begin{proof} Tomamos  
 \begin{align*}
 a_i &= \frac{\abs{x_i}}{\md{\gx}_p} \\
 b_i &= \frac{\abs{y_i}}{\md{\gy}_{p'}}
 \end{align*}
 
 Tenemos que \[ a_i b_i \leq \frac{1}{p}a_i ^ p + \frac{1}{p'} b_i^{p'} \]
 
 Sustituimos: 
 \[  frac{\abs{x_i}}{\md{x}_{p}} \cdot \frac{\abs{y_i}}{\md{y}_{p}} \leq \frac{\abs{x_i}^p}{p\cdot \md{x}_{p}^p} + \frac{\abs{y_i}^{p'}}{p'\cdot\md{y}_{p'}^{p}} \]
 
 Tomamos sumatorios y, teniendo en cuenta que $\md{x}_{p}^p = \sum|x_i|^p$, nos queda 
 
 \[ \frac{1}{\md{x}_{p}\md{y}_{p'}}\cdot \sum_{i=1}^N |x_iy_i| \leq \frac{1}{p\md{x}_{p}^p} \sum |x_i|^p + \frac{1}{p'\md{y}_{p'}^{p'}} \sum |y_i|^{p'} = \frac{1}{p} + \frac{1}{p'} = 1 \]
 
 \end{proof}
 
Una vez probadas las dos desigualdades anteriores, pasamos a probar la desigualdad triangular: 
 
\begin{proof} El objetivo es demostrar que \[ \md{\gx+\gy}_p \leq \md{\gx}_p+\md{\gy}_p \] y vamos a hacerlo en varios pasos.
 
 Para evitarnos las raíces empezamos con $\md{\gx+\gy}_p^p$

 \begin{gather*}
 \md{\gx+\gy}_p ^ p = \sum_1 ^ N |x_i+y_i| ^ p = \sum_ 1 ^ N |x_i+y_i| \cdot |x_i+y_i| ^ {p-1} =\\
 = \sum_1 ^ N |x_i|\cdot|x_i+y_i| ^ {p-1} + \sum_1^N |y_i| \cdot |x_i+y_i|^{p-1}
 \end{gather*}

 Utilizando la desigualdad de Hölder (\ref{thmHolder}) tenemos:
 
 \begin{multline*} \md{\gx+\gy}_p ^ p \leq \\
 \sum \left(|x_i|^p\right)^{\frac{1}{p}} \cdot \underbrace{\sum \left(\left(|x_i+y_i|^{p-1}\right)^{p'}\right)^{\frac{1}{p'}}}_* +
 \sum \left(|y_i|^p\right)^{\frac{1}{p}} \cdot \underbrace{\sum \left(\left(|X_i+y_i|^{p-1}\right)^{p'}\right)^{\frac{1}{p'}}}_* 
 \end{multline*}
 
 Por ser $p$ y $p'$ exponentes conjugados es fácil comprobar que $1-\frac{1}{p'} = \frac{1}{p}$\\
 Sacamos factor común y pasamos al otro lado obteniendo (PASO INTERMEDIO?)
\[ \left(\sum_1^N |x_i+y_i|^p\right)^{\overbrace{\left(1-\frac{1}{p'}\right)}^p} = \md{\gx+\gy}_p \leq \md{x}_{p} + \md{y}_{p} \]

\textit{Guille: esta demostración es muy, muy rara.}
\end{proof}


EJERCICIO PROPUESTO: Tomamos en el plano el conjunto de los puntos cuya norma es 1. Tomando en la norma p=2 sale la circunferencia. ¿Y en p=3? 


\begin{remark} Estos argumentos se pueden utilizar para demostrar
\[ \int \abs{f\cdot g}\, dx \leq \left(\int\abs{f}^p\, dx\right) ^ \frac{1}{p} \cdot \left(\int\abs{g}^{p'}\,dx\right)^\frac{1}{p'} \]

\textit{COMPLETAR.}
\end{remark}

\begin{defn}[Distancia\IS euclídea]
\[d(\gx,\gy) = \md{\gy - \gx} \]
\end{defn}

Propiedades:
\begin{itemize}
 \item La distancia es siempre positiva: $d(\gx,\gy)\ge 0$
 \item $d(\gx,\gx) = 0$
 \item Simetría: $d(\gx,\gy) = d(\gy, \gx)$
 \item Desigualdad triangular $d(\gx,\gz) \leq d(\gx,\gy) + d(\gy,\gz)$. La distancia entre 2 puntos es menor o igual en línea recta que pasando por un punto intermedio.
\end{itemize}


\begin{defn}[Distancia] Cualquier operacion que cumpla las 3 propiedades anteriores es una distancia. \end{defn}

\paragraph{Recapitulando}
Con un producto escalar puedo definir una norma y con esa norma puedo definir una distancia. Pero... ¿Podemos definir una norma 
que no venga de un producto escalar y/o alguna distancia que no provenga de una norma? Sí, por ejemplo

\[ \tilde{d} (\gx,\gy) = \abs{\arctg(y) - \arctg(x)} \] 

No cuesta mucho comprobar que cumple las 3 propiedades de una distancia. Además, distancia es cuanto menos curiosa porque nunca será mayor de $\pi$.

 Podemos comprobar que si existiera una norma que midiese esta distancia tendríamos \[\tilde{\md{\gx}} = \tilde{d} (\gx,\gor{0}) = \abs{\arctg (x)} \]
 pero esto no cumple la propiedad: $\tilde{\md{\lambda x}} = \abs{\arctg \lambda x} \neq \abs{\lambda}\abs{\arctg x} = 
 \abs{\lambda x}\tilde{||x||}$
 ya que ninguna distancia puede ser mayor que $\pi$ y tomando un $\lambda > \pi$ se produciría la contradicción.

\subsection{Relación norma - producto escalar}
\label{secNormaprodEsc}
\begin{theorem}
Supongamos que tengo un producto escalar $\ast$ y una norma asociada \[ \md{\gx} = (\gx\ast \gx) ^ {\frac{1}{2}}\]. Entonces \[ \md{\gx + \gy}^ 2 =  \md{\gx} ^ 2 + \md{\gy} ^ 2+2(\gx\ast\gy) \]
\end{theorem}

\begin{proof}
\[ \md{\gx+\gy}^2 = (\gx+\gy)\ast(\gx+\gy) = \gx \ast \gx + \gx \ast \gy + \gy \ast \gx + \gy \ast \gy=\md{\gx} ^ 2 + \md{\gy} ^ 2+2(\gx\ast\gy) \]
\end{proof}

Esa norma asociada al producto escalar tiene dos propiedades importantes:
\begin{itemize}
\item Paralelogramo: $\md{\gx+\gy}^ 2 + \md{\gx-\gy} ^ 2 = 2\left(\md{x}^2+\md{x}^2\right) $
\item Polarización: $\md{\gx+\gy} ^ 2 - \md{\gx-\gy} ^ 2 = 4(\gx*\gy)$
\end{itemize}


\subsection{Equivalencia de normas}
Sea $\md{\cdot}$ una norma en $\real^N$. Si intento calcular la norma de un vector $\gx$

\[ \md{\gx} = \md{\sum x_i e_i} \leq \sum_{i=1}^N \md{x_1 e_1} = \sum_{i=1}^N|x_i|\cdot\md{e_i} \]

Tenemos: $\md{\gx} \leq \sum_{i=1}^N c_i |x_i|$ siendo $c_i = \md{e_i}$. Aplicando Cauchy-Schwarz  nos queda
\[ \sum_{i=1}^N \left(c_i^2\right)^\frac{1}{2} \cdot \sum \left(|x_i|^2\right)^\frac{1}{2} \]
Es decir, puedo controlar cualquier norma con una constante y la norma euclídea:
$$|||\gx||| \leq C \md{x}_{2}$$
En particular, $0 \leq |||\gor{x_n} - \gx|||\leq c ||\gor{x_n}-\gx||$. 

\paragraph{Observación:} Aplicar Holder en vez de Cauchy, sale la igualdad con la norma p y no con la euclídea.

\paragraph{Aplicación:}
Sea $F(\gx = |||\gx|||$ y $F:\real^N \rightarrow \real^N$ 
$$|F(\gx)-F(\gy)| = \left| |||\gx - \gy||| \right| = |||\gx - \gy||| \leq C ||\gx - \gy||$$
Utilizando: $|||\ga-\gb||| \ge |||\ga||| - |||\gb|||$ \footnote{(la desigualdad triangular con restas, que se saca con un simple cambio de variable)}

\paragraph{Interpretación del resultado:}
Cualquier norma en $\real^n$ es CONTINUA respecto de la norma euclídea. \footnote{Continua si la tomas como una función de $\real^N$ en $\real$}

\begin{theorem}[Relación norma $\leftrightarrow$ producto escalar]
$\md{\cdot}$ una norma cualquiera de $\real^N$ proviene de un producto escalar si y sólo si la norma satisface la identidad del paralelogramo. 
\end{theorem}
\begin{proof}
En el apartado anterior (\ref{secNormaprodEsc}) demostramos la implicación hacia la derecha. Vamos a demostrar la recíproca:
Suponemos que la norma satisface la identidad del paralelogramo:

\begin{equation}
 \md{\ga+\gb}^2 + \md{\ga-\gb}^2 = 2 \md{\ga}^2 + 2\md{\gb}^2 \label{eqParalelogramo}
\end{equation}
Queremos probar que existe un producto escalar $\ast$ tal que $\md{\gx} = (\gx\ast \gx)^\frac{1}{2}$, así que definimos uno utilizando la identidad de polarización: \[ \gx\ast\gy = \frac{1}{4} \left( \md{\gx+\gy} ^ 2 - \md{\gx-\gy} ^ 2\right) \]

Queremos probar que, efectivamente, $\ast$ es un producto escalar, así que tenemos que demostrar las siguientes propiedades:
\begin{enumerate}
 \item $\gx\ast\gy = \gy\ast\gx$ .
 \item $\gx\ast\gx \ge 0\quad \forall\; \gx$
 \item $(\gx\ast\gx) = 0 \dimplies \gx=\gor{0}$ 
 \item $(\lambda \gx) \ast \gy = \lambda\left(\gx\ast\gy\right)$
 \item $(\gx+\gy)\ast\gz =\gx\ast\gx + \gy\ast\gz $
\end{enumerate}

Las propiedades 1, 2 y 3 son triviales. Vamos con 4 y 5

\paragraph{Demostración de la 4ª propiedad}

Demostraremos que se cumple por inducción cuando $\lambda\in\nat$. Primero probamos para $\lambda = 2$.

\begin{align*}
(2\gx)\ast\gy = \frac{1}{4} \left(|||2\gx+\gy |||^2 - |||2\gx-\gy |||^2\right) = && \\
= \frac{1}{4} \left(|||\underbrace{\gx}_a + \underbrace{\gx+\gy}_b|||^2 - |||\underbrace{\gx}_a+\underbrace{\gx-\gy}_{-b}|||^2\right) = && \text{usando (\ref{eqParalelogramo})} \\
= \frac{1}{4} \left(2|||\gx|||^2 + 2|||\gx + \gy|||^2\right) = && \\
= 2 \frac{1}{4} \left(|||\gx+\gy|||^2 - |||\gx-\gy|||^2\right) = 2 (\gx*\gy) &&
\end{align*}

Conclusión: si $\lambda = 2$ vemos que sale fuera.\\
Paso 2: $\lambda = n$ con $n \in \mathbb{N}$\\
Inducción completa: suponemos el resultado cierto para 1,...,n-1. Queremos probar el resultado para n.
$$(n\gx)*\gy = \frac{1}{4} \left(|||n\gx+\gy|||^2 - |||n\gx-\gy|||^2\right) =$$
$$ \frac{1}{4} \left(|||(n-1)\gx+\underbrace{\gx+\gy}_b|||^2 - |||\underbrace{(n-1)\gx}_a+ \underbrace{\gx-\gy}_b|||^2\right)=$$
$$...=2(\gx*\gy) + (n-2)(\gx*\gy) = n (\gx*\gy)$$

Paso 3: $\lambda = n$ con $n \in \mathbb{Z}$\\
Utilizaremos $(-\gx) * \gy = -(\gx*\gy$ y se acaba saliendo.

Paso 4: $\lambda = n$ con $\frac{p}{q} \in \mathbb{Q}$
$$\left(\frac{p}{q}\gx\right)*\gy = q\left(\left(\frac{p}{q}\gx\right)*\gy\right) = (q\cdot\frac{p}{q}\gx)*\gy = \left(p\gx*\gy\right) = p\left(\gx*\gy\right)$$ Multiplicamos por q y lo metemos, simplifica con el denominador y la p si puede salir.

Paso 5: $\alpha \in \real$

$\alpha = \liminf{n} r_n$. Utilizaremos el resultado previo de que cualquier norma es continua.

$\gx, \gy$ fijos.

Revisar: Los $|||r_n \gx + \gy|||^2$ y  $|||r_n \gx - \gy|||^2$ son continuos.
$$\alpha \gx*\gy = \frac{1}{4}\left(||r_n \gx + \gy|||^2||| -|||r_n \gx - \gy|||^2||| \right) =$$
$$\lim_{n\to\infty} \left(||r_n \gx + \gy|||^2||| -|||r_n \gx - \gy|||^2||| \right) = \lim_{n\to\infty} (r_n\gx*\gy) = $$
$$\lim_{n\to\infty} r_n(\gx*\gy) = \alpha (\gx*\gy)$$
\paragraph{Demostración de la 4 propiedad:}
$$A = (\gx+\gy)*\gz = \frac{1}{4} \left( |||\gx+\gy+\gz|||^2 - |||\gx+\gy-\gz|||^2\right)$$
$$B =\gx*\gz = \frac{1}{4} \left( |||\gx+\gz|||^2 - |||\gx-\gz|||^2\right)$$
$$C =\gy*\gz = \frac{1}{4} \left( |||\gy+\gz|||^2 - |||\gy-\gz|||^2\right)$$
Demostraremos que $A-B-C=0$\\
COMPLETAR la comprobación.
\end{proof}
\begin{remark}
$d(\gx,\gy) = |||\gx-\gy||| \text{para alguna norma}|||\cdot||| \dimplies (d(\gx+\gz)+d(\gy+\gz) = d(\gx,\gy) \wedge d(\lambda \gx,\lambda \gy = \abs{\lambda} d(\gx,\gy))$
\end{remark}

\section{Topología}
\definition{}
$$B_R(\gx_0) = {\gx \in \real^N \tq \underbrace{d(\gx,\gx_0)}_{||\gx-\gx_0||} < R }$$
Para evitar jaleos, al tratar la distancia vamos a tomar la norma euclídea
\begin{align*}
A \subset \real^N  \text{abierto} \dimplies \forall \ga\in A, \exists \varepsilon(a) > 0 \tq B_\varepsilon(A) \subset A.\\
B \subset \real^N \text{cerrado} \dimplies B^c(= \real^N-B) \text{abierto}.
\end{align*}
\begin{theorem}[Caracteriación de cerrados en términos de sucesiones]
$$B \subset \real^N \text{cerrado} \dimplies \text{para cuaquier sucesión convergente} {X_n} \subset B \text{entonces} \lim X_n \in B$$
 
\end{theorem}
\begin{theorem}
Suponemos conjuntos de dimensión finita:
\begin{itemize}
 \item Unión arbitraria de abiertos $\rightarrow$ abierto
 \item Intersección finita de abiertos $\rightarrow$ abierto
 \item Union finita de cerrados $\rightarrow$ cerrado
 \item Intersección arbitraria de cerrados $\rightarrow$ cerrado
\end{itemize} 
\end{theorem}

\definition{Punto de acumulación}
La idea intuitiva es aquellos puntos a los que puedo llegar en el límite, es decir, puntos que a su alrededor a una distancia arbitrariamente pequeña existen otros puntos del conjunto.
$$A\subset \real^N a \in A \subset(A) \dimplies  \forall \varepsilon > 0 ( B_{\varepsilon} (a) - {a} ) \cap \neq \text{\O} $$
Siendo $A \subset (A)$ es el conjunto de los puntos de acumulación.
\definition{Frontera }%$\dh{}$}
La frontera son aquellos puntos que en su entorno (para cualquier $\varepsilon$) hay puntos
$$A \subset %\dh{A}
\dimplies \forall \varepsilon>0 ; B_\varepsilon (a) \cap A \neq \text{\O} \wedge B_{\varepsilon} (a) \cap A^C \neq \text{\O} $$
\definition{Interior}
Es el conjunto abierto más grande subconjunto del conjunto total.
\definition{Cierre}
El conjunto cerrado más pequeño en el que está contenido el conjunto total.

\begin{remark}
Cierre e interior no los vamos a definir formalmente porque se dan por supuesto.
\end{remark}

\begin{theorem}[Conjunto cerrado y acotado]
Sea $K \subset \real^N$. Son equivalentes:
\begin{itemize}
 \item K cerrado y acotado.
 \item Para cualquier sucesión ${X_n} \subset K$, podemos encontrar una subsucesión convergente ${X_{n_j}} \subset{X_n}$.
 \item Dado cualquier recubrimiento ${A_i}$ abierto de modo que $K \subset \cup A_i$ puedo encontrar un recubrimiento finito ${A_j}, j=1,...,M \tq K \subset \cup_{i=1}^M A_i$
\end{itemize}
\end{theorem}

\definition{Conjunto compacto}
Un conjunto que cumpla cualquiera de las 3 propiedades anteriores.
\begin{theorem}
 En $\real^N$ se cumple que las 3 propiedades anteriores son equivalentes 1 $\dimplies$ 2 $\dimplies$ 3
\end{theorem}
\begin{proof}
\begin{proof}
 2 $\implies$ 1)\\
 Si no estuviera acotado entonces no sería posible encontrar una subsucesión convergente en la sucesión de elementos de norma 1, norma 2, norma 3... lo que contradice 2.
 
 Y si no fuera cerrado, lo que pasaría sería: ... que contradice 2
 \end{proof}
 \begin{proof}
 1 $\implies$ 2)\\
 Sea $K \subset \real^N$, cerrado y acotado. Sea ${X_n}\subset K$
 Vamos a utilizar que: sea $\gor{X_n} = (X_i^i)$ tenemos
 $$\gor{X_n}\subset K \subset B_R(\gor{0}) \implies \md{X_N} \leq R, \forall N \implies \left|X_n^j\right| \leq R$$
 Ahora vamos a aplicar el argumento de bisección \footnote{COMPLETAR: Divido un intervalo infinito en 2 mitades. Al menos en una mitad tendrá infinitos términos. Seleccionamos el segundo elemento de una mitad que tenga infinitos términos y volvemos a dividir el intervalo en 2 mitades. Y así una y otra vez} para seleccionar la sucesión que nos interesa.
 
 Para ello recordamos el criterio de Cauchy:
 $${A_N} \subset \real \text{convergente} \dimplies \forall \varepsilon > 0 \exists n_0 \tq n,m>n_0 \implies |a_n - a_m| < \varepsilon$$
 
 Con el criterio de bisección podemos encontrar una subsucesión convergente de la siguiente manera: definimos una subsucesión que converja a la primera coordenada, después con el mismo criterio podemos arreglar la sucesión para que converja a la segunda coordena.

 \end{proof}
 
\end{proof}
\begin{theorem}
 $K\subset \real$ un conjunto compacto.
 
 $f:\real \rightarrow \real \text{ continua en } K,  \exists  x_m,x_M \in K \tq F(X_m) \leq F(x) \leq F(X_M), \forall x \in K$, es decir, $F$ alcanza su máximo y su mínimo.
\end{theorem}
\begin{proof}

 Paso 1.1) Vamos a demostrar que el conjunto está acotado superiormente:
 
 Por reducción al absurdo, se puede encontrar una subsucesión convergente que por ser compacto tiene que ser convergente a un elemento pertenciente al conjunto, pero esa sucesión por ser no acotada tenemos: $\exists x_n \in K \tq F(\gor{X_n}) \ge n$
 
 Paso 1.2) De manera totalmente análoga.
 
 Paso 2) Vamos a demostrar la existencia de $X_M$:
 Por ser un conjunto acotado, tenemos un supremo en ese conjunto. ¿Ese supremo pertenecerá al conjunto?. Si no perteneciera, podríamos encontrar una subsucesión convergente al supremo lo que contradice la condición de acotado.
\end{proof}

\paragraph{Aplicación:}
$F(\gor{x}) = |||\gor{x}|||$ una norma (que ya sabemos que es continua):

\emph{Conclusión:} $m \md{x} \leq |||\gor{x}||| \leq C||\gor{x}||$

\begin{theorem}
En $\real^N$ TODAS las normas son equivalentes. 
\end{theorem}

\subsection{Conexión}
\definition{Conexión por caminos}
Dados $a,b \in C$ podemos encontrar una aplicación continua $\appl{\varphi}{[0,1]}{\real^N}$ tal que $\varphi(0) = a, \varphi(1) = b, \varphi(t) \in C, \forall t \in [0,1]$
\definition{Conexión}
Para cualquier par de abiertos $A,B \subset \real^N \tq C\subset A\cup B$ si $A\cap C\neq \O \y B\cap C \neq \O \implies A\cap B \neq \O$.
\begin{remark}
Es curioso comprobar que estas 2 definiciones no son equivalentes. Si tomamos el conjunto
$$\displaystyle\{(x,0), x\in (0,1]\} \bigcup \{(0,y), y \in (0,1]\} \bigcup \{\displaystyle\bigcup_{n=1}^{\infty}{\left(\frac{1}{n},y\right), y \in [0,1]}\}$$
Podemos razonar que sí es conexo por caminos, pero no según la otra definición.
\end{remark}
\section{Funciones continuas, abiertos y cerrados}
Sea $F$ continua:

1) A abierto $\nimplies F(A)$ abierto.

2) B cerrado $\nimplies F(B)$ cerrado.

\[\appl{F}{\real^N}{\real^N}\]


Definimos
$$F^{-1} (A) = \{\gor{x}\in \real^N \tq F(\gor{x})\in A\}$$
\begin{theorem}[Función inversa]
\begin{itemize}
\item F continua $\y$ A abierto $\implies F^{-1} (A)$ abierto.
\item F continua $\y$ B cerrado $\implies F^{-1} (A)$ cerrado.
\end{itemize}
\end{theorem}

\app
$$M=\{(x,y,z) \in \real^3 \tq x^2 + cos(x|y|) - e^z < 1 \}$$
$$F(x,y,z) = x^2 + cos(x|y|) - e^z < 1 $$
\[M=\{(x,y,z) \in \real^3\}\]
COMPLETAR

\begin{proof}
1)

\[\gor{x} \in F^{-1} (A) \text {Queremos hallar } R>0 \tq B_R(\gor{x})\subset F^{-1} (A)\]
\[\gor{x}\in F^{-1} (A) \dimplies F(\gor{x})\in A\]
\[\exists \varepsilon>0 \tq B_{\varepsilon}(F(\gor{x}))\subset A\]
\[\text{es decir: si} ||\gor{z}-F(\gor{x})|| < \varepsilon \implies \gor{z}\in A\]
\[\text{F continua} \implies \exists \delta > 0 \tq \underbrace{||\gor{x}-\gor{s}||<\delta}_{\gor{s}\in B_R?(\gor{x})} \implies\]
\[\implies ||F(\gor{x})-F(\gor{s})||<\varepsilon. \text{en particular,} F(\gor{s}) \in A; s \in F^{-1} (A)\]
Conclusión: Hemos encontrado un $\delta > 0$ tal que $s \in B_R(\gor{x}) \implies s \in F^{-1} (A)$.
\end{proof}
\subsection{Aplicaciones lineales}
Sea: $\appl{L}{\real^N}{\real^M}$

$L$ es lineal $\dimplies L(\lambda \gor{x}) = \lambda L(\gor{x}) \y L(\gor{x}+\gor{y}) = L(\gx)+L(\gy)$

Además, toda aplicación lineal se puede escribir en forma de matriz.

\[L(\gor{x})=A\gor{x} =
\begin{pmatrix}
a_{11} 	& \cdots & a_{1n}		\\
\vdots	& \ddots &  \vdots 	\\
a_{n1}	& \cdots & a_{nn} 
\end{pmatrix}\]

\begin{theorem}
 L lineal $\implies$ L continua.
\end{theorem}
\begin{proof}
 $$L(\gor{x} = \left(
 \begin{matrix}
  A_1 & \rightarrow \\
  \vdots & \vdots\\
  A_n & \rightarrow
 \end{matrix}\right)$$
 
 COMPLETAR

\end{proof}
\subsection{Norma de matrices}
$$\appl{F}{\real^N}{\real}$$
$$\gor{x} \rightarrow F(\gx) = \underbrace{||A\gx||}_{L(\gx)}$$
F es continua.

Sabemos que existe $c>0$ tal que $||A\gx|| \leq C||\gx||$, es decir,  $||A\gx||\leq C$ si $||\gx||=1$.
$$M = \{||A\gx|| \tq ||\gx||=1\}\subset \real$$\footnote{Conjunto esfera unidad}
La mejor constante $C$ es la cota superior mínima (supremo) que vamos a llamar $\alpha$.\\
$\alpha \in^{?} M$\\
$\alpha$ se alcanza en M, porque F es continua y M es compacto.

\definition{}
$$|||A||| = \alpha = max{||A\gx||\tq||\gx||=1}$$
Ejercicio propuesto: demostrar que $|||\cdot|||$ es una norma.

Demostración de la 4' porpiedad:
$$|||A+B||| = max{||(A+B)\gx|||} = ||(A+B)\gx_{A,B}|| = ||A\gx_{AB} + B\gx_{AB}|| \leq$$
$$
\underbrace{||A\gx_{AB}||}_{\leq max||A\gx|| =
|||A|||} + \underbrace{||B\gx_{AB}||}_{|||B|||} $$

\paragraph{Ejemplo:}COMPLETAR
Sea $A = \begin{pmatrix}
      1&2\\-3&1\\3&0
     \end{pmatrix}$. Calcular su norma.
Resolución:
Acabamos teniendo que maximizar (sabiendo que $|x|+|y| = 1$: $|x+2y| + |-3x+y| + |2x| \leq |x|+|2y| + |3x| + |y| + 2|x| = 6|x|+3|y| \leq 6 (|x|+|y|) =6$
¿Podemos encontrar un vector $(x_0,y_0)$ tal que $||A(x_0,y_0)^T||_1 = 6$?\\
Tomando $x_0 = 1$ y $y_0 = 0$ lo encontramos.
\obs Coincide con la suma de los valores absolutos de las columnas y escoger el más grande.

Aplicando lo mismo con la norma infinito: COMPLETAR
COMPLETAR

\begin{lemma}
 Sea A una matriz, $A^TA$ es simétrica.
\end{lemma}
\begin{lemma}
 $\underbrace{<\gx,A\gy>}_{\text{Producto en } \real^n} = \underbrace{<A^T\gx,\gy>}_{\text{Producto en } \real^M}$
\end{lemma}

 $A^TA$ diagonalizable ($N \times N$). Dado $\gx \in \real^N$.

 Desarrollamos en $B$: $\gx = \sum \alpha_i \gor{v}_i$. Con $<v_i,v_j> = 0$ con $i \neq j$.

 Calculamos $||\gx|| = \sum \alpha_i^2<v_i,v_i>$
 
 $$A^TA\gx = (A^TA)(\sum \alpha_i v_i) = \sum (\alpha_i\lambda_i\gor{v_i})$$
 
 Queremos hallar el máximo de $||A\gx||$ cuando $||\gx|| = 1$.
 
 $$||A\gx||^2 = <A\gx,A\gx> = <A^TA\gx,\gx> = \underbrace{<\sum \lambda_i \alpha_i v_i,\sum\alpha_i v_i>}_{<v_i,v_j> =0 \text{ con } i \neq j.}$$
 $$= \sum \lambda_i \alpha_i^2 \leq \lambda_{max} (\sum \alpha_i^2) = \lambda_{max}$$
 
 %\conclusion
 Hemos demostrado que:
 $$ max||A\gx|| \underbrace{\leq}_{=} ({\lambda_{max}})^{\frac{1}{2}}$$
 Este máximo se puede alcanzar tomando $x$ como el autovector asociado, por lo que el $\leq$ se convierte en un $=$.
 
 \section{Limite}
 $\appl{F}{\real^N}{\real^M}$
 
 \definition{Limite}
 $$\lim_{\gx \rightarrow \ga} F(\gx) = \gor{L} \dimplies \forall \varepsilon > 0,  \exists \delta>0 \tlq 0<||\gx-\ga|| <\delta \implies ||F(\gx) - L||<\varepsilon$$
 Importante el detalle de $0 < ||\gx-\ga||$, no es un $\leq$, porque no se necesita que la función esté si quiera definida en el punto $\ga$.
 \begin{theorem}
  
  Sean $\gor{x_n} = (x_1,...,x_n) \in \real^N$ y $\gor{L} = (L_1,...,L_n) \in \real^N$.
 
  $$x_n \rightarrow \gor{L} \dimplies (x_1 \rightarrow L_1) \y (x_2 \rightarrow L_2) \y ... \y (x_n \rightarrow L_n)$$
 \end{theorem}
 
 Idea para el cálculo de límites: 
 \begin{itemize}
  \item $\displaystyle\mylim{x}{a}{x} = \lim_{\gy\rightarrow 0} F(\gy+\ga)$.
  \item Límite a lo largo de rectas. $\displaystyle\mylim{x}{a}{\gx} \sim \lim F({t\gor{v}})$
 
 \end{itemize}
 
 Si $\displaystyle\lim F({t\gor{v}})$ toma valores distintos dependiendo de $\gor{v} \dimplies \nexists \displaystyle\mylim{x}{0}{\gx}$
 
 Pero, si $\forall t \in \real, \mylim{x}{a}{t\gor{v}} = L \implies \gor{L}$ es el candidato a ser el límite (no tiene porque serlo). El siguiente paso sería demostrar con argumentos de comparación (Sandwich) u otros que $\mylim{x}{a}{\gx} = L$.
 
 El contraejemplo es $f(x,y) = \frac{x\cdot y^2}{x^2 + y^4}$. Veamos por que:
 
 Nos acercamos al límite por medio de rectas:
 $$f(x,y) = f(x,mx) = \frac{x\cdot(mx)^2}{x^2 + (mx)^4} = \frac{m^2x^3}{(1 + x^2m^4)x^2}.$$
 $$\displaystyle\lim_{x\rightarrow 0} (f(x,mx)) \rightarrow 0, \forall m \in \real$$
 Pero si nos acercamos al límite por medio de $x = y^2$ tenemos:
 $$f(x,y) = f(y^2,y) = \frac{y^2y^2}{y^4+y^4} = \frac{y^4}{2y^4} = \frac{1}{2}\neq 0$$
 \paragraph{Conclusión:}
 
 $$\lim_{(x,y) \rightarrow (0,0)} (f(x,y)) \neq 0$$
 
 
\begin{theorem}
 $F$ continua $\dimplies$ para cualquier abierto $A$, $F^{-1}(A)$ es abierto.
\end{theorem}
\obs
Si $\appl{F}{\omega\subset\real^N}{\real^M} \dimplies$ para cualquier abierto $A, F^{-1}(A) = \omega \cup V$, con $V$ abierto.

\begin{proof}
$\implies$ De este teorema ya teníamos demostrada la implicación $\implies$.

$\Leftarrow$ Queremos probar: $$\forall \varepsilon > 0, \exists \delta>0 \tlq ||\gx-\ga||<\delta \implies \underbrace{||F(\gx)-F(\ga)||<\delta}_{F(\gx)\in B_{\varepsilon}(F(\ga))}$$
Tomamos  $$A = B_{\varepsilon}(F(\ga)) \rightarrow F(\ga) \in A \implies \ga \in F^{-1}(A)$$
Por hipótesis, $F^{-1}(A)$ abierto $\y$ $\ga \in F^{-1}(A)$
$$\exists B_{\delta}(\ga) \subset F^{-1}(A) \text{.Es decir, } \gor{s}\in B_{\delta}(\ga) \subset F^{-1}(A), s\in F^{-1}(A) \implies F(s) \in A = B_{\varepsilon}(F(\ga))$$

\end{proof}
\begin{remark}
Este teorema también se cumple para cerrados.
\end{remark}
\section{Diferenciación}
\definition{$F$ diferenciable en $\gor{a}$}
$$\exists \text{ aplicación lineal L} \tlq \frac{F(\gx)-F(\ga)-L(\gx-ga)}{||\gx-\ga||} \underbrace{\rightarrow}_{\gx\rightarrow\ga} 0$$
$$ = \lim_{\gx \rightarrow \ga} \frac{||F(\ga+\gor{h}) - F(\ga) - L\gor{h}}{||\gor{h}||}$$
\obs
\begin{itemize}
\item Si existe, $L$ es única.
\end{itemize}

\begin{proof}
Supongamos que existen $L_1,L_2$.
\[0=\lim_{\gor{h} \rightarrow \gor{0}} \frac{F(\gor{a}+\gor{h}) - F(\ga)-L_1\gor{h}}{||\gor{h}||} = \lim_{\gor{h} \rightarrow \gor{0}} \frac{F(\gor{a}+\gor{h}) - F(\ga)-L_2\gor{h}}{||\gor{h}||}\].

$$\text{ Sumando: } 0 = \lim_{\gor{h} \rightarrow \gor{0}} \frac{||F(\gor{a}+\gor{h}) - F(\ga)-L_1\gor{h}|| + ||F(\gor{a}+\gor{h}) - F(\ga) -L_2\gor{h}||}{||\gor{h}||}$$

\obs $||A-B|| = ||A+(-B)|| \leq ||A||+||B||$

Completar la contradicción.

\end{proof}

\paragraph{Notación: (Diferencia de $F$ en $\ga$)}

$$L\equiv DF(\ga)$$ 

\paragraph{Proposición:}
$$F \text{ diferenciable en } \ga \implies F \text{ continua en } \ga$$
\begin{proof}
 $$\lim_{\gor{h} \rightarrow \gor{0}} \frac{||F(\ga+\gor{h}) - F(\ga) - L\gor{h}}{||\gor{h}||} = 0$$
 Esta es la definición de diferenciable. Para que este límite sea 0, el numerador tiene que tender a 0, por lo que $F(\ga+\gor{h}) - F(\ga) \rightarrow 0$
\end{proof}

\obs
$F$ diferenciable en $\ga$.
\begin{align*}
\appl{F}{\real^N}{\real^M}\\
\text{Sabemos:}\\
 0 = \lim_{\gor{h} \rightarrow \gor{0}} \frac{||F(\ga+\gor{h}) - F(\ga) - L\gor{h}}{||\gor{h}||} = 0 \\
 \text{En particular (tomando } h = t \gor{e_1})\\
 \lim_{\gor{t} \rightarrow \gor{0}}\frac{a}{b} = \lim_{\gor{t} \rightarrow \gor{0}}\norm{\underbrace{\frac{1}{t} F(\ga+t\gor{e_1})-F(\ga))-L\gor{e_1}}_{\gor{W}(t)\in\real^N}}\\
 \text{Tomando la componente k-esima}\\
 0 = \mylim{t}{0}{\frac{F_K(a+te_i)-F_k(a)}{t} - L_{ki}}\\
 L_{ki} = \mylim{t}{0} {\frac{F_k(a+te_i) - F_k(a)}{t}} = \deriv{ F_k}{x_i}(\ga)
\end{align*}

\paragraph{Nomenclatura: }
Aproximación lineal $\sim$ Diferencial.

Matriz jacobiana $\sim$ Jacobiana.

\begin{theorem}
 Matriz asociada a $DF(\ga) \equiv $ Matriz de las derivadas parciales de F.
 
% $$DF(\ga) \eqiv \begin{pmatrix}
                  
 %                \end{pmatrix}
%$$
\end{theorem}

\begin{theorem}
 $F$ diferencialbe en $\ga \implies \exists \deriv{F_k}{x_i}(\ga), i=1,2,...,N \y k = 1,2,...,M$
\end{theorem}

El contraejemplo para demostrar $\nimplies$ es el mismo que en los límites a lo largo de rectas.

$f(x,y) = \frac{xy^2}{x+y^2}, (x,y) = (0,0) \y 0, (x,y)=(0,0)$

\paragraph{Comentarios sobre notación: }
\begin{itemize}
 \item $\appl{\delta}{\real}{\real^M}$. Utilizamos notación vectorial en vez de matricial (porque tendríamos una matriz columna).\\
 Ejemplo: la velocidad (en un instante de tiempo, un punto en el espacio).
 \item $\appl{F}{\real^N}{\real}$ (función escalar):

 COMPLETAR
 Se suele llamar vector gradiente.
 \item 
\end{itemize}

\subsection{Regla de la cadena: }
\paragraph{Derivada de una composición:\\}

COMPLETAR DIBUJITO

$\appl{F}{\real^N}{\real^M}$. 

$\appl{G}{\real^M}{\real^K}$.

$\appl{H=G\circ F}{\real^N}{\real^K}$.

$ \gx \in \real^N, \gy \in \real^M$

$F$ diferenciable en $\ga$, $G$ diferenciable en $F(\gor{a})$. Entonces $H=G\circ F$ es diferenciable en $\ga $.
Además la expresión matricial es:

$\underbrace{DH(\ga)}_{K\times N} = \underbrace{DG(F(\ga))}_{K\times M}\cdot \underbrace{DF(\ga)}_{M\times N}$
 
\obs
\paragraph{Notación de Leibniz:}

Para calcular 1 único elemento de la matriz diferencial (el de la fila $i$, columna $j$):

$$\deriv{H_i}{x_j}(\ga) = \displaystyle\sum_{k=1}^M \deriv{G_i}{y_k}\cdot\deriv{F_k}{x_j}$$
Con cuidado de $\displaystyle \deriv{G_i}{y_k}$ evaluado en $F(\ga)$ y $\deriv{F_k}{x_j}$ evaluado en $\ga$.

\paragraph{Aplicaciones y ejemplos:}
\begin{itemize}
 \item 
 \[\appl{F}{\real^N}{\real}\]
 \[\appl{\sigma}{\real}{\real^N}\]
 Sea $g\equiv F\circ \sigma \equiv \text{ Comportamiento de F a lo largo de la curva } \sigma, \appl{g}{\real}{\real}$.

$$g'(t_0) = DF(\sigma(t_0))D\sigma(t_0) = \underbrace{...}_{\text{Notación matricial}} = \underbrace{\pesc{\triangledown F(\sigma(t_0))}{\sigma'(t_0)}}_{\text{Notación vectorial}}$$

 
 \item 
 $$\sigma(t) = t\gor{b} + (1-t)\ga$$
 \[\appl{F}{\real^N}{\real^M}\]
 %$$F\circ \sigma(t) \eqiv g(t)$$
 
 COMPLETAR
\end{itemize}

\subsection{Extensiones del teorema del Valor Medio}
\begin{itemize}
 \item Original:
 $\appl{f}{\real}{\real}$ diferenciable. 
 
 $f(b)-f(a) = f'(c)(b-a)$ para algún $c\in[a,b]$
 
 \item
  
 $F : \real^N  \longrightarrow \real$
 
 $\sigma(t)  = t(\gor{b})+(1-t)\gor{a}$
 
 $g  = F\circ \sigma$
 $\appl{g}{\real }{\real}$
 
 $F(\gor{b}-\gor{a}) = g(1)-g(0)  = g'(s) \text{ para algún }s\in[0,1]$
 
  
  Peeeeero...
  $$\appl{F}{\real^N}{\real^2}$$
  $$F(\gor{b})-F(\ga) = \begin{pmatrix}
                         \pesc{\triangledown F_1(\gor{c_1}),{\gor{b}-\ga}}\\
                         \pesc{\triangledown F_2(\gor{c_2}),\gor{b}-\ga}
                        \end{pmatrix}
$$
  Tenemos 2 $c$ distintos, uno para cada $f$, por lo que este teorema pierde sentido.
  
  \item Versión para funciones matriciales:
  
  $$\norm{F(\gor{b}-\ga)} \leq |||DF(\ga)|||\cdot\norm{\gor{b}-\ga}$$
  
  \begin{proof}
   i) $$\appl{f}{\real}{\real}$$
   $$f(\gor{b}-f(\ga) = g(1)-g(0) = \int_0^1g'(s)ds \text{ con } g(t) = f(tb+(1-t)a)$$
   
   ii)  $$\appl{h}{\real^N}{\real}$$
   $$h(\gor{b}-h(\ga) = g(1)-g(0) = \int_0^1g'(s)ds = \int_0^1 \pesc{\triangledown h(s\ga+(1-s)\gor{b}),(\gor{b}-\ga)}$$
   $$g(t) = h(tb+(1-t)a)$$
   
   iii) $$\gor{G} = (G_1,...,G_M), G_i = \int_0^1H_i(t)dt$$
   Con $\gor{H} = (H_1(t),...,H_M(t))$
   
   
   $$\norm{\gor{G}}^2 = \pesc{\gor{G},\gor{G}} = \sum_{i=1}^M \left(\int_0^1 H_i(t)dt\right) \underbrace{\left(\int_0^1 H_i(s)ds\right)}_{G_i}$$
 
   $$\norm{\gor{G}}^2 = \displaystyle\int_0^1 \left(\underbrace{\sum_{i=1}^M G_i H_i (t)}_{\pesc{\gor{G},\gor{H}(t)}\leq \norm{\gor{G}}\cdot \norm{\gor{H}}}\right)dt$$
   Conclusión:
   $$\norm{\gor{G}}^2 \leq \displaystyle\int_0^1\norm{G}\cdot\norm{H(t)}dt$$
   
   iv) $\appl{F}{\real^N}{\real^M}$
   
   $\appl{F_i}{\real^N}{\real}$
   
   $\underbrace{F_i(\gor{b})-F_i(\ga)}_{G_i} = \displaystyle\int_0^1 \underbrace{\pesc{\triangledown F_i (s\gor{b}+(1-s)\ga ),(\gor{b}-\ga)}}_{H_i(t)}dt$
   
   Por el apartado iii tenemos que:
   
   
   $$\gor{H}(t) = \begin{pmatrix}
                   H_1(t)\\
                   \vdots\\
                   H_M(t)
                  \end{pmatrix}
                = \begin{pmatrix}
                   \pesc{\triangledown F_1(tb+(1-t)a),b-a}\\
                   \vdots\\
                   \pesc{\triangledown F_M(tb+(1-t)a),b-a}
                  \end{pmatrix}
		= \left(DF(...)\right)\begin{pmatrix}
		                        b_1-a_1\\
		                        \vdots\\
		                        b_M-a_M
		                       \end{pmatrix}
$$

$$\norm{H(t)} = \norm{DF(...)(\gor{b}-\ga)} \leq \norm{DF}\cdot\norm{\gor{b}-\ga}$$


$$\norm{F(\gor{b})-F(\ga)} \leq \displaystyle\int_0^1 \pesc{\triangledown F_i(tb+(1-t)a),b-a}dt$$
   
Conclusión:

$$\norm{F(\gor{b})-F(\ga)} \leq \norm{DF}\cdot\norm{\gor{b}-\ga}$$
  \end{proof}

\end{itemize}



\newpage
WTF DONDE VA¿¿¿¿¿¿
\section{Convergencia y continuidad}
Al tener definida una norma podemos definir convergencia y continuidad:
\begin{itemize}
 \item\emph{Convergencia:} $\gor{x_n} \rightarrow \gx \longleftrightarrow \forall \varepsilon > 0 \exists n_0 tq n>n_0 \Rightarrow |||\gx-\gor{x_n}|||<\varepsilon$.
 \item\emph{Continuidad:} Sea $F: \mathbb{R}^N \rightarrow \mathbb{R}^M$ continua en $a$ $\dimplies \forall \varepsilon > 0 , \exists \delta > 0$ tal que $||\gx-\ga||_a < \delta \Rightarrow ||F(\gx)-F(\ga)||_b< \varepsilon$
\end{itemize}
\paragraph{Observación:} Es interensante ver que se puede hablar de continuidad tomando una norma en $\mathbb{R}^N$ y otra distinta en $\real^M$ sin por ello variar la definición de continuidad. Más adelante veremos que en $\real^N$ todas las normas son equivalentes y qué significa que las normas sean equivalentes. (Esto no sucederá en espacios de dimensión infinita)
?????????????????







\end{document}
