\chapter{Teoría de la información}


\section{Definición e idea intuitiva de entropía}

	\textit{``Una teoría matemática de la comunicación''} (Claude E. Shannon, 1948)

	\begin{defn}
		$S = \{ S_1, ... , S_n \}$  espacio de probabilidad, $p_i \equiv$ posibilidad de escoger $S_i$.

	\end{defn}

	Más adelante si serán ``caracteres'' que componran mensajes o ficheros.

	\begin{defn}
		Shanon definió una función $ H(p_1, ..., p_n)$ que mide la cantidad de ``información contenida'' en S. Que se puede interpretar también como la ``incertidumbre'' al extraer una muestra aleatoria de $S$.
	\end{defn}

	\begin{example}
		$p_1 = 1$ y $p_i = 0$ si $i \neq 1 \rightarrow $ no hay incertidumbre ya que sabemos el resultado.

		$p_i = \frac{1}{n} \rightarrow $ hay mucha incertidumbre (es máxima).
	\end{example}


	Shannon explicó las siguientes propiedades de H:
	\begin{itemize}
		\item $H$ es contínua
		\item $H$ es creciente: $H(\frac{1}{n},...,\frac{1}{n}) (\text{n veces}) \rightarrow$ más cosas, más incertidumbre $\Rightarrow$ más información.
		\item $H(\frac{1}{n},...,\frac{1}{n}) (\text{n veces})$ =  $H(\frac{b_1}{n},...,\frac{b_k}{n}) (\text{k veces})$ + $\sum^{k}_{i = 1} \frac{b_k}{n} H(\frac{1}{b_i},...,\frac{1}{b_i}) (b_i \text{ veces})$ para cualquier $b_i \in \mathbb{Z}^+, \sum^{k}_{i = 1} b_i = n$.
	\end{itemize}

	\begin{prop}
		Si divides un conjuntos en trozos, la incertidumbre no varía
	\end{prop}

	$\rightarrow$ La información de n objetos no varía s los dividimos en cajas de tamaño $b_i$.

	Probó que la única función con esta propiedad es:

	$$H(p1, ..., p_n) = \text{Cte} \sum^{n}_{i=1} p_i \log(p_i) \quad \text{Cte} < 0 $$

	Eligió $\text{Cte} = -\frac{1}{\log 2}$ pensando en comunicaciones digiales para que le saliese $\log_2 ( \quad) $.

	\begin{defn}{entropía}[entropia]
		$$H(p_1, ..., p_n) = -\sum^{n}_{i = 1} p_i \log_2 p_i $$

		Se define $0 \log_2 0 = 0$ porque $\lim_{x \rightarrow 0} x \log x = \lim_{x \rightarrow 0^+} \frac{\log(x)}{1/x} = \lim_{x \rightarrow 0} \frac{1/x}{-1 / x^2} = \lim_{x \rightarrow 0} \frac{-x^2}{x} = 0$
	\end{defn}

	\begin{example}

		$S$ = \{ bytes \}  $p_1 = \frac{1}{2^8}, ..., p_{256} = \frac{1}{2^8}$

		$$  H = - \sum_{1}^{256} \frac{1}{256} \log_2 (\frac{1}{256}) = - \sum_{1}^{256} \frac{1}{256} (-8) = \sum_{1}^{256} \frac{1}{32} = 2^8 2^{-5} = 2^3 = 8 (\text{ nº de bits}) $$

	\end{example}

	\begin{obs}
		Podemos medir la entropía en bits
	\end{obs}

\section{Códigos prefijo}

	\begin{defn}[código]
		Definimos un código (binário únicamente descifrable) como una función $C: S \rightarrow \{ $ cadena de bits $ \} $ (ceros y unos) que es inyectiva cuando actúa sobre cadenas de elementos de $S$ como:

		$$ S_{i_1} S_{i_2} ... S_{i_N} \rightarrow C(S_{i_1}) C(S_{i_2}) ... C(S_{i_n}) $$

	\end{defn}

	Tenemos dos necesidades prácticas:

	\begin{itemize}
		\item Crear códigos que añaden redundancia para poder corregir errores cuando se transmiten por un canal poco fiable (CD, DVD, telecomunicaciones).

		\item Crear códigos que no tengan ninguna información redundante para poder comprimir.
	\end{itemize}

	\begin{defn}[Longitud media]
		La longitud media de las cadenas de bits $C(S_i)$ es:

		$$ l(C) = \sum_i l(C(S_i))$$

		Siendo $l(C(S_i))$ la longitud de $C(S_i)$ o su número de bits.
	\end{defn}

	Si los elementos de S forman ``palabras'' (cadenas) $S_{i_1} S_{i_2} ... S_{i_N}$ apareciendo como variables aleatorias \textbf{independientes} entonces la longitud media de la codificación de una cadena de $N$ elementos de $S$ es:

	$$l_{N}(C) = \sum_{i_1,...,i_N = 1}^{n} P_{i_1} P_{i_2} ... P_{i_N} l(C(S_{i_1} S_{i_2} ... S_{i_N}))$$

	Siendo $l(C(S_{i_1} S_{i_2} ... S_{i_N}))$ la longitud en bits al codificar de la cadena.

\section{Los teoremas source coding y noisy channel coding de Shannon}

	\begin{theorem}{Source coding theorem\footnote{También conocido como primer teorema de Shannon} }[Source coding theorem] \label{SourceCoding}
		Sea $l^*$ el mínimo de $l(C)$ entre todos los posibles códigos (con $S$ y sus probabilidades fijados), entonces:

		$$ H \leq l^* \leq H + 1 $$

		donde $H$ es la entropía de $S$.

	\end{theorem}


	\begin{corol}
		\label{corolario minima longitud}
		Sea $l^{*}_N$ el mínimo de $l_N(C)$ para códigos definidos sobre N-cadenas es decir S es, para calcular el $l^{*}_N$ , el conjunto $\widetilde{S} = \{ S_{i1}  S_{i2}  .... S_{iN}\}^n_{i_1...i_N = 1}$  :

		$$ H \leq \frac{l^*}{N} < H + \frac{1}{N} \text{ , en particular  } \lim_{N \rightarrow \infty} \frac{l^*_N}{N} = H$$

		En breve, este resultado asegura que el mejor código permite, en media, codificar (comprimir) un fichero de $N$ caracteres en $NH$ bits.

	\end{corol}
\begin{proof}
	(suponiendo el teorema)
		Aplicamos el teorema cambiando $S = \{ S_1,S_2...S_n \}$ por $\widetilde{S} = \{S_{i1}, S_{i2} .... S_{iN} \} =$ 
		{N-cadenas de elementos de S} con prob $\rightarrow p_{i1}.... p_{iN}$
		
	$$H _{\text{para } \widehat{S}} \leq l^{*}_N < H _{\text{para } \widehat{S}} +1 $$
	
	Basta probar $H _{\text{para } \widehat{S}} = NH$
		
	$$H _{\text{para } \widehat{S}} = - \sum_{i_1...i_N} p_{i1}....p_{iN} \log_2 (p_{i1}...p_{iN})$$
		
	Vemos que $\log_2 (p_{i1}...p_{iN}) = \log_2 p_{i1} + .... + \log_2 p_{iN}$
	
	¿Cuál sería el coeficiente de $\log_2 p_1$ al desarrollar la suma?
	$$coef = - \sum_{k = 1}^{N}\sum_{i_1...i_N = 1} p_{i1} ... p_{iN} = - p_1 \cdot \sum_{k=1}^{N} \sum_{j_1 ... j_{N-1} = 1}^{n} p_{j1} ... p_{jN-1}$$
	
	Como $\sum_{j_1 ... j_{N-1} = 1}^{n} p_{j1} ... p_{jN-1} = 1$
	
	$$ coef = - p_1 \cdot N \cdot 1 = - p_1 \cdot N \cdot ( p_1 + ... + p_n)^{N-1} = -N\cdot p_1$$
	
	Entonces, en general
	
	$$\begin{cases}
	\text{coef de } \log p_1 \rightarrow -Np_1\\
	\text{coef de } \log p_j \rightarrow -Np_j\\
	\end{cases}  \implies H _{\text{para } \widehat{S}} = -N \sum_{j=1}^{n} p_j \log p_j = N\cdot H$$
	
\end{proof}

	\textbf{Recordatorio} $C$ está en principio definido en $S$ pero la definición se extiende a cadenas de símbolos concatenando resultados. Exigimos que $C$ sea inyectiva actuando sobre cadenas.

	\begin{example}

		$S = \{A,B,C\} \quad C(A) = 0 \quad C(B) = 1 \quad C(C) = 10$

		C no es un código válido ya que $C(BA) = C(C) = 10$.

		$\widetilde{C}(A) = 01 \quad \widetilde{C}(B) = 001 \quad \widetilde{C}(C) = 0001 \Rightarrow \widetilde{C}(AC) = 010001 $ si es un código válido.

	\end{example}


	\begin{example}


		$S = \{A,B\} \quad p_1 = p_2 = \frac{1}{2} \quad H = -\frac{1}{2} \log_2 \frac{1}{2} - \frac{1}{2} \log_2 \frac{1}{2} = 1$

		$N = 10^6$: el fichero en ASCII ocupa $10^6$ bytes.

		Del corolario \ref{corolario minima longitud} obtenemos que el mejor código sólo permitiría llegar a $l^*_N$ hasta $NH$ bits $= 10^6 1$ bits $\simeq 125000$ bytes.

		Chamizo hizo unas pruebas en su ordenador en las que con archivos aleatorios los resultados fueron los siguientes:

		\begin{table}[h]
			\begin{tabular}{r|c}
			gzip & 159068 bits
			\end{tabular}
		\end{table}


	\end{example}


	\begin{example}

		$S = \{0,1\} \quad p_1 = p_2 = \frac{1}{2}$

		$$ 1 = H \leq \frac{l^*_N}{N} \Rightarrow l^*_N \geq N \Rightarrow \text{no hay compresión} $$

	\end{example}

\begin{example}
 ¿Cuanto se puede comprimir un fichero con $N$ bits con el 75\% de ceros y 25\% de 1s? 
	
	$ S= \{0,1\} p_1 = 3/4, p_2 = 1/4, \quad H = 0.811 \rightarrow  l^*_N \simeq 0.8111 N $
	
	Digamos que un fichero tiene tamaño M = 1000 bits y su mejor codif = R.
	
	Si codificamos bit a bit N = 1 (corolario, teorema)
	
	$$0.811 < \frac{l^{*}_1}{1} < 1.811$$
	
	Como $R = M \cdot l^{*}_1 \implies 811 \leq R < 1811$
	
	Si codificamos de 10 en 10 bits N=10 (corolario), entonces
	$$0.811 < \frac{l^{*}_1}{10} < 0.911$$
	
	Como $R = \frac{M}{10} \cdot l^{*}_1 \implies 811 \leq R < 911$
	
	\textbf{Por lo tanto tomando bloques de bits grandes ( lo cual en la práctica tiene límites) la "tasa de compresión" se acerca cuanto queramos a H} 
\end{example}
	

\section{Codificación de Huffman}

Es un algoritmo fácilmente programable para diseñar un código que verifique $H \leq l(C) < H +1$

De hecho $l(C) = L^{*}$

Se basa en la contrucción de un árbol binario a partir de $S_0 = S$ hasta llegar a $S_{n-1}$ que sólo tiene un elemento.

Las reglas para e árbol son:
\begin{enumerate}
	\item Se toman los dos elementos de $S_i$ con menos probabilidad y se ponen en el árbol como hijos de un padre al que se le asugna la suma de probabilidades.
	\item $S_{i+1} = S_i - $ {los hijos de antes} $\cup$ {padre}
\end{enumerate}
\begin{example}
	$$S =\{ a, b , c, d, e , f \}$$
	
	Siendo $a = \frac{1}{2}$ , $b, c,d = \frac{1}{8}$ y $e, f = \frac{1}{16}$
	
	Vemos que el padre de e y f ($\widehat{ef}$) tiene probabilidad $\frac{1}{8}$
	
	Nos queda
	$$S_1 = \{a,b,c,d,\widehat{ef}\}$$
	
	
	Ahora vemos el padre de d con $\widehat{ef}$, nos queda:
	$$S_2\{ a, b,c,\widehat{df}\}$$
	
	De forma que $\widehat{df}$ tiene probabilidad $\frac{1}{4}$
	
	Siguiendo estos pasos llegamos a:
	$$S_4 = \{ a , \widehat{bf}\}$$
	
	con las probabilidades de a y $\widehat{bf} = \frac{1}{2}$ y la raiz tendría probabilidad 1.
	
	Una vez que ya tenemos el árbol, partiendo de la raíz hasta las hojas codificamos con $0$ ir a la izquierda y con $1$ ir a la derecha.
	
	$$\begin{cases}
	a \rightarrow 0 \\
	b \rightarrow 100 \\
	c \rightarrow 101 \\
	d \rightarrow 110 \\
	e \rightarrow 1110 \\
	f \rightarrow 1111 \\
	\end{cases} \implies \begin{cases}
	l(C) = \frac{17}{8}\\
	H = \frac{17}{8}
	\end{cases}$$ 
\end{example}

\begin{example}
	$$S = \{ a , b , c , d\}$$
	Con probabilidades
	$\begin{cases}
	a = 0.35\\b = 0.25\\ c= 0.24\\ d = 0.16
	\end{cases}$
	
	 Me fijo en los dos que tengan probabilidades más pequeñas y los junto como hermanos de forma que el padre de c y d ($\widehat{cd}$) tiene probabilidad 0.4.
	 
	 Luego junto las dos siguientes probabilidaes más pequeñas de forma que nos queda $\widehat{ab}$ con probabilidad 0.6
	 
	 El padre de todos ($\widehat{abcd}$) tiene probabilidad 1.
	 
	 La codificación quedaría:
	 $$\begin{cases}
	 a \rightarrow 00\\ b \rightarrow 01 \\c \rightarrow 10 \\ c \rightarrow 11
	 \end{cases}$$
	 con longitud media $l(C) = 2$ y $H = 1.9472$
	 
	  
	
	
\end{example}

\begin{example}
	Vamos a hacer una pequeña variación sobre el ejemplo anterior.
	Tomamos ahora $\begin{cases}
	a = 0.45 \\ b = 0.25\\c = 0.24 \\ d = 0.06
	\end{cases}$
	
	Empezamos igual uniendo c y d. Nos queda $\widehat{cd}$ con probabilidad 0.30.
	
	Ahora unimos $\widehat{cd}$ con b. Entonces probabilidad de $ \widehat{bcd}$ es 0.55. Y finalmente unimos el nodo a.
	
	La codificación quedaría: $\begin{cases}
	a \rightarrow 0 \\ b \rightarrow 10 \\ c \rightarrow 110\\ d \rightarrow 111
	\end{cases}$
	
	La longitud media(lo que mide típicamente una cadena de bits) se calcularía:
	$$l(C) = 0.45 \cdot 1 + 0.25 \cdot 2 + 0.24 \cdot 3 + 0.06 \cdot 3 = 1.91$$
	
	Y la entropía $H = 1.7560$
\end{example}
Viendo esto se nos plantean algunas preguntas:

¿cómo demuestro que esto siempre da lugar a un código?

¿cómo demuestro que el resultado es óptimo?

\subsection{La codificación de Huffman da lugar a un código válido}

La codificación de Huffman verdadderamente da lugar a un código válido ( descodificable de manera única , C es inyectiva actuando sobre cadenas)

Vamos a verlo con el ejemplo anterior:
\begin{example}
	 $$\begin{cases}
	 a \rightarrow 0 \\ b \rightarrow 10 \\ c \rightarrow 110\\ d \rightarrow 111
	 \end{cases}$$
	 De esta forma : $ abacd \rightarrow 010011010$
	 
	 Vemos que no hay ambigüedad. Esto es debido a una propiedad que veremos ahora.
\end{example}

\begin{defn}[Código prefijo]
	
	
	Una función $C : S \rightarrow \{ \text{cadena de bits}\}$ tal que si $b_1 b_2 .... b_N \in Im C$ entonces $b_1 b_2 .... b_k \notin Im C$ para $k < N$ se dice que es un \textbf{código prefijo}
	
	Un ejemplo de código prefijo es el \textbf{UTF-8}
\end{defn}


\paragraph{¿Porqué un código prefijo es un código?}

$$C (S_{i1} S_{i2} .... S_{ik}) = C(S_{j1} S_{j2} .... S_{jk}) \implies C(S_{i1}) = C(S_{j1})$$
o bien se viola la propiedad de prefijo $\implies S_{i1} = S_{j1}$ , iterando $S_{i2} = S_{j2}$ , etc...

Con esto acabamos de ver porqué los codigos prefijos son códigos. Vamos ahora a ver \textbf{porqué los códigos de Huffman son códigos}.

Los códigos prefijos generan y provienen de árboles binarios ( no necesariamente completos. Un padre puede tener un solo hijo)

Esta correspondencia es como en Huffman : interpretando 
$$0 \rightarrow \text{rama a la izquierda}$$
$$1 \rightarrow \text{rama a la derecha}$$

Vamos a ver cómo quedaría el árbol si tenemos la siguiente codificación : $\begin{cases}
a \rightarrow 01\\ b \rightarrow 001\\ c \rightarrow 0001\\ d \rightarrow 0000\\
\end{cases}$

DIBUJO ÁRBOL

Con esta correspondencia tenemos que la codificación de Huffman es un código prefijo (porque proviene de un árbol)

La conclusión que podemos sacar de esto es que la codificación de Huffman es "óptima" en cuanto a la longitud media.

\begin{theorem}
	El mínimo de $l(C)$ sobre todos los códigos prefijo se alcanza con la codificación de Huffman
\end{theorem}
\obs Hay un resultado que dice que dado un código cualquiera siempre hay un código prefijo con el mismo valor $l(C)$.

Sabiendo esto , el teorema $\implies l^{*}= l(Huffman)$



\begin{theorem}[Desigualdad de Kraft]
	Dado un código prefijo $C : S = \{S_1, ..., S_n\} \rightarrow $ {cadenas de bits} , sea $l_i = l (C(S_i))$ entonces
	$$\sum_{i=1}^{n} 2 ^{-li} \leq 1$$
	Además , dados $l_i \in \ent^{+}$ cualesquiera que cumplan la desigualdad, existe un código prefijo tal que $l_i = l(C(S_i))$ con $1\leq i \leq n$
\end{theorem}
\begin{proof}
	Empezaos demostrando $l_i = l (C(S_i)) \implies
	\sum_{i=1}^{n} 2 ^{-li} \leq 1$
	
	Suponemos que $l_1 \leq l_2 ... \leq l_r < l_{r+1} = l_{r+2} = ... = L_n$
	
	Consideramos $B =${cadenas de $l_n$ bits} de forma que 
	$$2^{l_n} = |B| = |B \cap Im C| + |B - B \cap IM C| = n-r + \sum_{i=1}^{r} 2^{l_n - l_i} = \sum_{i=1}^{n} 2^{l_n - l_i}$$
	$B \cap Im C$ no contiene ninguna de las $2^{l_n - l_i}$ cadenas de bits que comienzan por $C(S_i)$
	%Es decir, cuenta todas las cadenas que empiezan %por $C(S_i)$ , que son $2^{l_n - l_i}$ cadenas, %y ve que es imposible que todas ellas sean un %código y dice que 
	
	El recíproco se basa en el algoritmo ilustrado con el siguiente ejemplo:
	$$l_1 = 1 , l_2=2, l_3=3$$
	$$2^{-1} + 2^{-2} + 2 ^{-3} \leq 1$$
	Queremos un código prefijo con estas longitudes.
	Tomamos la longitud mayor $l_3 = 3$ y dibujamos el árbol binario completo de altura esa logitud.
	DIBUJO ARBOL
	
	El algoritmo se basa en podar este árbol de la siguiente forma:
	\begin{enumerate}
		\item En $l_1 = 1$ borramos los descendientes de un vértice que estén en altura/profundidad 1.
		
		\item Luego hago lo mismo con $l_2$, con lo cual borramos los descendientes que estén en altura/profundidad 2.
		
		\item Una vez hecho esto, si sobran algunas hojas que no corresponden a las longitudes, también las borramos (en nuestro caso nos quedan dos con longitud 3, y solo quiero 1, por lo tanto borro el otro)
		
		\item Las codificaciones que nos quedarían serían $\begin{cases}
		1\\00\\010
		\end{cases}$
	\end{enumerate}
	Necesitamos alguna condición para que el algoritmo se pueda llevar a cabo, para que o se nos acaben los vértices.
	
	Mirando el ejemplo lo que hemos ido hav¡ciendo es:
	
	Al árbol con 8 hojas le he quitado 4, luego 2 y abajo quedan $8-4-2 = 2$ por lo tanto puedo escoger el $l_3$ de altura 1.
	
	En general, con la notación anterior $l_1 \leq l_2 \leq .... \leq l_{r} < l_{r+1} = .. = l_n$ el algoritmo parte de un árbol con $2^{l_n}$ hojas. De ellas borramos primero $2^{l_n - l_1}$ y en sucesivos pasos $2^{l_n - l_2} , .... , 2^{l_n - l_r}$ y me deben sobrar al menos $n-r$ de las hojas iniciales.
	
	Entonces necesitamos que $2^{l_n} - \sum_{i=1}^{r} 2^{l_n - l_i} \geq n-r$ para que el algoritmo funcione.
	
	Entonces $2^{l_n} \geq \sum_{i=1}^{n} 2^{l_n - l_i}$
\end{proof}

\obs Se puede probar (desig de McMillan) que para cualquier código se cumple $\sum_{i=1}^{n} 2^{-l_i} \leq 1$ aunque no sea prefijo.

Entonces dado un código , este satisface la desigualdad de Kraft y por el recíproco de Kraft sabemos que eiste un código prefijo con las mismas logitudes.

$\implies$ Cualquier teorema que hable de las longitudes y sea verdad para códigos prefijo, lo es para todos.


Una vez que hemos visto esto podemos probar el teorema del Source Coding \ref{SourceCoding}. Para la demostración vamos a utilizar la desigualdad de Kraft:

\begin{proof}
	\textbf{Demostración del Source coding theorem}
	Tenemos
	$$f(x_1 , x_2 , ...., x_n) = -\sum_{i=1}^{n} p_i \log x_i$$
	restringimos esta función de forma que
	$$\sum_{i=1}^{n} x_i = 1 \text{   con   } 0 \leq x_i \leq 1$$
	Entonces f alcanza un mínimo cuando $x_i = p_i$
	
		Tomamos $x_i = \frac{2^{-l_i}}{\sum_{j=1}^{n} 2^{-l_j}}$ , siendo $l_i$ como en la desigualdad de Kraft.
			
			Entonces
			$$H= f(p_1,...,p_n) \leq f (\frac{2^{-l_1}}{\sum ...} .... , \frac{2^{-l_n}}{\sum ...})$$
			Y esto, por la desigualdad de Kraft
			$$\leq f(2^{-l_1} , .... , 2^{-l_n}) = l(C)$$

	\end{proof}

\section{Codificación aritmética}
\section{Compresión LZ77, DEFLATE y LZW}
\section{El algoritmo ID3 de aprendizaje automático}

