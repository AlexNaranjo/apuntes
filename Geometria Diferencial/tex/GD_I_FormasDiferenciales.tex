% -*- root: ../GeometriaDiferencial.tex -*-

\chapter{Formas diferenciales en abiertos de $ℝ^n$}

\section{Introducción y motivación}

Vamos a hacer un pequeño repaso de formas diferenciales. A lo largo de esta sección vamos a considerar la función $\appl{f}{U⊆ℝ^n}{ℝ}$, con $f∈C^∞(U)$.

Las formas diferenciales parten del concepto de derivadas y la búsqueda de ``tangentes'' a un punto. En esta línea, recordamos lo que es la matriz diferencial de una función que, por así decirlo, nos da la tasa de variación de una función cuando nos movemos a lo largo de un vector.

\begin{defn}[Diferencial\IS de una función] Dada $\appl{f}{U⊆ℝ^n}{ℝ}$, con $f∈C^∞(U)$ y $x_0 ∈ U$, entonces el diferencial en un punto es $\Dif f(x_0) = f'(x_0) = (\Dif f)_{x_0}$, que se considera como una aplicación lineal
\begin{align*}
\appl{\Dif f}{ℝ^n&}{ℝ} \\
(λ_1, \dotsc, λ_n)^T &\longmapsto \sum \dpa{f}{x_i} (x_0) λ_i
\end{align*}
\label{defDiferencialD}
\end{defn}

Hay que tener en cuenta que el espacio de partida de $\Dif f$, $ℝ^n$, no es el mismo espacio de partida de $f$. Diremos que el $ℝ^n$ de $\Dif f$ es el espacio tangente de $f$ en $x_0$ (denotado por $Π_{x_0} U = ℝ^n$).

Por ser una aplicación lineal de $ℝ^n$ en $ℝ$, el diferencial está en el espacio dual de $ℝ^n$ o, más concretamente, del espacio tangente de $U$: \[ \Dif f(x_0) ∈ \left(Π_{x_0} U\right)^* ≝ Π_{x_0}^* U\] donde $Π_{x_0}^* U$ será el espacio cotangente.

El espacio tangente se identifica con su dual no de forma única: depende de la elección de la base. Es decir, son isomorfos pero no son canónicamente isomorfos, ya que hay muchos isomorfismos y no hay uno privilegiado.

Para los que no recuerden cláramente qué es el espacio dual:

\begin{defn}[Espacio dual]
Dado cualquier espacio vectorial V sobre un cierto cuerpo F, definimos el \textbf{espacio dual V*} como el conjunto de todas las funcionales lineales en F, es decir, transformaciones lineales en V a valores escalares (en este contexto, un ``escalar'' es un miembro del cuerpo-base F). El propio V* se convierte en un espacio vectorial sobre F bajo la definiciones habituales ('punto a punto') de la adición y de la multiplicación escalar.
\end{defn}

Como el diferencial está en el espacio dual, sus elementos serán llamados \concept[Covector]{covectores}. Esto nos permitirá evitar líos de notación al considerar vectores en el espacio y por otro lado vectores en el espacio tangente.

Por otra parte, queremos introducir otra notación, la de las formas diferenciales, a partir de la diferencial exterior.

\begin{defn}[Diferencial\IS exterior] Dada $\appl{f}{U⊆ℝ^n}{ℝ}$, con $f∈C^∞(U)$ y $x_0 ∈ U$, entonces la diferencial en un punto es \[ (\dif f)_{x_0} = \sum \dpa{f}{x_i} (\dif x_i)_{x_0} \] En general, la diferencial será \[ \dif f = \sum \dpa{f}{x_i} \dif x_i \] \label{defDifrenciald}
\end{defn}

En realidad, la única diferencia entre esta definición y la dada en \ref{defDiferencialD} es que cambiamos los $λ_i$ por $\dif x_i$. En ambos casos tenemos, por definición, la diferencial es una 1-forma en el abierto $U$. Sabemos ya que una 1-forma es una expresión \[ ω = f_1 \dif x_1 + \dotsb + f_n \dif x_n \] con $f_i ∈ C^∞(U)$.

Cada $(\dif x_i)_{x_0}$ es un covector de $Π^*_{x_0}$, y de hecho $\set{(\dif x_i)_{x_0}}$ es una base del espacio de covectores.

\paragraph{Deducción de las reglas de cálculo} Vamos a considerar ahora una diferencial \[ \dif h = \dpa{h}{x_1} \dif x_1 + \dotsb + \dpa{h}{x_n} \dif x_n \] y una 1-forma \[ ω = f_1 \dif x_1 + \dotsb + f_n \dif x_n \]

Dado ω, nos preguntamos si existe una función $h$ tal que $\dif h = ω$. Es una propiedad muy deseable, porque nos da de alguna manera unas ciertas garantías de ``operación cerrada'' en formas diferenciales.

Esto es lo mismo que plantearse la resolución del sistema de ecuaciones diferenciales dado por \[ \dpa{h}{x_i} = f_i\quad i=1,2,\dotsc,n \]

Esta cuestión está resuelta por el teorema de Poincaré, que veremos más adelante durante el curso.

A primera vista, parece difícil que haya solución. Si uno elije las $f_i$ de forma aleatoria, tendremos demasiadas ecuaciones y pocas incógnitas. Luego debemos esperar condiciones de integrabilidad: las $f_i$ deben cumplir ciertas posibilidades para que la solución $h$ exista. Habitualmente estas condiciones son necesarias pero no siempre suficientes, y una vez que las encontremos suponemos que se cumplen (si no no hay solución) y veremos si bajo esas condiciones el problema tiene solución.

Dado que queremos que exista $h$, vamos a suponer que efectivamente existe $h$. Entonces va a ocurrir que $\dpa{h}{x_i} = f_i$, y si volvemos a derivar tendremos que $\frac{∂h}{∂x_j∂x_i} = \dpa{f_i}{x_j}$. Si derivamos en orden contrario, como el Teorema de Schwarz nos dice que las derivadas cruzadas son iguales, tendremos que tener \[ \dpa{f_i}{x_j} = \dpa{f_j}{x_i} \]

Hay una manera natural de poner toda esta información. Nosotros tenemos la matriz Hessiana de derivadas parciales segundas dada por \[ H = \left(\frac{∂h}{∂x_j∂x_i}\right)_{ij}\] que es simétrica por el Teorema de Schwarz.

Por otra parte, a partir de la forma diferencial $ω = f_1 \dif x_i + \dotsb + f_n \dif x_n$ podemos obtener la matriz $H_1$ dada por \[ H_1 = \del{\dpa{f_i}{x_j}}_{ij} \]

Habíamos visto que la diferencia entre la matriz diferencial y la diferencial exterior era simplemente de notación, luego parece lógico que $H = H_1$. Luego la condición de integrabilidad es que $H_1$ sea simétrica.

A partir de aquí podemos definir más formalmente la diferencial de una 1-forma

\begin{defn}[Diferencial\IS de una 1-forma] Dada una 1-forma ω, tenemos que \[ \dif ω ≝ H_1^Γ = \frac{H_1 - H^T_1}{2} \] donde $H_1^Γ$ es la parte antisimétrica de la matriz $H_1$.
\end{defn}

Esto nos lleva a poder dar una condición concreta para que exista la función $h$ que comentábamos antes. Como queremos que $H_1$ sea simétrica, la parte antisimétrica deberá ser 0. Podemos enunciar entonces el siguiente lema:

\begin{lemma} Si existe $h$ tal que $\dif h = ω$, entonces $\dif ω = 0$. \end{lemma}

Vamos a ver cómo aplicar todo ese tocho de antes para calcular la diferencial de una 1-forma $ω$ dada por \[ ω = f_1 \dif x_1 + f_2 \dif x_2 \] definida en un abierto $U ⊆ ℝ^2$. En este caso, su matriz $H_1$ será \[ H_1 = \begin{pmatrix} \dpa{f_1}{x_1} & \dpa{f_1}{x_2} \\ \dpa{f_2}{x_1} & \dpa{f_2}{x_2} \end{pmatrix} \] así que su parte antisimétrica será

\[ H_1^Γ = \begin{pmatrix} 0 & \displaystyle\frac{\dpa{f_1}{x_2} - \dpa{f_2}{x_1}}{2} \\ \displaystyle\frac{\dpa{f_2}{x_1}- \dpa{f_1}{x_2}}{2} \end{pmatrix} \]

Y como ese 2 molesta multiplicamos por dos y nos lo quitamos de en medio.

Como al diferenciar estamos derivando y la derivada es lineal, deberíamos esperar que \[ \dif ω = \dif (f_1\dif x_1) + \dif (f_2 \dif x_2)\] luego tenemos que ver cuánto vale $\dif(f_1\dif x_1)$.

Dado que la derivada tiene que seguir cumpliendo la regla de Leibniz, tenemos que tener \[ \dif(f_1\dif x_1) = \dif f_1 \dif x_1 + f_1 \dif \dif x_1 \]

Aquí ya empiezan a pasar cosas importantes de este cálculo de diferenciales. Principalmente que $ \dif \dif x_1 = 0$, ya que la matriz $H$ tiene que ser simétrica.

Ahora tenemos que calcular $ \dif f_1 \dif x_1 $, que será \[ \dif f_1 \dif x_1  = \left(\dpa{f_1}{x_1} \dif x_1 + \dpa{f_1}{x_2} \dif x_2\right)\dif x_1 = \dpa{f_1}{x_1}\dif x_1 \dif x_1 + \dpa{f_1}{x_2} \dif x_2 \dif x_1 \] luego sumando nos queda que

\[ \dif ω = \dpa{f_1}{x_1}\dif x_1 \dif x_1 + \dpa{f_1}{x_2} \dif x_2 \dif x_1  + \dpa{f_2}{x_1} \dif x_1 \dif x_2 + \dpa{f_2}{x_2} \dif x_2 \dif x_2 \]

Comparamos esto con la matriz antisimétrica dada por  \[ 2H_1^Γ = \begin{pmatrix} 0 & \dpa{f_1}{x_2} - \dpa{f_2}{x_1} \\ \dpa{f_2}{x_1}- \dpa{f_1}{x_2} & 0\end{pmatrix} \] vemos que los términos $\dpa{f_1}{x_1}$ y $\dpa{f_2}{x_2}$ no aparecen, luego tiene que ser \[ \dpa{f_1}{x_1} \dif x_1 \dif x_1 = \dpa{f_2}{x_2} \dif x_2 \dif x_2 = 0 \] y por otra parte que el producto de diferenciales tiene que ser anticonmutativo, esto es, que $\dif x_1 \dif x_2 = - \dif x_2 \dif x_1$. Dado que este producto (producto exterior) es distinto al habitual, lo denotaremos como $\dif x_1 \y \dif x_2$.

Es decir, hemos extraído las dos reglas siguientes para el producto de diferenciales o \concept[Producto\IS exterior]{producto exterior}: \begin{align*}
\df{x_i, x_j} &= - \df{x_j, x_i} \\
\df{x_i, x_i} &= 0
\end{align*}

Vamos a extendernos un poco en el significado de la expresión $\df{x_i, x_j}$, y por tanto en el de la forma diferencial. En el fondo, no estamos más que definiendo unos objetos (las formas) y aplicando ciertas reglas razonables para calcular ciertas operaciones. Ciertamente esto es cierto.

\paragraph{¿Es $\dif ω = 0$ condición suficiente?} Tenemos que es una condición necesaria para que exista un $h$ tal que $\dif h = ω$. Ahora bien, ¿es condición suficiente?

Para demostrarlo, lo que vamos a hacer es construir esa función $h$. Es trivial ver que \[ h(x) = \int_γ \dif h \] para un cierto camino $\appl{γ}{I}{U}$. En este caso, aplicando la regla de Barrow

\[ \int_γ \dif h = \int_I \dif(h ○ γ) = h(γ(b)) - h(γ(a)) = h(x) - h(x_0)\] donde $x_0$ es el punto de inicio de $γ$. Es decir, que $h$ está definido salvo constante. Habría que ver, eso sí, que la definición que hemos construído no depende de la elección del camino $γ$ y que, además, se cumple efectivamente que $\dif h = ω$.

Vamos a demostrar que la definición no depende del camino. Tomemos $γ_1, γ_2$ dos caminos distintos que empiezan y acaban en el mismo punto. Consideremos entonces $Γ = γ_1 * γ_2^-$, y sea $D$ la región encerrada por Γ.

Por un lado, tenemos que como $\dif ω = 0$, entonces $0 = \int_D \dif ω$. Por otra parte, por el teorema de Stokes, \[ \int_D \dif ω = \int_Γ ω = \int_{γ_1} ω - \int_{γ_2} ω \implies \int_{γ_1} ω = \int_{γ_2} ω\] luego la integral no depende del camino

Lo único que necesitamos para aplicar Stokes es que $U$ sea simplemente conexo, es decir, que no haya agujeros y que siempre podamos deformar un camino a otro, de tal forma que el borde de $D$ sea Γ.

En el fondo, esto es un reflejo de lo que habíamos visto en cursos anteriores de cálculo con campos de vectores conservativos: cuando eran conservativos ($\grad V = 0$) la integral sobre caminos cerrados era cero y además existía una función potencial.

\subsection{Resumen: reglas del cálculo de formas diferenciales}



\section{Estudio formal de las formas diferenciales}

\subsection{Espacio tangente y cotangente}

Suponemos un abierto $U ⊆ ℝ^n$ y un punto $p ∈ U$.

\begin{defn}[Espacio\IS tangente] Se define el espacio tangente de $U$ en un punto como $Π_{p} U$, un espacio vectorial sobre $ℝ$ de dimensión $n$. Los elementos $D^\vv_{p} ∈ Π_{p} U $ son las derivadas direccionales (en la dirección $\vv$) locales (en el punto $p$), que operan las funciones y dan números.
\end{defn}

Lo primero que nos damos cuenta es de que la derivada direccional es local, sólo definida en un entorno de $p$. Vamos por ello a tratar de definir este espacio tangente de forma algebraica, sin saltar al análisis. Y para ello empezaremos con conceptos de anillos de funciones, funciones definidas en intervalos.

\subsubsection{Anillo de funciones}

Si tenemos dos funciones $\appl{f,g}{U}{ℝ}$ definidas en el abierto $U$ con $f,g ∈ C^∞ (U)$, podemos operar con ellas de forma sencilla:

\begin{align*}
(f+g)(x) &≝ f(x) + g(x)
(f·g)(x) &≝ f(x) · g(x)
\end{align*}

Es decir, que las funciones definidas en el abierto tienen estructura de anillo: todas tienen inverso con la suma. Tenemos una unidad y además es anillo conmutativo. Así, podemos definir el anillo de funciones de un abierto:

\begin{defn}[Anillo\IS de funciones] El anillo de funciones de un abierto $U$ se define como

\[ A(U) = C^∞ (U) = \set{\appl{f}{U}{ℝ},\; f∈C^∞}\]

, que es anillo conmutativo y con unidad.\end{defn}

De hecho, $A(U)$ no es sólo un anillo: también es una \concept{{$\real$}-álgebra}: es un anillo conmutativo y además espacio vectorial sobre $ℝ$, cumpliendo la igualdad

\[ (λ· f) · (β·g) = (λ · β) · (f·g) \]

donde $f,g ∈ A(U)$, $λ,β ∈ ℝ$ y teniendo cuidado de usar los productos que correspondan\footnote{$(λ·_{E.V.} f) ·_{A(U)} (β·_{E.V.}g) = (λ ·_{\real} β) \cdot_{E.V.} (f·_{A(U)}g)$.} (el producto de elementos de $ℝ$ no es lo mismo que de elementos del anillo de funciones).

\subsubsection{Gérmenes de funciones en $p$}

La idea de los gérmenes es que sólo nos interesa la función en un entorno del punto. Podemos definir así entonces el germen como

\begin{defn}[Germen\IS de función] Un germen de una función en un punto $p$ es un par $(V,f)$ de un abierto $p ∈ V ⊆ U$ y una función $\appl{f}{V}{ℝ}$ con $f∈C^∞$. \label{defGermenFuncion}
\end{defn}

La razón de buscar esta definición es que para la derivada no necesitamos definir la función en todo $U$, nos basta con sólo un poco, y eso nos facilitará las cosas.

Ahora bien, los gérmenes de funciones en realidad se trabajan con relaciones de equivalencia para simplificar. Diremos que dos gérmenes $(V,f)$, $(W,g)$ están relacionados ($(V,f) \sim (W,g)$) si y sólo si $p∈V∩W$ y además existe un abierto $V' ⊆ V∩W$ que contiene a $p$ y para el cual las restricciones de $f$ y $g$ coinciden, esto es $\restr{f}{V'} = \restr{g}{V'}$.

Tomando los gérmenes módulo esta relación de equivalencia, tendremos igualmente una $\real$-álgebra.

La ventaja de buscar ese abierto más pequeño es que no tenemos que decir exactamente cómo de pequeño es, nos basta simplemente que las funciones sean iguales en un pequeño entorno del punto.

A partir de esto, denotaremos como $A_p$ la $\real$-álgebra de gérmenes de funciones $C^∞$ definidas en un entorno de $p$. Por comodidad, para denotar un germen nos bastará con la función, $f∈A_p$, por ejemplo. También podemos pasar a definir lo que es una derivación:

\begin{defn}[Derivación] Una derivación en $p$ es una aplicación
\begin{align*}
\appl{D}{A_p&}{ℝ} \\
f &\longmapsto D(f)
\end{align*}

Queremos que esta función conserve de alguna forma la noción de derivada en una dirección, así que buscaremos varias propiedades:

\begin{enumerate}
	\item $D$ es lineal.
	\item $D(λ) = 0$, donde $λ$ es una función constante $λ(x) = λ ∈ ℝ$.\footnote{En realidad, esta propiedad es consecuencia de las otras dos pero viene bien tenerla presente.}
	\item Si esto se parece una derivada, además de definir cómo se derivan las sumas\footnote{Por esto forzamos que $D$ sea lineal.} definiremos cómo se derivan los productos, según la regla de Leibniz: \[ D(f·g) = f · D(g) + D(f) · g\]
\end{enumerate}\label{defDerivacion}
\end{defn}

Una vez que hemos emulado las derivadas, podemos definir el espacio tangente:

\begin{defn}[Espacio\IS tangente] Diremos que el espacio tangente a $p$ en $U$ se define como

\[ Π_p U ≝ \set{\appl{D}{A_p}{ℝ}\tq D \text{ derivación }} \]
\end{defn}

Fijémonos que el espacio tangente es un subespacio vectorial del espacio dual, ya que $A^*_p ≝ \set{\appl{D}{A_p}{ℝ} \tq D \text{ lineal }}$. De hecho, tenemos bien definido el producto por escalares y la suma de elementos de la forma habitual.

\paragraph{Motivación de todo esto} Al final, hemos logrado llegar a una definición de espacio tangente sin tener que usar derivadas, una definición puramente algebraica.

Con esta definición algebraica vamos a estudiar el espacio tangente, principalmente su dimensión, y vamos a obtener una base cómoda para hacer cálculos.

\subsubsection{Cálculo de la dimensión del espacio tangente}
\label{secDimTangente}

Vamos a demostrar que la dimensión del espacio tangente de un abierto de $ℝ^n$ es $n$. Suponemos $p ∈ U ⊆ ℝ^n$, y $p = (p_1, \dotsc, p_n)$. Supongamos también que tenemos una derivación $\appl{D}{A_p}{ℝ}$. Vamos a ver qué podemos obtener de aquí sabiendo las propiedades de la derivación.

La observación fundamental es que en el conjunto de gérmenes $A_p$ tenemos un cierto subconjunto en los que el germen es 0: \[ m_p ≝ \set{f ∈ A_p \tq f(p) = 0}\] que además es un ideal por la propia construcción. Y resulta ser maximal\footnote{Aunque todos nos acordamos de Estructuras Algebraicas, recordamos por si acaso: un ideal es maximal si no hay ningún ideal propio que lo contenga.}, ya que el cociente $\quot{A_p}{m_p}$ es un cuerpo: por construcción, las clases de equivalencia estarán formadas por funciones cuya diferencia sea constante, luego $\quot{A_p}{m_p} = ℝ$, que es un cuerpo.

Aquí no sé qué está haciendo: $D(λ) = 0$. $m_p ⊃ m_p^2$, donde $m_p^2$ es el ideal generado por productos de elementos de $m_p$, esto es, \( \label{eqMp2} m_p^2 ≝ \set{\sum g_{ij} f_i f_j \tq f_i f_j ∈ m_p }\)

¿Qué ocurre si derivamos $f·g$ con $f,g ∈ m_p$? Por la regla de Leibniz, tenemos que \[ D(f·g) = d· D(g) + g · D(f) \] Como $f = g = 0$, tenemos que $D(f·g) = 0$. Entonces es claro que cuando aplicamos la derivación a elementos de $m_p^2$ nos sale 0. La conclusión de todo esto es que $\restr{D}{m_p^2} \equiv 0$. Es decir, que la regla de Leibniz nos nosequé algo sobre cosas finitas y espacios enormes. Que eso nos lleva a que $D$ está determinada en realidad por los valores que toma en $\quot{A_p}{m_p^2}$. Es decir, que si denotamos
\begin{align*}
\appl{D}{A_p&}{ℝ} \\
\appl{π}{A_p&}{\quot{A_p}{m_p^2}} \\
\appl{\adh{D}}{\quot{A_p}{m_p^2}&}{ℝ}
\end{align*} nos queda que \[ D(f) = \adh{D}(π(f)) \]

Al final, hay un motivo por el cual este chorro nos dice que algo es finito.

El argumento es que una derivación $\appl{D}{A_p}{ℝ}$ induce una aplicación $\appl{\adh{D}}{\quot{A_p}{m_p^2}}{ℝ}$, que se puede ver de otra forma como $\appl{\adh{D}}{\quot{m_p}{m_p^2}}{ℝ}$ ya que si $f ∈ A_p \setminus m_p$ $D(f) = 0$. Entonces $\adh{D}$ es, por construcción, un elemento del espacio vectorial de $\quot{m_p}{m_p^2}$, y además a partir de $\adh{D}$ podemos reconstruir la $D$ original.

Veamos cómo hacer eso mismo: si tenemos $f∈A_p$ con $f(p) ≠ 0$, entonces consideramos la función $f_1(x) ≝ f(x) - f(p)$. Entonces $f_1(x) ∈ m_p$ por construcción\footnote{Recordamos que $m_p$ son las funciones que se anulan en $p$.}. Es claro que yo debo definir \[ D(f) ≝ \adh{D}\left([f_1]_{m^2_p}\right) \]

Es decir, que sólo con saber lo que vale $\adh{D}$ nos vale ya que simplemente trasladamos las funciones $f$ con valores desconocidos $f ∉ m_p$ a una clase de equivalencia de la cual sí sabemos el valor. Sólo tenemos que ver que efectivamente cumple la propiedades de derivación que habíamos pedido (\ref{defDerivacion}). Es trivial ver que se cumple la linealidad, pero Leibniz es más interesante. Empezamos\footnote{Se ve que por comodidad estamos escribiendo $f(x) \equiv f$.} con

\[ D(f·g) = \adh{D}\left([f·g - f(p) g(p)]_{m_p^2}\right) \]

¿Qué hacemos ahora\footnote{Sugerencia de Guille: llorar y dedicarnos a magisterio.}? Sólo sabemos que $\adh{D}$ es lineal. Podemos fijarnos en que \[ α = (f-f(p))· (g-g(p)) ∈ m_p^2\] por ser producto de funciones de $m_p$, luego $\adh{D}(α) = 0$. Si operamos, nos queda \[ (fg - f(p) g - g(p) f + f(p) g(p) \] y si obtenemos su derivación tenemos que

\[ \adh{D}(α) = \adh{D}(f·g) - f(p) \adh{D}(g) - g(p)\adh{D}(f) + \adh{D}(f(p)· g(p)) \]

\begin{example}[Estudio del espacio tangente con funciones polinomiales]

Vamos a suponer que trabajamos con polinomios (espacio $ℝ[x,y]$), que son más sencillos: con ellos podemos derivar formalmente sin definiciones de límites ni nada, y además, los gérmenes son los mismos polinomios: si tenemos todos los valores que toma en un abierto de $ℝ$, tenemos completamente determinado el polinomio.

Supongamos $p = (0,0)$. Entonces, ¿qué es $m_p ⊆ ℝ[x,y]$? Será\footnote{Teniendo en cuenta que $p(x,y) = a_0 + a_1 x + a_2 y + \dotsb$} \[m_p = \set{p(x,y) ∈ ℝ[x,y] \tq p(0,0) = 0} = \set{p(x,y) \tq a_0 = 0} \]

Queremos ver qué es $m_p^2$. Está claro que $x^2, y^2, xy ∈ m_p^2$. Afirmamos que $m_p^2 = \gen{x^2, y^2, xy}$, y vamos a demostrarlo. Si fuese así y tuviésemos $F(x,y) ∈ m_p^2$, podríamos escribirlo como \[ F(x,y) = \sum A_{ij}(x,y) · p(x,y) · q(x,y) \] siguiendo la definición que teníamos en \eqref{eqMp2}, y con $p, q ∈ m_p$. Por lo tanto, $p,q$ sólo pueden ser polinomios con término independiente 0 y entonces nos quedaría que efectivamente

\[ m_p^2 = \gen{x^2 + y^2 + xy} = \set{A(x,y) x^2 + B(x,y) y^2 + C(x,y) xy} \]

Una vez que tenemos $m_p$, queremos ver quién es $\quot{m_p}{m_p^2}$, que es el espacio cotangente a algo $Π_{(0,0)}^*$. Si $p(x) ∈ m_p^2$, entonces \[ p(x) = a_1 x + a_2 y + a_3 x^2 + a_4 xy + \dotsb \] Si $p, q$ están en la misma clase de equivalencia, su resta tiene que estar en $m_p^2$, luego los polinomios $r(x) ∈ \quot{m_p}{m_p^2}$ son de la forma \[ r(x) = a_1 x + a_2 y\] o, dicho de otra forma, \[ \quot{m_p}{m_p^2} \equiv ℝ·[x] + ℝ· [y] \] Es decir, que estamos en un espacio de dimensión dos que es lo que tenía que ser.
\end{example}

Una vez que hemos logrado manejar algo mejor y ver la aplicación de todos estos conceptos, podemos pasar a la formalización de la demostración de la dimensión del espacio tangente.

\begin{theorem} \label{thmDimQuotMp} Sea $p ∈ U ⊆ ℝ^n$, con $p = (x_1, x_2, \dotsc, x_n)$. Entonces el espacio $\quot{m_p}{m_p^2}$ es un R-espacio vectorial de dimensión $n$.\end{theorem}

Hasta ahora todo lo que hemos visto han sido manipulaciones algebraicas, aunque para demostrar este teorema hace falta usar análisis.

\begin{proof}[Idea] Suponemos que $n = 1$, y consideramos el desarrollo de Taylor de una función $f ∈ A_p$:

\[ f(x) - f(p) = f'(p)(x - p) + \frac{1}{2!} f''(p)(x - p)^2 + \dotsb\]

Vemos que $f''(p)(x - p)^2  ∈ m_p^2$. Dado que trabajamos en módulo $m_p^2$, tenemos que $[f(x) - f(p)] = f'(p) [ x- p]$. Esto demuestra que $\quot{m_p}{m_p^2}$ tiene base $[x - p]$, luego efectivamente la dimesión es 1.

Vamos ahora con la demostración más rigurosa. Para una función $f ∈ A_p$, tenemos\footnote{ Por el teorema de Taylor e incluyendo el término de error, en este caso con la formulación integral } que

\[ f(x) - f(p) = \sum_{i=1}^n \dpa{f}{x_i} (x_i - p_i) + \sum_{\substack{i=1 \\ j=1}}^n \frac{1}{2} (x_i - p_i)(x_j - p_j) ·\int_0^1 (1-t) \left(\frac{∂^2f}{∂x_i ∂x_j}(p + tx)\right) \dif t \]

Entonces nos queda que \[ [f(x) - f(p)] = \sum_{i=1}^n \dpa{f}{x_i} (p) [x_i - p_i] \mod m_p^2 \] así que está claro que $\set{[x_i-p_i]}_{i=1}^n$ son generadores de $\quot{m_p}{m_p^2}$. No hemos demostrado que sean base, eso sí.

Para terminar, demostramos que ese conjunto de generadores son linealmente independientes y que por lo tanto serán base. Lo haremos por demostración al absurdo: si no fueran independientes, podríamos tener una función \[ F = \sum λ_i (x_i - p_i) ∈m_p^2\]

Esto es un poco absurdo ya de por sí\footnote{El profesor ha dicho por qué pero yo no lo veo nada claro.}, pero vamos a demostrarlo.

Sabemos que $∀D$ $D(F) = 0$. Entonces, elegimos un conjunto de derivaciones $D_j$ tales que $D_j ( x_i - p_i) = δ_{ij}$ con $δ$ la delta de Kronecker (ver \ref{defDeltaKronecker}). En ese caso, \[ D_j(F) = \sum λ_i D_j (x_i - p_i) = λ_j\] pero hemos dicho que $D(F) = 0$, contradicción.

¿Qué $D_j$ nos valen? Si tomamos $D_j = \left(\dpa{}{x_j}\right)_p$, cumplen las propiedades de derivación y también que $D_j (x_i - p_i) = \dpa{(x_i - p_i)}{x_j} = δ_{ij}$\footnote{Es fácil ver que $D_j(x_i - p_i)$ será 1 cuando $j = i$.}.

En ese caso, tenemos efectivamente que $\set{[x_i-p_i]}_{i=1}^n$ es base.
\end{proof}

Con esta demostración hemos llegado a algo interesante, que es ver qué es un diferencial de cada coordenada \[ (\dif x_i)_p ≝ [x_i - p_i]_{m_p^2} \] y, de la misma forma, el diferencial de una función \[ (\dif f)_p ≝ \left[f - f(p)\right]_{m_p^2} ∈ \quot{m_p}{m_p^2} \]

De esta notación y del teorema anterior \ref{thmDimQuotMp} podemos llegar a otra notación:

\( (\dif f)_p = \sum \dpa{f}{x_i}(p) (\dif x_i)_p \label{eqDiferencialNotacion} \)

En realidad, lo que estamos haciendo es similar a un desarrollo de Taylor despreciando los términos de segundo orden (que es lo que logramos con el módulo $m_p^2$): aproximaciones lineales de funciones.

De momento estamos trabajando localmente, siempre con la diferencial definida en un punto $p$. Pronto pasaremos a definir las diferenciales en abiertos o incluso en variedades directamente.

\subsubsection{Construcción del espacio tangente}

Recapitulemos: hemos demostrado hasta ahora el siguiente isomorfismo entre el espacio tangente y el dual  \[ Π_p U \simeq \left(\quot{m_p}{m_p^2}\right)^* \] que $\dim_{ℝ} \quot{m_p}{m_p^2} = n$ y que además hay una base natural \[ \quot{m_p}{m_p^2} = \gen{(\dif x_1)_p, \dotsc, (\dif x_n)_p} \]

A partir de aquí también podemos escribir una base del tangente $Π_p U$. Como podemos identificar el tangente con el dual de $\quot{m_p}{m_p^2}$, si definimos $\set{(D_i)_p}_{i=1}^n$ como la base de $Π_p U$, tenemos que cada $D_i$ tiene que ser un elemento del dual\footnote{Me estoy perdiendo mucho.}: \[ (D_i)_p (x_j - x_j(p)) = δ_{ij}\] aunque por notación más cómoda tomaremos \[ (D_i)_p ≝ \left(\dpa{}{x_i}\right)_p \] y por lo tanto podemos reescribir la base del espacio tangente en un punto como \[ Π_pU = \gen{\left(\dpa{}{x_1}\right)_p, \dotsc, \left(\dpa{}{x_n}\right)} \] y entendiendo cada $\left(\dpa{}{x_i}\right)_p$ como derivaciones lineales que se aplican a gérmenes de funcioens de $p$ y que cumplen la regla de Leibniz.

Así, cualquier elemento $D_p ∈ Π_p U$ se puede expresar como  \[ D_p = \sum a_i \left(\dpa{}{x_i}\right)_p \] con $a_i ∈ ℝ$.

De hecho, como cada $D_p$ sólo tiene la información de los $a_i$, podemos considerar los $D_p$ como vectores del espacio ambiente $ℝ^n$. Análogamente, se puede obtener cada coordenada del vector tangente derivando con respecto a las coordenadas en $U$: \[ D_p (x_j) = a_j \]

Podemos preguntarnos por qué tanto lío para llegar al final a que los elementos del espacio tangente son vectores de $ℝ^n$. La respuesta si nos da por hacer eso es que de esta forma tenemos una definición estricta y formal de las formas diferenciales que nos permitirá extenderlas naturalmente\footnote{Naturalmente mis narices.} a variedades.

Otra forma de definir esto sería usar clases de equivalencia de curvas, construyendo vectores tangentes sin acabar de hablar de tangentes. Veremos (y el profesor espera convencernos de eso) que esta forma que hemos desarrollado es en realidad mejor, ya que se comportan mejor respecto a ciertas operaciones. No es lo estándar, desde luego, pero de hecho se va imponiendo que es mejor.\footnote{Resumen: las formas molan y los tensores son caca.}

\subsection{Relación con cálculo}

Suponemos que tenemos abiertos $U⊆ℝ^n, V⊆ℝ^m$, y una aplicación $\appl{F}{U}{V}$ con $F ∈ C^∞$. Sea $p ∈U$ y $F(p) ∈V$, y supongamos $D_p$ un vector tangente (o derivación) en $p$. La pregunta es si le podemos asociar una derivación en $F(p)$, o dicho de otra forma, si $F$ manda vectores tangentes en vectores tangentes.

Esto puede ocurrir o no, pero como pregunta es importante. Lo que haremos será estudiar variedades y funciones entre variedades, y ver si las propiedades del cálculo diferencial se mantienen por funciones.

Definiremos entonces $F_{*,p}(D_p)$ como un vector tangente a $F(p)$ en $V$. Este elemento existe, se podrá definir con una fórmula, y se podrá aplicar a funciones $g ∈ A_{F(p)}$ donde $A_{F(p)}$ es el conjunto de gérmenes de funciones en $F_{(p)}$ (ver \ref{defGermenFuncion}).

Sabemos derivar en $U$ pero no en $V$, entonces lo que vamos a hacer va a ser una permutación de símbolos que tiene sentido\footnote{Metageometría diferencial.}. En este caso, nos queda \[ F_{*,p}(D_p)(g) = D_p (g ○ F)\] donde $g ○ F ∈ A_p$ y sí sabremos derivarlo. Queda demostrar que efectivamente $F_{*,p}(D_p)$ es una derivación\footnote{Se deja como ejercicio.}. Con la suma, tendríamos algo como esto:

\[ F_{*,p}(D_p + D_p')(g) = [D_p + D_p'](g○F) \]

Querríamos escribir ahora  $[D_p + D_p'](g○F) = D_p (g○F) + D_p'(g○F)$, y de hecho podemos hacerlo por ser $D_p, D_p'$ elementos del dual de los gérmenes y por la definición de la suma en el dual\footnotemark y por lo tanto nos quedaría efectivamente que \[ F_{*,p}(D_p + D_p')(g) = F_{*,p}(D_p)(g) + F_{*,p}(D_p')(g) \]

\footnotetext{La suma de dos elementos del espacio dual aplicada a un elemento es la suma de sus aplicaciones al mismo elemento. Es decir, si $A, B ∈ E^{*}$ entonces $(A+B)(x) = A(x) + B(x)$ con $x ∈ E$.}

Bien, ¿quién es esa función $F_{*,p}$ en el cálculo diferencial (lo que vimos en Análisis Matemático)? No es más que la diferencial de $F$, que recordemos que era una aplicación \[ F_{*,p} = \appl{DF(p)}{Π_p U} {Π_{F(p)}V}\] cuya matriz era la de derivadas parciales, \[ DF(p) = \left(\dpa{F_j}{x_i}\right)_{ij} \]

Vamos a ver que efectivamente eso es lo que sale. Si tenemos coordenadas $x_1, \dotsc, x_n$ en $Π_pU$ y $y_1, \dotsc, y_m$ en $Π_{F(p)} V$, con $F = (F_1, \dotsc, F_m)$ y $y_j = F_j (x_1, \dotsc, x_n)$; nos queda que \[ F_{*, p} \left(\dpa{}{x_i}_p\right)(y_j) = \dpa{F_j}{x_i} (p) \] efectivamente la matriz jacobiana.

Con esto, nos queda que podemos usar los teoremas de análisis que ya conocemos: regla de la cadena, teorema de la función inversa (\ref{thmInv}) y teorema de la función implícita (\ref{thmFImp}). Son teoremas demostrados por definición de límite pero que usaremos libremente.

\section{Coordenadas locales}

En Geometría nos interesará cambiar las coordenadas para adaptarlas al problema. Supongamos que tenemos las coordenadas $(x_1, \dotsc, x_n)$ para un abierto $U⊆ℝ^n$, y una serie de funciones $f_i$ tales que \[ y_i = f_i(x_i, \dotsc, x_n)\] son otro conjunto de coordenadas.

La pregunta que nos podemos hacer es, si dado un $p ∈ U$, las diferenciales \[ (\dif y_j)_p = \sum \dpa{f_j}{x_i}(p) \dif x_i (p)\] forman una base de tal forma que \[ \gen{(\dif y_1)_p, \dotsc, (\dif y_n)_p} = Π_p^* U \]

Es claro que la condición necesaria es que la matriz jacobiana formada por las derivadas parciales $\dpa{f_j}{x_i}(p)$ del cambio de coordenadas sea invertible o de rango máximo ($n$).

\begin{wrapfigure}{r}{0.4\textwidth}
\centering
\inputtikz{I_EspiralDifeomorfismo}
\caption{La aplicación $F$ que lleva la espiral a la circunferencia cumple el T.F.Inv. en todo punto, pero por no ser inyectiva no hay un difeomorfismo.}
\label{figEspiralDifeomorfismo}
\end{wrapfigure}

¿Es esta condición también suficiente? Aquí podemos usar el teorema de la función inversa (\ref{thmInv}) nos dice que es ``casi'' suficiente, ya que nos dice que en un entorno $V$ de $p$ quizás más pequeño que $U$ la función $F$ del cambio de coordenadas (la formada por las $f_i$) es un difeomorfismo (\ref{defDifeomorfismo}) con su imagen.


Para completarlo, suponemos que las condiciones del teorema de la función inversa se cumplen $∀p ∈ U$. En este caso, nos preguntamos si $F$ define un difeomorfismo con su imagen.

Dado que la diferencial es local, no se ``entera'' de lo que ocurre lejos del punto que estemos considerando. Si $F$ no es inyectiva, nos estropea el difeomorfismo global aunque en cada entorno sí que sea inyectiva sobre su imagen (por ejemplo, una espiral que llevamos a una circunferencia, figura \ref{figEspiralDifeomorfismo}). Luego para que $F$ defina un difeomorfismo con su imagen en todo $U$ tiene que ser inyectiva.

\section{Álgebra multilineal o teoría de tensores}

Consideraremos un espacio vectorial $E$ sobre un cuerpo $\kbb$ con dimensión finita $n$, y su espacio dual $E^*$ de la misma dimensión con una base $e^*_i$ tal que $e_i^*(e_j) = δ_{ij}$.

\begin{defn}[Forma\IS mulilineal] Una forma\footnote{``Forma'' indica que los valores son escalares} multilineal es una función \[
\appl{F}{\underbrace{E^* × \dotsb × E^*}_{a} × \underbrace{E × \dotsb × E}_{b}}{\kbb} \] lineal en cada variable. Por convención, se dirá que es $a$ veces contravariante y $b$ veces covariante.
\end{defn}

Por ejemplo, el determinante de una matriz se puede considerar como una forma multilineal: es una aplicación de $n$ veces $E$ a $\kbb$.

Una cosa interesante es que el conjunto de todas las formas multilineales $a$ veces contravariantes y $b$ veces covariantes, que denotaremos por $\tens_b^a(E)$ es un espacio vectorial con las operaciones naturales:

\begin{gather*}
(Φ_1 + Φ_2)(x_1^*,\dotsc, x_a^*, x_1, \dotsc, x_b) = Φ_1(x_1^*,\dotsc, x_a^*, x_1, \dotsc, x_b) + Φ_2(x_1^*,\dotsc, x_a^*, x_1, \dotsc, x_b) \\
(λ ·Φ_1)(x_1^*,\dotsc, x_a^*, x_1, \dotsc, x_b) = λ (Φ_1(x_1^*,\dotsc, x_a^*, x_1, \dotsc, x_b))
\end{gather*}

También podemos definir el producto de tensores.

\begin{defn}[Producto\IS tensorial] Si tenemos $Φ_1 ∈ \tens_a^b(E), Φ_2 ∈ \tens_c^d(E)$, tenemos que

\begin{multline*} Φ_1 \otimes Φ_2 (x_1^*,\dotsc, x_{a+c}^*, x_1, \dotsc, x_{b+d}) = \\ Φ_1(x_1^*,\dotsc, x_a^*, x_1, \dotsc, x_b) ·_\kbb Φ_2(x_{a+1}^*,\dotsc, x_{a+c}^*, x_{b+1}, \dotsc, x_{b+d}) \end{multline*}

En este caso, tenemos que $Φ_1 \otimes Φ_2 ∈ \tens_{a+c}^{b+d}(E)$. \label{defProdTensorial}
\end{defn}

Este producto tiene propiedades deseables, como el hecho de que sea distributivo ($Φ_1 \otimes (Φ_2 + Φ_3) = Φ_1 \otimes Φ_2 + Φ_1 \otimes Φ_3$) al ser $\kbb$ un cuerpo con el producto distributivo respecto a la suma.

Lo que nos interesará es ver bases en estos espacios de tensores de dimensión finita, y también operaciones naturales con los tensores.

\subsection{Bases del espacio de tensores}

Es un proyecto complicado, así que vamos a dividir la tarea de escribir una base en tres partes. Primero, buscaremos la base para tensores covariantes ($(b,0)$ ó $\tens_0^a (E)$), después para tensores contravariantes ($(0,a)$ ó $\tens_0^a (E)$)) y después para covariantes y contravariantes.

\subsubsection{Base de tensores covariantes $(b,0)$}

Partimos de algo simple: tenemos $E$ un espacio vectorial sobre $\kbb$ de dimensión finita $n$, con una base $\base = \set{e_i}_{i∈I} \equiv \set{e_1, \dotsc, e_n}$. Sea $Φ ∈ \tens_0^b(E)$ un tensor covariante $(0,b)$, es decir, una aplicación multilineal que lleva $b$ vectores de $E$ a un valor en $\kbb$.

Queremos saber entonces cómo caracterizar $Φ$ como combinación lineal de elementos de la base. Más concretamente, querríamos poder escribir \[ Φ = \sum λ_m ρ_m \] donde $ρ_m$ serían los elementos de la base y $λ_m$ las coordenadas.

Para llegar a esa notación, vamos a empezar estudiando la imagen $Φ(\vx_1, \dotsc, \vx_b)$.Sabemos que cada $\vx_j$ se puede expresar como combinación lineal de elementos de la base de $E$, es decir, \[ \vx_j = \sum_{i = 1}^n x_i^j e_i\] donde $x_i^j$ es la coordenada $i$ del vector $j$.

Entonces, podemos escribir \[ Φ(\vx_1, \dotsc, \vx_b) = Φ\left(\sum_{i = 1}^n x_i^1 e_i, \dotsc, \sum_{i = 1}^n x_i^b e_i\right)\]

Por ser $Φ$ multilineal, podemos ver que esos sumatorios y productos por escalares se van a poder sacar. La cuestión es cómo. Veamos cómo sale el primer sumatorio:

\[ Φ\left(\sum_{i = 1}^n x_i^1 e_i, \dotsc, \sum_{i = 1}^n x_i^b e_i\right) = \sum_{{j_1}=1}^n x_{j_1}^1 · Φ\left(e_{j_1}, \sum_{i = 1}^n x_i^2 e_i, \dotsc, \sum_{i = 1}^n x_i^b e_i\right) \]

Parece claro entonces que a partir de aquí nos van a quedar $b$ sumatorios que van desde $1$ hasta $n$. Es decir, \[ Φ(\vx_1, \dotsc, \vx_b) = \sum_{{j_1}=1}^n \dotsb \sum_{{j_b}=1}^n x_{j_1}^1 · \dotsb x_{j_b}^b Φ(e_{j_1}, \dotsc, e_{j_b})\]

Vemos que, independientemente de los vectores $\vx_i$ que cojamos, hay una parte que se queda constante, que sólo depende de cómo es $Φ$: los escalares $Φ(e_{j_1}, \dotsc, e_{j_b}) ∈ \kbb$. Podríamos pensar entonces que eso son las coordenadas de $Φ$ en $\tens_0^b(E)$. La cuestión es, ¿quiénes son los elementos de la base?

Los elementos de la base deberían ser elementos que nos den $x_{j_1}^1 \cdot \dotsb x_{j_b}^b$ cuando sean aplicados a $\vx_1, \dotsc, \vx_b$. Por así decirlo, necesitamos ``selectores de coordenadas'', que cuando apliquemos a los vectores nos den un producto de determinadas coordenadas.

Haciendo un ejercicio de intuición matemática, podemos pensar en los elementos del espacio dual $E^*$. Consideramos la base $\base^* = \set{e_1^*, \dotsc, e_2^*}$ del espacio dual, donde $e_i^*(e_j) = δ_{ij}$. Visto de otro modo, $e_i$ es una aplicación lineal tal que $e_i^*(\vv) = v_i$, es decir, que nos da la coordenada $i$-ésima del vector que le pasemos como argumento\footnote{Algo más formalmente: si $\vv = \sum_{j=1}^n v_j e_j$, entonces $e_i^*(\vv) = \sum_{j=1}^n v_j e_i^*(e_j) = v_i e_i^*(e_i) = v_i$.}.

Parece que los elementos de la base del dual son lo que necesitamos, lo que actuará de ``selector de coordenadas''. ¿Cómo los componemos para poder aplicarlo a $b$ vectores? Usando lo que vimos antes del producto tensorial (\ref{defProdTensorial}). El truco es que podemos considerar un elemento del dual como un tensor $(0,1)$, y entonces el producto tensorial de $b$ tensores es un tensor $(0,b)$.

Es decir, que los elementos de la base del espacio de tensores $\tens_0^b(E)$ serían los elementos \begin{gather*}
ρ_{j_1,\dotsc,j_b} = e_{j_1}^* \otimes \dotsb \otimes e_{j_b}^* \\
\appl{ρ_{j_1,\dotsc,j_b}}{\underbrace{E×\dotsb×E}_{b}}{ℝ}
\end{gather*} donde $ρ_{j_1,\dotsc,j_b}$ es un tensor $(0,b)$ que coge la coordenada $j_1$ del vector $1$, la $j_2$ del vector $2$, etc\footnote{Por ejemplo, si tomamos $b=2$, tendríamos que $ρ_{1,2}((1,2), (3,4)) = 1 · 4$.}, es decir, con
\[
ρ_{j_1,\dotsc,j_b} (\vx_1, \dotsc, \vx_b) = \left(e_{j_1}^* \otimes \dotsb \otimes e_{j_b}^*\right)(\vx_1, \dotsc, \vx_b) = x_{j_1}^1 \dotsb x_{j_b}^b
\]
donde $x_{j_b}^k$ es la coordenada $j_b$ del vector $k$, con $j_b = 1, \dotsc, n$ y $k = 1, \dotsc, b$.

Así, nos quedaría que podemos expresar un tensor $Φ$ como \[ Φ = \sum_{j_1=1}^n \dotsb \sum_{j_b = 1}^n \underbrace{Φ(e_{j_1}, \dotsc, e_{j_b})}_{\text{Coordenada}} · \underbrace{e_{j_1}^* \otimes \dotsb \otimes e_{j_b}^*}_{\text{Elemento de la base}} \] de tal forma que la imagen de $\vx_1, \dotsc, \vx_b$ por $Φ$ se puede calcular como
\begin{align*}
Φ(\vx_1, \dotsc, \vx_b) &= \sum_{j_1=1}^n \dotsb \sum_{j_b = 1}^n Φ(e_{j_1}, \dotsc, e_{j_b}) · \left(e_{j_1}^* \otimes \dotsb \otimes e_{j_b}^*(\vx_1, \dotsc, \vx_b)\right) = \\
&= \sum_{j_1=1}^n \dotsb \sum_{j_b = 1}^n Φ(e_{j_1}, \dotsc, e_{j_b}) · x_{j_1}^1 \dotsb x_{j_b}^b
\end{align*}
donde, de nuevo, $x_{j_b}^k$ es la coordenada $j_b$ del vector $k$.

\paragraph{Ejemplo: base de un tensor $(0,2)$} Un tensor $Φ∈\tens_0^2(ℝ^2)$ se podrá descomponer de la siguiente forma: \[ Φ = λ_{1,1} · e_1^* \otimes e_1^* + λ_{1,2} · e_1^* \otimes e_2^* + λ_{2,1} · e_2^* \otimes e_1^* + λ_{2,2} · e_2^* \otimes e_2^*\] donde $λ_{j_1, j_2} ∈ ℝ$ y $e_1, e_2$ son una base de $ℝ^2$, que en este caso tomaremos como la canónica $e_1 = (1,0),\;e_2=(0,1)$. Así, los tensores $e_{j_1} \otimes e_{j_2}$ serán matrices cuadradas $2 × 2$. Por ejemplo, \[ e_1 \otimes e_2 = \begin{pmatrix} 1 & 0 \\ 0 & 1\end{pmatrix}\]

Entonces, los tensores $Φ∈\tens_0^2(ℝ^2)$ no son más que matrices $2 × 2$, que se pueden expresar como combinación lineal de cuatro matrices base, cada una obtenida con los productos tensoriales de los elementos de la base de $ℝ^2$.

\paragraph{El determinante como tensor $(0,b)$} Un ejemplo más concreto: antes decíamos que el determinante es un tensor, así que vamos a ir a ello. Sea $Ψ_b$ la operación que nos da el determinante de una matrix $b × b$, es decir, \[ \appl{Ψ_b}{M_{b×b}}{ℝ} \]. ¿Cómo lo expresamos como un tensor?

Lo primero es pasar del espacio de matrices al espacio en el que se definen los tensores. Podemos ver que una matrix $b × b$ no es más que un conjunto de $b$ vectores de dimensión $b$, es decir, que \[ M_{b×b} = \underbrace{E × \dotsb × E}_{b \text{ veces}}\] donde $E$ es el espacio vectorial que estemos trabajando. Tomemos por comodidad y por seguir con el ejemplo de antes $b = 2$ y $E = ℝ^2$, por lo que vamos a buscar el determinante de matrices de dimensión $2$.

Sea \[ A = \begin{pmatrix} a & c \\ b & d \end{pmatrix} = \begin{pmatrix} \vx_1 & \vx_2 \end{pmatrix} \] con $\vx_1 = \begin{pmatrix}a \\ b\end{pmatrix}$ y $\vx_2 = \begin{pmatrix}c \\ d\end{pmatrix}$. Entonces $Ψ_2(A) = ad - cb$. Mirándolo desde el punto de vista de ``selección de coordenadas'' que comentábamos antes, seleccionamos la coordenada 1 del primer vector y la 2 del segundo multiplicando por 1; y luego seleccionamos la coordenada 2 del primer vector y la 1 del segundo multiplicando por -1.

Expresando esto con los elementos de la base, tendremos que \( Ψ_2 = 0 · e_1^* \otimes e_1^* + 1 · e_1^* \otimes e_2^* - 1 · e_2^* \otimes e_1^* + 0 · e_2^* \otimes e_2^* \label{eqDet2Tensor} \)

Bajo esta notación, el determinante de una matriz $A$ sería \begin{align*}
Ψ_2(A) &= 0 · \left(e_1^* \otimes e_1^* (\vx_1, \vx_2) \right)+ 1 · \left(e_1^* \otimes e_2^* (\vx_1, \vx_2) \right) - 1  · \left(e_2^* \otimes e_1^* (\vx_1, \vx_2) \right) + 0 · \left(e_2^* \otimes e_2^* (\vx_1, \vx_2) \right) = \\
&= 0 · x_1^1 x_2^1 + 1 · x_1^1 x_2^2  - 1 · x_1^2 x_2^1 + 0 · x_2^2 x_2^2\end{align*}

Podríamos ver, por ampliar un poco, cómo escribiríamos el tensor que nos da el determinante de matrices de dimensión 3 (omitiendo las coordenadas que son 0)\footnote{Y probablemente con algún índice que me ha bailado también.}:
\begin{multline*} Ψ_3 = e_1^* \otimes e_2^* \otimes e_3^* + e_2^* \otimes e_3^* \otimes e_1^* + e_3^* \otimes e_1^* \otimes e_2^*  \\
 - e_3^* \otimes e_2^* \otimes e_1^* - e_2^* \otimes e_1^* \otimes e_3^* - e_2^* \otimes e_1^* \otimes e_3^* \end{multline*}

\subsubsection{Base de tensores contravariantes $(0,a)$}

La construcción de la base de tensores contravariantes es análoga a la de covariantes. Estudiaremos la imagen $Φ(\vx_1^*, \dotsc, \vx_a^*)$, que nos quedará como
\[ Φ(\vx_1^*, \dotsc, \vx_a^*) = \sum_{{j_1}=1}^n \dotsb \sum_{{j_b}=1}^n x_{j_1}^1 · \dotsb x_{j_b}^b Φ(e_{j_1}^*, \dotsc, e_{j_b}^*)\], donde $\base^* = \set{e_1^*,  \dotsc, e_n^*}$ es una base del dual $E^*$.

Igualmente, necesitaremos unos ``selectores de coordenadas'' que nos lleven los $\vx_1^*, \dotsc, \vx_a^*$ a los distintos productos $x_{j_1}^1 · \dotsb x_{j_b}^b$. Esos selectores de coordenadas serán aplicaciones lineales $\appl{e_{i}^{**}}{E^*}{\kbb}$, es decir, $e_{i}^{**} ∈ E^{**}$, elementos del dual del dual. Así, la base serían los elementos $e_{j_1}^{**} \otimes \dotsb \otimes e_{j_b}^{**}$. Por suerte para nosotros, se puede identificar naturalmente\footnote{Habría que decir bien por qué.} $E^{**}$ con $E$ a través de un isomorfismo, así que nuestra base serían los productos tensoriales de elementos de $E$, es decir, elementos de la forma $e_{j_1} \otimes \dotsb \otimes e_{j_b}$.

En definitiva, que podemos expresar tensores $(0,a)$ $Φ∈\tens_0^a$ como combinación lineal de la siguiente forma:

\[ Φ = \sum_{j_1=1}^n \dotsb \sum_{j_b = 1}^n \underbrace{Φ(e^*_{j_1}, \dotsc, e^*_{j_b})}_{\text{Coordenada}} · \underbrace{e_{j_1} \otimes \dotsb \otimes e_{j_b}}_{\text{Elemento de la base}} \]

\subsubsection{Base de tensores generales $(a,b)$}

Una vez que ya tenemos la bases de tensores covariantes y contravariantes, por separado, tenemos que juntarlos. Problema número uno: el infierno de notación que nos podemos encontrar.

\paragraph{Notación: dual arriba, normal abajo} Vamos a jugar con índices y subíndices. Cuando hablemos de vectores en el espacio vectorial $E$, usaremos subíndices tanto en los vectores de la base como en las coordenadas: \[  \vv = \sum_{i=1}^n λ_i e_i ∈ E\]

Análogamente, usaremos superíndices cuando trabajemos con el espacio dual \[ \vv^* = \sum_{i=1}^n λ^i e^i ∈ E^* \], con la ventaja de que nos evita tener que poner la estrella al referirnos a vectores del dual.

Bien, con esto vamos a ver cómo expresar un tensorial $Φ ∈ \tens_a^b (E)$. Podríamos seguir lo que hemos visto en las anteriores secciones, sacando los sumatorios usando la propiedad de mulitilinealidad de los tensores. Al final, nos quedaría que los elementos de la base serían los productos tensoriales \[ e_{i_1} \otimes \dotsb \otimes e_{i_a} \otimes e^{j_1} \otimes \dotsb \otimes e^{j_b} \] de tal forma que podemos expresar un tensor como  \[ Φ = \sum_{i_1 = 1}^n \dotsc \sum_{i_b = 1}^n \sum_{j_1 = 1}^n \dotsb \sum_{j_b = 1}^n Φ(e^{i_1}, \dotsc, e^{i_a}, e_{j_1},  \dotsc, e_{j_b}) · e_{i_1} \otimes \dotsb \otimes e_{i_a} \otimes e^{j_1} \otimes \dotsb \otimes e^{j_b} \]

Por comodidad, denotaremos las coordenadas $Φ(e^{i_1}, \dotsc, e^{i_a}, e_{j_1},  \dotsc, e_{j_b})$ con una letra mayúscula $T^{i_1, \dotsc, i_a}_{j_1, \dotsc, j_b}$, esto es, \[ T^{i_1, \dotsc, i_a}_{j_1, \dotsc, j_b} ≝ Φ(e^{i_1}, \dotsc, e^{i_a}, e_{j_1},  \dotsc, e_{j_b}) \]

Sólo tendremos que especificar estas coordenadas cuando hablemos un vector. Por ejemplo, siguiendo con el ejemplo que habíamos descrito en la ecuación \eqref{eqDet2Tensor} del determinante de matrices de orden 2 como tensor, tendríamos que dar las cuatro coordenadas como $T_{1,1} = 0,\, T_{1,2} = 1,\, T_{2,1} = -1,\, T_{2,2} = 0$ (no ponemos superíndices porque las entradas del determinante son vectores del espacio base, no hay nada de dual).

\subsection{Contracciones}

\begin{defn}[Contracción] La contracción, denotada como $C_1^1$, es una función lineal \[ \appl{C_1^1}{\tens_a^b(E)}{\tens_{a-1}^{b-1}(E)} \] que reduce en una unidad cada una de las componentes de la aplicación multilineal. Si $Φ ∈\tens_a^b(E)$ es una forma multilineal con coordenadas $T_{j_1, \dotsc, j_a}^{i_1, \dotsc, i_b}$ entonces las coordenadas $\adh{T}$ de $C_1^1(Φ)$ se definen de la forma \[ \adh{T}_{j_2, j_3, \dotsc, j_a}^{i_2, i_3, \dotsc, i_b} ≝ \sum_{α = 1}^n T_{α,j_2, j_3, \dotsc, j_a}^{α, i_2, i_3, \dotsc, i_b} \]

Es obvio que para que esta definición tenga sentido han de ser $a,b ≥ 1$.
\end{defn}

Por ejemplo, la contracción de una forma multilineal $Φ ∈\tens_2^2(E)$ dada por las coordenadas \[ \begin{matrix}
T_{1,1}^{1,1} = 1 & T_{1,2}^{1,1} = 1 & T_{2,1}^{1,1} = 1 & T_{2,2}^{1,1} = 0 \\
T_{1,1}^{1,2} = 2 & T_{1,2}^{1,2} = 0 & T_{2,1}^{1,2} = 4 & T_{2,2}^{1,2} = 0 \\
T_{1,1}^{2,1} = 3 & T_{1,2}^{2,1} = 1 & T_{2,1}^{2,1} = 0 & T_{2,2}^{2,1} = 1 \\
T_{1,1}^{2,2} = 0 & T_{1,2}^{2,2} = 2 & T_{2,1}^{2,2} = 1 & T_{2,2}^{2,2} = 0 \\
 \end{matrix} \]
 estaría dada por las siguientes coordenadas\footnote{Me ha bailado algún índice ssssseguro.}:
\begin{align*}
\adh{T}_1^1 &= \sum_{α = 1}^2 T_{α,1}^{α,1} = T_{1,1}^{1,1} + T_{2,1}^{2,1} = 1 + 0 = 1 \\
\adh{T}_1^2 &= \sum_{α = 1}^2 T_{α,1}^{α,2} = T_{1,1}^{1,2} + T_{2,1}^{2,2} = 2 + 2 = 4 \\
\adh{T}_2^1 &= \sum_{α = 1}^2 T_{α,2}^{α,1} = T_{1,2}^{1,1} + T_{2,2}^{2,1} = 1 + 1 = 2 \\
\adh{T}_2^2 &= \sum_{α = 1}^2 T_{α,2}^{α,2} = T_{1,2}^{1,2} + T_{2,2}^{2,2} = 0 + 0 = 0
\end{align*}

Podemos preguntar de forma más general cómo actúan las contracciones sobre los vectores de la base de tensores $e_{i_1} \otimes \dotsb \otimes e_{i_a} \otimes e^{j_1} \otimes \dotsb \otimes e^{j_b}$. Ese elemento de la base se puede considerar como un tensor $Φ$ con coordenada $T_{j_1, \dotsc, j_a}^{i_1, \dotsc, i_b} = 1$ y resto de coordenadas 0. Entonces, $C_1^1(Φ)$ será un tensor con todas las coordenadas $0$ salvo $R_{j_2, \dotsc, j_a}^{i_2, \dotsc, i_b}$, dada por \[ R_{j_2, \dotsc, j_a}^{i_2, \dotsc, i_b} = \sum_{α = 1}^n T_{α, j_2, \dotsc, j_a}^{α, i_2, \dotsc, i_b} = T_{j_1, j_2, \dotsc, j_a}^{i_1, i_2, \dotsc, i_b} = 1 \]

Esta coordenada corresponde además al vector de la base de $\tens_{a-1}^{b-1}(E)$ $e_{i_2} \otimes \dotsb \otimes e_{i_a} \otimes e^{j_2} \otimes \dotsb \otimes e^{j_b}$. Es decir, que la actuación de la contracción sobre vectores de la base es la siguiente: \[
C_1^1(e_{i_1} \otimes \dotsb \otimes e_{i_a} \otimes e^{j_1} \otimes \dotsb \otimes e^{j_b}) ≝ e^{j_1}(e_{i_1}) · \left(e_{i_2} \otimes \dotsb \otimes e_{i_a} \otimes e^{j_2} \otimes \dotsb \otimes e^{j_b}\right)\]

\paragraph{Algo de magia} Si tenemos un tensor $(1,1)$, un endomorfismo\footnote{¿Ah sí?}, su contracción es un tensor $(0,0)$, un número\footnote{WTF $×2$.}, y vemos, escribiendo su definición, que es exactamente la traza del endomorfismo\footnote{Zasca final.}

Mi intento de explicación: un tensor $(1,1)$ es una aplicación bilineal $\appl{Φ}{E^*×E}{ℝ}$. Uno puede tratar de representarlo como una matriz $A$ de tal forma que \[ Φ(\vv^*, \vv) = \begin{pmatrix} v_1^* & \cdots & v_n^* \end{pmatrix} · \begin{pmatrix} A \end{pmatrix} · \begin{pmatrix} v_1 \\ \vdots \\ v_n \end{pmatrix} \] que se puede ver que es un escalar. Ahora bien, que eso sea endomorfismo es difícil. Lo que sí es claro es que, viendo cómo actúa la contracción, tendríamos que \[ C_1^1 (e_{i_1} \otimes e^{j_1}) =  e^{j_1}(e_{i_1}) \], es decir, que los elementos de la base son escalares.

\seprule

Vamos a recuperar un poco la perspectiva: queremos buscar una función $f$ tal que $\dif f = ω$, donde ω es una 1-forma. Buscábamos también el significado de la diferencial de una variable $(\dif x_i)_{x_0}$ o de una función $(\dif f)_{x_0}$, que veíamos en la sección \ref{secDimTangente}. Ahora, con las definiciones del álgebra multilineal, lo que estamos haciendo es darle sentido a la expresión del producto exterior $\dfl{x_{i_1}}{x_{i_a}}$.

Ahora cosas sobre isomorfismos raros. Identificación entre $E×E\dotsb × E^* \dotsb $ y $E\otimes$ etc con una aplicación lineal $U_Φ$ entre el tensorial y $\kbb$.

\subsection{Funtorialidad}

Teniendo en cuenta las construccioned que veremos más adelante, partimos de cuatro espacios vectoriales $E_1, E_2, F_1, F_2$ y dos aplicaciones lineales $\appl{u}{E_1}{F_1}$ y $\appl{v}{E_2}{F_2}$. Nos podemos preguntar cómo definir la aplicación $\appl{u\otimes v}{E_1 \otimes E_2}{F_1 \otimes F_2}$.

Si tenemos $\set{e_i}_{i=1}^n$, $\set{f_j}_{j=1}^m$, $\set{\hat{e}_i}_{i=1}^q$, $\set{\hat{f}_j}_{j = 1}^p$ como bases de $E_1, E_2, F_1, F_2$ respectivamente, ¿cuánto vale $u(e_i) \otimes v(f_j)$?

\subsection{Cambios de base}

De álgebra lineal sabíamos que los cambios de base se hacían con una matriz de cambio. ¿Cómo extendemos esto a las formas multilineales?

Es algo que nos interesará mucho hacer, ya que más tarde o más temprano tendremos que pasar de una variedad a otra y cambiar de coordenadas. Tenemos, por lo tanto, que entender bien cómo funcionan los cambios de base.

Partimos de dos bases $\base = \set{e_1, \dotsc, e_n}$, $\base' = \set{e_1', \dotsc, e_n'}$. Entonces, sabemos que si tenemos $\vv ∈ E$, lo podemos escribir de dos formas:
\[\vv = \sum_{i=1}^n λ^ie_i = \sum_{i=1}^n μ^i e_i' \] y que además podemos pasar de una base a la otra \[ e_i = \sum_j p_i^j e_j' \qquad e_i' = \sum_j q_i^j e_j\], es decir, multiplicando por las matrices de cambio de base $P = (p_i^j), Q=(q_i^j)$, que cumplen que $PQ=QP=I$.

Nos podemos preguntar también cómo cambian las cosas si tenemos dos bases del espacio dual $E^*$: $\base^* = \set{e_1, \dotsc, e_n}$, $\base'^* = \set{e'^1, \dotsc, e'^n}$. No hay duda de que tiene que existir una forma de cambiar entre las dos bases tal que \[ e'^j = \sum_i a_i^j e^i \] con $A = (a_i^j)$ es una matriz invertible. Para calcularlo, querremos evaluar esos vectores del dual en vectores de la base de $E$ (los $e_t$) y ver qué sale\footnote{Usamos que $e^j(e_i) = δ_i^j$, esto es, $1$ si los índices son iguales y 0 si no (ver \ref{defDeltaKronecker})}:
\begin{align*}
e'^j (e_t) &= \left(\sum_i a_i^j e^i \right) (e_t) \\
e'^j \left( \sum_r p_t^r e_r' \right) &= a_t^j \\
p_t^j &= a_t^j \\
\end{align*}

En definitiva, yendo al cambio de coordenadas tendremos que, en $E$, \[ μ^j = \sum p_i^j λ^i\] y en el dual $E^*$ se usa la otra matriz, \[ μ_j = \sum q_i^j λ_i\]

\paragraph{Cambio de coordenadas en matrices} Supongamos que tenemos una aplicación lineal $\appl{U}{E}{E}$ dada por una matriz, de la siguiente forma: \[ U(e_i) = \sum a_i^j e_j\]

Podemos decir que $U ∈ \tens_1^1(E)$, espacio que es isomorfo a $E\otimes E^*$. Es decir, que podemos escribir \[ \sum_{ij} a_j^i e_i \otimes e^j = \sum_{ij} a_j^i \left(\sum_l p_i^le'_l\right) \otimes \left(\sum_t q^j_t e'^t\right)\]

Ahora sólo hay que usar que el producto es distributivo respecto de la suma, por lo tanto nos quedaría \[ \sum_{ij} a_j^i \left(\sum_l p_i^le'_l\right) \otimes \left(\sum_t q^j_t e'^t\right) = \sum_{lt}\left(\sum_{ij} a_j^1 p_i^l q_t^j \right)e_l' \otimes e'^t\]

\paragraph{Cambio de coordenadas en tensores en general} Tomamos un tensor $Φ =T = (T_{j_1,\dotsc, j_b}^{i_1,\dotsc, i^a}) ∈ \tens_b^a(E)$, donde podemos identificar al espacio de tensores con \[ \tens_b^a(E) \simeq \underbrace{E\otimes \dotsb \otimes E}_{a} \otimes \underbrace{E^* \otimes \dotsb \otimes E^*}_{b} \equiv E^{\otimes a} \otimes E^{*\otimes b}\]

Sabemos que \[ T = \sum T_{j_1,\dotsc, j_b}^{i_1,\dotsc, i^a} e_{i_1} \otimes \dotsb \otimes e_{i_a} \otimes e^{j_1} \otimes \dotsb \otimes e^{j_b} \]

Siguiendo los mismos cálculos que antes, nos quedaría que el cambio de coordenadas a $T'$ estaría dado por la fórmula \[ T'^{l_1, \dotsc l_a}_{k_1,\dotsc, k_b} = \sum_{\substack{i_a, \dotsc, i_a \\ j_1, \dotsc, j_b}} T_{j_1,\dotsc, j_b}^{i_1,\dotsc, i^a} p_{i_1}^{l_1} \dotsb p_{i_a}^{l_a} q_{j_1}^{k_1} \dotsb q_{j_b}^{k_b} \]

De aquí podemos retomar una definición algo antigua en la historia de las matemáticas:

\begin{defn}[Tensor\IS clásico] Un tensor clásico es una función $\appl{T}{\mathfrak{B}}{\kbb^{n^{a+b}}}$, donde $\mathfrak{B}$ es el conjunto de todas las bases en $E$, de tal forma que \begin{gather*} T(\base) = \left(\tc\right) \\ T(\base') = \left(\tc[k][l]\right) \end{gather*}

\end{defn}

\subsection{Potencias y álgebra tensorial}

\begin{defn}[Potencia] Se define la potencia tensorial como \[ \otimes^n E ≝ \underbrace{E\otimes \dotsb \otimes E}_{n} \]
\end{defn}

\begin{defn}[{Á}lgebra\IS tensorial] Dado un espacio vectorial $E$, se define el álgebra tensorial $\tens^\bullet E$ como \[ \tens^\bullet E ≝ \bigoplus_{n∈ℕ} \left(\otimes^n E\right) \]
\end{defn}

\begin{defn}[{Á}lgebra\IS pinchorial]\footnote{No soy capaz de escribir pinchorial sin reírme.} Se define el álgebra pinchorial como el conjunto de todos los tensoriales antisimétricos
\end{defn}

No sé qué mierdas ha escrito aquí. Pinchorial. Desisto.Λ

\subsection{Tensores antisimétricos}

¿Qué ocurre con un tensor antisimétrico? Si tenemos $Φ(\vx^*, \vy^*) = -Φ(\vy^*, \vx^*)$, con $Φ∈Λ^2 E$, entonces operando \begin{align*}
Φ(\vx^*, \vy^*) &= -Φ(\vy^*, \vx^*) \\
\sum_{ij} A^{ij} x_i y_j &= - \sum_{ji} A^{ji} x_j y_i \\
\end{align*} nos quedan nos condiciones sobre la matriz $A$: $A^{ii} = 0$ y $A^{ij} = - A^{ji}$. En este caso, podemos simplificar y nos quedaría \[\sum_{i<j} A^{ij} (x_i y_j - x_j y_i) \] y, por lo tanto, podemos escribir \[ Φ = \sum_{i<j} A^{ij} e_i \y e_j \]

En resumidas cuentas, lo que concluimos es que $\dim Λ^2 E = \comb{n}{2}$.

En general, tenemos que $Λ^p E ⊂ \otimes^p E$ con $e_{i_1} \y \dotsb \y e_{i_p}$ los elementos de la base definidos tal que \[ e_1 \y \dotsb \y e_p (f^1, \dotsc, f^p) ≝ \det \left(f^i(e_j)\right)\], y con \[ \dim Λ^p E = \comb{n}{p} \]

Como ejercicio, habría que ver que si $Φ ∈ Λ^p E$ entonces se puede escribir como \[ Φ = \sum_{i_1, \dotsc, i_p} A^{i_1, \dotsc, i_p} e_{i_1} \y \dotsb \y e_{i_p} \]

Ahora vamos a ir a lo que nos interesa. Si tenemos $α∈ Λ^p E, β∈ Λ^q E$, ¿qué es $α\y β$? Uno querría poder escribir $α\y β = k (α\otimes β)$, pero eso no es antisimétrico. Por lo tanto, tendremos que ``antisimetrizarlo''. Recordamos que si tenemos una aplicación $\appl{Φ}{\underbrace{E^* × \dotsb × E^*}_{m}}{\kbb}$, su antisimetrización es \[ Φ_a ≝ \frac{1}{m!}\sum_{σ∈S_m} (-1)^{\mop{sig} σ} Φ(f^{σ(1)}, \dotsc, f^{σ(m)}) \]

Así, justificamos que \[  e_1 \y \dotsb \y e_p (f^1, \dotsc, f^p) = \frac{p!}{p!} \sum_{σ∈S_p} (-1)^{\mop{sig} σ} \prod_{j=1}^p f^{σ(j)}(e_j)\]

\begin{defn}[Producto\IS exterior] Dados dos tensores antisimétricos \begin{gather*} α = \sum A^{i_1, \dotsc, i_p} e_{i_1} \y \dotsb \y e_{i_p} \\ β = \sum B^{j_1, \dotsc, j_q} e_{j_1} \y \dotsb \y e_{j_q} \end{gather*}, su producto se define gracias a la propiedad distributiva como \[ α \y \beta = \sum A^{i_1, \dotsc, i_p} B^{j_1, \dotsc, j_q} e_{i_1} \y \dotsb \y e_{i_p} \y e_{j_1} \y \dotsb \y e_{j_q}  \]
\end{defn}

En cuanto al cambio de signo, es fácil ver que \[ α \y β = (-1)^{pq} β \y α \]


\subsection{Aplicaciones en el espacio vectorial libre}

\begin{figure}[hbtp]
\centering
\inputtikz{I_EspacioVectorialLibre}
\caption{Diagrama de las aplicaciones del espacio vectorial libre.}
\label{imgVecLibre}
\end{figure}

Tenemos un conjunto $X$ y una aplicación $\appl{f}{X}{F}$. Tomando una cierta aplicación $\appl{u}{X}{\kbb^X}$,\footnote{$\kbb^X$ es el espacio vectorial libre, ver \ref{secEspacioVectorialLibre}.} demostrar que existe una aplicación $\appl{\hat{f}}{\kbb^X}{F}$ tal que $f = u ○ \hat{f}$ (ver figura \ref{imgVecLibre}).

Tomaremos $u$ de forma natural, es decir, que $u(x) = g_x ∈ \kbb^X$, $\appl{g_x}{X}{\kbb}$ definida como \[ g_x(α) =
\begin{cases}
1_\kbb & α = x \\
0_\kbb & α ≠ x \end{cases} \]

Tomamos un elemento $\vv ∈ \kbb^X$, que se puede expresar como

\section{Formas diferenciales}

Con toda la base del álgebra multilineal podemos definir más prácticamente las formas diferenciales.

\begin{defn}[p-forma] Una p-forma es un tensor contravariante que expresamos de la siguiente manera: \[ ω_p = \sum a_{i_1, \dotsc, i_p} (x_1, \dotsc, x_n) \dfl{x_{i_i}}{x_{i_p}} = \sum a_I \dif x_I \]

con $a_I ∈ C^∞(U)$, donde $U ⊆ ℝ^n$.
\end{defn}

Así, si tenemos dos p-formas \begin{align*}
ω &= \sum a_I \dif x_I \\
τ &= \sum b_J \dif x_J
\end{align*}

podemos operarlas con el \concept[Producto\IS exterior]{producto exterior} de la siguiente forma: \[ ω \y τ ≝ \sum a_I b_J \dif x_I \y x_J \], teniendo en cuenta que $\dif x_i \y \dif x_i = 0$.

Este producto cumple las propiedades distributiva y asociativa.

\subsection{Pullback}

También podemos ver qué ocurre con las formas diferenciales cuando tenemos aplicaciones entre espacios tangentes. Suponemos que tenemos $\appl{F}{U}{V}$, con $x_1, \dotsc, x_n$ coordenadas en $U$ y $y_1, \dotsc, y_m$ coordenadas en $V$, y con \[ F = (y_1(x_1, \dotsc, x_n), \dotsc, y_m(x_1, \dotsc, x_n)) \]

Tenemos una forma diferencial $ω = \sum a_I \dif y_I$: podemos ``traerla de vuelta'' a $U$ simplemente sustituyendo:

\begin{align*}
ω &= \sum a_I(y_1, \dotsc, y_m) \dif y_I \\
&= \sum a_I(y_1(x_1, \dotsc, x_n), \dotsc, y_m(x_1, \dotsc, x_n)) \dif y_I (x_1, \dotsc, x_n) \\
&= \sum algo \dpa{algo}{otra cosa} \dif x_{algomas}
\end{align*}

Estoy muy dormido como para seguir copiando ese churro.

\subsubsection{Campos como formas diferenciales y relación con el pullback}

Vimos en algún momento el operador de campo \[ D_p = \sum a_1(x_1, \dotsc, x_m) \left(\dpa{}{x_i}\right)_p \] que se se aplica a fuciones, de tal forma que \[ D_p(f) = \sum_i a_i \left(\dpa{f}{x_i}\right)_p ∈ C^∞ (U) \]

Una forma de ver estos campos vectoriales es como vectores que nos indican la velocidad de un fluido en cada punto (suponiendo que esta no dependa del tiempo). Veremos que en Geometría nos interesará buscar curvas tales que en todo punto su vector tangente sea el vector del campo, cosa que haremos con EDOs.

Si consideramos una 1-forma \[ ω_p = \sum b_i(x_1(p), \dotsc, x_m(p)) (\dif x_i)_p\], no es más que una aplicación lineal $\appl{ω_p}{Π_p U}{ℝ}$. Podemos pensar en el núcleo $N_p = \ker ω_p ⊂ Π_p U$ con codimensión\footnote{La codimensión es la dimensión del núcleo menos la del total, esto es, $\dim N_p = n - 1$.} 1\footnote{Por no sé qué teorema.}.

En este caso, en cada punto tendríamos un hiperplano $N_p$ diferenciable que varía con el punto. Así, lo que buscaremos serán las hipersuperficies de algo.

Es decir, que cuando tenemos una 1-forma diferencial buscaremos encontrar las hipersuperficies resolviendo la ecuación $ω = 0$.

Sabemos que $F^*(ω)$, con $ω$ una $p$-forma y $\appl{F}{U}{V}$, actúa sobre $p$ vectores $D_1, \dotsc, D_p$ del espacio tangente $Π_{x_0} U$ en $x_0$. Entonces, definiremos \[ F^*(ω)(D_1, \dotsc, D_p)≝ ω(F_* D_1, \dotsc, F_* D_p)\] donde cada $F_* D_i ∈ Π_{F(x_0)} V$ se define como \[ F_* D(g) ≝ D(g○F)\]

Las propiedades de esta imagen inversa son las siguientes (por definición):

\begin{enumerate}
\item $F^*(ω + τ) = F^* ω + F^* τ$.
\item $F^*(gω) = F^*(g) F^*(ω)$ donde $g$ es una función en $U$ y $F^*(g) = F ○ g $ es una función en V.
\item $F^*(ω_1 \y \dotsb \y ω_k) = F^*ω_1 \y \dotsb \y F^*ω_k$ para $ω_i$ 1-formas.
\item $F^*(ω \y τ) = F^*ω \y F^* τ$ para formas en general.
\item Si tenemos dos aplicaciones $G, F$, tenemos que $(G ○ F)^*ω = F^*(G^*(ω))$.
\end{enumerate}

Vamos a hacer la demostración de la propiedad 3.

\begin{proof}[Propiedad 3] $F^*(ω_1 \y \dotsb \y ω_k)$ va a ser una $k$-forma en $U$, luego va a actuar sobre $k$ vectores tangentes. Entonces, por definición, tenemos que \[ F^*(ω_1 \y \dotsb \y ω_k)(D_1, \dotsc, D_k) = ω_1 \y \dotsb \y ω_k(F_*D_1, \dotsc, F_* D_k)\] y entonces por definición\footnote{Decimoquinta vez que en los apuntes pongo ``por definición''. Estoy seguro de que lo ha dicho muchas veces más.} del producto pinchorial o exterior esto es igual a \[ \det\left(ω_i (F_*D_j)\right)\]

Volviendo a aplicar la definición\footnote{Decimosexta.} de $F^*$ nos quedaría que
\[ \det\left(ω_i (F_*D_j)\right) = \det (F^*ω_i (D_j)) = F^* ω_1 \y \dotsb \y F^* ω_k (D_1, \dotsc, D_k) \] y ya está.
\end{proof}

Veamos un ejemplo. Supongamos que tenemos una $p$-forma diferencial $ω = \sum a_I \dif y_I$ en $V$ donde $a_I ∈ C^∞(V)$ y $I = i_1, \dotsc, i_p$, de tal forma que $\dif y_I = \dif y_{i_1} \y \dotsb \y \dif y_{i_p}$. En este caso, la función inversa nos quedaría \[ F^* ω = F^*\left(  \sum a_I \dif y_I \right) = \sum F^*(a_I) F^*(\dif y_I) \] usando las propiedades que hemos visto antes. Esto demuestra que las dos definiciones que hemos visto de qué es $F^*$ son la misma: la primera era sustituir y operar y la segunda es la aplicación a los vectores tangentes.


La propiedad cuatro también es interesante, veámosla.

\begin{proof}[Propiedad 4] Queremos ver que si $ω$ es una $p$-forma y $τ$ una $q$-forma, entonces $F^*(ω \y τ) = F^*ω \y F^* τ$. Si partimos de que \begin{gather*}
ω = \sum a_I \dif y_I \\
τ = \sum b_J \dif y_J
\end{gather*}, entonces tenemos que \[ ω \y τ = \sum a_I b_J \dif y_I \y \dif y_J \] por definición del producto exterior.

Operamos entonces: \[ F^*\left(\sum a_I b_J \dif y_I \y \dif y_J\right) = \sum F^*(a_I) F^*(b_J) F^*(\dif y_I \y \dif y_J) \]

Tenemos que $\dif y_I \y \dif y_J$ son 1-formas, luego podemos escribir $F^*(\dif y_I \y \dif y_J) = F^*(\dif y_I) \y F^*(\dif y_J)$ y entonces nos queda directamente que eso es igual a $ F^*ω \y F^* τ$.
\end{proof}

\subsection{Derivada exterior}

La derivada exterior nos lleva de una $p$-forma a una $(p + 1)$-forma, como si derivásemos una vez más. Veamos la definición formal.

\begin{defn}[Derivada\IS exterior] Dada $ω_p = \sum a_I \dif x_I$, entonces se define la derivada exterior como \[ \dif ω_p ≝ \sum \dif a_I \y \dif x_I \] teniendo en cuenta que \[ \dif f ≝ \sum \dpa{f}{x_i} \dif x_i \]\label{defDerivadaExterior}
\end{defn}

Vamos a ver sus propiedades:

\begin{enumerate}
\item Linealidad: $\dif (ω_1 + ω_2) = \dif ω_1 + \dif ω_2$.
\item \concept{Regla\IS de Leibniz}: $\dif (ω_1 \y ω_2) = \dif ω_1 \y ω_2 + (-1)^p ω_1 \y \dif ω_2$, donde $ω_1$ es una $p$-forma.
\item \concept{Lema\IS de Poincaré}: $\dif (\dif ω) = 0$.
\item $F^*(\dif ω) = \dif F^*(ω)$.
\end{enumerate}

Vamos a ver las demostraciones de las propiedades 2,3 y 4.

\begin{proof}[Propiedad 2] Partimos de que $ω_1 \y ω_2 = \sum a_I b_J \dif x_I \y \dif x_J$, entonces \[ \dif (ω_1 \y ω_2) = \dif\left(\sum a_I b_J \dif x_I \y \dif x_J\right) = \sum \dif(a_I b_J) \y \dif x_I \y \dif x_J \]

Con $\dif(a_I b_J)$ aplicamos la regla de Leibniz de funciones así que $\dif(a_I b_J) = \dif a_I · b_J + a_I · \dif b_J$. Podemos separar entonces y \[ \sum \dif(a_I b_J) \y \dif x_I \y \dif x_J = \sum b_J \dif a_I \y \dif x_I \y \dif x_J + \sum a_I \dif b_J \y \dif x_I \y \dif x_J \]

Por definición, el primer sumando es lo que queremos: $\sum b_J \dif a_I \y \dif x_I \y \dif x_J = \dif ω_1 \y ω_2$. El segundo casi lo es, salvo porque $\dif x_I$ no está donde debe.Tenemos que ``mover'' $\dif x_I$ a la izquierda, y eso son $p$ cambios. Es decir, que $\dif b_J \y \dif x_I = (-1)^p \dif x_I \y \dif b_J$ así que $\sum a_I \dif b_J \y \dif x_I \y \dif x_J = (-1)^p ω_1 \y \dif ω_2$.h
\end{proof}

\begin{proof}[Propiedad 3]
Queremos demostrar que el diferencial de la diferencial es 0. Ya lo teníamos en el caso de las 0-formas, y ahora vamos a verlo en general para $ω = a_I \dif x_I$. En este caso, tenemos que \[\dif ω = \dif a_I \y \dif x_I + a_I \dif(\dif x_I) \]

Sabemos que $\dif (\dif x_I) = 0$\footnote{Es fácil verlo así: $\dif (\dif x_I) = \dif (1 · \dif x_I) = \dif 1 \y \dif x_I = 0$, que es $0$ porque la derivada de una constante ($1$) es $0$.}, así que nos queda $\dif ω = \dif a_I \y \dif x_I$\footnote{Esto no es más que llegar de forma algo más detallada a la derivada exterior que ya hemos definido en \ref{defDerivadaExterior}.}.

Tomamos de nuevo la derivada exterior: \[ \dif(\dif ω) = \dif(\dif a_I \y \dif x_I) = \underbrace{\dif(\dif a_I)}_{0} \y \dif x_I - \dif a_I\y \underbrace{\dif(\dif x_I)}_{0} = 0\] y ya lo tenemos.
\end{proof}

\begin{proof}[Propiedad 4]
Vamos a demostrar que $F^*(\dif ω) = \dif F^*(ω)$. Empezaremos primero viendo qué ocurre cuando $ω$ es una 0-forma y por lo tanto sólo tenemos el diferencial de una función. En este caso, \[ \dif ω = \dif f = \sum_j \dpa{f}{y_j}\dif y_j \] y aplicando el pullback
	\begin{multline*} F^* (\dif ω )
		= F^*\left(\sum_j \dpa{f}{y_j}\dif y_j \right)
		= \sum_{i,j} \dpa{f}{y_j} \dpa{y_j}{x_i} \dif x_i = \\
		\eqreason{Usando la regla de la cadena.} \sum_i \dpa{(f○F)}{x_i}
		\eqreason{Por definición de $\dif$.}\dif (F○f) = \dif F^*(f)
	\end{multline*}

Con esto podemos saltar al caso general, cuando $ω = \sum a_I \dif y_I$. Necesitaremos un lemilla que nos dice que \[ \dif\left(F^* (\dif y_I)\right) = \dif(\dif F_{i1} \y \dif F_{i2}\y \dotsb \y \dif F_{ip}) = 0\] y que no vamos a demostrar.

Siguiendo con la demostración, por definición de pullback tenemos que  \[ \dif (F^* ω) = \dif \left(\sum_I F^*(a_I) F^*(\dif y_I)\right) \]. Con la propiedad distributiva, tenemos que eso es igual a \[ \sum_I \left(\dif F^*(a_I) \y F^*(\dif y_I) + F^*(a_I) \dif\left(F^* (\dif y_I)\right)\right)\]. Aquí vemos rápidamente que $\dif \left(F^*(\dif y_I)\right) = \dif \dif F^* y_I = 0$ (acabamos de ver que el pullback y la diferencial se pueden intercambiar con 0-formas). Luego, operando, \[ \dif (F^* ω) = \sum_I \dif F^*(a_I) \y F^*(\dif y_I) = F^*\left(\sum_I \dif a_I \y \dif y_I\right) \eqreason{Por la propia definición del pullback.} F^* (\dif ω) \]
\end{proof}

Con las propiedades demostradas, vamos a ir a por el primer teorema importante: el de Poincaré.

\begin{theorem}[Teorema\IS de Poincaré]
Sea $ω$ una p-forma en un abierto $U ⊂ ℝ^n$, con coordenadas $x_1, \dotsc, x_n$ en $U$. Vamos a suponer $\dif ω = 0$ (o, dicho de otra forma, que $ω$ sea cerrada).

Bajo ciertas hipótesis topológicas, existe $α$ una $(p-1)$-forma en $U$ tal que \[ ω = \dif α\]. Esto se denota como que $ω$ es una p-forma\IS exacta.
\end{theorem}

Para la demostración necesitaremos una definición previa

\begin{defn}[Abierto\IS contractible] Un abierto $U ⊂ ℝ^n$ es contractible si existe una función continua $\appl{Φ}{I×U}{U}$ con $Φ(1,x) = x\; ∀ x ∈ U$ y $Φ(0,x) = x_0 ∈ U$.\label{defContractible}
\end{defn}

\begin{proof}
La demostración usa un truco con mucha utilidad en topología. En lugar de trabajar en $U$, vamos a considerar $I = [0,1)$ y trabajar en $I × U = U'$, una especie de cilindro, con coordenadas $t, x_1, \dotsc, x_n$.

Definiremos dos aplicaciones, $j_0, j_1$, de la siguiente forma:
\begin{align*}
\appl{j_1}{U&}{U'} & \appl{j_0}{U&}{U'} \\
x &\longmapsto (1,x) & x &\longmapsto (0,x)
\end{align*}

El pullback de ambas se haría cambiando $t = 1$ en $ω$ y $\dif t = 0$ con $j_1^*(ω)$, y cambiando $t = 0$ en $ω$ y $\dif t = 0$ con $j_0^*(ω)$. El truco es ahora usar las dos copias de $U$ (cada tapa del cilindro) y usar estas $j_i^*$ de una forma que vamos a ver ahora.

Sea $Ω^p(U)$ el espacio vectorial de todas las $p$-formas en $U$. Definimos la aplicación $\appl{K}{Ω^{p+1}(U')}{Ω^p(U)}$ de la siguiente forma: si $ω = a_I \dif x_I$, sin que aparezca la $t$, definimos $K(a_I \dif x_I) ≝ 0$ ya que en la derecha tengo $p$ diferenciales y no hay otra forma de quitar una.

Cuando $ω = a_I(t,x) \dif t \y dif x_I$, entonces $K(ω) ≝ \left(\int_0^1 a_I(t,x) \dif t\right) \dif x_I $. Los elementos son suma de estos dos tipos y como es lineal con definirlo en los ya se queda definido.

Una propiedad clave es que \( \label{eqPropKdif} K(\dif ω) + \dif (K(ω)) = j_1^* ω - j_0^* ω \), y vamos a demostrarlo.

Cualquier $ω$ se va a poder ver como suma de elementos de la forma $a_I(t,x)  \dif x_I$ y $a_I(t,x) \dif t \y \dif x_I$, así que vamos a escribir la fórmula según estos dos elementos. Si $ω = a_I (t,x) \dif x_I$, tenemos que $K(ω) = 0$ por la construcción de $K$. Por otra parte, como $\dif ω = \dpa{a_I}{t} \dif t \y \dif x_I + θ$ donde $θ$ son otros términos que no tienen $\dif t$, tenemos que \[ K(\dif ω ) = \left(\int_0^1 \dpa{a_I}{t} \dif t\right) \dif x_I = \left(a_I(1,x) - a_I(0,x) \right) \dif x_I \] que, por definición, es lo mismo que $j_1^*ω - j_0^*ω$ por la propia definición de $j_0$ y $j_1$.

Nos queda ahora demostrar \eqref{eqPropKdif} para $ω = a_I(t,x) \dif t \y \dif x_I$. Por la construcción de los $j_i$, el lado de la derecha de la igualdad es $0$ ya que ambas aplicaciones nos llevaban $\dif t$ $0$. Así, tenemos que demostrar que $K(\dif ω ) = \dif (K(ω))$.

Calculamos $K(\dif ω)$, que será
\begin{align*}
K(\dif ω) &= K \left[\left(\dpa{a_i}{t}\dif t + \sum_i \dpa{a_I}{x_i} \dif x_i\right) \y \dif t \y \dif x_I \right] \eqreason{$\dpa{a_i}{t}\dif t$ se va por repetir $\dif t$, y después cambiamos de signo por la permutación de una diferencial.} \\
& =
K \left( - \sum_i \dpa{a_I}{x_i} \dif t \y \dif x_i \y \dif x_I \right) = \\
&= - \sum_i\left(\int_0^1 \dpa{a_I}{x_i} \dif t\right) \dif x_i \y \dif x_I
\end{align*}

Calculando luego el $\dif K(ω)$ nos saldrá lo mismo pero con el signo cambiado:
\begin{align*}
\dif (K ω) &= \dif \left(\int_0^1 \left( a_I(t,x) \dif t \right) \dif x_I\right) = \\
&= \sum \dpa{\int_0^1 a_I(t,x) \dif t}{x_I} \dif x_i \y \dif x_I = \\
&= \sum \left(\int_0^1 \dpa{a_I}{x_I} \dif t\right) \dif x_i \y \dif x_I
\end{align*}

Cogemos ahora la contracción $Φ$ de \ref{defContractible}. Entonces, \[ K(\dif Φ^*ω) + \dif (K(Φ^*ω) = j_1^* ω - j_0^*ω = ω - 0 = ω \], luego el α que buscamos es algo.
\end{proof}

Vamos a ver un ejemplo de esto. Sea $ω = P \dif x + Q \dif y + R \dif z$ definida en todo $ℝ^3$. Entonces, \[ \dif ω = \left(\dpa{R}{y} - \dpa{Q}{x}\right) \dif y \y \dif z + \left(\dpa{P}{z} - \dpa{R}{x}\right) \dif z \y \dif x + \left(\dpa{Q}{x} - \dpa{P}{y}\right) \dif x \y \dif y\]

Si igualamos $\dif ω = 0$, eso es equivalente a un sistema de ecuaciones: \begin{align*}
\dpa{R}{y} &= \dpa{Q}{x} \\
\dpa{P}{z} &= \dpa{R}{x} \\
\dpa{Q}{x} &= \dpa{P}{y}
\end{align*}

Queremos obtener ahora el potencial $α$. Definiremos la contración $Φ(t,x,y,z) = (tx, ty,tz)$. Lo primero es calcular $Φ^*ω$, que es puramente un cambio de variable:
\begin{multline*}
Φ^* ω = P(tx,ty,tz) (t\dif x + x \dif t) + \\ Q(tx,ty,tz) (t\dif y + y \dif t) + R(tx,ty,tz) (t\dif z + z \dif t)
\end{multline*}

Sólo interesarán los términos $x \dif t, y \dif t, z \dif t$: los otros no tienen $\dif t$ y por $K$ se irán a cero de cabeza. Consideramos ahora $F(x,y,z) ≝ K(Φ^*ω(x,y,z))$, y vemos que nos queda \[ F(x,y,z) = x \int_0^1 P(tx,ty,tx) \dif t +  y \int_0^1 Q(tx,ty,tx) \dif t +  z \int_0^1 R(tx,ty,tx) \dif t \]

Ahora deberíamos comprobar que $\dif F = ω$, que como no es muy horroroso podemos hacerlo. Empezamos por la derivada parcial con respecto a $x$, que es\footnote{Omitimos argumentos por facilidad.} \[ \dpa{F}{x} = \int_0^1 P \dif t + x \int_0^1 \dpa{P}{x} t \dif t + y \int_0^1 \dpa{Q}{x}t \dif t + z \int_0^1 \dpa{R}{x}t \dif t \]

Tenemos que llegar ahora a que $\dpa{F}{x} = P$, que es el coeficiente de $\dif x$ en $ω$. Nos fijamos en las ecuaciones que teneíamos antes y podemos cambiar:
\[ \dpa{F}{x} = \int_0^1 P \dif t + x \int_0^1 \dpa{P}{x} t \dif t + y \int_0^1 \dpa{P}{x}t \dif t + z \int_0^1 \dpa{P}{z}t \dif t\]

Eso se puede poner como una sola integral, que es
\begin{align*}
\dpa{F}{x} &= \int_0^1 \left(P + xt \dpa{P}{x} + yt \dpa{P}{y} + zt \dpa{P}{z} \right)\dif t = \\
&= \int_0^1 \left(P + t \deriv{}{t} P(tx,ty,tz)\right) \dif t = \\
&= \int_0^1 \deriv{P · t}{t} \dif t = \\
&= P(x,y,z)
\end{align*}

Haríamos esto con el resto de parciales y nos saldría efectivamente ω.

Si considerásemos $ω = A \dif y \y \dif z + B \dif z \y \dif x + C \dif x \y \dif y$, la derivada exterior nos saldría como la divergencia: \[ \dif ω = \left(\dpa{A}{x} + \dpa{B}{y} + \dpa{C}{z}\right) \dif x \y \dif y \y \dif z \]


La 1-forma $α$ que cumple $\dif α = ω$ sería de la forma $α = P \dif x + Q \dif y + R \dif z$. Usando la misma contracción de antes, tendríamos que \[ Φ^* ω = A(tx,ty,tz) \dif (ty) \y \dif (tz) + \dotsb  = A(tx,ty,tz) \left((y\dif t +t\dif y) \y (z \dif t + t \dif z)\right) + \dotsb \], donde de nuevo sólo nos importan los términos con $\dif t$, y donde $y \dif t \y z \dif t$ se van porque repiten diferencial. Entonces \[ Φ^* ω = A(tx,ty,tz) (yt \dif t \y \dif z - zt \dif t \y \dif y) + \dotsb \], de tal forma que \[ α = K(Φ^* ω) = \left(\int_0^1 At \dif t\right) (y \dif z - z \dif y)\]

Aquí podríamos ponernos de nuevo a derivar para ver que sale lo que sale, pero ahora sí es un engendro y no es plan.
