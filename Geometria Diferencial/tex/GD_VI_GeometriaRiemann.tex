% -*- root: ../GeometriaDiferencial.tex -*-
\chapter{Introducción a la Geometría Riemanniana}

\hfill \textit{Prometo que yo he hecho lo mejor que he podido con esto.}

Tomaremos $X$ una variedad compacta y una métrica Riemanniana en $X$ dada por el producto escalar $\pesc{}_p$ en $\tgs_p X$. La condición es que dados $D, D'$ campos en $X$, la aplicación \begin{align*}
p & \longmapsto \pesc{D_p, D_p'}_p \\
x & \longmapsto ℝ
\end{align*} es infinitamente diferenciable en $X$.

% Autonota: Creo que lo que este hombre está tratando de explicarnos es cómo la métrica riemanniana permite replicar las nociones que ya teníamos de ℝ^n.

En una variedad riemanniana\footnote{Variedad compacta con métrica riemanniana.} algo.

En geometría de primero hacíamos cosas como medir la longitud de una curva $\appl{γ}{I}{X}$ dada por \[ \mop{long} γ ≝ \int_I \md{γ'(t)} \dif t \]

Dado que esta longitud no dependía de la parametrización, es una propiedad intrínseca de la imagen. Por alguna razón, se tiene que la distancia entre dos puntos es \[ \dst(x, x') ≝ \inf_{γ\, curva} \mop{long} γ \], que es una distancia que nos da la misma topología que teníamos antes\footnote{La topología de la variedad es la topología de subespacio inducida por el espacio ambiente.}. Esto será lo que nos dé una distancia riemanniana.

Para demostrar que es una distancia lo más difícil es ver que si la distancia es cero entonces los dos puntos son el mismo.

La métrica riemanniana también nos da una noción de ángulo. No sé cómo lo hace pero lo hace, parece ser.

Otra cosa que querríamos hacer es definir una isometría.

\begin{defn}[Isometría] Dadas dos variedades $X, X'$ y un difeomorfismo $\appl{Φ}{X}{X'}$, se dice que Φ es isometría si y sólo si para todo $p∈X$ y para todo par $D_p, D_p'$ de vectores tangentes se tiene que \[ \pesc{D_p, D_p'} = \pesc{Φ_{*,p}(D), Φ_{*,p}(D')} \]
\end{defn}

Dos variedades diferenciales isomorfas son la misma, aunque no tienen por qué tener la misma métrica riemanniana. En cambio, dos variedades isométricas sí que tienen la misma geometría riemanniana.

Las isometrías de una variedad forman un grupo y, en fin.

% TODO: Esto es una proposición.

Un último comentario: si hay particiones de la unidad para una variedad diferenciable compacta $X$ entonces hay una métrica. Tenemos la partición $(U_i, ρ_i)$. En cada abierto $U_i$ la métrica se puede definir como \[ \pesc{\dpa{}{x_j}, \dpa{}{x_k}}_i = δ_ij \], tomando esas parciales como los elementos de la base del tangente. Así, podemos combinarlo todo y decir que, dados dos vectores $D,D'$, su producto escalar es\footnote{No estoy muy seguro de esto porque lo ha puesto sin $D,D'$, pero a mí me gustan las ecuaciones sin vacíos.} \[ \pesc{D, D'} = \sum_i ρ_i \pesc{D, D'}_i \]

La cuestión es que puede haber muchas.

\section{Ecuaciones de estructura}

Nosequé de un libro del hijo de Cartan que se llama formas diferenciales y que está bien explicado.

La idea es construir ciertas formas diferenciales que son más o menos canónicaas y, usando esas formas diferenciales y las operaciones estándar construir nuevas formas y se ve que así aparecen muchas formas y llega un momento en el que no pueden ser indepentientes, hay tantas que tiene que haber relaciones entre ellas. Las relaciones entre las formas vendrán dadas por coeficientes que darán invariantes de la variedad, como curvatura o algo.

La idea va a ser coger formas, y aplicarles operaciones hasta que aparezcan estas relaciones porque no queda otra.

Lo primero que hacemos es definir una transformación afín $T$, que es un vector $M$ (una traslación) y una parte lineal $L$. Por notación, llamaremos \[ e_i ≝ L((0,\dotsc, 0, 1, 0, \dotsc, 0))\] donde el $1$ está en la posición $i$-ésima, y además \[ M ≝ T((0,\dotsc, 0))\], con $\det(e_1, \dotsc, e_n)$. Con esto diremos que tenemos una referencia afín dada por $(M, e_1, \dotsc, e_n)$. El conjunto de todas las referencias afines será $\mathcal{R} ⊆ ℝ^n \underbrace{× \dotsb ×}_{n+1} ℝ^n ⊂ ℝ^{(n+1)n}$. Es un abierto por alguna razón. % Creo que porque es todo el espacio.

Un nosequé móvil es un $S ⊂ \mathcal{R}$ es una referencia parametrizada de los puntos de $S$.En particular, uno puede parametrizar las referencias por el mismo punto que estamos... No sé ni cómo escribirlo.

De aquí viene el nombre: el método de la referencia móvil consiste en usar referencias móviles\footnote{Capitán obvio al rescate.} que varían de punto a punto.

Podemos considerar funciones $\mathcal{R}\longmapsto ℝ^n$, donde las funciones serán $M, e_1, \dotsc, e_n$, que a cada elemento de $\mathcal{R}$ le manda un elemento de la referencia. Por ejemplo, \begin{align*}
\appl{M}{\mathcal{R}&}{ℝ^n} \\
(M, e_1, \dotsc, e_n) &\longmapsto M
\end{align*}

Aquí por alguna razón se puede derivar $M$ con la diferencial exterior. Para funciones a valores vectoriales derivar dos veces da cero porque se hace componente a componenente \[ \dif(\dif M) = 0\]

Entonces ahora empieza a aparece ya estas uno-formas y dos-formas que son las que van a dar lugar a las ecuaciones de estructura. Yo considero $\dif M$, que se obtiene derivando las componentes de $M$. Y esto, va a ocurrir digamos en una referencia $r$ y se va a aplicar a un vector tangente $ξ ∈ ℝ^{n+1}$, luego \[ (\dif M)_r\, ξ = \sum_i ω_{i,r} (ξ) (e_i)_r\] porque estamos escribiendo un vector en la base y un vector son coordenadas por coeficientes numéricos, los $ω_{i,r}(ξ)$. Al final no se van a escribir casi nunca, no hace falta, y esto al final se va a escribir como  \[ \dif M = \sum_i ω_i e_i \] y sobreentendemos los puntos y los vectores.

Los coeficientes actúan sobre vectores tangentes y dan números, luego los $ω_i$ son 1-formas en $\mathcal{R}$. Los mismos cálculos se hacen para las otras funciones $e_j$, y tenemos que \( \dif e_j = \sum_k υ_{j,k} e_k \label{eqDerivadaE} \), donde los $ω_{j,k}$ son igualmente $1$-formas.

Así, tenemos $n^2 + n$ 1-formas que nos vienen de gratis. Son 1-formas normales. Lo que hacemos es derivarlas\footnote{Espíritu Guijarro: no sé qué hacer, luego derivo.} y ver qué pasa.

Lo que sabemos es que $\dif(\dif M) = 0$ y que $\dif(\dif e_i) = 0$. Por otro lado, tenemos que $\dif M = \sum ω_i e_i$. Ahora tenemos que volver a derivar, así que calculamos: \[ \dif(ω_i e_i) = \dif ω_i e_i - ω_i \y \dif e_i \]

Ahora, la derivada de $\dif e_i$ la tenemos en \eqref{eqDerivadaE}, luego sustituimos y nos queda que eso es igual a \[ \dif ω_i e_i - ω_i \y \left(\sum_k ω_{i,k} e_k\right) \]. Sumando todas estas cosas nos queda que \( 0 = \dif(\dif M) = \sum_i \dif ω_i e_i - \sum_i ω_i \y \sum_k ω_{i,k} e_k \label{eqRiemann2} \)

¿Qué quiero hacer con esto? Ya hemos escrito como algo que tiene formas diferenciales por los elementos de la base, luego vamos a usar que los $e_k$ son independientes, por lo que nos quedaría lo siguiente \[ 0 = \sum_k \left(\dif ω_k -\sum_i ω_i \y ω_{i,k} \right) e_k \], de donde se concluye ya que los $e_k$ son base que \[ \dif ω_k = \sum_i ω_i \y ω_k \]

Lo que se ve es que cuando se deriva, los coeficientes de $M$ que es lalalalalala traslación en la referencia en la que estamos, se obtienen unas $ω_i$ que al volverlas a derivar salen cosas que siguen siendo dependientes de la $ω_i$. Ahora, el mismo cálculo se puede hacer para las otras $e_i$ y lo que queda es que \[ \dif ω_{i,j} = \sum_k ω_{i,k} \y ω_{k,j} \] haciendo el mismo cálculo.

A estas ecuaciones se le llama las ecuaciones de estructura. A las $ω_{i,j}$ se les llama la forma de conexión.

Queda una cosa más, que es que usando las referencias afines se puede hacer geometría afín, pero normalmente querremos hacer geometría euclídea (Riemanniana). Luego lo que habrá que suponer es que la referencia sea ortonormal, luego querremos obligar a que $\pesc{e_i, e_k} = δ_{ij}$.

Las referencias ortonormales $\mathcal{O} \overset{i}{⊂} \mathcal{R}$ no es un abierto pero es una subvariedad de $\mathcal{R}$. En este conjunto se pueden definir $\gor{ω}_i = i^* ω_i$ y $\gor{ω}_ij = i^* ω_{ij}$ usando el cambio de variable $i$ de la inmersión (creo), y van a definir las mismas ecuaciones de estructura. Y entonces es lo mismo hacer la derivada y luego la imagen y nosequé cosas. Cuando estamos en geometría riemanniana les quitamos la barra porque es un abuso razonable.

La cuestión es que además de estas ecuaciones de estructura hay otra ecuación más que viene de aquí (no sé qué es aquí) ya que $\dif (e_i · e_j) = 0$ por ser base ortonormal, pero por Leibiniz \[ \dif (e_i · e_j) = \dif e_i e_j + e_i \dif e_j \] y sustituyendo nos va a quedar que $ω_{ij} = -ω_{ji}$. Entonces en el caso de referencias ortonormales se añade otra ecuación a las ecuaciones de estructura. Esto sguiere que con estas ω se va a poder hacer una matriz antisimétrica de 1-formas que se le llama Ω, matriz de conexión.

Con esto lo único que se ha hecho es la base digamos en $ℝ^n$ de la geometría que se va a hacer. La base viene luego de considerar variedades de $ℝ^n$. Lo que veremos el próximo día es qué ocurre cuando en $ℝ^n$ hay una subvariedad. De hecho, consideraremos el caso de $n=3$ con una subvariedad como superficie.

\begin{theorem}[Lema\IS de Cartan I] (doCarmo, p. 80) Sea $E$ un espacio vectorial de dimensión $n$. Sean $ω_1, \dotsc, ω_r$ aplicaciones lineales de $E$ en $ℝ$ linealmente independientes\footnote{Es decir, linealmente independientes como covectores del espacio dual $E^*$.}. Supongamos que existen otras formas lineales de $E$ en $ℝ$ $θ_1, \dotsc, θ_r$ tal que \[ Λ^2 E^* \ni \sum ω_i \y θ_i = 0\]

Entonces, existen $a_{ij}$ tales que \[ θ_i = \sum_j a_{ij} ω_j \] y con $a_{ij} = a_{ji}$.\label{thmCartanI}
\end{theorem}

\begin{proof} Dado que las $ω_i$ que tenemos son independientes, podemos completarlas a una base $ω_1, \dotsc, ω_r, \dotsc, ω_n$ del espacio vectorial. Por lo tanto, podremos reescribir las $θ_i$ como \[ θ_i = \sum_{j≤r} a_{ij} ω_j + \sum_{l>r} b_{il} ω_l \]

Queremos buscar que los $b_{il}$ son todos cero para que se cumpla el teorema. Usamos que $\sum ω_i \y θ_i = 0$, y operamos: \[
0 = \sum_{i ≤ r} ω_i \y θ_i = \sum_{\substack{i ≤ r \\ j ≤ r}} a_{ij} · ω_i \y ω_j + \sum_{\substack{i ≤ r \\ l > r}} b_{il} · ω_i \y ω_l \eqreason{Agrupamos las formas usando la antisimetría del producto exterior.} \sum_{i < j} (a_{ij} - a_{ji}) ω_i \y ω_j + \sum b_{il} ω_i \y ω_l
\]

Ahora usamos que el conjunto de los $ω_i \y ω_j, ω_i \y ω_l$ es linealmente independiente y concluimos: como todo eso tiene que ser cero, por un lado $a_{ij} = a_{ji}$ y $b_{il} = 0$.
\end{proof}

\begin{theorem}[Lema\IS de Cartan II] Sea un abierto $U ⊂ ℝ^n$ y $ω_1, \dotsc, ω_n$ 1-formas linealmente independientes. Supongamos que existen 1-formas $ω_{ij}$ con $1≤i,j≤n$ tales que \[ ω_{ij} = - ω_{ji},\qquad \dif ω_j = \sum ω_k \y ω_{kj} \]

Entonces, las $ω_{ij}$ son únicas. \label{thmCartanII}
\end{theorem}

\begin{proof} Vamos a demostrarlo por reducción al absurdo, suponiendo que existen otras $\gor{ω}_{ij}$ que cumplen las condiciones del teorema. En concreto, dado que la diferencial de $\dif ω_j$ tiene que ser la misma, si restamos vamos a tener que  \[ \dif ω_j - \difω_j = \sum_k ω_k \y (\gor{ω}_{kj} - ω_{kj}) = 0\]

Usando el lema anterior \eqref{thmCartanI}, tenemos una suma de productos diferenciales que es nula, luego sabemos que existen unos coeficientes simétricos, que llamaremos $B_{ki}^j$, tales que \[ \gor{ω}_{kj} - ω_{kj} = \sum_i B_{ki}^j ω_i \] y \( B_{ki}^j = B_{ik}^j \label{eqCartanII_1}\). Querremos demostrar que esos coeficientes son 0.

El truco es usar la antisimetría del producto de formas, y entonces \[ \gor{ω}_{kj} - ω_{kj} = -(\gor{ω}_{jk} - ω_{jk}) \]

Por otra parte, cambiando el signo al sumatorio tendríamos que \[-(\gor{ω}_{jk} - ω_{jk}) = - \sum B_{ji}^k ω_i \]

De ahí sacamos que \( B_{ji}^k = - B_{ki}^j \label{eqCartanII_2} \). Ahora operamos rotando los índices para conseguir demostrar que un coeficiente es igual a menos él mismo, y que por lo tanto sólo puede ser cero:
\[ B_{ji}^k \eqexpl{\eqref{eqCartanII_2}} -B_{ki}^j \eqexpl{\eqref{eqCartanII_1}} -B_{ik}^j \eqexpl{\eqref{eqCartanII_2}} B_{jk}^i \eqexpl{\eqref{eqCartanII_1}} B_{kj}^i \eqexpl{\eqref{eqCartanII_2}} -B_{ij}^k \eqexpl{\eqref{eqCartanII_1}} -B_{ji}^k  \]

Por lo tanto, nos queda que $\gor{ω}_{kj} = ω_{kj}$ y entonces las formas son únicas.
\end{proof}

Ahora vamos a aplicar esto a nosequé. Superficies, creo. Recordamos el conjunto de las referencias ortonormales \[ \mathcal{O} = \set{ r = (M, e_1, \dotsc, e_n) \tq e_i · e_j = δ_{ij} } \]

%% Dibujo del marco de referencia aquí

Entonces, cuando en $ℝ^3$ tenemos una superficie $S$, en cada punto vamos a tener el vector $M$, que nos dice qué punto es, y luego tendremos un montón de referencias. Nosotros estaremos interesados en las referencias adaptadas.

Suponemos que tenemos la superficie $S \overset{i}{⊂} ℝ^3$, donde $i$ es una inmersión compatible con la topología. Entonces, para cualquier $p∈S$, tendremos $U,V$ entornos de $p$ en $S$ y $ℝ^3$ respectivamente tales que $V∩S = U$.

En cada punto de la superficie vamos a tener un punto y un espacio tangente $\tgs_p S$. En cada uno de esos puntos elegiremos una referencia ortonormal que esté adaptada. ¿Qué quiere decir eso?

Para elegir una referencia adaptada, elegiremos un campo normal\footnote{Módulo 1, con el producto escalar inducido por el de $ℝ^3$.} $e_1 ∈ \tgs_p S$ que no se anule, que será el primer vector de la referencia. Lo que tiene sentido ahora es tomar el ortogonal a $e_1$. La cuestión es que hay dos ortogonales. Para elegir entre ellos, lo que haremos será usar la orientación inducida del plano. Así, $e_2$ será unitario, perpendicular a $e_1$ y además, para la orientación inducida $Ψ_{\tgs_p S}(e_1, e_2) > 0$ donde $Ψ_{\tgs_p S}$ es la aplicación bilineal inducida de $ℝ^3$\footnote{No sé qué es esto.}. Por último, elegiremos un último vector ortogonal a esos tres tal que \[ Ψ_{ℝ^3} (e_1, e_2, e_3) > 0\]

A esto le llamaremos una referencia adaptada. Esta referencia definirá una función $\appl{σ}{U⊂S}{\mathcal{O}(U)}$ donde $\mathcal{O}(U) = \set{r∈\mathcal{O} \tq M ∈ U}$. La aplicación inversa será la proyección $π$, tal que $π○σ = I_U$.

La σ asocia una referencia a cada punto de $U$. Se puede ver como una referencia móvil porque cambia según el punto.

Ahora, en $\mathcal{O}(U)$ teníamos 1-formas $ω_1, ω_2, ω_3$ y 1-formas $ω_{12}, ω_{13}, ω_{23}$. Uno puede definir entonces sus antiimágenes por $σ$, y veremos que esas formas $σ^*(ω_i), σ^*(ω_{ij})$ cumplen las mismas ecuaciones de estructura que habíamos visto antes, dado que el pullback es compatible con el producto exterior y la diferencial. Por abuso de notación, muchas veces se omitirá el $σ^*$ y se hablará simplemente de las $ω_i, ω_{ij}$: el contexto nos dirá de cuáles estamos hablando.

Nos damos cuenta de que en una superficie (dimensión 2), las tres 1-formas que tenemos no son independientes. Tiene que haber una relación de dependencia. En la referencia adaptada, esta relación es bastante tonta: $ω_3 = 0$. Lo interesante es lo que se deduce de esto: si $ω_3$ es $0$, entonces cuando escribimos las ecuaciones de estructura tendremos que, como $\dif ω_3 = 0$, ocurrirá que \[ \dif ω_3 = ω_1 \y ω_{13} + ω_2 \y ω_{23} = \]. Usando el lema de Cartan \eqref{thmCartanI}, podremos escribir \begin{align*}
ω_{13} &= h_{11} ω_1 + h_{12} ω_2 \\
ω_{23} &= h_{12} ω_1 + h_{22} ω_2
\end{align*}

Podemos escribir esos $h_{ij}$ como una matriz simétrica \[ \begin{pmatrix} h_{11} & h_{12} \\ h_{12} & h_{22}\end{pmatrix} \], que tendrá como invariantes el determinante y la traza. Entonces, definiremos la \concept[Curvatura\IS Gaussiana]{curvatura gaussiana} \[ K = h_{11} h_{22} - h_{12}^2 \] y la \concept[Curvatura\IS Media]{curvatura media} \[ H = \frac{h_{11} + h_{22}}{2} \]

Lo interesante será ver que no dependen de la referencia que hemos usado, ya que el cambio de referencia es simplemente un cambio de base que mantiene los invariantes de la matriz de antes.

Vamos a ver lo que comentábamos antes de que en superficies $ω_3 = 0$. Usando la notación es fácil: \[ \dif M (e_j) = \sum ω_i(e_j) e_i = e_j \], luego los coeficientes tienen que ser la delta de Kronecker, es decir, $ω_i(e_j) = δ_{ij}$. Así, las formas $ω_i$ que definíamos no son más que las formas duales de los vectores. Y ahora $ω_3$ es cero. Wat.

Hay una manera de definir la curvatura sólo con formas. Recordamos que teníamos  \begin{align*}
ω_{13} &= h_{11} ω_1 + h_{12} ω_2 \\
ω_{23} &= h_{21} ω_1 + h_{22} ω_2
\end{align*} con $h_{12} = h_{21}$. Podemos hacer la siguiente cuenta: \[ \dif ω_{12} \eqreason{Por las ecuaciones de estructura.} ω_{11}\y ω_{12} + ω_{12} \y ω_{22} + ω_{13}\y ω_{32} \eqreason{Dado que estas formas son antisimétricas, $ω_{ii} = 0$.} -K ω_1 \y ω_2 \] donde $K$ es de nuevo la curvatura Gaussiana. Al sólo depender de $ω_1, ω_2$, es intrínseca.

Podemos hacer lo mismo con la curvatura media: \[ ω_{13} \y ω_2 + ω_1 \y ω_{23} = (h_{11} + h_{22}) ω_1 \y ω_2 = 2Hω_1\y ω_2 \]

Vamos a demostrar que la curvatura gaussiana no depende de la inmersión.

\begin{theorem}[Teorema\IS egregio de Gauss] Sea $M_2$ una superficie (variedad de dimensión 2) con dos inmersiones $i, i'$ en $ℝ^3$. Supongamos que las dos inmersiones producen la misma métrica Riemanniana. Sea $K$ la curvatura usando la inmersión $i$ y $K'$ la curvatura usando la inmersión $i'$. Entonces, para todo $p ∈ M_2$, se tiene que $K(p) = K'(p)$, es decir, la curvatura es independiente de la inmersión elegida.
\end{theorem}

\begin{proof} Sean $e_1, e_2$ campos ortonormales en un abierto $U ⊂ M_2$ que contiene a $p$. Consideramos las formas duales $ω_1, ω_2$ de estos campos, que son iguales independientemente de la inmersión elegida, dado que las métricas son la misma.

Usando el lema II de Cartan (\ref{thmCartanII}), tenemos que \begin{align*}
\dif ω_{12} &= \dif ω_{12}' \\
-K ω_1 \y ω_2 &= -K' ω_1' \y ω_2' \\
-K ω_1 \y ω_2 &= -K' ω_1 \y ω_2 \\
-K &= -K'
\end{align*}
\end{proof}

\begin{example}
Vamos a hacer un ejemplo de cálculo de esto, usando la helicoide. Esta variedad está dada de forma paramétrica por \begin{align*}
\appl{f}{ℝ^2&}{ℝ^3} \\
(s,t) & \longmapsto (s \cos t, s \sin t, at)
\end{align*} con $a∈ℝ$.

Lo primero es calcular la referencia ortonormal. Calculamos las derivadas parciales:
\begin{align*}
f_s = \dpa{f}{s} &= (\cos t, \sin t, 0) \\
f_t = \dpa{f}{t} &= (-s \cos t, s \cos t, a)
\end{align*}

Este caso es simple porque $f_s$ y $f_t$ son perpendiculares en $ℝ^3$. De hecho, $f_s$ está normalizado, así que podemos definir fácilmente los vectores $e_1, e_2$ como \begin{align*}
e_1 &≝ f_s \\
e_2 &≝ \frac{f_t}{a^2 + s^2} \\
\end{align*}

Para tener la base ortonormal, nos falta el vector $e_3$. Lo calcularemos usando el producto vectorial, que tiene implícita la orientación de la superficie. Resulta \[ e_3 = e_1 × e_2 = \frac{1}{a^2+s^2} (a \sin t, -a \cos t, s) \]

Ahora hay que escribir las ecuaciones de estructura para esta referencia móvil. $M$ es el punto que estamos considerando, así que tendremos que $M = f(s,t)$. Luego podemos calcular la diferencial como \[ \dif M = \dif f = f_s \dif s + f_t \dif t \]

Tenemos que reescribir esa diferencial en función de la base de la referencia móvil. No es difícil lograrlo viendo las expresiones que habíamos definido para $e_1, e_2$: \[ \dif M = \dif s · e_1 + \sqrt{a^2 + s^2} \dif t · e_2\], de tal forma que las dos formas $ω_1, ω_2$ de las ecuaciones de estructura serán los coeficientes de $e_1, e_2$: \begin{align*}
ω_1 &= \dif s \\
ω_2 &= \sqrt{a^2 + s^2} \dif t
\end{align*}

Sabemos que ahora podremos escribir \[ \dif e_3 = ω_{31} e_1 + ω_{32} e_2 + ω_{33} e_3 \], con $ω_{33} = 0$ por estar en una referencia ortonormal. Como teníamos $e_3$ escrito explícitamente, podemos escribirlo derivando: \begin{multline*} \dif e_3 = \left( -s \frac{1}{\sqrt{s^2 + a^2}} · (a \sin t, - a \cos t, s)+ \frac{1}{\sqrt{s^2 + a^2}} · (0,0,1)\right)\dif s \\ + \frac{1}{\sqrt{s^2 + a^2}} · (a \cos t, a \sin t, 0) \dif t \end{multline*}

Reescribiendo para que el coeficiente de $\dif t$ esté en función de $e_1$ obtenemos $ω_{31}$: \[ \frac{a}{\sqrt{s^2+a^2}} · \frac{ω_2}{\sqrt{s^2 + a^2}} = ω_{31} \] y, haciendo lo mismo con el coeficiente de $\dif s$, nos queda que \[ \frac{a}{a^2 + s^2} ω_1 = ω_{32} \]

Finalmente, podemos escribir ya las curvaturas. La curvatura media es $H = 0$ porque no aparece algo en nosédónde, y la curvatura gaussiana es el determinante de la matriz que sale de $H$ y resulta ser \[ K = - \frac{a^2}{(s^2 + a^2)^2}\]

Las superficies con curvatura media $0$ se llaman \concept[Superficie\IS mínima]{superficies mínimas} y son importantes porque son las que minimizan el área para un borde dado.
\end{example}

\subsection{Formas fundamentales}

Vamos a revisar las formas fundamentales que veíamos en Geometría de Curvas y Superficies. La primera forma fundamental era $I_p(\vv) = \pesc{\vv, \vv}_p$, la métrica en el espacio tangente. Como podemos escribir \[ \vv = ω_1(\vv) e_1 + ω_2 (\vv) \], entonces podemos decir que \[ ω_1 (\vv) · ω_1 (\vv) + ω_2(\vv) · ω_2(\vv) \eqexpl{Not.} (ω_1^2 + ω_2^2)(\vv)\], luego podemos reescribir la primera forma fundamental como \[ I_p(\vv) = (ω_1^2 + ω_2^2) \]

De forma similar, la segunda forma fundamental se escribe como\footnote{Como abuso de notación, aquí no hay producto exterior al multiplicar las formas: $(ω_i ω_j) (\vv) ≝ ω_i(\vv) · ω_j(\vv)$.}  \[ II_p (\vv) ≝ (ω_{13} · ω_1 + ω_{23} ω_2)(\vv) = \left(h_{11}ω_1·ω_1 + h_{12} ω_1ω_2 + h_{21} ω_2 ω_1 + h_{22} ω_2 ω_2 \right)(\vv)\], que se parece sospechosamente a la expresión de la segunda forma fundamental que veíamos en GCS.

\begin{theorem} Sean $S, S'$ dos superficies en $ℝ^3$. Sea $\appl{f}{S}{S'}$ un difeomorfismo que mantiene las dos formas fundamentales. Entonces, existe una isometría $\appl{ρ}{ℝ^3}{ℝ^3}$ global tal que $ρ(S) = S'$.
\end{theorem}

Este teorema nos da una forma de ver cuándo dos superficies son la misma pero colocadas en distinta posición.
