% -*- root: ../GeometriaDiferencial.tex -*-
\chapter{Campos vectoriales y formas diferenciales: Teoremas de Frobenius y Poincaré}

\section{Curvas integrales}

No sé si esto va aquí.

Si tenemos una aplicación $\appl{F}{X}{Y}$, entonces la definición de la diferencial en un punto \[ (DF)_x = \appl{F_{*, x}}{\tgs_x }{\tgs_{F(x)} Y} \] es la misma que para abiertos de $ℝ^n$.

Si $F$ es inyectiva y $D$ es un campo en $X$, entonces sí existe un campo $F_*(D)$ definido no en $Y$ sino en $F(X) ⊂ Y$.

Vamos a ver varios resultados en esto.

\begin{defn}[Curva\IS integral] Sea $\appl{γ}{I}{X}$ una curva donde $I = [-a, a]$ es un intervalo simétrico alrededor del origen, y $D$ un campo en $X$. Decimos que γ es una curva integral de $D$ por el punto $x_0$ si se cumple que $γ(0) = x_0$ y además $γ_*\left(\dpa{}{t}\right) = \restr{D}{γ(I)}$ donde $\dpa{}{t}$ es un campo de vectores que a cada punto le asigna el vector tangente a la curva.
\end{defn}

Supongamos por ejemplo que $D = \sum a_i(x_1, \dotsc, x_n) \dpa{}{x_i}$. Entonces, dada $γ(t) = (x_1(t), \dotsc, x_n(t))$, tendríamos que \[ γ_*\left(\dpa{}{t}\right) = \sum x_i'(t) \dpa{}{x_i}\] ya que recordemos que aplicar la función es sólo permutar los símbolos: \[ γ_* \left(\dpa{}{t}\right) f = \dpa{}{t} (f○γ) \]

Al igualar $D$ con la composición esa tendríamos que, para $i = 1, \dotsc, n$, \[ x_i'(t) = a_i(x_1(t), \dotsc, x_n(t))\]. Todo eso junto es un sistema de ecuaciones diferenciales ordinarias autónomo. Es decir, que el campo da lugar a un sistema de ecuaciones locales, que al resolverlo dan cosas.

Hay una definición y un teorema útiles en geometría. Vamos a verlos.

\begin{defn}[Caja] Dado $x_0$ un punto y $D$ un campo en $X$, se dice que una caja para $D$ en $x_0$ es una tripleta formada por un entorno abierto $U_0$ de $x_0$, un real $a > 0$ (o infinito) y una función $Φ$ que cumplen ciertas condiciones.

$Φ$ debe ser una función $\appl{Φ}{U_0 × (-a, a)}{X}$ diferenciable ($C^∞$). Además $∀x∈U_0$ fijo se obtiene una función $γ_x ≝ \appl{Φ(x, t)}{(-a, a)}{X}$ que define una curva integral en $X$. Dicho de otra forma, la función $Φ_0$ ``pega'' todas las curvas.

La última condición es que, $∀t ∈ (-a, a)$ fijo podemos definir una función $\appl{τ_t}{U_0}{X}$ que por definición es simplemente $τ_t ≝ Φ(x,t)$ y que cumple el ser un difeomorfismo de $U_0$ con su imagen.
\end{defn}

\begin{theorem} Para toda $D$ y $x_0 ∈ X$, existe una caja y además es única.

Además, si $t_1, t_2, t_1 + t_2 ∈ (-a, a)$, entonces $τ_{t_1+t_2} = τ_{t_1} ○ τ_{t_2} = τ_{t_2} ○ τ_{t_1}$.
\end{theorem}

Ese último enunciado del teorema nos da una especie de propiedad de grupo para τ. De hecho, a τ se le llama el \concept[Flujo\IS de un campo]{flujo de un campo}.

Por ejemplo, si vemos el campo de velocidades de un fluido estacionario, en el que la velocidad sólo depende del punto $x$ del fluido, la función $τ_t(x)$ nos da una progresión del líquido, cómo se mueven las partículas a lo largo del tiempo por la curva que pasa por $x$.

Geométricamente, un campo lo que hace es definir un flujo cuando se integra: para cada $t$, se obtiene una función que va de cada punto en el punto imagenpor el flujo, y ese movimiento determina las curvas solución y está determinado por ellas, por supuesto.

En este caso, se dice que el campo es el generador infinitesimal del flujo y el flujo se dice que es el flujo del campo flujo flujo flujo.

Hay dos teoremas importantes sobre los campos.

\begin{theorem} Sea $D$ un campo en $X$ y $x_0$ un punto en $X$ donde el campo no se anula ($D_{x_0} ≠ 0$). Entonces existe un sistema $y_1, \dotsc, y_n$ de coordenadas locales en un entorno de $x_0$ en el cual $D = \dpa{}{y_1}$. \label{thmEnderezamientoCampos}
\end{theorem}

Este teorema lo que está diciendo es que se puede cambiar de carta de forma que en la nueva carta el campo se escribe de esa forma. Es necesario que el campo no sea nulo, porque si no la derivada esa no se anularía.

Esto es importante porque en Geometría Diferencial a veces los problemas se resuelven de manera intrínseca, sin usar coordenadas locales. Pero cuando hagamos los cálculos en coordenadas locales, elegiremos un sistema mejor adaptado al problema para hacer el cálculo como si fuera en abiertos de $ℝ^n$. Lo que dice el teorema es que nos conviene usar las coordenadas locales esas para tener una expresión lo más simple posible.

La idea de la demostración del teorema es la siguiente. Tenemos el campo que no se anula en un punto, y consideramos una hipersuperficie dada por una función $f(x_1, \dotsc, x_n) = K$ constante. La curva solución del sistema que nos daría las nuevas coordenadas será una curva transversal a la superficie.

Entonces, para cada punto cerca de $x_0$, consideramos la curva solución y la cortamos con la hipersuperficie. Como el campo es transversal, todos los vectores hacen cosas y yo ya estoy muy perdido porque hasta el hobre este creo que se está peridiendo. Estoy empezando a escribir cosas sin sentido. Lo que $D = \dpa{}{y_1}$ significa es que $D(y_i)$ es cero si $i ≠ 1$.

Escribir el campo así es lo mismo que encontrar funciones tales que al derivarlas da cero y luego una función $y_1$ tal que vale 1. En definitiva, buscamos funciones que cumplen el sistema
\begin{align*}
D(y_2) = D(y_3) = \dotsb = D(y_n) &= 0 \\
D(y_1) &= 1
\end{align*}

Encontrar las que valen $0$ no es demasiado difícil.

Por definición, cada nosequé le pasa algo y es constante sobre las curvas solución, que implica ese sistema, que es lo que falta y es cierto porque si yo tengo una función en un entorno $x_0$ en el que estamos $f$ restringida a las curvas solución de $D$ es constante en tonces $D(f) = 0$. Esto es lo que me faltaría ver. Fijaros que $f$ restringida a las curvas solución es constante sobre cada curva, no constante entre diferenteces curvas. ¿Por qué es cierto esto? Por la definición de curva solución.

\textit{Guille: No he entendido puto nada de lo que ha dicho ni de lo que ha escrito. Pablo dice que sí pero no se lo cree ni él. Dudo que siquiera el profesor sepa qué puñetas ha dicho.}

\seprule

\begin{defn}[Integral\IS primera] La integral primera es algo que viene en la hoja

\end{defn}

Es fácil de ver que las $y_j$ de ese nuevo sistema de coordenadas son las integrales primeras.

Algo sobre las hojas. Tengo hambre y no voy a copiar mucho.

Habría que demostrar que todo eso es una carta. Habría que calcular el determinante de la matriz jacobiana (el wronskiano) y ver que no se anula en el punto.

\subsubsection{Ejemplos}

Supongamos la recta real $ℝ$ con coordenada $x$, y una partícula sometida a un potencial $U(x)$. Es decir, que la fuerza sobre la partícula es $F = -U'(x)$. Sabemos que $F = ma$, luego \[ -U'(x) = F = m a = m \od[2]{x(t)}{t} = \od{p}{t} \] donde $p = m \od{x}{t} = mv$ es el momento lineal.

Vamos a hacer el cálculo en un plano $xp$, donde $x$ es la coordenada y $p$ es el momento. En estas coordenadas, $\od{x}{t} = \frac{p}{m}$. Es decir, que con este truco hemos pasado a un sistema de ecuaciones de primer orden.

Esto se puede hacer como un campo \[ D = \frac{p}{m}\pd{}{x} - U'(x) \pd{}{p} \] en el plano. Busquemos las integrales primeras del campo: funciones $H$ tales que $D(H) = 0$. Sabemos que las integrales primeras existen localmente en entornos de puntos en los que el campo no se anule.

Para encontrarla, tenemos que resolver la ecuación en derivadas parciales \[ \frac{p}{m} \pd{H}{x} - U'(x) \pd{H}{p} = 0 \]

Para resolver eso escribimos \[ \pd{H}{x} = U'(x) \qquad \pd{H}{p} = \frac{p}{m}\]

De la primera ecuación sacamos que $H(x,p) = U(x) + C(p)$. Derivando esto con respecto a $p$, tenemos que $\pd{H}{p} = C'(p) = \frac{p}{m}$, luego $C(p) = \frac{p^2}{2m}$, de tal forma que \[ H = U(x) + \frac{p^2}{2m} \]

Esta es la ecuación de la energía total del sistema o mecánica, donde $\frac{p^2}{2m}$ es la energía cinética. La energía mecánica se conserva así que eso de ahí es cosntante (y su derivada es cero).

Ahora bien, ¿qué haríamos para resolver el movimiento de la partícula? ¿Qué habría que hacer para continuar? ¿Cómo usamos el hecho de que la energía se mantiene constante?

Queremos conocer no ya la curva en el plano de fases $xp$, sino también su proyección en el eje $x$ en función del tiempo. Lo que hacemos es sustituir en la ecuación de antes $p$ por $m\od{x}{t}$, luego \[ H = U(x) + \frac{1}{2m}\left(m\od{x}{t}\right)^2 = E_0 \] donde $E_0$ es constante. Es decir, hemos pasado una ecuación de primer orden separable, que se reduce a una integral, y despejando y haciendo cosas e integrando se sale.

Otro ejemplo: consideramos un punto $(a,b) ∈ ℝ^2$, y consideramos para cada punto $(a,b)$ la parábola $y = x^2 + (b-a^2)$, que se peude parametrizar por $γ(t) = (t, t^2 + (b-a^2))$. ¿Cómo calculamos el campo cuyas curvas solución son esas? Tenemos que derivar. Es claro que el campo será \[ D_{(x,y)} = (1,2t) \] Resolviendo el sistema $D = \pd{}{x} + 2x \pd{}{y}$ nos queda que $x(t) = t +c_1$ y que $y(t) = t^2 + 2c_1 t + c_2$, es decir, las mismas curvas de las familias de antes con $a = c_1$ y $b =c_2$. El flujo está dado por \[ τ_t(x,y) = (t, t^2 + (y-x^2))\]

Vamos a buscar también enderezar el campo. Consideramos \[ D(H) = 0 = \pd{H}{x} + 2x \pd{H}{y} \] y haciendo lo mismo de antes la solución es $H(x,y) = -x^2 + y$, que está diciendo que las curvas solución están contenidas en parábolas, cosa que ya sabíamos desde el principio.

El resto del ejemplo es demostrar que el determinante de la mariz jacobiana es no nulo en todo punto y entonces las coorenadas locales que enderazan el campo son coordenadas globales en este caso, porque en todo punto la matriz jacobiana tiene detemrinante no nulo y se comprueba de hecho que es una carta loca, lse calcula la inversa, y luego hay un segundo ejemplo que no da tiempoa mirar hoy que es en una sola variable y que nosequé.

El otro ejemplo es el campo dado por \[ D = x^2 \pd{}{x} \] al que le corresponde la ecuación $\od{x}{t} = x^2$, luego $t = \frac{-1}{x} + C$. Despejando, \[ x = \frac{-1}{t-C} \]

Así, cuando $t = 0$, la posición en el instante inicial $x_0 = \frac{1}{C}$ y sustituyendo de vuelta tenemos que \[ x = \frac{x_0}{1-tx_0} \]

El flujo del campo es por lo tanto \[ τ_t(x) = \frac{x}{1-xt} \], que nos da para cada punto $x$ nos va a dar la trayectoria en función del tiempo. Es curioso ver que el flujo no es global porque cuando $t = \frac{1}{x}$ se hace infinito.

Esto quiere decir que hay una cierta limitación si queremos definir el flujo en todo $t$. Es una justificación de la hipótesis que vamos a enunciar ahora, o que ya enunciamos en su momento. No sé.

\begin{defn}[Soporte] Dado un abierto $U ⊂ ℝ^n$ y un campo $D$ en $U$. El soporte del campo se define como \[ \mop{sop}(D) ≝ \adh{\set { p ∈ U \tq D_p ≠ 0}} \] \end{defn}

\begin{theorem} Si $\mop{sop}(D)$ es compacto y está contenido en $D$, entonces el flujo $τ_t$ está definido para todo $t∈ℝ$.
\end{theorem}

\begin{proof}
En la hoja está la idea.
\end{proof}

\section{Corchete de Lie}

Dados dos campos \[ D = \sum a_i \pd{}{x_i} \qquad \quad D' = \sum b_j \pd{}{x_j} \] y una función $f$, podemos considerar el siguiente cálculo: \[ D(D'(f)) = D\left(\sum b_j \dpa{f}{x_j} \right) = \sum_{i,j} a_i \pd{b_j}{x_i} \pd{f}{x_j} + \sum_{i,j} a_i b_j \mder{f}{2}{x_1}{}{x_j}{} \]

Así, queda claro que $D(D')$ no es un campo. Ahora bien, hay una cierta simetría que me he perdido y ahora hay cosas con corchetes y voy a escribir pero no sé nada de esto:

\[ [D, D'] ≝ D○D' - D'○D = \sum_j\left(\sum_i a_i \dpa{b_j}{x_i} - b_i \dpa{a_j}{x_i}\right) \dpa{}{x_j}\]

Por alguna razón esto simplifica mucho las cosas. Tiene ropiedades interesantes:

\begin{enumerate}
\item $[D, D'] = - [D', D]$.
\item Bilinealidad; $[aD + bD', D''] = a[D, D''] + b[D', D'']$.
\item \concept{Identidad\IS de Jacobi}: la suma de permutación cíclica es cero: $[[D, D'], D''] + [[D',D''], D] + [[D'',D], D'] = 0$.
\item $[fD, gD'] = fg[D, D'] + fD(g)D' - gD'(f) D$
\item $[[D, D'], D''] = [DD' - D'D, D''] = DD'D'' -D''DD' - D'DD'' + D''D'D$.
\item $[fD, yD'] h = fD(gD'(h)) - gD'(fD(h))$
\end{enumerate}

Ahora bien, ¿para qué sirve esto? Vamos a verlo empezando con subvariedades.

\section{Subvariedades}

Cosas de la hoja, teorema del rango constante.

Esto va en la misma línea que el teorema de enderezamiento de campos (\ref{thmEnderezamientoCampos}): nos permite cambiar las coordenadas para llegar a un sistema más fácil de manejar.

Ejemplo de la hoja.

\begin{wrapfigure}{r}{0.4\textwidth}
\centering
\inputtikz{III_Pez}
\caption{Un pez. Glugluglu.}
\label{figPez}
\end{wrapfigure}

Luego lo de inmersión. Cuidado porque lo que tiene que ser inyectivo es la diferencial, no la función. Por ejemplo, si consideramos la curva de ecuación $y^2 - x^2 + x^3 = 0$ (imagen \ref{figPez}) parametrizada por \[ F(λ) = (1-λ^2, λ(1-λ^2)) \], su diferencial es inyectiva $∀λ$, a pesar de que la función en sí no es inyectiva (el $(0,0)$, el punto naranja se cruza dos veces).


\begin{wrapfigure}{l}{0.4\textwidth}
\centering
\inputtikz{III_BucleCero}
\caption{La función no llega al cero.}
\label{figBucleCero}
\end{wrapfigure}

Un ejemplo de función que no es compatible con la topología es el looping que cruza una vez por el cero y luego no llega (imagen \ref{figBucleCero}).

Luego habla de la submersión y demostraciones.

\section{Teorema de Frobenius}

Luego el teorema de frobenius. Nosequé. Algo de que los campos generan el espacio tangente.

Si los vectores son tangentes a la variedad, el corchete de Lie debe depender sólo de esos dos ($[D_1, D_2] = fD_1 + gD_2$ para algunas funciones $f,g$) por nosequé. Que el corchete no apunte fuera\footnote{Esto empieza a ser ridículo.}. Algo de todo esto es condición necesaria y el teorema de Frobenius dice que es también suficiente. La condición está en la hoja, creo.

\subsection{Algunos ejemplos}

Un ejemplo. Empezamos con una función en $ℝ^3$ dada por \[ F(x,y,z) = x^2 + y^2 + z^2\], con diferencial exterior \[ \dif F = 2x \dif x + 2y \dif y + 2x \dif z \]. Buscamos campos $D = a_1 \dpa{}{x} + a_2 \dpa{}{y} + a_3 \dpa{}{z}$ que se anulen con $\dif F$: \[ 0 = (\dif F) D = a_1 2x + a_2 2y + a_3 2z\]

Las soluciones son un espacio algo de dimensión dos, así que tomamos \begin{gather*} D_1 = y \dpa{}{x} - x \dpa{}{y} \\ D_2 = z \dpa{}{x} - x \dpa{}{y} \end{gather*}. El ejercicio sería ver que cuando $x≠0$ son linealmente independientes, y como ejercicio calcular $[D_1, D_2]$ y comprobar que $Δ ≝ \gen{D_1, D_2}$ es involutivo, y entonces algo que no sé qué pone. Y luego no sé por qué salen las esferas de centro el origen. Y el espacio son las superficies integrales de la distribución.

Vamos a calcular entonces $[D_1, D_2]:$ \[ \lie{D_1, D_2} = \lie{y \dpa{}{x}, z \dpa{}{x}} - \lie{y\dpa{}{x}, x\dpa{}{z}} - \lie{x\dpa{}{y}, z \dpa{}{x}} + \lie{x\dpa{}{y}, x \dpa{}{z}} \] simplemente usando la propiedad distributiva. Ahora usamos que $[fD, gD'] = fg[D, D'] + fD(g)D' - gD'(f) D$ y entonces pasan cosas que ha borrado.

De alguna parte ha salido la 1-forma \[ ω = 2 x \dif x + 2y \dif y + 2z \dif z\], que es ortogonal a los dos campos: $ω(D_1) = ω(D_2) = 0$\footnote{Esto ocurre porque cambias $\dif x$ por la coordenada de $\dpa{}{x}$ en el campo, y análogamente con el resto de coordenadas. Entonces $ω(D_1) = 2x · y + 2y · (- x) +  2z · 0 = 0$. Matemáticamente parece lo menos formal del mundo pero funciona, que ya es algo.}. Entonces, ω restringida a la superficie $S$ es nula.

Así, otra forma de ver el problema de Frobenius es, dadas formas diferenciales perpendiculares (incidentes) al campo, cuáles son nulas. El sistema de ecuaciones sería, en términos de formas \[ \restr{ω}{S} = 0 \]

\paragraph{Ejercicio 2} Tenemos un campo $D = x \dpa{}{x} + y \dpa{}{y}$. Este campo está definido en todo el plano pero no cumple el teorema de enderezamiento en $D_{(0,0)} = 0$. Lo que nos dicen es que si tenemos una función $\appl{H}{ℝ^2}{ℝ}$ entonces $D(H) = 0$ y $H$ es constante.

¿Por qué es constante?\footnote{El hombre este se queda en silencio y nos mira como si supiésemos de qué nos está hablando.} Lo primero que hay que hacer es ver cómo es el campo. En cada punto $(x,y)$ es el vector $(x,y)$. Así, las rectas solución son las rectas que pasan por el origen $y = λ x$ (ver figura \ref{imgCampoRadial})

\begin{figure}[hbtp]
\centering
\inputtikz{III_CampoRadial}
\caption{En cada punto $\vx$, el campo es el mismo vector $\vx$. Las rectas solución son las que son tangentes a los vectores del campo, es decir, rectas que pasan por el origen.}
\label{imgCampoRadial}
\end{figure}

$H$ debe ser constante por cada curva solución, ya que $D(H)$ es cero. Y para cada recta tiene que ser la misma constante, porque todas las rectas comparten el origen. Si no se anulase, el teorema de enderezamiento de campos diría que tiene que haber una integral primera. Así, nos quedaría $H_1(x,y) = \frac{y}{x}$.

\paragraph{Otro ejercicio} Tenemos \[ D = \dpa{}{x} + \sin(x) \dpa{}{y} \] y nos piden algo tal que $D(f) = 1$ y $D(g) = 0$. La primera es fácil: $f(x,y) = x$. La otra tiene el sistema de ecuaciones \begin{align*} \od{x}{t} &= 1 \\ \od{y}{t} &= \sin x \end{align*}, lo que nos da como resultado \begin{gather*}x = t + c \\ \dif y = \sin(t+c) \dif t \implies y(t) = - \cos(t + c) + c' \end{gather*}

Esto quiere decir que el grupo uniparamétrico de automorfismos que manda cada punto a vete tú a saber dónde es \[ τ_t(x,y) = \left(t+x, \cos x + y - \cos(t+x)\right)\]

Buscamos ahora la integral primera: dado un punto en el que el campo no es cero, cogemos una hipersuperficie que lo contiene (por ejemplo, $x=0$) y luego para cada punto $(x,y)$ vemos la intersección de la curva integral que pasa por él y que interseca con $x = 0$. Nos interesará entonces la segunda coordenada y por construcción sale nosequé. De verdad que no me he distraído y no he sido capaz de seguirle.

La función que sale es $g(x,y) = y + \cos x - 1$. Ahora, el campo en el sistema con coordenadas $f \equiv x_1,g \equiv x_2$, se escribe como \[ D = \dpa{}{x_1} \]

Es fácil ver que $f,g$ son un sistema de coordenadas locales en todo punto del plano. Es decir, que el determinante del jacobiano del cambio es distinto de cero en todo punto. Falta el paso de calcular usando la solución el punto de intersección y su coordenada $y$ que es la que pondremos en la $g$.
