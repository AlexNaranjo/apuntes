\documentclass{apuntes}

\title{Geometría Diferencial}
\author{Guillermo Julián Moreno}
\date{14/15 C2}

% Paquetes adicionales

% --------------------

\begin{document}
\pagestyle{plain}
\maketitle

\tableofcontents
\newpage

\chapter{Formas diferenciales en abiertos de $ℝ^n$}

\section{Introducción y motivación}

Vamos a hacer un pequeño repaso de formas diferenciales. A lo largo de esta sección vamos a considerar la función $\appl{f}{U⊆ℝ^n}{ℝ}$, con $f∈C^∞(U)$.

Las formas diferenciales parten del concepto de derivadas y la búsqueda de ``tangentes'' a un punto. En esta línea, recordamos lo que es la matriz diferencial de una función que, por así decirlo, nos da la tasa de variación de una función cuando nos movemos a lo largo de un vector.

\begin{defn}[Diferencial\IS de una función] Dada $\appl{f}{U⊆ℝ^n}{ℝ}$, con $f∈C^∞(U)$ y $x_0 ∈ U$, entonces el diferencial en un punto es $\Dif f(x_0) = f'(x_0) = (\Dif f)_{x_0}$, que se considera como una aplicación lineal
\begin{align*}
\appl{\Dif f}{ℝ^n&}{ℝ} \\
(λ_1, \dotsc, λ_n)^T &\longmapsto \sum \dpa{f}{x_i} (x_0) λ_i
\end{align*}
\label{defDiferencialD}
\end{defn}

Hay que tener en cuenta que el espacio de partida de $\Dif f$, $ℝ^n$, no es el mismo espacio de partida de $f$. Diremos que el $ℝ^n$ de $\Dif f$ es el espacio tangente de $f$ en $x_0$ (denotado por $Π_{x_0} U = ℝ^n$).

Por ser una aplicación lineal de $ℝ^n$ en $ℝ$, el diferencial está en el espacio dual de $ℝ^n$ o, más concretamente, del espacio tangente de $U$: \[ \Dif f(x_0) ∈ \left(Π_{x_0} U\right)^* ≝ Π_{x_0}^* U\] donde $Π_{x_0}^* U$ será el espacio cotangente.

El espacio tangente se identifica con su dual no de forma única: depende de la elección de la base. Es decir, son isomorfos pero no son canónicamente isomorfos, ya que hay muchos isomorfismos y no hay uno privilegiado.

Para los que no recuerden cláramente qué es el espacio dual:

\begin{defn}[Espacio dual]
Dado cualquier espacio vectorial V sobre un cierto cuerpo F, definimos el \textbf{espacio dual V*} como el conjunto de todas las funcionales lineales en F, es decir, transformaciones lineales en V a valores escalares (en este contexto, un ``escalar'' es un miembro del cuerpo-base F). El propio V* se convierte en un espacio vectorial sobre F bajo la definiciones habituales ('punto a punto') de la adición y de la multiplicación escalar.
\end{defn}

Como el diferencial está en el espacio dual, sus elementos serán llamados \concept[Covector]{covectores}. Esto nos permitirá evitar líos de notación al considerar vectores en el espacio y por otro lado vectores en el espacio tangente.

Por otra parte, queremos introducir otra notación, la de las formas diferenciales, a partir de la diferencial exterior.

\begin{defn}[Diferencial\IS exterior] Dada $\appl{f}{U⊆ℝ^n}{ℝ}$, con $f∈C^∞(U)$ y $x_0 ∈ U$, entonces la diferencial en un punto es \[ (\dif f)_{x_0} = \sum \dpa{f}{x_i} (\dif x_i)_{x_0} \]. En general, la diferencial será \[ \dif f = \sum \dpa{f}{x_i} \dif x_i \] \label{defDifrenciald}
\end{defn}

En realidad, la única diferencia entre esta definición y la dada en \ref{defDiferencialD} es que cambiamos los $λ_i$ por $\dif x_i$. En ambos casos tenemos, por definición, la diferencial es una 1-forma en el abierto $U$. Sabemos ya que una 1-forma es una expresión \[ ω = f_1 \dif x_1 + \dotsb + f_n \dif x_n \] con $f_i ∈ C^∞(U)$.

Cada $(\dif x_i)_{x_0}$ es un covector de $Π^*_{x_0}$, y de hecho $\set{(\dif x_i)_{x_0}}$ es una base del espacio de covectores.

\paragraph{Deducción de las reglas de cálculo} Vamos a considerar ahora una diferencial \[ \dif h = \dpa{h}{x_1} \dif x_1 + \dotsb + \dpa{h}{x_n} \dif x_n \] y una 1-forma \[ ω = f_1 \dif x_1 + \dotsb + f_n \dif x_n \]

Dado ω, nos preguntamos si existe una función $h$ tal que $\dif h = ω$. Es una propiedad muy deseable, porque nos da de alguna manera unas ciertas garantías de ``operación cerrada'' en formas diferenciales.

Esto es lo mismo que plantearse la resolución del sistema de ecuaciones diferenciales dado por \[ \dpa{h}{x_i} = f_i\quad i=1,2,\dotsc,n \]

Esta cuestión está resuelta por el teorema de Poincaré, que veremos más adelante durante el curso.

A primera vista, parece difícil que haya solución. Si uno elije las $f_i$ de forma aleatoria, tendremos demasiadas ecuaciones y pocas incógnitas. Luego debemos esperar condiciones de integrabilidad: las $f_i$ deben cumplir ciertas posibilidades para que la solución $h$ exista. Habitualmente estas condiciones son necesarias pero no siempre suficientes, y una vez que las encontremos suponemos que se cumplen (si no no hay solución) y veremos si bajo esas condiciones el problema tiene solución.

Dado que queremos que exista $h$, vamos a suponer que efectivamente existe $h$. Entonces va a ocurrir que $\dpa{h}{x_i} = f_i$, y si volvemos a derivar tendremos que $\frac{∂h}{∂x_j∂x_i} = \dpa{f_i}{x_j}$. Si derivamos en orden contrario, como el Teorema de Schwarz nos dice que las derivadas cruzadas son iguales, tendremos que tener \[ \dpa{f_i}{x_j} = \dpa{f_j}{x_i} \]

Hay una manera natural de poner toda esta información. Nosotros tenemos la matriz Hessiana de derivadas parciales segundas dada por \[ H = \left(\frac{∂h}{∂x_j∂x_i}\right)_{ij}\], que es simétrica por el Teorema de Schwarz.

Por otra parte, a partir de la forma diferencial $ω = f_1 \dif x_i + \dotsb + f_n \dif x_n$ podemos obtener la matriz $H_1$ dada por \[ H_1 = \del{\dpa{f_i}{x_j}}_{ij} \].

Habíamos visto que la diferencia entre la matriz diferencial y la diferencial exterior era simplemente de notación, luego parece lógico que $H = H_1$. Luego la condición de integrabilidad es que $H_1$ sea simétrica.

A partir de aquí podemos definir más formalmente la diferencial de una 1-forma

\begin{defn}[Diferencial\IS de una 1-forma] Dada una 1-forma ω, tenemos que \[ \dif ω ≝ H_1^Γ = \frac{H_1 - H^T_1}{2} \] donde $H_1^Γ$ es la parte antisimétrica de la matriz $H_1$.
\end{defn}

Esto nos lleva a poder dar una condición concreta para que exista la función $h$ que comentábamos antes. Como queremos que $H_1$ sea simétrica, la parte antisimétrica deberá ser 0. Podemos enunciar entonces el siguiente lema:

\begin{lemma} Si existe $h$ tal que $\dif h = ω$, entonces $\dif ω = 0$. \end{lemma}

Vamos a ver cómo aplicar todo ese tocho de antes para calcular la diferencial de una 1-forma $ω$ dada por \[ ω = f_1 \dif x_1 + f_2 \dif x_2 \] definida en un abierto $U ⊆ ℝ^2$. En este caso, su matriz $H_1$ será \[ H_1 = \begin{pmatrix} \dpa{f_1}{x_1} & \dpa{f_1}{x_2} \\ \dpa{f_2}{x_1} & \dpa{f_2}{x_2} \end{pmatrix} \], así que su parte antisimétrica será

\[ H_1^Γ = \begin{pmatrix} 0 & \displaystyle\frac{\dpa{f_1}{x_2} - \dpa{f_2}{x_1}}{2} \\ \displaystyle\frac{\dpa{f_2}{x_1}- \dpa{f_1}{x_2}}{2} \end{pmatrix} \]

Y como ese 2 molesta multiplicamos por dos y nos lo quitamos de en medio.

Como al diferenciar estamos derivando y la derivada es lineal, deberíamos esperar que \[ \dif ω = \dif (f_1\dif x_1) + \dif (f_2 \dif x_2)\], luego tenemos que ver cuánto vale $\dif(f_1\dif x_1)$.

Dado que la derivada tiene que seguir cumpliendo la regla de Leibniz, tenemos que tener \[ \dif(f_1\dif x_1) = \dif f_1 \dif x_1 + f_1 \dif \dif x_1 \]

Aquí ya empiezan a pasar cosas importantes de este cálculo de diferenciales. Principalmente que $ \dif \dif x_1 = 0$, ya que la matriz $H$ tiene que ser simétrica.

Ahora tenemos que calcular $ \dif f_1 \dif x_1 $, que será \[ \dif f_1 \dif x_1  = \left(\dpa{f_1}{x_1} \dif x_1 + \dpa{f_1}{x_2} \dif x_2\right)\dif x_1 = \dpa{f_1}{x_1}\dif x_1 \dif x_1 + \dpa{f_1}{x_2} \dif x_2 \dif x_1 \], luego sumando nos queda que

\[ \dif ω = \dpa{f_1}{x_1}\dif x_1 \dif x_1 + \dpa{f_1}{x_2} \dif x_2 \dif x_1  + \dpa{f_2}{x_1} \dif x_1 \dif x_2 + \dpa{f_2}{x_2} \dif x_2 \dif x_2 \]

Comparamos esto con la matriz antisimétrica dada por  \[ 2H_1^Γ = \begin{pmatrix} 0 & \dpa{f_1}{x_2} - \dpa{f_2}{x_1} \\ \dpa{f_2}{x_1}- \dpa{f_1}{x_2} & 0\end{pmatrix} \] vemos que los términos $\dpa{f_1}{x_1}$ y $\dpa{f_2}{x_2}$ no aparecen, luego tiene que ser \[ \dpa{f_1}{x_1} \dif x_1 \dif x_1 = \dpa{f_2}{x_2} \dif x_2 \dif x_2 = 0 \], y por otra parte que el producto de diferenciales tiene que ser anticonmutativo, esto es, que $\dif x_1 \dif x_2 = - \dif x_2 \dif x_1$. Dado que este producto (producto exterior) es distinto al habitual, lo denotaremos como $\dif x_1 \y \dif x_2$.

Es decir, hemos extraído las dos reglas siguientes para el producto de diferenciales o \concept[Producto\IS exterior]{producto exterior}: \begin{align*}
\df{x_i, x_j} &= - \df{x_j, x_i} \\
\df{x_i, x_i} &= 0
\end{align*}

Vamos a extendernos un poco en el significado de la expresión $\df{x_i, x_j}$, y por tanto en el de la forma diferencial. En el fondo, no estamos más que definiendo unos objetos (las formas) y aplicando ciertas reglas razonables para calcular ciertas operaciones. Ciertamente esto es cierto.

\paragraph{¿Es $\dif ω = 0$ condición suficiente?} Tenemos que es una condición necesaria para que exista un $h$ tal que $\dif h = ω$. Ahora bien, ¿es condición suficiente?

Para demostrarlo, lo que vamos a hacer es construir esa función $h$. Es trivial ver que \[ h(x) = \int_γ \dif h \] para un cierto camino $\appl{γ}{I}{U}$. En este caso, aplicando la regla de Barrow

\[ \int_γ \dif h = \int_I \dif(h ○ γ) = h(γ(b)) - h(γ(a)) = h(x) - h(x_0)\] donde $x_0$ es el punto de inicio de $γ$. Es decir, que $h$ está definido salvo constante. Habría que ver, eso sí, que la definición que hemos construído no depende de la elección del camino $γ$ y que, además, se cumple efectivamente que $\dif h = ω$.

Vamos a demostrar que la definición no depende del camino. Tomemos $γ_1, γ_2$ dos caminos distintos que empiezan y acaban en el mismo punto. Consideremos entonces $Γ = γ_1 * γ_2^-$, y sea $D$ la región encerrada por Γ.

Por un lado, tenemos que como $\dif ω = 0$, entonces $0 = \int_D \dif ω$. Por otra parte, por el teorema de Stokes, \[ \int_D \dif ω = \int_Γ ω = \int_{γ_1} ω - \int_{γ_2} ω \implies \int_{γ_1} ω = \int_{γ_2} ω\] luego la integral no depende del camino

Lo único que necesitamos para aplicar Stokes es que $U$ sea simplemente conexo, es decir, que no haya agujeros y que siempre podamos deformar un camino a otro, de tal forma que el borde de $D$ sea Γ.

En el fondo, esto es un reflejo de lo que habíamos visto en cursos anteriores de cálculo con campos de vectores conservativos: cuando eran conservativos ($\grad V = 0$) la integral sobre caminos cerrados era cero y además existía una función potencial.

\subsection{Resumen: reglas del cálculo de formas diferenciales}



\section{Estudio formal de las formas diferenciales}

\subsection{Espacio tangente y cotangente}

Suponemos un abierto $U ⊆ ℝ^n$ y un punto $p ∈ U$.

\begin{defn}[Espacio\IS tangente] Se define el espacio tangente de $U$ en un punto como $Π_{p} U$, un espacio vectorial sobre $ℝ$ de dimensión $n$. Los elementos $D^\vv_{p} ∈ Π_{p} U $ son las derivadas direccionales (en la dirección $\vv$) locales (en el punto $p$), que operan las funciones y dan números.
\end{defn}

Lo primero que nos damos cuenta es de que la derivada direccional es local, sólo definida en un entorno de $p$.

\subsubsection{Anillo de funciones}

Si tenemos dos funciones $\appl{f,g}{U}{ℝ}$ definidas en el abierto $U$ con $f,g ∈ C^∞ (U)$, podemos operar con ellas de forma sencilla:

\begin{align*}
(f+g)(x) &≝ f(x) + g(x)
(f·g)(x) &≝ f(x) · g(x)
\end{align*}

Es decir, que las funciones definidas en el abierto tienen estructura de anillo: todas tienen inverso con la suma. Tenemos una unidad y además es anillo conmutativo. Así, podemos definir el anillo de funciones de un abierto:

\begin{defn}[Anillo\IS de funciones] El anillo de funciones de un abierto $U$ se define como

\[ A(U) = C^∞ (U) = \set{\appl{f}{U}{ℝ},\; f∈C^∞}\]

, que es anillo conmutativo y con unidad.\end{defn}

De hecho, $A(U)$ no es sólo un anillo: también es una \concept{.$\real$-álgebra}: es un anillo conmutativo y además espacio vectorial sobre $ℝ$, cumpliendo la igualdad

\[ (λ· f) · (β·g) = (λ · β) · (f·g) \]

donde $f,g ∈ A(U)$, $λ,β ∈ ℝ$ y teniendo cuidado de usar los productos que correspondan\footnote{$(λ·_{E.V.} f) ·_{A(U)} (β·_{E.V.}g) = (λ ·_{\real} β) \cdot_{E.V.} (f·_{A(U)}g)$.} (el producto de elementos de $ℝ$ no es lo mismo que de elementos del anillo de funciones).

\subsubsection{Gérmenes de funciones en $p$}

La idea de los gérmenes es que sólo nos interesa la función en un entorno del punto. Podemos definir así entonces el germen como

\begin{defn}[Germen\IS de función] Un germen de una función en un punto $p$ es un par $(V,f)$ de un abierto $p ∈ V ⊆ U$ y una función $\appl{f}{V}{ℝ}$ con $f∈C^∞$.
\end{defn}

La razón de buscar esta definición es que para la derivada no necesitamos definir la función en todo $U$, nos basta con sólo un poco, y eso nos facilitará las cosas.

Ahora bien, los gérmenes de funciones en realidad se trabajan con relaciones de equivalencia para simplificar. Diremos que dos gérmenes $(V,f)$, $(W,g)$ están relacionados ($(V,f) \sim (W,g)$) si y sólo si $p∈V∩W$ y además existe un abierto $V' ⊆ V∩W$ que contiene a $p$ y para el cual las restricciones de $f$ y $g$ coinciden, esto es $\restr{f}{V'} = \restr{g}{V'}$.

Tomando los gérmenes módulo esta relación de equivalencia, tendremos igualmente una $\real$-álgebra.

La ventaja de buscar ese abierto más pequeño es que no tenemos que decir exactamente cómo de pequeño es, nos basta simplemente que las funciones sean iguales en un pequeño entorno del punto.

A partir de esto, denotaremos como $A_p$ la $\real$-álgebra de gérmenes de funciones $C^∞$ definidas en un entorno de $p$. Por comodidad, para denotar un germen nos bastará con la función, $f∈A_p$, por ejemplo. También podemos pasar a definir lo que es una derivación:

\begin{defn}[Derivación] Una derivación en $p$ es una aplicación
\begin{align*}
\appl{D}{A_p&}{ℝ} \\
f &\longmapsto D(f)
\end{align*}

Queremos que esta función conserve de alguna forma la noción de derivada en una dirección, así que buscaremos varias propiedades:

\begin{enumerate}
	\item $D$ es lineal.
	\item $D(λ) = 0$, donde $λ$ es una función constante $λ(x) = λ ∈ ℝ$.\footnote{En realidad, esta propiedad es consecuencia de las otras dos pero viene bien tenerla presente.}
	\item Si esto se parece una derivada, además de definir cómo se derivan las sumas\footnote{Por esto forzamos que $D$ sea lineal.} definiremos cómo se derivan los productos, según la regla de Leibniz: \[ D(f·g) = f · D(g) + D(f) · g\]
\end{enumerate}\label{defDerivacion}
\end{defn}

Una vez que hemos emulado las derivadas, podemos definir el espacio tangente:

\begin{defn}[Espacio\IS tangente] Diremos que el espacio tangente a $p$ en $U$ se define como

\[ Π_p U ≝ \set{\appl{D}{A_p}{ℝ}\tq D \text{ derivación }} \]
\end{defn}

Fijémonos que el espacio tangente es un subespacio vectorial del espacio dual, ya que $A^*_p ≝ \set{\appl{D}{A_p}{ℝ} \tq D \text{ lineal }}$. De hecho, tenemos bien definido el producto por escalares y la suma de elementos de la forma habitual.

\paragraph{Motivación de todo esto} Al final, hemos logrado llegar a una definición de espacio tangente sin tener que usar derivadas, una definición puramente algebraica.

Con esta definición algebraica vamos a estudiar el espacio tangente, principalmente su dimensión, y vamos a obtener una base cómoda para hacer cálculos.

\subsubsection{Cálculo de la dimensión del espacio tangente}

Vamos a demostrar que la dimensión del espacio tangente de un abierto de $ℝ^n$ es $n$. Suponemos $p ∈ U ⊆ ℝ^n$, y $p = (p_1, \dotsc, p_n)$. Supongamos también que tenemos una derivación $\appl{D}{A_p}{ℝ}$. Vamos a ver qué podemos obtener de aquí sabiendo las propiedades de la derivación.

La observación fundamental es que en el conjunto de gérmenes $A_p$ tenemos un cierto subconjunto en los que el germen es 0: \[ m_p ≝ \set{f ∈ A_p \tq f(p) = 0}\], que además es un ideal por la propia construcción. Y resulta ser maximal\footnote{Aunque todos nos acordamos de Estructuras Algebraicas, recordamos por si acaso: un ideal es maximal si no hay ningún ideal propio que lo contenga.}, ya que el cociente $\quot{A_p}{m_p}$ es un cuerpo: por construcción, las clases de equivalencia estarán formadas por funciones cuya diferencia sea constante, luego $\quot{A_p}{m_p} = ℝ$, que es un cuerpo.

Aquí no sé qué está haciendo: $D(λ) = 0$. $m_p ⊃ m_p^2$, donde $m_p^2$ es el ideal generado por productos de elementos de $m_p$, esto es, \[ m_p^2 ≝ \set{\sum g_{ij} f_i f_j \tq f_i f_j ∈ m_p }\]

¿Qué ocurre si derivamos $f·g$ con $f,g ∈ m_p$? Por la regla de Leibniz, tenemos que \[ D(f·g) = d· D(g) + g · D(f) \]. Como $f = g = 0$, tenemos que $D(f·g) = 0$. Entonces es claro que cuando aplicamos la derivación a elementos de $m_p^2$ nos sale 0. La conclusión de todo esto es que $\restr{D}{m_p^2} \equiv 0$. Es decir, que la regla de Leibniz nos nosequé algo sobre cosas finitas y espacios enormes. Que eso nos lleva a que $D$ está determinada en realidad por los valores que toma en $\quot{A_p}{m_p^2}$. Es decir, que si denotamos
\begin{align*}
\appl{D}{A_p&}{ℝ} \\
\appl{π}{A_p&}{\quot{A_p}{m_p^2}} \\
\appl{\adh{D}}{\quot{A_p}{m_p^2}&}{ℝ}
\end{align*} nos queda que \[ D(f) = \adh{D}(π(f)) \]

Al final, hay un motivo por el cual este chorro nos dice que algo es finito.

El argumento es que una derivación $\appl{D}{A_p}{ℝ}$ induce una aplicación $\appl{\adh{D}}{\quot{A_p}{m_p^2}}{ℝ}$, que se puede ver de otra forma como $\appl{\adh{D}}{\quot{m_p}{m_p^2}}{ℝ}$ ya que si $f ∈ A_p \setminus m_p$ $D(f) = 0$. Entonces $\adh{D}$ es, por construcción, un elemento del espacio vectorial de $\quot{m_p}{m_p^2}$, y además a partir de $\adh{D}$ podemos reconstruir la $D$ original.

Veamos cómo hacer eso mismo: si tenemos $f∈A_p$ con $f(p) ≠ 0$, entonces consideramos la función $f_1(x) ≝ f(x) - f(p)$. Entonces $f_1(x) ∈ m_p$ por construcción\footnote{Recordamos que $m_p$ son las funciones que se anulan en $p$.}. Es claro que yo debo definir \[ D(f) ≝ \adh{D}\left([f_1]_{m^2_p}\right) \]

Es decir, que sólo con saber lo que vale $\adh{D}$ nos vale ya que simplemente trasladamos las funciones $f$ con valores desconocidos $f ∉ m_p$ a una clase de equivalencia de la cual sí sabemos el valor. Sólo tenemos que ver que efectivamente cumple la propiedades de derivación que habíamos pedido (\ref{defDerivacion}). Es trivial ver que se cumple la linealidad, pero Leibniz es más interesante. Empezamos\footnote{Se ve que por comodidad estamos escribiendo $f(x) \equiv f$.} con

\[ D(f·g) = \adh{D}\left([f·g - f(p) g(p)]_{m_p^2}\right) \]

¿Qué hacemos ahora\footnote{Sugerencia de Guille: llorar y dedicarnos a magisterio.}? Sólo sabemos que $\adh{D}$ es lineal. Podemos fijarnos en que \[ α = (f-f(p))· (g-g(p)) ∈ m_p^2\] por ser producto de funciones de $m_p$, luego $\adh{D}(α) = 0$. Si operamos, nos queda \[ (fg - f(p) g - g(p) f + f(p) g(p) \], y si obtenemos su derivación tenemos que

\[ \adh{D}(α) = \adh{D}(f·g) - f(p) \adh{D}(g) - g(p)\adh{D}(f) + \adh{D}(f(p)· g(p)) \]



\chapter{Variedades diferenciables}

\section{Ejemplos de variedades diferenciables}

\chapter{Campos vectoriales y formas diferenciales: Teoremas de Frobenius y Poincaré}

\chapter{Integración en variedades: teorema de Stokes}

\chapter{Geometría de superficies. Curvatura de Gauss-Bonnet}

\chapter{Introducción a la geometría riemanniana}

%% Apéndices (ejercicios, exámenes)
\appendix

\chapter{Ejercicios}
\input{tex/GeometriaDiferencial_Ejs.tex}
\printindex
\end{document}
