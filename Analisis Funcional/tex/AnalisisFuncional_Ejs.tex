% -*- root: ../AnalisisFuncional.tex -*-
\newcommand{\hard}{\hspace{-3pt}(\dag)\hspace{5pt}}

Los ejercicios marcados con (\dag) están marcados como de ``dificultad especial'' en las hojas.

\section{Hoja 1}


\begin{problem}
\ppart Probar, usando el \nref{thm:CategoriaBaire}, que $I = \set{x ∈ ℝ \tq x ∉ ℚ} ≠ ∅$ y que, de hecho, $I$ es un \nlref{def:ConjuntoGDelta}.

\ppart Probar que $ℝ$ no es numerable.

\ppart Sea $X = ℤ$ con $\dst(x,y) = \abs{x-y}$. Probar que \sdst es un espacio métrico completo y que, sin embargo, es numerable. ¿Por qué no contradice esto al Teorema de Baire?

\solution

\spart

Sabemos que $ℚ$ es numerable, así que podemos enumerar todos los racionales en una serie $q_1, q_2, \dotsc, q_n, \dotsc ∈ ℚ$. Definimos entonces $X_n = ℝ \setminus \set{q_n}$ como una serie de conjuntos abiertos y densos. La intersección de todos ellos son todos los $x ∈ ℝ$ no racionales, que es el conjunto $I$ que buscábamos. Además, por el \nref{thm:CategoriaBaire}, esa intersección es un $G_δ$ denso en $ℝ$, y por eso mismo es no vacío.

\spart

Si $ℝ$ fuese numerable, entonces podríamos enumerarlo: $ℝ \equiv\set{ x_1, x_2, \dotsc, x_n, \dotsc}$. Por otra parte, los conjuntos formados por un único punto son diseminados, por lo que podríamos definir que $ℝ = \bigcup_{n≥1} \set{x_n}$. Sin embargo, esto entraría en contradicción con el Teorema de Baire, que nos dice que no podemos escribir $X$ como una unión numerable de conjuntos diseminados.

\spart

Para que \sdst sea un espacio métrico completo, toda \nlref{def:SucesionCauchy} ha de converger en el espacio. Que una sucesión sea de Cauchy implica que $∀ε> 0$ existe un $N ∈ ℕ$ tal que si $m,n ≥ N$, entonces $\dst(x_m, x_n)$. La cuestión es que, como estamos en $ℤ$, si tomamos\footnote{Cosa que no sé si podemos hacer.} un $ε < 1$, entonces $\dst(x_m, x_n) = 0$ (no podemos tener distancias fraccionarias entre elementos de $ℤ$). Por lo tanto, por ser de Cauchy llega un momento en el que la sucesión se repite constantemente. El límite será entonces es elemento que se repite, que por ser parte de la misma sucesión está en $ℤ$.

Esto no contradice el Teorema de Baire porque en $ℤ$ no hay conjuntos densos, y lo vamos a demostrar. Sea $Y \subsetneq ℤ$ un conjunto cualquiera de $ℤ$, y sea $z ∈ ℤ \setminus Y$. La bola de radio $\sfrac{1}{2}$ centrada en $z$ tiene intersección vacía con $Y$ (no hay ningún entero a distancia $\sfrac{1}{2}$ de $z$), por lo que $Y$ no puede ser denso. \triple

Como no hay conjuntos densos, no puede haber tampoco conjuntos diseminados y por lo tanto sigue cumpliéndose el Teorema de Baire: no podemos escribir $ℤ$ como unión numerable de conjuntos diseminados.
\end{problem}

\begin{problem} \hard Sean $\set{a_n}_n≥1$, $\set{b_n}_{n≥1}$ dos sucesiones de números reales y $a_n$ absolutamente convergente. Probar que:

\ppart Sea $f$ dada por \[ f(x) = \sum_{n≥1} a_n φ(b_n) \] con \[ φ(x) = \begin{cases} [x] & 0 ≤ [x] ≤ \frac{1}{2} \\ 1 - [x] & \frac{1}{2} ≤ [x] ≤ 1 \end{cases} \] siendo $[x]$ la parte decimal de $x$.

Demostrar que $f$ es continua y $f ∈ C_{[0,1]}$. Además, la serie que define a $f(x)$ es absolutamente convergente.

\ppart Sea $a_n = 2^{-n}$, $b_n = 2^n$, y $h_m = ε_n 2^{-m}$ con $ε_m = \pm 1$ para todo $m$. Probar que \[ \frac{f(x + h_m) - f(x)}{h_m} = ε_m \sum_{n=1}^{m-1} 2^{m-n} \left(φ(2^n(x+h_m)) - φ(2^nx)\right)\]

\ppart Si escribimos $x = [x] + \sum_{k>0} α_k 2^{-k}$ con $α_k ∈ \set{0,1}$, entonces \[ φ(2^nx) = φ\left(\sum_{l≥1} α_{n +l} · 2^{-l}\right)\] y además $\sum_{l≥1}α_{n +l} · 2^{-l} ∈ [0,1]$. Del mismo modo, \[ φ(2^n(x+h_m)) = φ(ε_m2^{2-m} + \sum_{l≥1} α_{n +l} · 2^{-l})\] y \[ ε_m2^{n-m} + \sum_{k≥1} α_{n +l} · 2^{-l} = \sum_{l≥1} α_{n +l}' · 2^{-l}\], siendo $α_{n+l}' = α_{n+l} + δ_{m-n, k} ε_m$, con $δ_{i,j}$ la delta de Kronecker\footnote{Esto es, $δ_{i,j} = 1$ si $i = j$, y $0$ si $i ≠ j$.}.

\ppart Tomamos $ε_m = (-1)^{α_m}$. Entonces $α_{n+l}' ∈ {0,1}\;∀l≥1$. Fijemos $m > 1$. Entonces $\sum_{l≥1} α_{n+l} 2^{-l}$ y $\sum_{l≥1}α_{n+l} 2^{-l}$ están ambos en la misma mitad del intervalo $[0,1]$. Usar esto para probar que \[ \frac{f(x+h_m) - f(x)}{h_m} = m-1\]

\ppart Del apartado anterior se sigue que $f(x)$ no es diferenciable en ningún $x ∈ ℝ$. Sin embargo, $f$ no está muy lejos de serlo en el sentido siguiente: si $\abs{h} ≤ 1$, entonces $∃ C ∈ (0,∞)$ independiente de $x$ y $h$ tal que \[ \abs{f(x+h) - f(x)} ≤ C\abs{h} \left(1 + \log \frac{1}{\abs{h}}\right) \]

\textbf{Indicación}: Dado $0 < \abs{h} ≤ 1$, existe un único $k ∈ ℕ$ con $2^{-k-1} < \abs{h} ≤ 2^{-k}$. Entonces estimar $f(x+h) - f(x)$ dividiendo la suma en los términos $n < k$ y $n ≥ k$ y estimando cada suma por separado.

\solution

\spart

\spart

\spart

\spart

\spart

Lo que está diciendo es que el módulo de continuidad de esta función es sólo un poquito peor que Lipschitz.

Para demostrarlo, vemos que existe un único $k ∈ ℕ$ tal que $2^{-k-1} < \abs{h} ≤ 2^{-k}$. En otros términos, $k$ es la parte entera de $\log_2 \frac{1}{\abs{h}}$. Entonces podemos escribir \begin{align*}
\abs{f(x+h) - f(x)} &=
	\abs{\sum_{n≥1} 2^{-n} \left([2^nx + 2^n h] - [2^nx]\right)}\\
&≤ \sum_{n≥1} 2^{-n} \abs{[2^nx + 2^n h] - [2^nx]} \\
&= \underbrace{\sum_{1≤n≤k} 2^{-n} \abs{[2^nx + 2^n h] - [2^nx]}}_A + \underbrace{\sum_{n≥k} 2^{-n} \abs{[2^nx + 2^n h] - [2^nx]}}_B
\end{align*}

Vamos a estimar ambos sumatorios por separado. El más simple es el $B$, que haciendo cuentas sale que $B ≤ 2 \abs{h}$. Para la estimación de $A$, usamos que $\abs{[x] - [y]} ≤ \abs{x-y}$ y haciendo todavía más cuentas nos queda que $A ≤ k\abs{h}$.

\end{problem}

\begin{problem} Sea \sdst un espacio métrico compacto. Probar que es completo. ¿Es cierto el recíproco?
\solution

Sea $\set{x_n}$ una sucesión de Cauchy en $X$, y sea $\set{ε_n}$ otra sucesión que tiende a $0$. Por ser $\set{x_n}$ de Cauchy, para cada $ε_n$ existe un $M_n ∈ ℕ$ tal que si $m,n ≥ M_n$ entonces $\dst(x_m, x_n) < ε_n$. Equivalentemente, para todo $n ∈ N$ tendremos que $x_n ∈ \bola_{ε_n} (x_{M_n}) = B_n$. Entonces, por ser $X$ compacto existe un subrecubrimiento finito $B_{n_i}$ de $\bigcup B_n$.

Por ser un subrecubrimiento, tendremos que a partir de un cierto $n$ suficientemente grande, las bolas $\bola_{ε_n} (x_{M_n})$ que definíamos antes están estrictamente contenidas en él, por lo que el límite debe de estar ahí también. % Me convence esto más bien poco.

\end{problem}

\begin{problem} \hard Sea $\set{f_n}_{n∈ℕ}$ una familia uniformemente acotada de funciones diferenciables en $[a,b]$, un intervalo compacto de $ℝ$, y cuyas derivadas están uniformemente acotadas.

\ppart Probar que cierta subsucesión $\set{f_{n_j}}$ converge uniformemente a una función $f$ Lipschitz. ¿Es cierto que $f$ es necesariamente diferenciable en todo $[a,b]$?

\ppart Probar que si se omite la condición de que la familia sea uniformemente acotada, existe una sucesión de constantes $c_n$ tales que $f_n - c_n$ converge uniformemente a una función $f$ Lipschitz continua.

\solution

\begin{figure}[hbtp]
\centering
\inputtikz{FuncionLipschitz}
\caption{La idea de la función Lipschitz es que siempre hay un doble cono (el verde) que contiene a toda la función.}
\label{fig:FuncionLipschitz}
\end{figure}

Recordamos la definición de función continua Lipschitz (\fref{fig:FuncionLipschitz}):

\begin{defn}[Función\IS Lipschitz continua] Una función $\appl{f}{(X, \dst_X)}{(Y, \dst_Y)}$ entre dos espacios métricos se dice Lipschitz continua si existen una constante $C ∈ ℝ$, $C ≥ 0$ tal que para todos $x_1, x_2 ∈ X$ se tenga que \[ \dst_Y(f(x_1), f(x_2)) ≤ C \dst_X(x_1, x_2) \]
\end{defn}

\spart

Si la familia de funciones está uniformemente acotada, eso significa que tenemos una cota $0 ≤ C_0 < ∞$ tal que $C_0 > f_n\; ∀n∈ℕ$. Igualmente, si sus derivadas están uniformemente acotadas tenemos que $f_n' < C_1$.

El espacio $C_{[a,b]}$ con la norma del supremo, que es donde vive esta sucesión, es un espacio métrico completo, por lo que existe una subsucesión $\set{f_{n_j}}$ convergente uniformemente a $f ∈ C_{[a,b]}$. Vamos a demostrar ahora que esa $f$ es Lipschitz.

Por otra parte, como las $f_n$ tienen derivada acotada por $C_1$, podemos decir que son funciones Lipschitz con cota $C_1$. Vemos ahora qué ocurre con $\abs{f(x) - f(y)}$ para $x,y ∈ [a,b]$. Fijando un $ε>0$, podemos encontrar un $n_0 ∈ ℕ$ tal que si $n > n_0$ entonces $\abs{f_n(x) - f(x)} < ε$ por ser las $f_{n_j}$ convergentes a $f$. Sumando y restando: \begin{align*}
\abs{f(x) - f(y)} &= \abs{f(x) - f_n(x) + f_n(x) - f(y) + f_n(y) - f_n(y)} \\
&≤ \abs{f(x) -f_n(x)} + \abs{f(y) - f_n(y)} + \abs{f_n(x) - f_n(y)} \\
&≤ ε + ε + C_1\abs{x-y}
\end{align*}, por lo que haciendo tender $ε \to 0$ tenemos que $f$ es Lipschitz con la misma cota $C_1$ de las funciones de la sucesión.

Sin embargo, $f$ no tiene por qué ser necesariamente diferenciable: $f$ sólo es Lipschitz pero Lipschitz no implica diferenciable en todo punto. Por ejemplo, $f(x) = \abs{x}$ es continua Lipschitz pero no es derivable en $x = 0$.

\spart

Tomamos $c_n = f_n(a)$, ya que por el teorema del valor medio tenemos que \[ \abs{f_n(x) - c_n} = \abs{f_n(x) - f_n(a)} ≤ C\abs{x-a} ≤ C\abs{b-a} < ∞ \], luego $f_n - c_n$ está equiacotada y estamos en las condiciones del teorema anterior.

\end{problem}

\begin{problem} Construir en $C_{[0,1]}$ un subconjunto acotado de funciones que no sea un \nlref{def:ConjTotalmenteAcotado}.

\solution

Nos basta aplicar \nref{thm:AscoliArzela}. El ejemplo canónico es $f_n(x) = x^n$ para $x ∈ [0,1]$, que converge a $0$ cuando $x ∈ [0,1)$ y $1$ si $x = 1$.

\end{problem}

\begin{problem} Sea \sdst un espacio métrico completo. Demuestra que:

\ppart Si $K ⊂ X$ es cerrado, entonces $K$ es completo.
\ppart Si $K$ es completo y totalmente acotado, entonces es compacto.

\solution

\spart

Fácil: una sucesión de Cauchy en $K$ es convergente en $X$ por ser $X$ completo. Como $K$ es cerrado, el límite de la sucesión ha de estar en $K$, así que $K$ es completo.

\spart

Recordamos el \nref{thm:HeineBorel}, que nos dice que un subconjunto $K ⊂ X$ es compacto si y sólo si es cerrado y totalmente acotado. Partimos de que $K$ es completo y totalmente acotado, por lo que sólo necesitamos probar que además es cerrado.

Ahora bien, esto es sencillo de probar: por ser $K$ completo, podemos tomar una sucesión de Cauchy $\set{x_n} ⊂ K$, que convergerá a un $x ∈ K$. Por lo tanto $K$ es cerrado y podemos demostrar el ejercicio aplicando Heine-Borel.

\end{problem}


\begin{problem} Sea $f ∈ C_c(ℝ)$ una función continua en $ℝ$ y de soporte compacto. Fijamos $η ∈ C_c^1 (ℝ)$ con $\int η = 1$, y sea $η_t(x) = tη(tx)$ para $t > 0$. Demuestra que:

\ppart Sea $f_t ≝ f * η_t$. Entonces $f_t \convs[][t] f$ uniformemente.
\ppart $\norm{f_t'}_∞ ≤ C(1+t)$ con $t > 0$ y $C$ finita e independiente de $t$.

\solution

Este ejercicio es una demostración de un teorema conocido sobre familias de aproximaciones de la identidad \citep[Def. III.11, Teorema III.20]{ApuntesVarReal} que viene bien demostrado en \citep[Teoremas 8.14, 8.15]{folland99}. Vamos a demostrarlo todo formalmente.

\begin{defn}[Familia\IS de aproximaciones de la identidad] \label{def:AproxIdentidad} Una familia de aproximaciones de la identidad es una sucesión $\set{ρ_n}_{n∈ℕ}$ de funciones $\appl{ρ_n}{ℝ^N}{ℝ}$ no negativas, infinitamente derivables, de soporte compacto, con $\sop ρ_n ∈ \bola_{\frac{1}{n}} (0)$ y con norma $\norm{ρ_n}_1 = \int_ℝ ρ_n = 1$.
\end{defn}

Esta definición nos vale igualmente para una familia no numerable. Es fácil ver que $η_t$ son una familia de aproximaciones de la identidad, aunque en este caso miramos el límite $t \to ∞$ en lugar de $t \to 0$. No es un cambio relevante.

Vamos ahora con la demostración.

\spart

Demostramos primero que las $η_t$ mantienen la misma integral sobre $ℝ$ que η: \[ \int_{ℝ} η_t(x) \dif x = \int_ℝ tη(tx) \dif x \eqreasonup{$y = tx$} \int_ℝ t · \frac{1}{t} · η(y) \dif y = \int_{ℝ} η(y) \dif y = 1\]

Una vez hecho esto, calculamos la convolución: \[ f_t(x) = \int_ℝ f(x-y) η_t(y) \dif y \]

Por ser $f$ continua podremos acotar su valor en entornos pequeños, esto es, que $∀ ε > 0$ existe un $δ > 0$ tal que $f(y) ∈ \bola_ε(f(x))$ si $x - y ∈ \bola_δ(0)$. Además, dado que las $η_t$ tienen un soporte cada vez más pequeño, esto es, $\sop η_t \convs[][t] \set{0}$, podemos encontrar un $t_ε$ suficientemente grande tal que $\sop η_t ⊆ \bola_δ(0)$ si $t > t_ε$. En este caso podemos acotar y ver que, si $t > t_ε$, entonces \begin{align*}
f_t(x) &= \int_ℝ f(x-y) η_t(y) \dif y \\
&= \int_{\bola_δ(0)} f(x-y) η_t(y) \dif y \\
&≤ \int_{\bola_δ(0)} (f(x) \pm ε) η_t(y) \dif y \\
&= (f(x) \pm ε) \int_{\bola_δ(0)} η_t (y) \dif y = f(x) \pm ε \end{align*}

Haciendo tender $ε \to 0$, $t$ se irá a infinito y tendremos la convergencia uniforme que buscábamos.

\spart

Vamos a usar una propiedad de la convolución, y es la siguiente: dadas $f,g$, entonces \[ \od{f*g}{x} = \od{f}{x} * g = f* \od{g}{x}\]

Esto nos permitirá sacar la derivada de la convolución aprovechando que η es derivable. Calculamos su derivada: \[ η_t'(x) = t^2 η'(tx) \], lo que nos deja que \[ f_t' (x) = f * η_t'(x) = \int_{ℝ} f(x-y) t^2 η'(tx) \dif y \]

Ahora vamos a tratar de acotar esta integral acotando las dos funciones que tenemos. $η'$ está acotada por ser $η$ derivable, y $f$ también está acotada por ser continua en un compacto. Llamaremos $K_f, K_η$ a las respectivas cotas para $f$ y $η'$.

El siguiente paso es ver que, dado que tanto $f$ como η (y por lo tanto $η_t'$) tienen soporte compacto, no tenemos que evaluar la integral en todo $ℝ$ sino sólo donde alguna de las dos funciones sea distinta de cero. Podemos suponer que siempre hay un soporte contenido en el otro (si no, la intersección será más pequeña que cualquiera de los dos y la cota será menor).

Empezaremos viendo qué ocurre cuando $\sop η_t' ⊆ \sop f(x-y)$. En este caso, tendremos que \[ \int_{ℝ} f(x-y) t^2 η'(tx) \dif y = \int\limits_{\sop η_t'} f(x-y) t^2 η'(tx) \dif y ≤ t^2 K_f K_η \int\limits_{\sop η_t'} \dif y \]

Sabemos que $\int_{\sop η_t'} \dif y = m(\sop η_t')$, es decir, es la medida del soporte. Sin embargo, es fácil ver que $m(\sop η_t') ≤ \frac{m(\sop η)}{t}$ (el soporte de la derivada está contenido en el soporte de la función, y al multiplicar por $t$ el parámetro estamos contrayendo el soporte). Como η tiene soporte compacto, su medida es finita y lo que nos queda es que \[ K_f K_η t^2 \int\limits_{\sop η_t'} \dif y = K_f K_η t · m(\sop η_t') ≤ C_1 t \]

Nos falta ver el caso contrario: qué ocurre cuando  $\sop f(x-y) ⊆ \sop η_t'$. De manera análoga, nos quedaría que \[ \int\limits_{\sop f(x-y)} f(x-y) t^2 η'(tx) \dif y ≤ K_f K_η t^2 · m(\sop f(x-y)) \]

Ahora bien, este caso ($\sop f(x-y) ⊆ \sop η_t'$) ocurre sólo cuando $t$ es pequeño (para $t$ grande el soporte de $η_t$ se hace tan pequeño como queramos y acabará estando contenido en el de $f(x-y)$). Luego en este caso tendremos $t$ acotado y por lo tanto podremos decir que \[ K_f K_η t^2 · m(\sop f(x-y)) ≤ C_2 \] con $C_2 ∈ ℝ$ independiente de $t$. Sumando ambas cotas, tenemos que $f_t' ≤ C_1 t + C_2$, y tomando $C = \max \set{C_1, C_2}$ nos queda que $f_t' ≤ C t + C = C(1+t)$.

\end{problem}

\section{Hoja 2}

\begin{problem} Sea $(X, \norm{·})$ un espacio vectorial normado. Demuestra que

\ppart  Dado $\mathcal{U} ⊂ X$ abierto, el conjunto $x + \mathcal{U} = \set{ x+ y \tq y ∈ \mathcal{U}}$ es abierto, así como $r\mathcal{U} = \set{r · x \tq x ∈ \mathcal{U}}$ para $r > 0$. En otras palabras, demuestra que la topología de un espacio normado es compatible con la estructura de espacio vectorial.

\ppart La norma es una función Lipschitz continua de constante $1$: \[ \abs{\norm{x} - \norm{y}} ≤ \norm{x-y} \quad ∀x,y ∈ X \]

\ppart Si $x_n \to x$ en $X$, entonces $\norm{x_n} \to \norm{x}$.

\ppart Dado $E ⊂ X$ definamos $\dst(x,E) = \inf \set{\norm{x-y} \tq y ∈ E}$. Entonces $\dst(x,E)$ es también Lipschitz continua de constante $1$.

\solution

\spart \label{ej:Hoja2:1A}

Sea $\mathcal{U} ⊂ X$. Ya hemos visto que $x + \mathcal{U}$ es un abierto para un $x ∈ V ⊂ \mathcal{U}$. Si tomamos entonces $z ∈ x + \mathcal{U}$, existirá un $y ∈ \mathcal{U}$ con $z = x + y$. Como $\mathcal{U}$ es abierto, podemos tomar una bola $\bola_δ(y) ⊂ \mathcal{U}$ y entonces $x + \bola_δ(x) ⊂ x + \mathcal{U}$, luego efectivamente $x + \mathcal{U}$ es abierto.

Un argumento similar valdrá para demostrar que $r\mathcal{U}$ es abierto. No merece demasiado la pena pararse en ello.

\spart

Sumando y restando, podemos ver que \[ \norm{x} = \norm{(x+y) - y} ≤ \norm{x-y} + \norm{y} \], luego $\norm{x} - \norm{y} ≤ \norm{x-y}$.

Eso sí, no hemos conseguido el valor absoluto. Sin embargo, si repetimos el mismo proceso desde la norma de $y$ tenemos que \[\norm{y} = \norm{(y-x) + x} ≤ \underbracket{\norm{y-x}}_{= \norm{x-y}} + \norm{x} \], luego $\norm{x} - \norm{y} ≥ - \norm{x-y}$. Juntando ambas conclusiones, \[ \abs{\norm{x} - \norm{y}} ≤ \norm{x-y} \] tal y como nos pedían.

\spart

Se resuelve simplemente por continuidad de la norma.

\spart

Si $\dst(x,E)$ es Lipschitz de constante 1, entonces lo que queremos ver es que \[ \abs{\dst(x,E) - \dst(x',E)} ≤ \abs{x - x'} \]

Esto se puede entender como una generalización del apartado B, si en ese caso tomábamos $E = \set{0}$.

Para demostrarlo, tomamos $x,x' ∈ V$ y $ε > 0$. Entonces existen $y, y' ∈ E$ con \begin{gather*}
 \dst(x,E) ≤ \norm{x-y} ≤ \dst(x,E) + ε\\
 \dst(x',E) ≤ \norm{x'-y'} ≤ \dst(x',E) + ε\\
\end{gather*}, de tal forma que nos acercan todo lo que queramos a esa distancia\footnote{No tiene por que ser igual porque está definido como el ínfimo, no tiene por qué alcanzarse.}. Además, podemos hacer dos estimaciones adicionales por ser la distancia el ínfimo: \[ \dst(x', E) ≤ \norm{x' - y} \qquad \dst(x,E) ≤ \norm{x-y'} \]

Supongamos además sin pérdida de generalidad que $\dst(x,E) ≥ \dst(x',E)$. Entonces \begin{multline*}
\abs{\dst(x,E) - \dst(x',E)} =
	\underbracket{\dst(x,E)}_{≤\norm{x-y'}} - \underbracket{\dst(x', E)}_{≥ \norm{x' - y'} - ε} ≤ \norm{x + y'} - \norm{x'-y'} + ε ≤ \\ ≤ \abs{ \norm{x + y'} - \norm{x'-y'} } + ε
	≤ \norm{(x-y') - (x'-y')} + ε = \norm{x-x'} + ε
\end{multline*}, y haciendo tender $ε \to 0$ ya tenemos lo que buscábamos.

\end{problem}

\begin{problem} \label{ej:Hoja2:2} Sea $V$ un espacio vectorial sobre $\kbb = ℝ$ ó $ℂ$. Diremos que $A ⊂ V$ es convexo si $∀x,y ∈ A$ y $∀t ∈ [0, 1]$, entonces $(1-t)x + ty ∈ A$ (en otros términos, el segmento $[x,y] ⊂ A$ si $x,y ∈ A$).

\ppart Demuestra que \[ A + A = \set{x + y \tq x,y ∈ A} = 2A = \set{2x \tq x ∈ V} \]

\ppart Si $W$ es otro espacio vectorial sobre \kbb y $\appl{T}{V}{W}$ es lineal, entonces $T(A)$ es convexo en $W$.

\ppart Si $(V, \norm{\cdot})$ es normado, entonces dados $x ∈ V$ y  $r > 0$, los conjuntos \begin{align*}
\bola_r (x) &= \set{x ∈ V \tq \norm{x} < r} \\
\adh{\bola_r}(x) &= \set{x ∈ V \tq \norm{x} ≤ r}
\end{align*} son, respectivamente, la bola abierta y la cerrada, centradas en $x ∈ V$ y de radio $r > 0$. Demuestra que ambas son convexas.

\ppart Si $(V,\norm{\cdot})$ es normado y $A ⊂ V$ es convexo, entonces $\adh{A}$ es convexo.

\solution

\spart

Está claro que $2 A = \set{x + x \tq x ∈ A} ⊂ A + A$, y para esto no necesitamos para nada la convexidad. La inclusión no será demasiado complicada.

Sea $z ∈ A + A$. Entonces $z = x + y$ con $x,y ∈ A$. Dividiendo por dos, \[ \frac{1}{2} z = \frac{1}{2} x + \frac{1}{2} y\], que está en $A$ por ser convexo. Luego si $\frac{1}{2} z ∈ A$, sólo queda ver que $z ∈ 2A$.

\spart

Tomamos $T$ en $(1-t)x + ty ∈ A$ y listos, podemos hacerlo por linealidad.

\spart

Empezamos primero con la bola cerrada. Tomamos $p,q ∈ \adh{\bola_r}(x)$ y $ 0 ≤ t ≤ 1$. Entonces \begin{multline*} \norm{(1-t)p + t q - x} = \norm{(1-t)(p - x) + t(q - x)} ≤ \\ ≤ (1-t)\norm{p - x} +(t) \norm{q - x} ≤ (1-t) r + t (r) = r \end{multline*}, así que como $\norm{((1-t)p + t q) - x} ≤ r$, tenemos que $(1-t)p + t q ∈ \adh{\bola_r}(x)$.

Para la bola abierta se hace igual, sólo que aparece un menor estricto en $\norm{p-x} < r,\norm{q-x} < r$ y nos sale.

\spart

Sean $x,y ∈ \adh{A}$ y $t ∈ [0,1]$. Por ser $\adh{A}$ cerrado, existirán sendas sucesiones $\set{x_n}, \set{y_n} ⊂ A$  que convergen respectivamente a $x$ e $y$. Consideramos ahora los puntos $(1-t)x_n + ty_n ∈ A$ por ser $A$ convexo. Como $\adh{A}$ es cerrado, la sucesión de esos puntos tenderá a $(1-t)x + t y$ que está en $\adh{A}$.

\end{problem}

\begin{problem} Sea $X = (C([0,1]), \norm{·}_∞)$. Entonces, si $f ∈ L^1[0,1]$ y \[ λ_f(g) ≝ \int_0^1 f(x) g(x) \dif x\], entonces $λ_f$ define un funcional acotado en $X$ cuya norma es exactamente $\norm{f}_1$. Demuéstralo siguiendo los siguientes pasos:

\ppart La desigualdad $\norm{λ_f}_{X^*} ≤ \norm{f}_1$ es trivial (justificarlo).
\ppart Si $f ∈ C([0,1])$ probar la desigualdad opuesta; junto con el apartado anterior esto prueba igualdad en este caso.
\ppart Si $f ∈ L^1[0,1]$ es cualquiera, usar que hay una sucesión de funciones continuas $f_n ∈ C([0,1])$ con $f_n \to f$ en $L^1[0,1]$

\solution

\spart

Podemos hacer la siguiente estimación: \[
\abs{λ_f(g)} = \abs{\int_0^1 f(x) g(x) \dif x} ≤ \int_0^1 \abs{f(x)} \abs{g(x)} \dif x ≤ \norm{g}_∞ \int_0^1 \abs{f(x)} \dif x = \norm{f}_1 \norm{g}_∞
\], luego $λ_f ∈ X^*$.

\spart

Si $f ∈ C([0,1])$, entonces $\norm{λ_f}_{X^*} ≥ \norm{f}_1$. Supongamos que $f ∈ C([0,1])$ y $f \not\equiv 0$. Consideramos \[ g (x) = \sign f(x) = \begin{cases} \frac{\abs{f(x)}}{f(x)} & f(x) ≠ 0 \\ 0 & f(x) = 0 \end{cases} \]

Entonces \[ λ_f(g) = \int_0^1 f(x) \sign f(x) \dif x = \int_0^1 \abs{f(x)} \dif x = \norm{f}_1 \]

Además, como $f \not \equiv 0$, $g(x) ≠ 0$ y $\abs{g(x)} = 1$, si $g(x) ≠ 0$, por lo que $\norm{g}_∞ = 1$ y entonces \[ \frac{\abs{λ_f (g)}}{\norm{g}_∞} = \frac{\norm{f}_1}{1} = \norm{f}_1 \implies \norm{λ_f}_{X^*} ≥ \norm{f}_1 \]

Ahora bien, tenemos un problema con esto, y es que $g$ no es continua y por lo tanto $g ∉ X$. Para arreglar el argumento vamos a regularizar $g$ con $\set{θ_t}_{t>0}$ una serie de  \textit{mollifiers}\footnote{\nref{def:AproxIdentidad}}. Para construirlos, fijamos $θ ∈ C_c(ℝ)$ par con $\sop θ ⊆ [-1, 1]$, $\norm{θ}_1 = 1$ y $θ ≥ 0$; y entonces \[ θ_t(x) = \frac{θ(\inv{t} x)}{t} \]

Tenemos que ampliar la definición de $g$ a $ℝ$, así que lo que haremos será extenderla con ceros a todo $ℝ$. Definimos entonces ahora \[ g_t = g * θ_t = \int_{ℝ} g(y) θ_t(x-y) \dif x\]

Supongamos adicionalmente que $\sop f ⊆ [ε, 1-ε]$ para cierto $0 < ε < \sfrac{1}{2}$, y que entonces tendremos $\sop g = \sop f$. Ahora vemos que $\sop g_t = \sop g + \sop θ_t = [ε - t, 1 - ε +t] ⊂ [0,1]$ si $0 < t ≤ ε$.

Calculamos ahora $λ_f(g_t)$ con $t ∈ (0,ε]$, ya que esta $g_t$ siempre será continua para $t > 0$. \begin{align*}
λ_f(g_t) &= \int_0^1 f(x) (g * θ_t)(x) \dif x = \\
	&= \int_{ℝ} f(x) (g * θ_t)(x) \dif x = \\
	&= \int_ℝ f(x) \left(\int_{ℝ} g(y) θ_t(x-y) \dif y \right) \dif x = \\
	&= \int_{ℝ} g(y) \left(\int_{ℝ} f(x) \underbracket{θ_t (x-y)}_{ = θ_t(y-x)} \dif x \right) \dif y = \\
	&= \int_{ℝ} g(y) (f*θ_t)(y) \dif y
\end{align*}

Como $f ∈ C[0, 1]$, entonces $f * θ_t \convs[L^∞][t][0] f$, luego podemos la integral converge igualmente: \[ \int_{ℝ} g(y) (f*θ_t)(y) \dif y \convs[][t][0] \int_{ℝ} g(y) f(y) \dif y \], así  que $λ_f(g_t) \convs[][t][0] λ_f (g) = \norm{f}_1$. Entonces \[ \norm{λ_f}_{X^*} ≥ \frac{\abs{λ_f(g_t)}}{\norm{g_t}_∞} \quad ∀t > 0\], ya que la definición del supremo era que  \[ \norm{λ_f}_{X^*} = \sup_{g ∈ X \setminus\set{0}} \frac{\abs{λ_f(g)}}{\norm{g}_X} \]

Lo único que nos falta ver es que $\norm{g_t}_∞ \convs[][t][0] 1$. Sabemos que $\norm{g_t}_∞ ≤ \norm{g}_∞ = 1$, como consecuecnia de que $θ ≥ 0$ y $\norm{θ}_1 = 1$. Pero si $f \not \equiv 0$, entonces $g$ es de módulo $1$ y continua en cualquier abierto donde $f ≠ 0$, y entonces $g_t(x) \convs[][t][0] g(x)$. Para verlo, si $g$ es continua en $x$ pues sale, y si no el módulo es exactamente $1$, y juntándolo tenemos que la norma infinito converge hacia uno cuando $t$ se hace pequeñito. Y juntando de nuevo esto con otra cosa, juraría que lo de la norma de antes, entonces nos queda que \[ \norm{λ_f}_{X^*} ≥ \liminf_{t \to 0} \frac{\abs{λ_f(g_t)}}{\norm{g_t}_∞} = \frac{\liminf_{t \to 0} \abs{λ_f(g_t)}}{\liminf_{t \to 0} \norm{g_t}_∞} = \norm{f}_1 \]

Y ya casi lo hemos probado si $\sop f ∈ [ε, 1-ε]$. Nos queda probarlo para el último caso de $f ∈ C([0,1])$. Sea $ε > 0$. Consideramos $f_ε $ como una versión truncada, formalmente \[ f_ε(x) = \begin{cases}
f(x) & x ∈ [ε, 1-ε] \\
f(ε)\left(1 - 2\frac{ε-x}{ε}\right) & x ∈ [\sfrac{ε}{2}, ε] \\
f(1-ε)\left(1 - 2\frac{1-ε-x}{1-ε}\right) & x ∈ [1- ε, 1- \sfrac{ε}{2}] \\
0 & x ∈ [0, \sfrac{ε}{2}] ∪ [ 1- \sfrac{ε}{2}, 1]
\end{cases}
\]

Vamos a ver entonces que \[ \norm{f-f_ε}_1 ≤ \dotsb ≤ 2ε\norm{f}_1 \], y entonces podemos escribir \[ λ_f(g) = λ_{f_ε}(g) + λ_{f-f_ε}(g) \], y como $\abs{λ_{f-f_ε}(g)} ≤ \norm{f-f_ε}\norm{g}_1 \convs[][ε][0] 0$, y luego \[\norm{λ_f}_{X^*} = \norm{λ_{f-f_ε} + λ_{f_ε}} ≥ \norm{λf_ε}_{X^*} - \norm{λ_{f-f_ε}}_{X^*} = \norm{f_ε}_1 - \norm{f - f_ε}_1 \convs[][ε][0] \norm{f} \], y esto ya sí termina la prueba cuando $f$ es continua. La idea sería la misma si tuviésemos $f ∈ L^1[0,1]$, que es lo que queríamos probar, usando una aproximación por continuas.


\end{problem}

\begin{problem}[4] Dada $f ∈ C([-\sfrac{1}{2}, \sfrac{1}{2}])$ (que entenderemos extendida por periodicidad de período 1 a todo $ℝ$), sus coeficientes de Fourier son \[ \hat{f}(n) ≝ \int_{-\sfrac{1}{2}}^{\sfrac{1}{2}} f(x) e^{-2πinx} \dif x\quad n ∈ ℤ\] y las sumas parciales de su serie de Fourier formal $\sum_{n∈ℤ} \hat{f}(n) e^{2πinx}$ como \[ S_Nf(x) ≝ \sum_{\abs{n} ≤ N} \hat{f}(n) e^{2πinx} \quad N ∈ ℕ \]

Demostrar que:

\ppart Dado $D_N(t) ≝ \sum_{\abs{n}≤N} e^{2πint}$ el llamado \textbf{Núcleo de Dirichlet}.\index{Núcleo!de Dirichlet}, entonces  \[ S_N f(x) = \int_{-\sfrac{1}{2}}^{\sfrac{1}{2}} f(y) D_n (x-y) \dif y = (D_N * f) (x) \]

\ppart Probar que \[ D_N(t) = \frac{\sin 2πt(N + \sfrac{1}{2})}{\sin πt} \]

Indicación: las sumas $\sum_{1≤ n ≤ N} e^{2πint}$ y $\sum_{-N ≤ n ≤ -1} e^{2πint}$ son ambas geométricas y conjugadas entre sí. Además, según la notación del \fref{ej:Hoja2:2}, $S_N(f)(0) = λ_{D_N} (f)$.

\ppart Probar que existe $C > 0$ tal que \[ \norm{D_N}_{L^1} = C \log N + \mathcal{O}(1), \; N \to ∞ \]

Indicación: si $\abs{t} ≤ \sfrac{1}{2}$, entonces $\frac{1}{\sin πt} - \frac{1}{πt} = \mathcal{O}(1)$.

\ppart Usar el \fref{ej:Hoja2:2} y los apartados anteriores para concluir que existe una $f ∈ C([-\sfrac{1}{2}, \sfrac{1}{2}])$ cuya serie de Fourier diverge en $x = 0$; y que de hecho existe todo un $G_δ$ denso de funciones con esa propiedad.

\solution

\spart

Los núcleos de Dirichlet están dados por \[ D_N(t) ≝ \sum_{\abs{n} ≤ N} e^{2πint} \]

Operando tenemos que \[
S_Nf(x) = \sum_{\abs{n}≤N} \hat{f}(n) e^{2πinx} = \sum_{\abs{n}≤N} \left(\int_{-\sfrac{1}{2}}^{\sfrac{1}{2}} f(y) e^{-2πiny} \dif y \right) e^{2πinx}
\]

Podemos pasar $e^{2πinx}$ dentro de la integral por ser constante con respecto a $y$, y dado que la suma que estamos haciendo es finita, podemos pasar el sumatorio dentro de la integral por linealidad de ésta, y lo que nos queda es que \begin{gather*}
\sum_{\abs{n}≤N}
	\left(\int_{-\sfrac{1}{2}}^{\sfrac{1}{2}} f(y) e^{-2πiny} \dif y \right)
	e^{2πinx} =
\int_{-\sfrac{1}{2}}^{\sfrac{1}{2}}
	\sum_{\abs{n}≤N} f(y)·e^{-2πiny}·e^{2πinx} = \\
= \int_{-\sfrac{1}{2}}^{\sfrac{1}{2}}  \sum_{\abs{n}≤N} f(y) e^{2πn(x-y)} \dif y
= \int_{-\sfrac{1}{2}}^{\sfrac{1}{2}}  f(y) \underbracket{\sum_{\abs{n}≤N} e^{2πn(x-y)}}_{D_N(x-y)}
= (D_N * f)(x) \end{gather*}
, que es lo que se pedía probar.

\spart

Definimos \[
D_N^+(t) ≝ \sum_{1≤n ≤N} e^{2πint} \qquad
D_N^-(t) ≝ \sum_{-N ≤ n ≤ -1} e^{2πint} \]
de tal forma que \[ D_N(t) = D_N^+(t) + D_N^-(t) + e^{2πit·0} = D_N^+(t) + \conj{D_N^+(t)} + 1 = 2 \Re D_N^+(t) + 1 \]

Dado que $D_N^+(t)$ es una suma geométrica, podemos hallar su valor: \[ D_N^+(t) = e^{2πit} \frac{1 - e^{2πiNt}}{1-e^{2πit}} = \frac{e^{2πit} - e^{2πi(N+1)t}}{1-e^{2πit}} \]

Multiplicando arriba y abajo por $e^{-πit}$ tenemos que \[ D_N^+(t) = \frac{e^{2πit} - e^{2πi(N+1)t}}{1-e^{2πit}} = \frac{e^{πit} - e^{2πi(N + \sfrac{1}{2})t}}{e^{-πit} - e^{πit}}\], de forma que \[ \conj{D_N^+(t)} = \frac{e^{-πit} - e^{-2πi(N + \sfrac{1}{2})t}}{-e^{-πit} + e^{πit}} = \frac{-e^{-πit} + e^{-2πi(N + \sfrac{1}{2})t}}{e^{-πit} - e^{πit}} \]

Ahora sólo queda sumar y ver que \begin{align*}
D_N(t) &=
	\frac{e^{πit} - e^{2πi(N + \sfrac{1}{2})t}}{e^{-πit} - e^{πit}}
+ 	\frac{-e^{-πit} + e^{-2πi(N + \sfrac{1}{2})t}}{e^{-πit} - e^{πit}}
+	\frac{e^{-πit} - e^{πit}}{e^{-πit} - e^{πit}} = \\
&= 	\frac{e^{-2πi(N + \sfrac{1}{2})t} - e^{2πi(N + \sfrac{1}{2})t}}{e^{-πit} - e^{πit}}
=	\frac{\sin -2π(N + \sfrac{1}{2})t}{\sin -πt} = \\
&= 	\frac{\sin 2πt(N + \sfrac{1}{2})}{\sin πt}
\end{align*} tal y como se pedía.

\spart


Una primera observación es que podemos reducirnos a estudiar la integral en el intervalo $[0,\sfrac{1}{2}]$, ya que el valor absoluto del seno es una función par: \(
\norm{D_N}_{L^1[-\sfrac{1}{2}, \sfrac{1}{2}]}
	= \int_{-\sfrac{1}{2}}^{\sfrac{1}{2}} \abs{\frac{\sin 2πt(N+\sfrac{1}{2})}{\sin πt}} \dif t
	= 2 \int_{0}^{\sfrac{1}{2}} \frac{\abs{\sin 2πt(N+\sfrac{1}{2})}}{\sin πt} \dif t \label{eq:NormaDNPrev} \)

Usando la sugerencia, sabemos que $\frac{1}{\sin πt} - \frac{1}{πt} = \mathcal{O}(1)$ cuando $\abs{t} ≤ \sfrac{1}{2}$, luego

Hay dos problemas que sortear en la integral: por un lado, la indeterminación cuando $t \to 0$ y el valor absoluto en el numerador. Podemos dividir el intervalo de integración en subintervalos en los que el numerador no cambie de signo, y además ``aislaremos'' la indeterminación y podremos tratarla por separado. La división será la siguiente: \begin{gather*}
[0, \sfrac{1}{2}] = \left(\bigcup_{k=0}^{N-1} I_k \right) ∪ I_E \\
I_k = \left[ \frac{k}{2(N+\sfrac{1}{2})},  \frac{k+ 1}{2(N+\sfrac{1}{2})} \right) \qquad
	I_E = \left[ \frac{N}{2(N+\sfrac{1}{2})}, \frac{1}{2} \right]
\end{gather*}

Por comodidad, denotaremos $f(t) = \frac{\abs{\sin 2πt(N+\sfrac{1}{2})}}{\sin πt} $.

\paragraph{Integral en $I_0$} El primer intervalo a resolver será $I_0 = \left[0, \frac{1}{2(N+\sfrac{1}{2})}\right)$, que es donde tenemos la indeterminación cuando $t \to 0$. Pero si resolvemos ese límite tenemos que \[ \lim_{t \to 0}  \frac{\sin 2πt(N+\sfrac{1}{2})}{\sin πt} \eqreasonup{L'Hopital} \lim_{t \to ∞} \frac{2π(N + \sfrac{1}{2}) · \cos 2πt(N+\sfrac{1}{2})}{π · \cos πt} = 2(N + \sfrac{1}{2}) \]

Además, derivando $f(t)$ tenemos que \begin{align*}
f'(t) &= \frac{\sin πt ·\cos 2πt(N + \sfrac{1}{2}) - \cos πt · \sin 2πt(N + \sfrac{1}{2})}{\sin^2 πt} = \frac{\sin πt(1 - 2N - 1)}{\sin^2 πt} \\
&= \frac{\sin -2πtN}{\sin^2 πt} = \frac{- \sin 2πtN}{\sin^2 πt} ≤ 0 \quad ∀t ∈ \left[0, \frac{1}{2(N+\sfrac{1}{2})}\right)
\end{align*}, por lo que $f$ es decreciente en el intervalo, luego su máximo ocurre en $t = 0$ y finalmente podemos acotar y ver que \( 0 ≤ \int_{I_0} f(t) \dif t ≤ \int_0^{\frac{1}{2(N+\sfrac{1}{2})}} 2(N + \sfrac{1}{2}) \dif t = 1 = \mathcal{O}(1) \label{eq:CotaI0} \)

\paragraph{Integral en $I_k$} Para resolver esta integral, usamos que $\frac{1}{\sin πt} - \frac{1}{πt} ≤ M$ para $M < ∞$ cuando $\abs{t} ≤ \sfrac{1}{2}$. Entonces podemos separar la integral: \begin{align*}
\int_{I_k} \frac{\abs{\sin 2πt(N+\sfrac{1}{2})}}{\sin πt} \dif t &= \int_{I_k}\underbracket{\sin (2πt(N+\sfrac{1}{2}))}_{≤ 1}·\left(\underbracket{\frac{1}{\sin πt} - \frac{1}{πt}}_{≤M} + \frac{1}{πt} \right) \dif t = \\
 &≤ \int_{\frac{k}{2(N+\sfrac{1}{2})}}^{\frac{k+ 1}{2(N+\sfrac{1}{2})}} M + \frac{1}{πt} \dif t = \frac{M}{2(N+\sfrac{1}{2})} + \left(\frac{\log t}{π}\right|_{t = \frac{k}{2(N+\sfrac{1}{2})}}^{\frac{k+ 1}{2(N+\sfrac{1}{2})}} = \\
 &= \frac{M}{2(N+\sfrac{1}{2})} + \frac{\log (k+1) - \log k}{π}
\end{align*}

Como los logaritmos tienen signo alternado, se van a cancelar al sumar las integrales de intervalos sucesivos y por lo tanto quedará \( \label{eq:CotaIkSup}
0 ≤ \sum_{k = 1}^{N-1} \int_{I_k} f(t) \dif t ≤ \frac{M(N-1)}{2N + 1} + \frac{\log N - \log 1}{π} = \mathcal{O}(1) + \frac{1}{π}\log N
	\)

Falta ahora una cota inferior para comprobar que efectivamente $\sum_{k=1}^{N-1} = \mathcal{O}(1) + C \log N$. Para ello, observamos dos cosas: que $\sin πt ≤ πt$ cuando $\abs{t} ≤ \sfrac{1}{2}$ y que $\frac{1}{πt}$ es monótona decreciente. Entonces podemos operar y ver que \begin{align*}
\int_{I_k} \frac{\abs{\sin 2πt(N+\sfrac{1}{2})}}{\sin πt} \dif t
	&≥ \int_{I_k} \frac{\abs{\sin 2πt(N+\sfrac{1}{2})}}{πt} \dif t
	≥ \int_{I_k} \frac{\abs{\sin2πt(N+\sfrac{1}{2})}}{π\left(\frac{k+ 1}{2(N+\sfrac{1}{2})} \right)} \dif t = \\
	&= \frac{1}{π^2k} \abs{\left(\cos 2πt(N+\sfrac{1}{2})\right|_{t=\frac{k}{2(N+\sfrac{1}{2})}}^{\frac{k + 1}{2(N+\sfrac{1}{2})}}} = \frac{2}{π^2k}
\end{align*}

Sumando ahora todas esas integrales, vemos que \( \sum_{k=1}^{N-1} \int_{I_k}f(t) \dif t ≥ \sum_{k=1}^{N-1} \frac{2}{π^2k} > \frac{2\log N}{π^2} \label{eq:CotaIkInf}\), luego juntando esta cota y la de \eqref{eq:CotaIkSup} lo que tenemos es que efectivamente \( \sum_{k=1}^{N-1} \int_{I_k} f(t) \dif t = C \log N + \mathcal{O}(1) \label{eq:CotaIk} \) para alguna constante $0 < C < ∞$ (de hecho, según estos cálculos, $C$ debería de estar entre $\frac{2}{π^2}$ y $\frac{1}{π}$).

\paragraph{Integral en $I_E$} La función en este intervalo $I_E = \left[ \frac{N}{2(N+\sfrac{1}{2})}, \frac{1}{2} \right]$ es continua y por lo tanto acotada, y como la medida del intervalo $m(I_E) \convs[][N] 0$ se va a cero, es fácil ver que \( \label{eq:CotaIE} \int_{I_E} f(t) \dif t = \mathcal{O}(1) \)

\paragraph{Conclusión} Dividiendo el intervalo de integración en tres ``zonas'' \eqref{eq:CotaI0}, \eqref{eq:CotaIk} y \eqref{eq:CotaIE} hemos logrado las cotas que buscábamos (un logaritmo y funciones $\mathcal{O}(1)$), así que sólo falta juntarlas volviendo a la integral original \eqref{eq:NormaDNPrev}, de tal forma que \( \norm{D_N}_{L^1} = 2 \int_{0}^{\sfrac{1}{2}} f(t) \dif t = C \log N + \mathcal{O}(1), \; N \to ∞ \label{eq:CotaFinal}\), con $C ∈ [\sfrac{4}{π^2}, \sfrac{2}{π}]$.

\spart

Según la notación del \fref{ej:Hoja2:2}, $λ_{D_N} (f) = S_Nf(0)$, con $λ_{D_N}$ un operador acotado. De hecho, $\norm{λ_{D_N}}_{\mathcal{L}(X, ℝ)} = \norm{D_N}_1$, y $λ_{D_N} ∈ \mathcal{L}(X, ℝ)$ donde $X = (C([-\sfrac{1}{2}, \sfrac{1}{2}], \norm{·}_∞)$ que es un espacio Banach y $ℝ$ normado.

Como los funcionales $\set{λ_{D_N}}_{N ≥ 1}$ no están acotados uniformemente ($\norm{D_N}_1 \convs[][N] ∞$ según hemos demostrado en \eqref{eq:CotaFinal}), el Teorema de acotación uniforme de Banach-Stainhaus nos dice que existe un conjunto $B$ $G_δ$ denso en $X$ tal que \( \label{eq:BanachStainhaus} \sup_{N ≥ 1} \norm{λ_{D_N} (f)}_ℝ = \sup_{N ≥ 1} \norm{S_Nf(0)}_ℝ = ∞ \quad ∀f ∈ B \), y como $λ_{D_N}$ está acotado, \[ \norm{λ_{D_N} (f)}_ℝ ≤ \norm{λ_{D_N}}_{\mathcal{L}(X, ℝ)} · \norm{f}_ℝ < ∞ \quad ∀N ∈ ℕ \], luego la única posibilidad para que \eqref{eq:BanachStainhaus} se cumpla es que $\norm{S_Nf(0)}_ℝ$ diverja para $f ∈ B$, siendo $B$ un $G_δ$ denso en $X$, tal y como se pedía demostrar.

\end{problem}

\begin{problem}[5] \label{ej:Hoja2:5} Sean $X$ e $Y$ dos espacios normados. Demostrar que:

\ppart Si $\dim X < ∞$ y $A$ es una aplicación lineal $\appl{A}{X}{Y}$, entonces es continua.

\ppart Si $X$ es de dimensión finita, existe una aplicación lineal $\appl{A}{X}{Y}$ discontinua. Indicación: Usar que todo espacio vectorial admite una base de Hamel, esto es, un conjunto $B ⊂ X$ linealmente independiente y maximal para esa propiedad.

\solution

\spart

Como la dimensión de $X$ es finita, podemos fijar $\mathcal{B} = \set{\ve_1, \dotsc, \ve_n}$ base de $X$ y la base dual algebraica $\mathcal{B}^* = \set{\ve_1^*, \dotsc, \ve_n^*}$, ambas ortonormales. Con estas dos bases podemos expresar cualquier elemento $x ∈ X$ como \[ x = \sum_{i = 1}^n \ve_i^*(x) \ve_i \]

Con eso podremos acotar $A$ de la siguiente forma:
\begin{align*}
A(x) &=
	A(x) = A\left(\sum_{i=1}^n \ve_i^* (x) \ve_i \right) = \\
	&= \sum_{i=1}^n \ve_i^* (x) A(\ve_i)
	≤ \sum_{i=1}^n \underbracket{\norm[0]{\ve_i^*}_{\dual}}_{=1} \norm{x}_X A(\ve_i) \\
\norm{A(x)} &≤ \norm{x}_X \norm{\sum_{i=1}^n A(\ve_i)}
\end{align*}

Dado que la suma de los $A(\ve_i)$ es finita y cada uno de los sumandos es finito (si no, $A$ no sería lineal) esa norma es finita y ya tenemos la cota que buscábamos.

\spart

% TODO.

\end{problem}

\begin{problem}[6] \label{ej:Hoja2:InversaBiyeccion} Sean $X$ e $Y$ espacios de Banach.

\ppart Si $T ∈ \linapp$ es una biyección, entonces $\inv{T} ∈ \linapp$ (\fref{crl:AppAbiertInversa} del \nref{thm:AppAbierta}).

\ppart Si $V$ es un espacio vectorial con dos normas $\norm{·}_1$ y $\norm{·}_2$ para las cuales $V$ es completo, con la primera dominando a la segunda (\fref{def:NormaDominante}), entonces la segunda domina también a la primera (\fref{crl:AppAbiertaAcotacionNormas}). \textit{Indicación: usar el apartado anterior con $T$ la identidad}.

\solution

\spart

La existencia de la inversa viene por el hecho de que la aplicación sea biyectiva. Tenemos que demostrar que es lineal y acotada.

Para la linealidad, por un lado $\inv{T}(x+y) = c \iff T(c) = x + y$. Como $T$ es biyectiva, existen $a,b ∈ X$ tales que $T(a) = x, T(b) = y$, luego $T(c) = T(a) + T(b) = T(a+b)$, luego $c = a+ b$ por ser inyectiva y $\inv{T}(x+y) = a + b = \inv{T}(x) + \inv{T}(y)$.

Por otro, $\inv{T}(λy) = a \iff λy = T(a)$. Como $T$ es biyectiva, existe $b ∈ X$ tal que $T(b) = y$, luego $λT(b) = T(a) \implies \inv{T}(λy) = λ\inv{T}(y)$.

Para la acotación, por el \nref{thm:AppAbierta} sabemos que $∃ δ  > 0$ tal que $T(\bola_1(0)) ⊃ \bola_δ(0)$. Sea ahora $y ∈ Y$, y queremos demostrar que $\norm{T(y)} < C$ para una cota $C > 0$. Lo que hacemos es ``reescalar'' $y$: $y' = \frac{δ}{2\norm{y}} y$, de tal forma que $y' ∈ \bola_δ(0) ⊂ Y$. Así, $\inv{T}(y') ∈ \bola_1(0) ⊂ X$. Ahora bien, como $\inv{T}$ es lineal entonces $\inv{T}(y') = \frac{δ}{2\norm{y}} \inv{T}(y)$, luego \begin{align*}
\frac{δ}{2\norm{y}} \norm{\inv{T}(y)} &≤ 1  \\
\norm{\inv{T}(y)} &≤ \frac{2\norm{y}}{δ} \\
\norm{\inv{T}} &≤ \frac{2}{δ}
\end{align*} así que ya tenemos la acotación que buscábamos.

\spart

Si $\norm{·}_1$ domina a $\norm{·}_2$, entonces (\fref{prop:NormaDominante}) existe una constante $C < ∞$ tal que $∀x ∈ X$ $\norm{x}_1 ≤ C \norm{x}_2$. Denotamos entonces $X = (V, \norm{·}_1)$ e $Y = (V, \norm{·}_2)$, y definimos la aplicación identidad entre ellos, que es obviamente una biyección y una aplicación lineal acotada (norma $1$).

Ahora sólo falta ver que $\norm{T(x)}_2 = \norm{x}_1 = \norm{x}_2$ y listos.

\end{problem}

\begin{problem}[7] Probar que el \nref{thm:GraficaCerrada} es equivalente a la siguiente afirmación: Dados $X$, $Y$ espacios de Banach y $\appl{T}{X}{Y}$ lineal, si $T$ tiene la propiedad de que siempre que $x_n \to x$ en $X$ y $T(x_n) \to y$ en $Y$ se cumple que $T(x) = y$, entonces $T$ es acotado.

\solution

Que ese enunciado implica el \nref{thm:GraficaCerrada} es sencillo: tomamos una sucesión $\set{(x_n, T(x_n))}_{n ≥ 1} ⊂ G(T)$ de Cauchy. Sabemos que $x_n \to x ∈ X$ y $T(x_n) \to y = T(x) ∈ Y$ por ser $X, Y$ Banach. Por un lado, esto implica que $G(T)$ es cerrado (el límite de sucesión de Cauchy se queda dentro del conjunto) y por otro implica que es acotado según el enunciado anterior, luego $T$ será continua y por lo tanto $T ∈ \linapp$, que es lo que dice el teorema de la gráfica cerrada.

En el otro sentido, sabemos que si $G(T)$ es cerrado eso implica que $T ∈ \linapp$. Dado que $G(T)$ es cerrado, podemos tomar una sucesión de Cauchy $(x_n, T(x_n)) \to (x, y) ∈ G(T)$. Como $T∈ \linapp$, está acotado y además $T(\lim x_n) = \lim T(x_n) \implies T(x) = T(y)$.
\end{problem}


\begin{problem}[8] Una isometría $I ∈ \linapp$ con $X,Y$ espacios normados es una aplicación con la propiedad de que $∀ x ∈ X$ $\norm{I(x)}_Y = \norm{x}_X$. Probar:

\ppart Si $X$ es Banach, $Y$ es normado e $\appl{I}{X}{Y}$ es una isometría sobreyectiva, entonces $Y$ también es de Banach.
\ppart Toda isometría es una aplicación inyectiva: $\ker I = \set{0}$.
\ppart Si $I$ es isometría sobreyectiva, entonces es un isomorfismo lineal entre $X$ e $Y$ y su inversa $\appl{I}{Y}{X}$ es también una isometría.
\ppart Si $I$ es una isometría de $X$ y $\dim X < ∞$ entonces $I$ es automáticamente sobreyectiva y, por tanto, un isomorfismo de $X$.
\ppart Construir un ejemplo de isometría no sobreyectiva de un espacio de Banach. Indicación: Tomar $X = \ell^2$ siendo \[ \ell^2 = \set{\set{a_n}_{n ≥ 1} \tq \norm{\set{a_n}}_2 ≝ \left(\sum_{n ≥ 1} \abs{a_n}^2 \right)^{\frac{1}{2}} < ∞ } \]

\solution

\spart
\label{ej:Hoja2:8}

Sea $\set{y_n}_{n ≥ 1} ⊂ Y$ una sucesión de Cauchy. Como $I$ es sobreyectiva, existe otra sucesión $\set{x_n}_{n ≥ 1} ⊂ X$ tal que $I(x_n) = y_n\;∀n ≥ 1$. Vamos a ver que $x_n$ es de Cauchy: sabemos que $∀ ε > 0$ existe un $N > 1$ tal que si $n,m ≥ N$ entonces $\norm{y_n - y_m} < ε$. Ahora podemos ver que \[ \norm{x_n - x_m} = \norm{I(x_n - x_m)} = \norm{I(x_n) - I(x_m)} = \norm{y_n - y_m} < ε\] luego $\set{x_n}$ es de Cauchy.

Como $X$ es Banach, entonces $x_n \to x ∈ X$. Afirmamos que $y = I(x)$ es el límite de $\set{y_n}_{n ≥ 1}$. Para eso vemos que, dado $ε > 0$ existe un $N ≥ 1$ tal que si $n > N$ entonces $\norm{y - y_n} < ε$. Análogamente a como lo hemos hecho antes, tomamos el $N$ que nos haga falta para conseguir la acotación con ε en la sucesión $\set{x_n}$ y nos queda \[ \norm{y - y_n} = \norm{I(x) - I(x_n)} = \norm{I(x - x_n)} < ε \]

\spart

Por ser espacios normados, $\norm{x} = 0$ si y sólo si $x = 0$. Luego si $I(x) = 0$, $\norm{I(x)} = 0 = \norm{x} \implies x = 0$, luego efectivamente $\ker I = \set{0}$.

\spart

Dado que las isometrías son inyectivas y nos dicen que es sobreyectiva, entonces $I$ es biyectiva y por lo tanto es un isomorfismo lineal. Que la inversa es isometría es trivial.

\spart

\textit{Nota: Voy a suponer que dice isometría de $X$ en $X$}

Tenemos dimensión $n$ finita, luego podemos dar una base $\set{e_1, \dotsc, e_n}$ para $X$.

Para que sea sobreyectiva, vemos que podemos construir una base de forma recursiva. Como $I(e_1) ∈ X$ podemos expresarlo como suma de elementos de la base, esto es, \[ I(e_1) = α_1 e_1 + \dotsb + α_n e_n \] y despejar: \[ e_1 = \frac{I(e_1) - α_2 e_2 - \dotsb - α_n e_n}{α_1} \]

Repitiendo, llegaremos a que $\set{I(e_1), \dotsc, I(e_n)}$ es una base y por lo tanto ya tenemos la sobreyectividad.

Por el camino me he dejado ver qué pasa si alguno de los coeficientes es 0 y no podemos despejar.

\spart

La aplicación que ``desplaza'' una sucesión es lineal e isometría pero no sobreyectiva: \[ T(x_1, x_2, \dotsc) = (0, x_1, x_2, \dotsc)\]
\end{problem}

\begin{problem}[9] \label{ej:Hoja2:9} Sean $X$ e $Y$ espacios de Banach y $\set{T_n}_{n ≥ 1} ⊂ \linapp$ tales que $∀x ∈X$ existe el límite $\lim_{n \to ∞} T_n(x) ≝ T(x)$. Demuestra que

\ppart $\sup_{n ≥ 1} \norm{T_n} < ∞$.
\ppart $T ∈ \linapp$.
\ppart $\norm{T} ≤ \limsup_{n \to ∞} \norm{T_n}$.

\solution

\spart

Suponemos que que exista el límite implica que es finito. En ese caso, aplicamos el \nref{thm:AcotacionUniforme}, y como el límite siempre es finito entonces están uniformemente acotadas.

\spart

Cuentas.

\end{problem}

\begin{problem}[10] Sea $X$ un espacio de Banach real, $0 < ε_n \to 0$ y $f_n ∈ X^*$ con la propiedad de que para todo $x ∈ X$ existe un $r > 0$ tal que $\norm{x}_X < r$ y una constante $C(x) ∈ ℝ$ tal que \( \label{eq:Hoja2:Ej10} \pesc{f_n, x} ≤ ε_n \norm{f_n}_{X^*} + C(x) \)

Entonces probar que $\set{f_n}$ es un subconjunto acotado en $X^*$ del modo siguiente:

\ppart Probar que \eqref{eq:Hoja2:Ej10} es una acotación válida para $\abs{\pesc{f_n,x}}$.
\ppart Considerar $g_n = \left(1 + ε_n \norm{f_n}_{X^*}\right)^{-1} f_n$, y probar que $\set{g_n}$ es acotado en $X^*$.
\ppart Deducir una contradicción si $\sup \norm{f_n}_{X^*} = ∞$

\solution

\spart

\spart

Tomamos un $x ∈ X$ con $\norm{x}_X < r$, y suponemos sin pérdida de generalidad que $C(x)$. Por el apartado anterior, \begin{align*}
\pesc{g_n, x} &≤ \frac{ε_n \norm{f_n}_X + C(x)}{1 + ε_n \norm{f_n}_{X^*}} =
	\underbracket{\frac{ε_n \norm{f_n}_X}{1 + ε_n \norm{f_n}_{X^*}}}_{< 1} +
	\underbracket{\frac{C(x)}{1 + ε_n \norm{f_n}_{X^*}}}_{< C(x)} < \\
& < 1 + C(x) < ∞
\end{align*}, luego hemos acotado $\abs{\pesc{g_n, x}}$ para los $x ∈ X$ con norma menor que $r$. Ahora bien, como todo $x ∈ X$ es proporcional a otro $x' ∈ X$ con $\norm{x'} < r$ tenemos que $\sup \abs{\pesc{g_n, x}} < ∞ \; ∀x ∈ X$.

Entonces podemos aplicar el \nref{thm:AcotacionUniforme}, que nos dice que existirá una constante $C < ∞$ tal que $\norm{g_n}_{X^*} ≤ C\; ∀n$, luego $\set{g_n}$ está acotado.

\spart

Si efectivamente las $f_n$ no estuviesen acotadas, entonces existiría una sucesión $n_1 < n_2 < \dotsb$ con $\lim_{j \to ∞} \norm[1]{f_{n_j}}_{X^*} = ∞$. Así, para un $x ∈ X$ podemos ver que $\pesc{f_{n_j}, x} = (1 + ε_{n_j} \norm[1]{f_{n_j}}_{X^*}) \pesc{g_{n_j}, x}$. Tomando valores absolutos, podemos estimar y ver que
\begin{align*}
\abs{\pesc{f_{n_j}, x}}
	&= \abs{(1 + ε_{n_j} \norm[1]{f_{n_j}}_{X^*}) \pesc{g_{n_j}, x}} ≤ \\
	&≤ (1 + ε_{n_j} \norm[1]{f_{n_j}}_{X^*}) \norm[1]{g_{n_j}}_{X^*} \norm{x}_X ≤ \\
	&≤ (1 + ε_{n_j} \norm[1]{f_{n_j}}_{X^*}) C \norm{x}_X  \\
\frac{\abs{\pesc{f_{n_j}, x}}}{1 + ε_{n_j} \norm[1]{f_{n_j}}_{X^*}}
	&≤ C\norm{x}_X  \\
\frac{\abs{\pesc{φ_{n_j}, x}}}{1 + ε_{n_j} \frac{1}{\norm[1]{f_{n_j}}_{X^*}}}
	&≤ C\norm{x}_X  \\
\end{align*} definiendo \[
	φ_{n_j} ≝ \frac{f_{n_j}}{\norm[1]{f_{n_j}}_{X^*}} ∈ \adh{\bola}_1(0)  ⊂ X^*
\]

El denominador de ahí se irá a cero,a así que para cada $j$ elegimos un $x_j$ con $\norm{x_j}_{X} = 1$, y el producto escalar es mayor que un medio y al final contradicción.

\end{problem}

\begin{problem}[11] Consideramos $X$ un espacio de Banach real, un operador $\appl{A}{D(A)}{X^*}$ en general no lineal con $D(A) ⊂ X$ y monótono: $∀x,y ∈ D(A)$ se tiene que $\pesc{A(x) - A(y), x - y} ≥ 0$. Demuestra que:

\ppart Sea $x_0 ∈ \inter D(A)$. Entonces existen dos constantes $R, C > 0$ tales que si $x ∈ \bola_R(x_0) ⊂ D(A)$, entonces $\norm{A(x)}_{X^*} ≤ C$. \hint{Argumentar por reducción al absurdo suponiendo que $x_n \to x_0$ y $\sup_{n}  \norm{A(x_n)}_{X^*} = ∞$ y aplicar el ejercicio anterior.}

\ppart Deducir que si $\appl{T}{X}{\dual[X]}$ es lineal con la propiedad \[ \pesc{T(x), x)} ≥ 0 \quad ∀x ∈ X \] entonces $T$ es acotado.

\ppart Mostrar mediante un ejemplo que si $T$ es como en el apartado anterior pero sólo en un subespacio $D(T)$ la conclusión ya no es válida, incluso aunque $D(T)$ sea denso.

\solution

\spart

Siguiendo la sugerencia, suponemos $x_n \to x_0$ una sucesión de Cauchy con $\sup \norm{A(x_n)}_{X^*} = ∞$.

% TODO

\end{problem}

\begin{problem}[12] Consideremos el espacio \[ \ell^1 = \set{\set{a_n}_{n ≥ 1} \tq \sum_{n≥1} \abs{a_n} < ∞ }\] de las sucesiones numéricas sumables. Sea así mismo \[ \ell^∞ = \set{\set{μ_n}_{n ≥ 1} \tq \sup_{n ≥ 1} \abs{μ_n} < ∞ } \] el espacio de las sucesiones numéricas acotadas. Probar que:

\ppart $\ell^∞ ⊂ \dual[\ell^1]$ en el siguiente sentido: si $\set{μ_n} ∈ \ell^∞$ está fijada y $\set{a_n} ∈ \ell^1$, entonces \( \label{eq:FuncionalEll} λ(\set{a_n}) ≝ \sum_{n ≥ 1} a_n μ_n = \pesc{λ, \set{a_n}}\) cumple $\abs{λ(\set{a_n})} ≤ \norm{μ_n}_∞ \norm{a_n}_1$; identificamos entonces $\set{μ_n}$ con el funcional acotado λ.

\ppart Sea $e_n = \set{δ_{nj}}_{j ≥ 1}$ para $n = 1, 2, \dotsc$ y sea $μ_n = λ(e_n)$ con $λ ∈ \dual[\ell^1]$. Entonces $\set{μ_n} ∈ \ell^∞$ y si $\set{a_n} ∈ \ell^1$ entonces $λ(\set{a_n}) = \sum_{n≥1} a_n μ_n$. De este modo todo funcional en $\dual[\ell^1]$ se representa por una sucesión acotada y por lo tanto $\dual[\ell^1] = \ell^∞$.

\ppart $\ell^1$ es separable pero $\ell^∞$ no lo es. \hint{Construir en $\ell^∞$ un conjunto no-numerable de sucesiones de norma $1$ y tales que la diferencia de dos de ellas distintas sea también de norma $1$.}

\ppart $\dual[\ell^∞]$ contiene estrictamente a $\ell^1$. \hint{Sea $E$ el subespacio de $\ell^∞$ dado por las sucesiones que tienen límite cuando $n \to ∞$, y sea λ el funcional dado por \extappl{λ}{E}{ℂ}{\set{μ_n}_{n≥1}}{\lim_{n \to ∞} μ_n}. Entonces $λ$ es acotado en $E$, y usar el \nref{thm:HahnBanach} para extenderlo a un funcional acotado en $\ell^∞$. Probar que no existe $\set{a_n} ⊂ \ell^1$ que represente a tal funcional.}

\solution

\spart

Es claro que \[ \abs{λ(\set{a_n})} = \abs{\sum_{n≥1} a_n μ_n} ≤ \sum_{n≥1} \abs{a_n} \abs{μ_n} ≤ \sum_{n ≥ 1} a_n \sup μ_n = \norm{a_n}_1 \norm{μ_n}_∞ \]

\spart

Operando por linealidad de $λ$, vemos que \[ \sum_{n ≥ 1} a_n μ_n = \sum_{n≥1} λ(e_n a_n) = λ \left(\sum_{n≥1} e_n a_n \right) = λ(\set{a_n})\] ya que si sumamos las sucesiones $a_0 e_0 = \set{a_0, 0, \dotsc}$, $a_1 e_1 = \set{0, a_1, 0, \dotsc}$ al final nos sale $\set{a_n}$.

\spart

El conjunto $A ⊂ \ell^∞$ de sucesiones que sólo constan de ceros y unos\footnote{Idea: \href{http://math.stackexchange.com/questions/702199/ell-infty-mathbb-n-is-not-a-separable-space}{Math.SX}.} es no numerable, y dos sucesiones distintas cumplen que $\norm{\set{a_n} - \set{b_n}}_∞ = 1$. Para ver que es no numerable simplemente hay que fijarse que ese espacio de sucesiones es isomorfo a $\parts{ℕ}$ (cada sucesión $\set{a_n}$ se corresponde con un conjunto $B ∈ \mathcal{P}(ℕ)$ donde $k ∈ B \iff a_k = 1$), que es no numerable.

Por otra parte, para demostrar que $\ell^1$ sí es separable, damos el siguiente conjunto\footnote{Idea: \href{http://math.stackexchange.com/questions/745888/i-would-like-to-show-that-ell1-is-separable?rq=1}{Math.SX} de nuevo.} de sucesiones de racionales con una cantidad finita de elementos no nulos: \[ Q = \set{\set{x_n}_{n ∈ ℕ} ⊂ ℚ \tq ∃N ∈ ℕ,\; ∀n ≥ N\, x_n = 0} \]

Para demostrar que podemos acercarnos todo lo posible ($ε > 0$) a un elemento cualquiera $\set{a_n}_{n∈ ℕ} ∈ \ell^1$, tomamos un $N$ tal que $\sum_{n > N} \abs{a_n} < \sfrac{ε}{2}$. Además, como $ℚ \densein ℝ$, tomamos valores $x_j ∈ ℚ$ tales que $\abs{a_j - x_j} < \frac{ε}{2N}$. La sucesión $\set{x_n}$ la construiremos con esos valores y con $x_j = 0$ si $j > N$. En ese caso, lo que tenemos es que \[ \norm{\set{x_n} - \set{a_n}}_{\ell^1} = \sum_{n = 1}^∞ \abs{x_n - a_n} = \sum_{n = 1}^N \abs{x_n - a_n} + \sum_{n > N} \abs{a_n} < N \frac{ε}{2N} + \frac{ε}{2} = ε \] tal y como queríamos.

\spart

Que el funcional es acotado es trivial. Que no existe una sucesión que nos valga es fácil. Definimos $a^k \equiv \set{a_n^k}_{n ≥ 1}$ como la sucesión con los $k$ primeros términos nulos y el resto $1$, por ejemplo: \[ \set{a_n^4}_{n≥ 1} = \set{0, 0, 0, 0, 1, 1, \dotsc }\]

Está claro que $\norm{a^k}_∞ = 1$ y que $λ(a^k) = 1$. Supongamos que ahora existe una sucesión $\set{α_n}_{n ≥ 1} ⊂ \ell^1$ que representa a λ. Pero entonces vemos que, para todo $k$, $λ(a^k) = λ(a^{k+1})$. Si aplicamos la fórmula de \eqref{eq:FuncionalEll} y restamos ambos resultados, tenemos que los $k$ primeros términos se nos van porque son cero, el $k + 1$ se queda por ser distinto en las dos sucesiones, y a partir de ahí se van porque son todos $1$. Es decir:
\begin{align*}
λ(a^k) - λ(a^{k+1}) &= \sum_{j ≥ 1} a^k_j α_j - \sum_{j ≥ 1} a^{k+1}_j α_j \\
0 &= \underbracket{a^{k}_{k+1}}_{1} α_{k+1} - \underbracket{a^{k+1}_{k+1}}_{0} α_{k+1} \\
0 &= α_{k+1}
\end{align*}

Haciendo este procedimiento, tenemos que $α_n = 0\; ∀n ∈ ℕ$, pero está claro que esa sucesión no representa al funcional, luego $λ$ no se corresponde con ningún elemento de $\ell^1$.
\end{problem}

\begin{problem}[13] Sean $E, F$ espacios de Banach y $\appl{a}{E×F}{\kbb}$ una aplicación bilineal tal que \begin{align*}
a(x,·) &∈F^*\;∀x ∈ E \\
a(·,y) &∈E^*\;∀x ∈ F
\end{align*}

Demuestra que entonces existe una constante $C < ∞$ tal que \[ \abs{a(x,y)} ≤ C \norm{x}_E \norm{y}_F \quad ∀x ∈ E,\, ∀y ∈ F \] (en otras términos, bilineal y continua en cada variable implica continua conjuntamente).

\solution

Para la resolución de este ejercicio trataremos de aplicar el principio de acotación uniforme. Sea \[ \mathcal{F} = \set{ a(x, ·) \tq x ∈ \adh{\bola_1}(0) ⊂ E} ⊂ F^* \]

Dados $x ∈ E\setminus\set{0}$ y $w ∈ F$, queremos ver cuánto vale $\abs{a(x,w)}$. Dado que $a(x,·) ∈ F^*$, tendremos que \[ \abs{a(x,w)} ≤ \norm[0]{a(x, ·)}_{F^*} \norm{w}_F \]

Dado que $x ≠ 0$ y $a$ es lineal en $x$, podemos ver que $a(x, ·) = \norm{x}_E a\left(\frac{x}{\norm{x}_E}, ·\right)$ y entonces lo que nos queda es que  \[ \abs{a(x,w)} ≤ \norm{a\left(\frac{x}{\norm{x}_E}, ·\right)}_{F^*} \norm{x}_E \norm{w}_F \]

Podremos estimar esa cantidad tomando entonces el supremo para todos los $z ∈ \adh{\bola_1}(0)$, y entonces tendremos que \( \label{eq:H2:E13} \abs{a(x,w)} ≤ \left(\sup \set{ \norm{a(z,·)}_{F^*} \tq z ∈ \adh{\bola_1}(0) ⊂ E}\right) · \norm{x}_E \norm{w}_F \)

Supongmaos que ese supremo de ahí sea infinito. Como $F$ es completo, usando el \nref{thm:AcotacionUniforme} existiría un cierto conjunto $F_0 ⊂ F$, un $G_δ$ contenido en $F$ y tal que para cualquier $w ∈ F_0$ se tenga que $\sup \set{ \norm{a(z,w)}_{F^*} \tq z ∈ \adh{\bola_1}(0) ⊂ E} = ∞$. Pero dado un $w_0 ∈ F_0$, si consideramos la aplicación $a(·, w_0) ∈ E^*$ tendríamos que $\sup \set{\abs{a(z, w_0)} \tq z ∈ \adh{\bola_1}(0) ⊂ E} = ∞$, contradicción porque entonces $a(·,w_0)$ no sería funcional lineal y continuo en $E^*$.

Así, existe una constante $C < ∞$ que acota uniformemente a todos los operadores y que además vale también para $x = 0$.

\end{problem}


\begin{problem}[14] Sea $X = (C([0,1]), \norm{·}_∞)$ un espacio de Banach y $δ_{x_0} = f(x_0)$ el funcional evaluativo o Delta de Dirac.

\ppart Demuestra que $δ_{x_0} ∈ X^*$ y, de hecho, $\abs{\pesc{δ_{x_0}, f}} = \abs{δ_{x_0}(x)} ≤ \norm{f}_∞$.
\ppart Determinar su núcleo $E ⊂ X$ y probar que es un subespacio cerrado.
\ppart Determinar el espacio cociente $\quot{X}{E}$, ¿es de dimensión finita o infinita?

\solution

\spart

El operador es obviamente lineal y la cota de la norma se cumple trivialmente. Tendríamos problemas si estuviésemos trabajando con $L^∞$ porque ahí no tenemos bien definido el valor de la función en un único punto (dos funciones equivalentes pueden tener valores distintos en un conjunto de medida cero de puntos), pero como estamos con funciones continuas no hay ningún problema.

\spart

El núcleo de $δ_{x_0}$ es $\ker δ_{x_0} = \set{ f ∈ C([0,1]) \tq f(x_0) = 0}$, que es cerrado porque $f(x_0) = 0$ es cerrado y el funcional es continuo.

\spart

Se puede ver que $\quot{X}{\ker δ_{x_0}} \cong ℂ$, ya que las funciones están determinadas por el valor en $x_0$, que es cualquiera dentro de $ℂ$.

\end{problem}

\begin{problem}[15] \label{ej:Hoja2:15} Sea $X$ un \nlref{def:EspacioReflexivo} y $E ⊂ X$ un espacio cerrado. Entonces $E$ es también reflexivo, considerando en $E$ la norma inducida por $X$.

\solution

Ver \fref{prop:SubespacioReflexivo}.

\end{problem}

\begin{problem}[16] Demostrar que
\ppart Si $X$ es uniformemente convexo entonces también lo es estrictamente.
\ppart Si $X = (L^p(Ω,\algbA, μ), \norm{}_2)$ con $p = 1$ ó $p = ∞$ y $(Ω,\algbA, μ)$ un espacio de medida, entonces $X$ no es uniformemente convexo (excluir trivialidades como espacios Ω que consten de un único punto).
\ppart Si $X = (L^2(Ω, \algbA, μ), \norm{}_2)$ con $(Ω, \algbA, μ)$ un espacio de medida, demostrar que para $f,g ∈ X$ entonces \[ \norm{\frac{f+g}{2}}_2^2 + \norm{\frac{f-g}{2}}_2^2 = \frac{\norm{f}_2^2 + \norm{g}_2^2}{2} \]

Deducir que $X$ es uniformemente convezo.
\solution

Primero, las definiciones:

\begin{defn}[Espacio\IS estrictamente convexo] Un espacio de Banach $X$ se dice estrictamente convexo si dados $x, y ∈ X$ con $\norm{x} = \norm{y} = 1$ y $x ≠ y$, entonces $\norm{(1-t)x + ty)} < 1\; ∀t ∈ (0,1)$.
\end{defn}

\begin{defn}[Espacio\IS uniformemente convexo] \label{def:EspacioUnifConvexo} Un espacio de Banach $X$ se dice uniformemente convexo si dados $x, y ∈ X$ con $\norm{x} = \norm{y} = 1$ y $x ≠ y$ y $∀ε > 0$, si $\norm{x-y} > ε$ entonces existe un $δ > 0$ tal que \[ \norm{\frac{x+y}{2}} < 1 - δ\]
\end{defn}

Y un lema que nos será útil.

\begin{lemma} \label{lem:ConvexaMaximo} Sea $\appl{φ}{[a,b]}{ℝ}$ convexa con $φ(a) = φ(b) = L$. Si  $∃c ∈ (a,b)$ con $φ(c) ≥ L$, entonces $φ$ es constante en $[a,b]$.
\end{lemma}

\begin{proof}
Lo primero que vamos a ver es que no puede ser que $φ(c) > L$, sólo que $φ(c) = L$. Dado que $c ∈ (a,b)$ podemos escribirlo como $c = (1-t) a + tb$ para algún $t ∈ (0,1)$.

Como $φ$ es convexa, entonces \[ L ≤ φ(c) = φ((1-t) a + tb) \eqreasonup[≤]{φ convexa} (1-t) φ(a) + tφ(b) = L\], luego $φ(c) = L$.

Sea ahora $a < x < c$, y supongamos que $φ(x) < L$. Como $c ∈ (x,b)$, entonces podemos escribir $c = (1-s)x + sb$ para $s ∈ (0,1)$ y haciendo lo mismo que antes \[ L= φ(c) = φ((1-s)x + sb) = (1-s)φ(x) + sφ(b) < L \], contradicción luego $∀x ∈ (a,c)$ tenemos que $φ(x) = L$. Haciendo lo mismo con $x ∈ (c,b)$ llegaríamos a lo mismo y listos.
\end{proof}

En otras palabras, este lema dice que si la función es convexa (es decir, que se va hacia abajo y luego sube) y en algún punto alcanza $L$, entonces la única posibilidad es que sea constante. Ver dibujito correspondiente.

\spart

Consideramos $φ(t) = \norm{z(t)}$ con $t ∈ [0,1]$ y $z(t) =(1-t)x + ty$. Está claro que $φ(0) = φ(1) = 1$. Vamos a demostrar que φ es convexa.

Sean $0 ≤ t ≤ x ≤ s ≤ 1$. Como  $x ∈ [t,s]$ existe un $α ∈ [0,1]$ con $x = (1-α)t + αs$, de tal forma que $z(x) = (1-α)z(t) + α z(s)$. Entonces, usando la desigualdad triangular \[ φ(x)  ≤ (1-α)\norm{z(t)} + α \norm{z(s)} = (1-α)φ(t) + αφ(s) \] luego efectivamente φ es convexa.

Ahora vemos que si $X$ es uniformemente convexo y $φ(t) ≥ 1$ para algún $0 < t < 1$, aplicando el \fref{lem:ConvexaMaximo} tendríamos que $φ(t) = 1$. En particular, $1 = φ(\sfrac{1}{2}) < 1 - δ$ por ser $X$ uniformemente convexo, lo que es una contradicción por alguna razón.

\spart

\spart

Vemos que \[ \norm{f\pm g}^2_2 = \int_Ω \abs{f\pm g}^2 = \int_Ω (f \pm g)\conj{(f \pm g)} = \int \abs{f}^2 + \abs{g}^2 \pm (g\conj{f} + \conj{g}f)\]

Sumando ambos, se nos va el $\pm$: \[ \norm{f+g}_2^2 + \norm{f-g}_2^2 = 2(\norm{f}_2^2 + \norm{g}_2^2)\]

Si $w > \norm{f-g}_2^2 > ε > 0$ y $\norm{f}_2^2 = \norm{g}_2^2 = 1$, entonces \[ \norm{\frac{f+g}{2}}_2^2 = \frac{\norm{f}_2^2 + \norm{g}_2^2}{2} - \frac{\norm{f-g}_2^2}{4} < 1 - \frac{ε^2}{4} \], y entonces $\norm{\frac{f+g}{2}} < \sqrt{1 - \frac{ε^2}{4}} = 1 - δ$ con $0 < δ = 1 - \left(1 - \frac{ε^2}{4}\right)^{\sfrac{1}{2}}$.

\end{problem}

\begin{problem}[17] El propósito de este ejercicio es probar la convexidad uniforme de $X = (L^p\meas, \norm{·}_p)$, con \meas un espacio de medida y con $1 < p < ∞$.

\ppart Sea $g(x) = (1 + x^2)^{\sfrac{p}{2}} - x^p - 1$ con $x ∈ [0,∞)$. Probar que $g$ es no-decreciente en su dominio y que, en consecuencia, $1 + x^p ≤ (1+x^2)^{\sfrac{p}{2}}$ si $x ≥ 0$

\solution

\end{problem}

\section{Hoja 3}

\begin{problem} Sea $X$ un espacio vectorial sobre $\kbb = ℝ$ ó $ℂ$ y $\appl{p}{X}{ℝ^+}$ una función en $X$ que cumple \begin{align*}
p(tx) &= tp(x) & x ∈ X, t≥ 0 \\
p(x+y) ≤ p(x) + p(y) & x,y ∈ X
\end{align*}

En adelante, a $p(x)$ lo llamaremos \index{Funcional!de gauge}\textbf{funcional de gauge}.

Sea $E ⊂ X$ un subespacio y $\appl{λ}{E}{ℝ}$ un funcional lineal \textit{real} que cumple $λ(x) ≤ p(x)\;∀x ∈ E$. Probar que $λ$ admite una extensión a un funcional lineal real en $X$ tal que $λ(x) ≤ p(x) \;∀x ∈ X$.

\hint{La prueba del \nref{thm:HahnBanach} se puede repetir punto por punto con ligeras modificaciones.}

\solution

\end{problem}

\begin{problem}[2] \label{ej:Hoja3:2}
Sea $X = (V, \norm{·})$ un espacio vectorial sobre $\kbb = ℝ$ ó $ℂ$ y normado. Sea λ un funcional lineal no nulo en $X$. Un hiperplano afín es un subconjunto $E_α$ de $V$ de la forma $E_α = \inv{λ}(α)$ para algún $α ∈ \kbb$ (esto es, una hipersuperficie de nivel de ese funcional). Demostrar:

\ppart $E_α$ es convexo.
\ppart Siempre podemos suponer $α ∈ ℝ$.
\ppart Si λ es continuo, $E_α$ es cerrado.
\ppart Si $E_0$ es cerrado, $E_α$ es cerrado para todo α.
\ppart Si $X' = (V', \norm{·})$ es una compleción de $X$ (esto es, $V ⊂ V'$ con $V$ denso en $V'$ y $X'$ de Banach) y $E_α$ es cerrado en $V$, lo sigue siendo en $V'$.
\ppart Por los apartados anteriores, basta considerar el caso $α = 0$ y $X$ de Banach. Consideremos entonces $\appl{\tilde{λ}}{\quot{X}{E}}{\kbb}$ la aplicación inducida en el cociente. Deducir, del \nref{thm:IsomorfiaBanach}, que $\tilde{λ}$ es un isomorfismo continuo; concluir que el propio λ también lo es.

\solution

\spart

Sean $x, y ∈ E_α$. Para demostrar que $E_α$ es convexo hay que demostrar que $∀t ∈ [0,1]$ se tiene que $(1-t) x + ty  ∈ E_α$. Si $x, y ∈ E_α$ entonces $λ(x) = λ(y) = α$, y por lo tanto \[ λ\left((1-t) x + ty\right) = λ((1-t)x) + λ(ty) = (1-t) λ(x) + t λ(y) = α \], por lo que todo el segmento $[x,y]$ está en $E_α$ y por lo tanto es convexo.

\spart

Si $α ∉ ℝ$, entonces es de la forma $α = r e^{iθ}$ con $r > 0$ y $θ ∈ (0, 2π)$. En ese caso, podemos multiplicar el funcional λ por el escalar $τ = e^{iθ}$, de tal forma que $\tilde{λ}(x) = τλ(x)$ y $\inv{\tilde{λ}}\left(\frac{α}{τ}\right) = \inv{λ}(α)$, con $\frac{α}{τ} = r e^0 ∈ ℝ$.

El funcional $\tilde{λ}$ es simplemente una rotación de λ: sigue manteniendo la misma norma: $\norm[1]{\tilde{λ}(x)} = \norm[1]{e^{iθ} λ(x)} = 1 · \norm{λ(x)}$. Al ser una multiplicación por un escalar sigue manteniendo las propiedades de linealidad de λ.

\spart

Para demostrar que $E_α$ es cerrado, se puede ver que su complementario $E_α^c$ ha de ser abierto. Tomamos $x ∈ E_α^c$ tal que $λ(x) ≠ α$. Sea $ε > 0$ tal que $ε < \abs{λ(x) - α}$. Por ser λ continuo, existe un $δ > 0$ tal que si $y ∈ \bola_δ(x)$ entonces $\abs{λ(x) - λ(y)} < ε < \abs{λ(x) - α}$. Es decir, que $λ(y) ≠ α$ (si fuera igual, ε sería 0 y hemos dicho que es estrictamente positivo) y entonces $y ∉ E_α$.

En resumen, para cualquier $x ∈ E_α^c$ podemos encontrar una bola abierta $\bola_δ(x)$ con $δ > 0$ tal que $\bola_δ(x) ⊂ E_α^c$, luego $E_α^c$ es abierto y por lo tanto $E_α$ es cerrado.

\spart

Si $E_α = ∅$, entonces es cerrado y no hace falta probar nada. Si no es vacío, entonces $∃ a ∈ E_α$ con $λ(a) = α$, y podemos expresar $E_α = E_0 + a$, que es cerrado.

\spart

Vamos a hacer la caracterización por sucesiones, buscando ver que no existen sucesiones cuyo límite está fuera de $E_α$ cuando lo miramos dentro de $V'$. Las únicas sucesiones problemáticas son las que convergen fuera de $V$ (si alguna convergiese con $V$ y no tuviese límite en $E_α$ entonces éste no sería cerrado).

\spart

Recordando el \nref{thm:IsomorfiaBanach}, lo que tenemos es que $E = E_0 = \ker λ$ y por lo tanto tenemos que $\tilde{λ}$ es un isomorfismo lineal y continuo con la norma de $\kbb$.

Por ser $\tilde{λ}$ isomorfismo, tenemos que al menos λ es sobreyectivo. El resto no lo veo claro y no sé si me falta alguna condición.

\end{problem}

\begin{problem} \label{ej:Hoja3:3} Sea $V$ un espacio vectorial sobre $\kbb = ℝ$ ó $ℂ$ y $C ⊂ V$ convexo. Decimos que $C$ es un \index{Espacio!balanceado}\textbf{espacio balanceado} si dados $x ∈ C$, $λ ∈ \kbb$ con $\abs{λ} = 1$ entonces $λx ∈ C$. Diremos que será un \index{Espacio!absorbente}\textbf{espacio absorbente} si $\bigcup_{t > 0} tC = V$. El \index{Funcional!de Minkowsky}\textbf{funcional de Minkowsky} o gauge de $C$ es \( \label{eq:FuncMinkowsky} p(x) ≝ \inf \set{λ > 0 \tq x ∈ λC} \)

Demuestra que:

\ppart Si $t ≥ 0$, $p(tx) = tp(x)$ para el gauge de un conjunto $C$ absorbente con $0 ∈ C$. Si además $C$ es balanceado, entonces $p(tx) = \abs{t} p(x)$ para cualquier escalar $t$.
\ppart Si $C$ es convexo, probar que $p(x+y) ≤ p(x) + p(y)$ para cualesquiera $x,y ∈ V$. \hint{Probar que si $λ,μ > 0$ y $C$ es convexo entonces $λC + μ C = (λ+μ)C$.}
\ppart Si $A = \set{ x ∈ V \tq p(x) < 1}$ y $B = \set { x ∈ V \tq p(x) ≤ 1}$, entonces se cumple que $A ⊂ C ⊂ B$.
\ppart Ver que si $X = (V, \norm{·})$ es normado, $C$ abierto y absorbente y $0 ∈ C$ entonces tenemos que \[
C = \set{x ∈ V \tq p(x) < 1} \qquad \adh{C} = \set{x ∈ V \tq p(x) ≤ 1}
\]

\solution

\spart

Si $C$ es absorbente, su unión es todo el espacio luego para un $α$ suficientemente grande, $x ∈ αC$. Como además $0 ∈ C$, ha de existir un $β ∈ (0, 1]$ $βx ∈ C$, ya que si no existiese nunca podríamos ``hacer crecer'' $C$ y llegar a cubrir todo $V$.

En particular, tomamos β como el supremo de todos los valores que cumplen esa condición. En ese caso, $p(x) = \frac{1}{β}$, y $p(tx) = \frac{t}{β}$ luego la igualdad sale directamente.

Si además $C$ es balanceado, lo que hacemos es lo siguiente: descomponemos $t = ατ$, con $α ∈ ℝ^+$ (de hecho, $α = \abs{t}$) y $\abs{τ} = 1$. Ya hemos visto que $p(αx) = α p(x)$, y como $C$ es balanceado, $τx ∈ C$. Si $p(αx)$ es el ínfimo, entonces $p(ταx) = p(tx)$ tendrá que ser menor o igual (no puede ser mayor porque $αx ∈ (p(αx) + ε)C $ con $ε \to 0$, luego obligatoriamente $tx = ατx ∈ (p(αx) + ε)C$). Haciendo el mismo razonamiento en el otro sentido llegamos a la igualdad.

\spart

Seguimos la sugerencia y vemos que si $λ,μ > 0$ entonces $λC + μC = (λ + μ)C$ con $C$ convexo.

Sea entonces $x ∈ λC + μC$, que será de la forma $x = λc_1 + μc_2$ con $c_1, c_2 ∈ C$. Consideramos ahora $c_0 = \frac{c_1 + c_2}{2}$, que está en $C$ por ser convexo. No sé seguir. % TODO

En el otro sentido es más sencillo: si $x ∈ (λ + μ)C$, entonces $x = (λ + μ)c$ para $c ∈ C$ y es fácil ver que $x = λc + μC$ luego $x ∈ λC + μC$.

Una vez visto esto, se puede probar lo que pide el enunciado viendo que, si $p(x) = λ$ y $p(y) = μ$, entonces\footnote{Voy a simplificar: sé que pone el ínfimo en la definición del gauge pero voy a hacer como si no. Si queremos ponernos formales, en lugar de decir que $x ∈ λC$ hay que sustituir por $x ∈ (λ + ε)C$ para $ε \to 0$, pero en el fondo es lo mismo.} $x ∈ λC$ y $y ∈ μC$, luego $x + y ∈ λC + μC = (λ + μ) C$, por lo que $p(x + y) ≤ λ + μ = p(x) + p(y)$.

\spart

\textit{Voy a suponer que $C$ tiene que seguir siendo convexo, porque entonces es falso de falsísima falsedad.}

Vamos por partes. Si $x ∈ A$, entonces eso significa que existe un $c ∈ C$ tal que\footnote{Ver la nota anterior: sé que es un ínfimo y no tiene por qué ser $x = λc$, pero para la demostración puedo pasarlo por alto porque todo sigue valiendo si lo hacemos formalmente y es menos lío de escribir.} $x = λc$ con $0 < λ < 1$. No sé seguir. % TODO

El otro caso es más sencillo: si $c ∈ C$, entonces en particular $c ∈ 1C$ luego $p(x) ≤ 1$ obligatoriamente.

\spart


\end{problem}

\begin{problem} Sea $X$ como en el \fref{ej:Hoja3:2} y sean $A$, $B$ dos subconjuntos de $V$ no vacíos. Consideramos un hiperplano $E = E_α = \inv{λ}(α)$ con $α ∈ℝ$ para λ un funcional real, tanto si $\kbb = ℝ$ como si es $ℂ$. Decimos que $E$ separa $A$ y $B$ si \begin{align*}
λ(x) ≤ α & \quad ∀x ∈ A \\
λ(x) ≥ α & \quad ∀x ∈ B \\
\end{align*}

Si en cambio para cierto $ε > 0$ se cumple que  \begin{align*}
λ(x) ≤ α - ε & \quad ∀x ∈ A \\
λ(x) ≥ α + ε& \quad ∀x ∈ B \\
\end{align*} decimos que $E$ separa estrictamente $A$ y $B$.

Queremos probar el \nref{thm:HahnBanachGeom1} siguiendo los siguientes pasos.

\ppart Si $E = \inv{λ}(α)$ con $α ∈ ℝ$ es un hiperplano cerrado, entonces los conjuntos \begin{align*}
A_α &= \set{x ∈ V \tq λ(x) ≤ α} \\
B_α &= \set{x ∈ V \tq λ(x) ≥ α}
\end{align*} son ambos convexos y cerrados con frontera común $E$.

\ppart Caso $A$ abierto y $B = \set{x_0}$. Probar que existe un funcional real y continuo λ con $λ(x) < λ(x_0)\;∀x ∈ A$. En particular, $E = \inv{λ}(x_0)$ separa $A$ y $\set{x_0}$.  \hint{Reducirlo al caso $0 ∈ A$, y entonces usar el gauge del abierto convexo $A$ y los ejercicios anteriores. Probar asimismo que si $\bola_r(0) ⊂ C$ para un $r > 0$, entonces $p(x) ≤ \frac{1}{r} \norm{x}\;∀x ∈ V$ y que $C ∩ (-C) ⊂ \inv{λ}[-1,1]$, lo cual implica la continuidad de $λ$.}

\solution

Reproducimos el enunciado del teorema:

\begin{theorem}[Teorema\IS de Hahn-Banach (Primera forma geométrica)] \label{thm:HahnBanachGeom1} Sea $X = (V, \norm{·})$ un espacio vectorial sobre $\kbb = ℝ$ ó $ℂ$ normado, y sean $A,B ⊂ V$ convexos, disjuntos y uno de ellos abierto. Entonces, existe un hiperplano cerrado $E$ que los separa.
\end{theorem}

\spart

Veamos que son convexos: sean $x, y ∈ A_α$ y $t ∈ [0,1]$. Queremos ver que $tx + (1-t)y ∈ A_α$. Operamos y vemos que \[ λ(tx + (1-t)y) = tλ(x) + (1-t) λ(y) ≤ tα + (1-t) α = α \] y se puede probar análogamente que $B_α$ es convexo.

Para ver que son cerrados, sea $\set{x_n}_{n ∈ ℕ} ⊂ A_α$ de Cauchy con $x_n \to x ∈ V$. Queremos ver que $x ∈ A_α$. Sabemos que $\set{λ(x_n)}_{n ∈ ℕ}$ es igualmente de Cauchy en $ℝ$ así que $α ≥ c \lim λ(x_n) = λ(\lim x_n) = λ(x)$, listos.

Lo de la frontera me parece demasiado obvio.

\spart

% LOLWTF. TODO

\end{problem}

\begin{problem}[4] Pretendemos probar la segunda versión del teorema anterior, siguiendo los siguientes pasos que siguen seguidamente:

\ppart Sea $A' = A - B$. Probar que $A'$ es convexo y cerrado y que $0 ∉ A'$. Concluir que $∃r > 0$ tal que $\bola_r(0) ⊂ (A')^c$.
\ppart Usando el \nref{thm:HahnBanachGeom1}, demuestra que $\bola_r(0)$ y $(A')^c$ pueden ser separados por un hiperplano y por tanto existe un funcional real, continuo y no nulo $λ$ con \[ λ(u) ≤ λ(v) \quad ∀u ∈ A', \; ∀v ∈ \bola_r(0) \]

\solution

Reproducimos el enunciado del teorema.

\begin{theorem}[Teorema\IS de Hahn-Banach (Segunda forma geométrica)] \label{thm:HahnBanachGeom2} Sea $X = (V, \norm{·})$ un espacio vectorial sobre $\kbb = ℝ$ ó $ℂ$ normado, y sean $A,B ⊂ V$ convexos, disjuntos, con $A$ cerrado y $B$ compacto. Entonces existe un funcional real y continuo $λ$ que los separa estrictamente.
\end{theorem}

\spart

Vamos a ver primero que $A'$ es convexo. Sean $x_1 = a_1 - b_1$ y $x_2 = a_2 - b_2$ elementos de $A'$, con $a_i ∈ A$ y $b_i ∈ B$. Vamos que ver que $∀t ∈ [0,1]$ se tiene que $tx_1 + (1-t) x_2$. Lo tenemos simplemente operando: \[ tx_1 + (1-t)x_2 = t a_1 - tb_1 + (1-t)a_2 - (1-t)b_2 = \underbracket{ta_1 + (1-t)a_2}_{∈ A} - \underbracket{\left(tb_1 + (1-t)b_2\right)}_{∈ B} \]

Para ver que es cerrado, tomamos $\set{x_n}_{n ∈ ℕ}$ una sucesión de Cauchy en $A'$ que converja a $x ∈ V$. Podemos escribir entonces cada elemento como $x_n = a_n - b_n$. Como $B$ es compacto, podemos encontrar una subsucesión $\set{b_{n_j}}_{j ≥ 1}$ convergente a $b ∈ B$. La subsucesión $\set{x_{n_j}}_{j ≥ 1}$ seguirá convergiendo a $x$. Ahora vemos que $a_{n_j} = x_{n_j} + b_{n_j} \convs[][j] x + b$. Como $\set{a_{n_j}}_{j ≥ 1}$ es una sucesión convergente en $A$ y $A$ es cerrado, entonces $a = x + b ∈ A$, y por lo tanto $x = a - b ∈ A'$.

Por último, es fácil ver que $0 ∉ A'$. Si estuviese, $0 = a - b \implies a = b$, luego $A$ y $B$ no serían disjuntos. Juntando esto con que $A'$ es cerrado, tenemos que podemos encontrar un $r  > 0$ tal que $\bola_r(0) ∩ A' = ∅$.

\spart

Lo cierto es que no tengo claro que pueda usar el teorema anterior porque veo muy, muy dudoso que $(A')^c$ sea convexo.

\end{problem}

\begin{problem}[5] \label{ej:Hoja3:5} Sea $E ⊂ V$ con $X = (V, \norm{·})$ un espacio normado. Demostrar que si $E$ es convexo, su cierre en la topología débil y en la fuerte coinciden. \hint{Basta probar que si $\adh{E}$ es el cierre de $E$ en la topología fuerte, su complementario $E^c$ es abierto en la topología débil, para lo cual basta aplicar el \nref{thm:HahnBanachGeom1}}

Probar, sin embargo, que la afirmación anterior es falsa si $\dual$ es un espacio dual en el cual damos la topología débil-$*$. \hint{¿Qué ocurre con $\bola_X$ si $X$ no es reflexivo?}

\solution

% TODO

\end{problem}

Enunciamos el siguiente lema para probarlo en el siguiente ejercicio.

\begin{lemma}[Lema\IS de Riemann-Lebesgue] \label{lem:RiemannLebesgue} Dado $λ ∈ ℝ$, definimos $e_λ(x) ≝ e^{iλx}$ con $x ∈ [0,1]$. Entonces $e_λ(x) ∈ L^p[0,1]$ cuando $1 ≤ p ≤ ∞$ y además $e_λ \wconvs[\abs{λ}] 0$.
\end{lemma}

\begin{problem}[8] \hard Demuestra el \nref{lem:RiemannLebesgue} en los siguientes pasos.

\ppart Probar que, efectivamente, $e_λ ∈ L^p[0,1]$ para $1 ≤ p ≤ ∞$ y que $\norm{e_λ}_p = 1$.
\ppart Sea \[ X = \set{ \appl{g}{[0,1]}{\kbb} \tq g∈C^1([0,1]),\,g(0) = g(1) = 0} \] el espacio de funciones 1-periódicas y continuas. Entonces, demuestra que \( \pesc{e_λ,g} = \int_0^1 e_λ(x) g(x) \dif x \convs[][\abs{λ}] 0 \label{eq:RiemannLebesgue:1} \)

\hint{Usar una integración por partes.}

\ppart Si $p = 1$, entonces $e_λ ∈ \dual[L^1] = L^∞$ y $\norm{e_λ}_∞ = 1$ para cualquier λ.
\ppart Si $p = 1$, entonces $X$ es denso en $L^1[0,1]$. Deducir de esto y de \eqref{eq:RiemannLebesgue:1} que $\pesc{e_λ, g} \convs[][\abs{λ}] 0$ para toda $g ∈ L^1$, esto prueba la afirmación para el caso $p = 1$.
\ppart Si $1 < p ≤ ∞$, usar que $L^p([0,1]) ⊂ L^1([0,1])$ para deducir el caso general.
\ppart Probar que si $p = 1$ y consideramos $L^1(ℝ)$ se sigue verificando que $e_λ \convs[][\abs{λ}] 0$.

\solution

\spart

Es trivial ver que $\abs{e_λ(x)} = 1$ y por lo tanto la norma $\norm{e_λ}_p = 1$ $∀p, λ$.

\spart

Siguiendo la sugerencia, integramos por partes. Tomamos $u,v$ de la siguiente forma:
\begin{align*}
u = g &\implies \dif u = g' \dif x\\
\dif v = e^{iλx} \dif x &\implies v = \frac{1}{iλ} e^{iλx}
\end{align*}

Así se nos simplifica la integral:
\[
\int_0^1 e^{iλx} g(x) \dif x = \eval{g(x) \frac{1}{iλ} e^{iλx}}_{x = 0}^1 - \int_0^1 \frac{1}{iλ} e^{iλx} g'(x) \dif x = - \int_0^1 \frac{1}{iλ} e^{iλx} g'(x) \dif x\]

Acotamos esa última integral en valor absoluto. Como $g ∈ C^1$, tenemos que $\sup_{x ∈ [0,1]} \abs{g'(x)} = M < ∞$, y $\abs{e^{iλx}} = 1$ luego \[ \abs{\int_0^1 \frac{1}{iλ} e^{iλx} g'(x) \dif x } ≤ \int_0^1 \frac{M}{\abs{i}\abs{λ}} \dif x = \frac{M}{\abs{λ}} \] que es obvio que se va a $0$ cuando $\abs{λ} \to ∞$.

\spart

Ya demostrado directamente en el primer apartado.

\spart

Como $X$ es denso en $L^1([0,1])$, para todo $ε > 0$ podemos encontrar una $g_ε ∈ X$ tal que $\norm{g - g_ε}_1 < ε$. Ahora operamos aprovechando la \nlref{prop:DesigualdadScharwz}: \[ \pesc{e_λ, g} = \pesc{e_λ, g - g_ε + g_ε} = \underbracket{\pesc{e_λ, g - g_ε}}_{≤ \norm{e_λ}\norm{g - g_ε} = ε} + \underbracket{\pesc{e_λ, g_ε}}_{\to 0} \to 0 \]

\spart

Pues eso.

\spart

Si consideramos el espacio $C_c^1(ℝ)$ de funciones derivables de soporte compacto podemos seguir aplicando la integración por partes y el argumento de densidad y todo se mantiene.

\end{problem}

\begin{problem} \label{ej:Hoja3:9} Sea $\dual$ un espacio de Banach dual con predual $X$ separable. Sea $\set{x_n}_{n≥1}$ una sucesión en $X$ acotada, tal que $∃ C < ∞$ con $\norm{x_n}_{\dual} ≤ C\;∀n ≥ 1$.

Demuestra que entonces existe una subsucesión $\set{x_{n_j}}_{j ≥ 1}$ y un $x ∈ X$ tal que $x_{n_j} \wconvs[j] x$. \hint{Recordar que $\bola_X$ es compacta y metrizable en la topología débil-$*$.}

\solution

Nos podemos traer la sucesión a la bola de la forma $z_n = \frac{x_n}{\norm{x_n}} ∈ \bola_X$. Como dice el enunciado, como $\bola_X$ es compacta y metrizable con la topología débil-$*$, ahí encontramos una subsucesión convergente y deshacemos la transformación.
\end{problem}

\begin{problem} Sea $X$ un espacio reflexivo. Queremos probar un resultado análogo al del \fref{ej:Hoja3:9}, esto es, si $\set{x_n}_{n≥ 1}$ es una sucesión en $X$ acotada, entones existe una subsucesión $\set{x_{n_j}}_{n ≥ 1}$ convergente débilmente a $x$ en los sigueintes pasos.

\solution

\end{problem}

\begin{problem}[13] Sea $X = (L^∞(ℝ), \norm{·}_∞)$. Sabemos que $X$ es el dual de $(L^1(ℝ), \norm{·}_1)$.

\ppart Sea \[ C_0(ℝ) = \set{\appl{f}{ℝ}{ℂ} \tq f\text{ continua}, \lim_{\abs{x} \to ∞} f(x) = 0} \]

Demostrar que es un subespacio cerrado y separable de $X$, aunque no denso en $X$. \hint{Explicar por qué $X$ no es separable.}

\ppart Dada $f ∈ L^∞$, definimos\footnote{Recordamos que $\ind_A(x)$ es $1$ si $x ∈ A$ y 0 en otro caso.} $f_n ≝ f \ind_{[-n, n]}$. Demostrar que $f_n \wconvs f$. Decudir que el espacio $Y = \set{f ∈ L^∞(ℝ) \tq \sop f \text{ compacto}}$ es denso en $X$ con la topología débil-$*$.

\ppart Fijemos $0 ≤ θ ∈ C_c^1(ℝ)$ una función diferenciable y no-negativa de soporte compacto, con $\norm{θ}_1$, y consideramos la correspondiente \nlref{def:AproxIdentidad} $\set{θ_ε}_{ε > 0}$, con $θ_ε (x) ≝ \inv{ε}θ(\inv{ε}x)$. Entonces las funciones $θ_ε$ tienen las mismas propiedades que θ.

Definimos, para $f ∈ Y$ y $ε > 0$, la convolución $f_ε ≝ θ_ε * f$ dada por \[ f_ε(x) = \int_{ℝ} f(x-y) θ_ε(y) \dif y \]

Demuestra que $f_ε \wconvs[ε][0^+] f$, que $f_ε ∈ C_0(ℝ)$ y concluir que $C_0(ℝ)$ es denso en $L^∞(ℝ)$ con la topología débil-$*$.

\ppart Explicar por qué la densidad de $C_0(ℝ)$ en $L^∞(ℝ)$ con la topología débil-$*$ implica que tal topología no es metrizable, a pesar de que su restricción a la bola unidad sí lo sea.

\solution

\spart


Para demostrar que es cerrado, tomamos $\set{f_n}_{n ≥ 1} ⊂ C_0(ℝ)$ una sucesión de Cauchy que convergerá $f_n \convs f ∈ L^∞$, y buscamos demostrar que $f ∈ C_0(ℝ)$.

En la topología de la norma $\norm{·}_∞$, la convergencia es la convergencia uniforme, luego si $f_n$ son continuas convergerán a una función continua, así que $f$ es continua.

Por otra parte, como $f_n \convs[L^∞] f$ y tanto $f_n$ como $f$ son continuas, sabemos que
\begin{multline*}
	0 = \sup_{x ∈ X} \abs{f(x) - f_n(x)}
	≥ \limsup_{\abs{x} \to ∞} \abs{f(x) - f_n(x)}
	≥ \\ ≥ \abs{
		\limsup_{\abs{x} \to ∞} \abs{f(x)}
		- \lim_{\abs{x} \to ∞} \abs{f_n(x)}
	} ≥ \limsup_{\abs{x} \to ∞} \abs{f(x)}
\end{multline*}

Así, $f(x)$  ha de converger a $0$ cuando $\abs{x} \to ∞$: si no lo hiciese, entonces $\limsup_{\abs{x} \to ∞} \abs{f(x)}$ sería mayor que cero y tendríamos contradicción con la ecuación anterior.

En conclusión, como $f_n \convs[L^∞] f ∈ C_0(ℝ)$ para cualquier sucesión de Cauchy $\set{f_n}_{n ≥ 1}$, entonces $C_0(ℝ)$ es cerrado.

Para ver que $C_0(ℝ)$ es separable, partimos primero de que el Teorema de aproximación de Stone-Weierstrass dice que $ℚ[x]$ es un subconjunto denso y numerable de $C([-r,r])$ para $r ∈ ℚ^+$. Consideramos ahora el siguiente conjunto: \[ \mathcal{Q}_{τ} = \set{ρ_τ * (q · \ind_{[-r, r]}) \tq q ∈ ℚ[x],\, r ∈ ℕ} \] con $τ > 0$ fijo y $ρ_τ$ un regularizador apropiado que haga los $(q · \ind_{[-r, r]})$ continuos y tales que \[ \norm{ρ_τ*(q · \ind_{[-r, r]}) - q · \ind_{[-r, r]}}_∞ < τ \]

Finalmente, construiremos el espacio de todos esos polinomios: $\mathcal{Q} = \bigcup_{τ ∈ ℚ^+} \mathcal{Q}_τ$. Es claro que $\mathcal{Q}$ es numerable por serlo $ℚ[x]$, y que además $\mathcal{Q} ⊂ C_0(ℝ)$.

Vamos a dar ahora la forma de aproximar una $f ∈ C_0(ℝ)$ a una distancia dada de $ε > 0$. Como $f$ se desvanece en infinito, existe un $c ∈ ℚ$ tal que $\norm{f - f \ind_{[-c, c]}}_∞ < \sfrac{ε}{3}$. Por comodidad, denotamos ahora $g = f \ind_{[-c, c]}$.

El Teorema de aproximación de Stone-Weierstrass asegura la existencia de un polinomio $q ∈ ℚ[x]$ tal que $\norm{g - q}_∞ < \sfrac{ε}{3}$ en $C([-c, c])$. Si en lugar de $q$ tomamos la regularización y restricción $ρ_{\sfrac{ε}{3}} * (q · \ind_{[-N, N]})$, tendremos que $\norm{g - ρ_{\sfrac{ε}{3}} * (q · \ind_{[-N, N]})}_∞ < \frac{2ε}{3}$. Finalmente, como $\norm{f - g}_∞ < \frac{ε}{3}$, podemos sustituir $g$ por el polinomio regularizado y tenemos que \[\norm{f - ρ_{\sfrac{ε}{3}} * (q · \ind_{[-N, N]})}_∞ < ε \]

Ahora bien, $C_0(ℝ)$ no es denso en $X$ simplemente porque $f(x) = c ∈ ℝ\setminus\set{0}$ está en $L^∞$. Dada cualquier $g ∈ C_0(ℝ)$, como $\lim_{\abs{x}\to ∞} g(x) = 0$, siempre tendremos que $\norm{f - g} ≥ c$ y no habrá forma de aproximar $f$.

\spart


Para tratar la convergencia débil-$*$, consideramos un $g ∈ L^1(ℝ)$, ya que $\dual[L^1(ℝ)] = L^∞(ℝ)$. Vamos a ver que $\pesc{f - f_n, g} \to 0$. Operando \[ \pesc{f - f_n, g} = \int_{ℝ} \underbracket{(f - f_n)}_{= 0 \text{ en } [-n, n]} g \dif μ = \int_{-∞}^{-n} (f - f_n) g \dif μ + \int_{n}^{∞} (f - f_n) g \dif μ \]

Así, podemos sustiuir $g$ por $γ_n = g · (1 - \ind_{[-n, n]})$ y $\pesc{f - f_n, g} = \pesc{f - f_n, γ_n}$, que está acotado en valor absoluto por $\norm{f - f_n}_∞ \norm{γ_n}_1$. Es obvio que $\norm{f - f_n}_∞ ≤ \norm{f}_∞$, luego querremos demostrar que $\norm{γ_n}_1 \to 0$. Pero para eso simplemente vemos que \[ \norm{g}_1 = \int_{ℝ} \abs{g} \dif μ = \int_{ℝ} \abs{g} \ind_{[-n, n]} \dif μ + \norm{γ_n}_1\]

Como $\int_{ℝ} \abs{g} \ind_{[-n, n]} ≥ 0$ tiende a $\norm{g}_1$, sólo queda que $\norm{γ_n}_1 \to 0$, por lo que $\abs{\pesc{f - f_n, g}} \to 0$ y $f_n \wconvs f$.

Dado que $f_n$ es obviamente de soporte compacto y hemos demostrado que podemos aproximar cualquier función $f$ tanto como queramos con esas funciones, queda claro que $Y = \set{f ∈ L^∞(ℝ) \tq \sop f \text{ compacto}}$ es denso en $X$ con la topología débil-$*$.

\spart

En primer lugar probamos que $f_ε$ es continua. Dado un $η > 0$, sean $a, b ∈ ℝ$. Vamos a buscar una restricción en la distancia de $a - b$ para dar la continuidad. Para ello, usamos que $f * θ_ε = θ_ε * f$ y operamos:
\begin{align*}
\abs{f_ε(a) - f_ε(b)} &= \abs{\int_{ℝ} f(y) θ_ε(a - y) \dif y - \int_{ℝ} f(y) θ_ε(b - y) \dif y} = \\
	&= \abs{\int_{ℝ}\left[θ_ε(a-y) - θ_ε(b-y)\right] f(y) \dif y}  \\
	&≤ \int_{ℝ} \abs{θ_ε(a-y) - θ_ε(b-y)} \abs{f(y)} \dif y
\end{align*}

Como $θ_ε$ es continua, dado $\tilde{η} = \frac{η}{\norm{f}_1 \abs{f(y)}}$ podemos encontrar un $δ > 0$ tal que si $\abs{(a-y) - (b-y)} = \abs{a-b} < δ$ entonces $\abs{θ_ε(a-y) - θ_ε(b-y)} < \tilde{η}$. Como $f ∈ L^∞$ es de soporte compacto, está claro que $f ∈ L^1 ∩ L^∞$. Además, podemos hacer la división por $\abs{f(y)}$ sin problemas: $f$ está esencialmente acotada, luego si vale infinito lo hace en un conjunto de medida cero que no afecta a la integral. Por otra parte, no hace falta considerar los puntos que $f(y) = 0$, ya que están fuera del soporte y tampoco afectan a la integral.

Así, podemos continuar y ver que \[ \int_{ℝ} \abs{θ_ε(a-y) - θ_ε(b-y)} \abs{f(y)} \dif y ≤ \int_{\sop f} \frac{η}{\norm{f}_1\abs{f(y)}} \abs{f(y)} \dif y = η \] que es precisamente lo que queríamos demostrar, luego $f_ε$ es continua.

Falta ver que $\lim_{\abs{x} \to ∞} f_ε (x) = 0$. Por la expresión de la convolución, $(f * θ_ε)(x) ≠ 0$ si y sólo si $(x + \sop f) ∩ \sop θ_ε ≠ ∅$. Cuando $x \to \pm ∞$, $(x + \sop f)$ y $\sop θ_ε$ acabarán siendo disjuntos y $(f* θ_ε)(x) = 0$, por lo que entonces $f_ε ∈ C_0(ℝ)$.

Finalmente, como con la topología débil-$*$ $\adh{C_0(ℝ)} = Y$ y $\adh{Y} = X$, $\adh{Y}$ es el cerrado más pequeño que contiene a $Y$, y como $\adh{C_0(ℝ)}$ es cerrado ha de ser $\adh{C_0(ℝ)} = X$, esto es, $C_0(ℝ)$ es denso en $X$ con la topología débil-$*$.

\spart

En el apartado anterior hemos visto que $C_0(ℝ)$ es denso en $L^∞$ si lo consideramos con la topología débil-$*$, pero en el primer apartado habíamos visto que no es denso en $L^∞$ con la topología de la norma. Como todas las topologías métricas son equivalentes, se sigue directamente que la topología débil-$*$ no puede ser metrizable porque si no $C_0(ℝ)$ no podría ser denso en una y no serlo en otra.

\end{problem}

\section{Exámenes resueltos}

\subsection{Junio 2015}

\begin{problem} Sea $X = (C^1([0,1]), \norm{·}_X)$ y $\norm{f}_X = \norm{f}_∞ + \norm{f'}_∞$.

\ppart Demuestra que $X$ es de Banach.
\ppart Sea $φ ∈ L^1([0,1])$ y \( λ_φ (f) = \pesc{φ, f} = \int_0^1 φ f' \dif μ \label{eq:Junio2015:1} \)

Demostrar que $λ_φ ∈ X^*$ y la estimación \[ \abs{λ_φ(f)} ≤ \norm{φ}_{L^1} \norm{f}_X \]

\ppart Sea $λ ∈ X^*$ tal que $\pesc{λ, f} = f'(0)$. Ver que efectivamente $λ$ es continuo pero no de la forma dada en \eqref{eq:Junio2015:1}.

\ppart Sea $V = \set{f ∈ C^1([0,1]) \tq f(0) = f'(0) = 0 }$. Ver que $V$ es un subespacio cerrado de $X$ y calcular la dimensión del espacio cociente $\quot{X}{V}$.

\solution

\spart

Basta ver que dada una sucesión $\set{f_n}_{n ≥ 1}$ de Cauchy en $X$ converge a $f ∈ X$. Definimos el límite como el límite puntual de las funciones: \[ f(x) ≝ \lim_{n \to ∞} f_n(x) \]

Como la sucesión es de Cauchy en $X$, tenemos que \[ 0 = \lim_{m,n \to ∞} \norm{f_m - f_n}_X = \lim_{m,n \to ∞} \left(\norm{f_m - f_n}_∞ + \norm{f_m' - f_n'}_∞ \right) \] lo que a su vez implica que ambos sumandos tienen límite cero por separado.

Entonces lo que tenemos es tanto las funciones como sus derivadas convergen uniformemente en $[0,1]$, y por lo tanto el límite existe y es una función continua: $f ∈ C([0,1])$.

Falta ver que $f$ es diferenciable. Como $f_n'$ es una sucesión que converge uniformemente, podemos decir que $∃g ∈ C([0,1])$ tal que $f_n' \to g$ uniformemente en $[0,1]$, y queremos ver que $g = f'$.

Para ello, usamos el Teorema Fundamental del Cálculo, y reescribimos $f_n(x)$ con $0 ≤ x ≤ 1$ como \[ f_n(x) = \underbracket{f_n(0)}_{\to f(0)} + \int_0^x \overbracket{f_n'(t)}^{\to g(t)} \dif t \to f(x) \]

Así, dado $x ∈ [0,1]$ tenemos que \[ f(x) = f(0) + \int_0^x g(t) \dif t \] con $g ∈ C([0,1])$, y por el Teorema Fundamental del Cálculo, vemos que la derivada $f'$ es precisamente $g(x)$.

\spart

Es obvio ver que $λ_φ$ es lineal, y la acotación también sale directamente: \[ \abs{λ_φ(f)} = \abs{\int_0^1 φf'} ≤ \int_0^1 \abs{φ} \underbracket{\abs{f'}}_{≤ \norm{f'}_∞} ≤ \norm{f}_X \int_0^1 \abs{φ} = \norm{φ}_1 \norm{f}_X \] luego $λ_φ ∈ X^*$.

\spart

Es obviamente lineal y obviamente acotado por la definición de la norma, que implica que la derivada (que es continua) está acotada.

Supongamos ahora que existe una función $φ ∈ L^1([0,1])$ tal que $\pesc{λ, f} = \int_0^1 φ f'$ para toda $f ∈ X$. Vamos a tratar de ver que $\essop φ = ∅$ (equivalentemente, $φ(x) = 0$ en casi todo punto de $[0,1]$).

Supongamos que dado $δ > 0$ existe un $A ⊂ [δ,1 - δ]$ medible y con $μ(A) > 0$ tal que $φ(x) ≠ 0$ si $x ∈ A$. Buscaremos csonstruir una función $f ∈ C([0,1])$ que cumpla \[ \begin{cases}
φ(x) f'(x) ≥ 0 & ∀ x ∈ [0,1] \\
φ(x) f'(x) > 0 & x ∈ A \\
f'(0) = 0
\end{cases} \]

Podemos tomar $f' = \sign φ · \ind_{A}$, de tal forma que $f'(0) = 0$, y $f'φ = φ \sign φ \ind_A = \abs{φ} \ind_A > 0$ y por lo tanto la integral es positiva, tendríamos una contradicción.

Lo malo es que $f'$ no es continua, aunque eso no es problema: podemos suavizarla con un regularizador o mollifier con la norma tan cercana a $f'$ como queramos y listos.

\spart

Tanto $f(0)$ como $f'(0)$ se pueden considerar dos funcionales continuos, y sus núcleos son subespacios cerrados\footnote{Lol wtf.}, luego la intersección de ambos es $V$ que será cerrado.

Para ver la dimensión del espacio cociente aplicamos el \nref{thm:IsomorfiaBanach}. Sea \extappl{π}{X}{ℂ^2}{f}{(f(0), f'(0))} que es claramente sobreyectiva\footnote{Si $(a,b) ∈ ℂ^2$, damos $f = bx + a$ por ejemplo.}. Por otra parte, $\ker π = V$ obviamente, así que el Teorema de isomorfía nos dice que $\quot{X}{V} \cong ℂ^2$, luego la dimensión es $2$.
\end{problem}

\begin{problem} Sea $X$ un espacio de Banach y \[ E = \adh{\bola_R(x_0)} = \set{ x∈ X \tq \norm{x - x_0} ≤ R} \]

\ppart Probar que $E$ es convexo y cerrado.
\ppart Sea $X = L^1([0,1])$ y $E = \set{f ∈ L^1([0,1]) \tq \norm{f - 2}_1 ≤ 1}$. Calcular la norma mínima de un elemento de $E$ y ver que se alcanza el mínimo.
\ppart Demostrar que el $E$ anterior contiene una infinidad de elementos de norma mínima.
\ppart Sea $X = C([0,1])$ con $\norm{f}_X = \norm{f}_∞$. Sea  \[ E = \set{f ∈ X \tq \int φf = 1 } \qquad φ = \ind_{[0, \sfrac{1}{2}]} - \ind_{[\sfrac{1}{2}, 1]} \]

Demostrar que $E$ es cerrado y convexo y $\inf_{f ∈ E} \norm{f}_∞ = 1$.

\ppart Probar que en $E$ dado como en el apartado anterior no hay elementos de norma $1$.

\solution

\spart

Para ver que es convexo, tomamos $x,y ∈ E$ y $t ∈ [0,1]$. Entonces es fácil ver que \[ \norm{tx + (1-t)y - x_0} ≤ t\norm{x - x_0} + (1-t) \norm{y - x_0} ≤ tR + (1-t) R = R \] luego $tx + (1-t)y ∈ E$.

Para ver que es cerrado, lo mejor es ver que su complementario es abierto. Para eso, vemos que si $x ∈ E^c$, entonces $\norm{x - x_0} > R$ así que tomamos $0 < δ < \abs{\norm{x - x_0}  - R}$ y es obvio ver que $\bola_δ(x) ∩ E = ∅$.

\spart

Sea $f ∈ E$, entonces \[ \norm{f}_1 = \int_0^1 \underbracket{\abs{(f - 2) + 2}}_{ ≥ 2 - \abs{f - 2}} ≥ 2  - \underbracket{\int_0^1 \abs{f - 2}}_{ ≤ 1} ≥ 2 - 1 = 1  \]

Además, tomando $f = 1$ es claro que $f ∈ E$ y $\norm{f}_1 = 1$, luego el mínimo se alcanza.

\spart

Sea $f ∈ E$ con $\norm{f - 2}_1 = 1$ y $\abs{f} = 2 - \abs{f - 2}$. En particular, si $0 ≤ f ≤ 2$ esa última desigualdad se cumple.

Fijando $A ⊂ [0,1]$ medible y con $μ(A) = \sfrac{1}{2}$, tomamos $f = \ind_A - \ind_{A^c} + 1$, es fácil ver que $0 ≤ f ≤ 2$, y a su vez $\abs{f - 2} = 1$ luego $f ∈ E$.

\spart

Como $E = \inv{λ_φ}(1)$ con $φ ∈ L^1[0,1]$, se deduce inmediatamente que $E$ es convexo y cerrado (ver \fref{ej:Hoja3:2}).

Para lo del ínfimo, sabemos que $\abs{\int_0^1 φf} ≤ \norm{φ}_1 \norm{f}_∞$.  Sabiendo que $\norm{φ}_1 = 1$ e imponiendo la condición del enunciado de $\int_0^1 φf = 1$, tenemos que $1 ≤ \norm{φ}_1 \norm{f}_∞ ≤ \norm{f}_∞$.

Si escogemos $f_0 = φ$, tendríamos que $\int_0^1 φ f_0 = 1$. El problema es que $f_0$ no es continua, aunque de nuevo podemos suavizarla un poquito y hacerla continua, de tal forma que $f_ε \to f_0$ y por lo tanto el ínfimo es $1$.

\spart

Supongamos que hay elementos de norma $1$ en $E$. Si $f ∈ E$ minimiza el ínfimo del apartado anterior, entonces $\abs{f(x)} ≤ 1 $ para todo $x ∈ [0,1]$. Ahora operamos con la integral: \[ 1 = \int_0^1 φ f = \int_0^{\sfrac{1}{2}} f - \int_{\sfrac{1}{2}}^1 f \]

En valor absoluto, esas dos integrales son $≤ \sfrac{1}{2}$. La única posibilidad entonces es que valgan $\sfrac{1}{2}$ y $\sfrac{-1}{2}$ respectivamente, y eso llevaría a que $f = φ$, que no es continua.
\end{problem}

\begin{problem} Comprobar que si $X$ es Banach y $F ⊂ X$ es un subespacio cerrado, entonces $x_0 ∈ \adh{F}$ si y sólo si todo funcional $φ ∈ \dual$ que se anula en $F$ también se anula en $x_0$ siguiendo los siguientes pasos.

\ppart Sea $φ ∈ \dual$ que se anula en $F$. Demostrar que si $x_0 ∈ \adh{F}$ entonces $φ(x_0) = 0$.

\ppart Sea $F ≠ X$, fijamos $x_0 ∉ F$ y tomamos $F' = F + \spn \set{x_0}$, donde $F \subsetneq F'$ y \[ F' = \set{x + λ x_0 \tq x ∈ F, \, λ ∈ \kbb } \]

Definimos $φ$ en $F'$ como $φ(x + λx_0) = λ$. Probar que $φ$ es lineal.

\ppart Si $x_0 ∉ \adh{F}$, probar que $∃ δ > 0$ tal que $\norm{x_0 - x} ≥ δ$ para todo $x ∈ F$.

\ppart Deducir del apartado anterior que el funcional definido en el apartado \textbf{b)} es acotado si $x_0 ∉ \adh{F}$. De hecho, \( \abs{φ(x + λx_0)} ≤ \frac{1}{δ} \norm{ x + λx_0} \label{eq:Junio2015:Acotacion} \)

\ppart Deducir la afirmación recíproca\footnote{¿Recíproca de qué?} usando el \nref{thm:HahnBanach}

\solution

\spart

Como $x_0 ∈ \adh{F}$ construimos una sucesión de Cauchy $\set{x_n}_{n ∈ ℕ} ⊂ F$ con $x_n \to x_0$. Como $φ$ es continuo en $X$, entonces $φ(x_n) \to φ(x_0)$ y como $φ(x_n) = 0\;∀n$ pues simplemente $φ(x_0) = 0$.

\spart

Simplemente vemos que $φ$ es lineal porque la descomposición de un $y ∈ F'$ como $y = x + λx_0$ con $x ∈ F$ es única (porque $x_0 ∉ F$).

\spart

Obvio porque es una reformulación de que $\adh{F}^c$ es abierto.

\spart

Vemos que $\abs{φ(x + λx_0)} = \abs{λ}$. Si $λ = 0$, ciertamente \eqref{eq:Junio2015:Acotacion} se cumple. Si $λ ≠ 0$, entonces \begin{align*}
\norm{x + λx_0} &= \norm{λ(x_0 + \inv{λ} x)} \\
	&= \abs{λ} \norm{x_0 - y}\qquad y = - \inv{λ}x ∈ F
	≥ δ\abs{λ}
	≥ δ\abs{φ(x_0 + λx)}
\end{align*}

\spart

Extendemos el funcional a un $φ∈ X^*$, así que $\restr{φ}{F} = 0$ por construcción y $φ(x_0) = 1$ (hemos encontrado un $φ∈\dual$ que no se anule en $x_0$).

\end{problem}
