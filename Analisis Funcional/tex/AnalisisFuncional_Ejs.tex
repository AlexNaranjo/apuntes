% -*- root: ../AnalisisFuncional.tex -*-
\newcommand{\hard}{\hspace{-3pt}(\dag)\hspace{5pt}}

Los ejercicios marcados con (\dag) están marcados como de ``dificultad especial'' en las hojas.

\section{Hoja 1}


\begin{problem}
\ppart Probar, usando el \nref{thm:CategoriaBaire}, que $I = \set{x ∈ ℝ \tq x ∉ ℚ} ≠ ∅$ y que, de hecho, $I$ es un \nlref{def:ConjuntoGDelta}.

\ppart Probar que $ℝ$ no es numerable.

\ppart Sea $X = ℤ$ con $\dst(x,y) = \abs{x-y}$. Probar que \sdst es un espacio métrico completo y que, sin embargo, es numerable. ¿Por qué no contradice esto al Teorema de Baire?

\solution

\spart

Sabemos que $ℚ$ es numerable, así que podemos enumerar todos los racionales en una serie $q_1, q_2, \dotsc, q_n, \dotsc ∈ ℚ$. Definimos entonces $X_n = ℝ \setminus \set{q_n}$ como una serie de conjuntos abiertos y densos. La intersección de todos ellos son todos los $x ∈ ℝ$ no racionales, que es el conjunto $I$ que buscábamos. Además, por el \nref{thm:CategoriaBaire}, esa intersección es un $G_δ$ denso en $ℝ$, y por eso mismo es no vacío.

\spart

Si $ℝ$ fuese numerable, entonces podríamos enumerarlo: $ℝ \equiv\set{ x_1, x_2, \dotsc, x_n, \dotsc}$. Por otra parte, los conjuntos formados por un único punto son diseminados, por lo que podríamos definir que $ℝ = \bigcup_{n≥1} \set{x_n}$. Sin embargo, esto entraría en contradicción con el Teorema de Baire, que nos dice que no podemos escribir $X$ como una unión numerable de conjuntos diseminados.

\spart

Para que \sdst sea un espacio métrico completo, toda \nlref{def:SucesionCauchy} ha de converger en el espacio. Que una sucesión sea de Cauchy implica que $∀ε> 0$ existe un $N ∈ ℕ$ tal que si $m,n ≥ N$, entonces $\dst(x_m, x_n)$. La cuestión es que, como estamos en $ℤ$, si tomamos\footnote{Cosa que no sé si podemos hacer.} un $ε < 1$, entonces $\dst(x_m, x_n) = 0$ (no podemos tener distancias fraccionarias entre elementos de $ℤ$). Por lo tanto, por ser de Cauchy llega un momento en el que la sucesión se repite constantemente. El límite será entonces es elemento que se repite, que por ser parte de la misma sucesión está en $ℤ$.

% Triplazo.
Esto no contradice el Teorema de Baire porque en $ℤ$ no hay conjuntos densos, y lo vamos a demostrar. Sea $Y \subsetneq ℤ$ un conjunto cualquiera de $ℤ$, y sea $z ∈ ℤ \setminus Y$. La bola de radio $\sfrac{1}{2}$ centrada en $z$ tiene intersección vacía con $Y$ (no hay ningún entero a distancia $\sfrac{1}{2}$ de $z$), por lo que $Y$ no puede ser denso.

Como no hay conjuntos densos, no puede haber tampoco conjuntos diseminados y por lo tanto sigue cumpliéndose el Teorema de Baire: no podemos escribir $ℤ$ como unión numerable de conjuntos diseminados.
\end{problem}

\begin{problem} \hard Sean $\set{a_n}_n≥1$, $\set{b_n}_{n≥1}$ dos sucesiones de números reales y $a_n$ absolutamente convergente. Probar que:

\ppart Sea $f$ dada por \[ f(x) = \sum_{n≥1} a_n φ(b_n) \] con \[ φ(x) = \begin{cases} [x] & 0 ≤ [x] ≤ \frac{1}{2} \\ 1 - [x] & \frac{1}{2} ≤ [x] ≤ 1 \end{cases} \] siendo $[x]$ la parte decimal de $x$.

Demostrar que $f$ es continua y $f ∈ C_{[0,1]}$. Además, la serie que define a $f(x)$ es absolutamente convergente.

\ppart Sea $a_n = 2^{-n}$, $b_n = 2^n$, y $h_m = ε_n 2^{-m}$ con $ε_m = \pm 1$ para todo $m$. Probar que \[ \frac{f(x + h_m) - f(x)}{h_m} = ε_m \sum_{n=1}^{m-1} 2^{m-n} \left(φ(2^n(x+h_m)) - φ(2^nx)\right)\]

\ppart Si escribimos $x = [x] + \sum_{k>0} α_k 2^{-k}$ con $α_k ∈ \set{0,1}$, entonces \[ φ(2^nx) = φ\left(\sum_{l≥1} α_{n +l} · 2^{-l}\right)\] y además $\sum_{l≥1}α_{n +l} · 2^{-l} ∈ [0,1]$. Del mismo modo, \[ φ(2^n(x+h_m)) = φ(ε_m2^{2-m} + \sum_{l≥1} α_{n +l} · 2^{-l})\] y \[ ε_m2^{n-m} + \sum_{k≥1} α_{n +l} · 2^{-l} = \sum_{l≥1} α_{n +l}' · 2^{-l}\], siendo $α_{n+l}' = α_{n+l} + δ_{m-n, k} ε_m$, con $δ_{i,j}$ la delta de Kronecker\footnote{Esto es, $δ_{i,j} = 1$ si $i = j$, y $0$ si $i ≠ j$.}.

\ppart Tomamos $ε_m = (-1)^{α_m}$. Entonces $α_{n+l}' ∈ {0,1}\;∀l≥1$. Fijemos $m > 1$. Entonces $\sum_{l≥1} α_{n+l} 2^{-l}$ y $\sum_{l≥1}α_{n+l} 2^{-l}$ están ambos en la misma mitad del intervalo $[0,1]$. Usar esto para probar que \[ \frac{f(x+h_m) - f(x)}{h_m} = m-1\]

\ppart Del apartado anterior se sigue que $f(x)$ no es diferenciable en ningún $x ∈ ℝ$. Sin embargo, $f$ no está muy lejos de serlo en el sentido siguiente: si $\abs{h} ≤ 1$, entonces $∃ C ∈ (0,∞)$ independiente de $x$ y $h$ tal que \[ \abs{f(x+h) - f(x)} ≤ C\abs{h} \left(1 + \log \frac{1}{\abs{h}}\right) \]

\textbf{Indicación}: Dado $0 < \abs{h} ≤ 1$, existe un único $k ∈ ℕ$ con $2^{-k-1} < \abs{h} ≤ 2^{-k}$. Entonces estimar $f(x+h) - f(x)$ dividiendo la suma en los términos $n < k$ y $n ≥ k$ y estimando cada suma por separado.

\solution

\spart

\spart

\spart

\spart

\spart

Lo que está diciendo es que el módulo de continuidad de esta función es sólo un poquito peor que Lipschitz.

Para demostrarlo, vemos que existe un único $k ∈ ℕ$ tal que $2^{-k-1} < \abs{h} ≤ 2^{-k}$. En otros términos, $k$ es la parte entera de $\log_2 \frac{1}{\abs{h}}$. Entonces podemos escribir \begin{align*}
\abs{f(x+h) - f(x)} &=
	\abs{\sum_{n≥1} 2^{-n} \left([2^nx + 2^n h] - [2^nx]\right)}\\
&≤ \sum_{n≥1} 2^{-n} \abs{[2^nx + 2^n h] - [2^nx]} \\
&= \underbrace{\sum_{1≤n≤k} 2^{-n} \abs{[2^nx + 2^n h] - [2^nx]}}_A + \underbrace{\sum_{n≥k} 2^{-n} \abs{[2^nx + 2^n h] - [2^nx]}}_B
\end{align*}

Vamos a estimar ambos sumatorios por separado. El más simple es el $B$, que haciendo cuentas sale que $B ≤ 2 \abs{h}$. Para la estimación de $A$, usamos que $\abs{[x] - [y]} ≤ \abs{x-y}$ y haciendo todavía más cuentas nos queda que $A ≤ k\abs{h}$.

\end{problem}

\begin{problem} Sea \sdst un espacio métrico compacto. Probar que es completo. ¿Es cierto el recíproco?
\solution

Sea $\set{x_n}$ una sucesión de Cauchy en $X$, y sea $\set{ε_n}$ otra sucesión que tiende a $0$. Por ser $\set{x_n}$ de Cauchy, para cada $ε_n$ existe un $M_n ∈ ℕ$ tal que si $m,n ≥ M_n$ entonces $\dst(x_m, x_n) < ε_n$. Equivalentemente, para todo $n ∈ N$ tendremos que $x_n ∈ \bola_{ε_n} (x_{M_n}) = B_n$. Entonces, por ser $X$ compacto existe un subrecubrimiento finito $B_{n_i}$ de $\bigcup B_n$.

Por ser un subrecubrimiento, tendremos que a partir de un cierto $n$ suficientemente grande, las bolas $\bola_{ε_n} (x_{M_n})$ que definíamos antes están estrictamente contenidas en él, por lo que el límite debe de estar ahí también. % Me convence esto más bien poco.

\end{problem}

\begin{problem} \hard Sea $\set{f_n}_{n∈ℕ}$ una familia uniformemente acotada de funciones diferenciables en $[a,b]$, un intervalo compacto de $ℝ$, y cuyas derivadas están uniformemente acotadas.

\ppart Probar que cierta subsucesión $\set{f_{n_j}}$ converge uniformemente a una función $f$ Lipschitz. ¿Es cierto que $f$ es necesariamente diferenciable en todo $[a,b]$?

\ppart Probar que si se omite la condición de que la familia sea uniformemente acotada, existe una sucesión de constantes $c_n$ tales que $f_n - c_n$ converge uniformemente a una función $f$ Lipschitz continua.

\solution

\begin{figure}[hbtp]
\centering
\inputtikz{FuncionLipschitz}
\caption{La idea de la función Lipschitz es que siempre hay un doble cono (el verde) que contiene a toda la función.}
\label{fig:FuncionLipschitz}
\end{figure}

Recordamos la definición de función continua Lipschitz (\fref{fig:FuncionLipschitz}):

\begin{defn}[Función\IS Lipschitz continua] Una función $\appl{f}{(X, \dst_X)}{(Y, \dst_Y)}$ entre dos espacios métricos se dice Lipschitz continua si existen una constante $C ∈ ℝ$, $C ≥ 0$ tal que para todos $x_1, x_2 ∈ X$ se tenga que \[ \dst_Y(f(x_1), f(x_2)) ≤ C \dst_X(x_1, x_2) \]
\end{defn}

\spart

Si la familia de funciones está uniformemente acotada, eso significa que tenemos una cota $0 ≤ C_0 < ∞$ tal que $C_0 > f_n\; ∀n∈ℕ$. Igualmente, si sus derivadas están uniformemente acotadas tenemos que $f_n' < C_1$.

El espacio $C_{[a,b]}$ con la norma del supremo, que es donde vive esta sucesión, es un espacio métrico completo, por lo que existe una subsucesión $\set{f_{n_j}}$ convergente uniformemente a $f ∈ C_{[a,b]}$. Vamos a demostrar ahora que esa $f$ es Lipschitz.

Por otra parte, como las $f_n$ tienen derivada acotada por $C_1$, podemos decir que son funciones Lipschitz con cota $C_1$. Vemos ahora qué ocurre con $\abs{f(x) - f(y)}$ para $x,y ∈ [a,b]$. Fijando un $ε>0$, podemos encontrar un $n_0 ∈ ℕ$ tal que si $n > n_0$ entonces $\abs{f_n(x) - f(x)} < ε$ por ser las $f_{n_j}$ convergentes a $f$. Sumando y restando: \begin{align*}
\abs{f(x) - f(y)} &= \abs{f(x) - f_n(x) + f_n(x) - f(y) + f_n(y) - f_n(y)} \\
&≤ \abs{f(x) -f_n(x)} + \abs{f(y) - f_n(y)} + \abs{f_n(x) - f_n(y)} \\
&≤ ε + ε + C_1\abs{x-y}
\end{align*}, por lo que haciendo tender $ε \to 0$ tenemos que $f$ es Lipschitz con la misma cota $C_1$ de las funciones de la sucesión.

Sin embargo, $f$ no tiene por qué ser necesariamente diferenciable: $f$ sólo es Lipschitz pero Lipschitz no implica diferenciable en todo punto. Por ejemplo, $f(x) = \abs{x}$ es continua Lipschitz pero no es derivable en $x = 0$.

\spart

Tomamos $c_n = f_n(a)$, ya que por el teorema del valor medio tenemos que \[ \abs{f_n(x) - c_n} = \abs{f_n(x) - f_n(a)} ≤ C\abs{x-a} ≤ C\abs{b-a} < ∞ \], luego $f_n - c_n$ está equiacotada y estamos en las condiciones del teorema anterior.

\end{problem}

\begin{problem} Construir en $C_{[0,1]}$ un subconjunto acotado de funciones que no sea un \nlref{def:ConjTotalmenteAcotado}.

\solution

Nos basta aplicar \nref{thm:AscoliArzela}. El ejemplo canónico es $f_n(x) = x^n$ para $x ∈ [0,1]$, que converge a $0$ cuando $x ∈ [0,1)$ y $1$ si $x = 1$.

\end{problem}

\begin{problem} Sea \sdst un espacio métrico completo. Demuestra que:

\ppart Si $K ⊂ X$ es cerrado, entonces $K$ es completo.
\ppart Si $K$ es completo y totalmente acotado, entonces es compacto.

\solution

\spart

Fácil: una sucesión de Cauchy en $K$ es convergente en $X$ por ser $X$ completo. Como $K$ es cerrado, el límite de la sucesión ha de estar en $K$, así que $K$ es completo.

\spart

Recordamos el \nref{thm:HeineBorel}, que nos dice que un subconjunto $K ⊂ X$ es compacto si y sólo si es cerrado y totalmente acotado. Partimos de que $K$ es completo y totalmente acotado, por lo que sólo necesitamos probar que además es cerrado.

Ahora bien, esto es sencillo de probar: por ser $K$ completo, podemos tomar una sucesión de Cauchy $\set{x_n} ⊂ K$, que convergerá a un $x ∈ K$. Por lo tanto $K$ es cerrado y podemos demostrar el ejercicio aplicando Heine-Borel.

\end{problem}


\begin{problem} Sea $f ∈ C_c(ℝ)$ una función continua en $ℝ$ y de soporte compacto. Fijamos $η ∈ C_c^1 (ℝ)$ con $\int η = 1$, y sea $η_t(x) = tη(tx)$ para $t > 0$. Demuestra que:

\ppart Sea $f_t ≝ f * η_t$. Entonces $f_t \convs[][t] f$ uniformemente.
\ppart $\norm{f_t'}_∞ ≤ C(1+t)$ con $t > 0$ y $C$ finita e independiente de $t$.

\solution

Este ejercicio es una demostración de un teorema conocido sobre familias de aproximaciones de la identidad \citep[Def. III.11, Teorema III.20]{ApuntesVarReal} que viene bien demostrado en \citep[Teoremas 8.14, 8.15]{folland99}. Vamos a demostrarlo todo formalmente.

\begin{defn}[Familia\IS de aproximaciones de la identidad] \label{def:AproxIdentidad} Una familia de aproximaciones de la identidad es una sucesión $\set{ρ_n}_{n∈ℕ}$ de funciones $\appl{ρ_n}{ℝ^N}{ℝ}$ no negativas, infinitamente derivables, de soporte compacto, con $\sop ρ_n ∈ \bola_{\frac{1}{n}} (0)$ y con norma $\norm{ρ_n}_1 = \int_ℝ ρ_n = 1$.
\end{defn}

Esta definición nos vale igualmente para una familia no numerable. Es fácil ver que $η_t$ son una familia de aproximaciones de la identidad, aunque en este caso miramos el límite $t \to ∞$ en lugar de $t \to 0$. No es un cambio relevante.

Vamos ahora con la demostración.

\spart

Demostramos primero que las $η_t$ mantienen la misma integral sobre $ℝ$ que η: \[ \int_{ℝ} η_t(x) \dif x = \int_ℝ tη(tx) \dif x \eqreasonup{$y = tx$} \int_ℝ t · \frac{1}{t} · η(y) \dif y = \int_{ℝ} η(y) \dif y = 1\]

Una vez hecho esto, calculamos la convolución: \[ f_t(x) = \int_ℝ f(x-y) η_t(y) \dif y \]

Por ser $f$ continua podremos acotar su valor en entornos pequeños, esto es, que $∀ ε > 0$ existe un $δ > 0$ tal que $f(y) ∈ \bola_ε(f(x))$ si $x - y ∈ \bola_δ(0)$. Además, dado que las $η_t$ tienen un soporte cada vez más pequeño, esto es, $\sop η_t \convs[][t] \set{0}$, podemos encontrar un $t_ε$ suficientemente grande tal que $\sop η_t ⊆ \bola_δ(0)$ si $t > t_ε$. En este caso podemos acotar y ver que, si $t > t_ε$, entonces \begin{align*}
f_t(x) &= \int_ℝ f(x-y) η_t(y) \dif y \\
&= \int_{\bola_δ(0)} f(x-y) η_t(y) \dif y \\
&≤ \int_{\bola_δ(0)} (f(x) \pm ε) η_t(y) \dif y \\
&= (f(x) \pm ε) \int_{\bola_δ(0)} η_t (y) \dif y = f(x) \pm ε \end{align*}

Haciendo tender $ε \to 0$, $t$ se irá a infinito y tendremos la convergencia uniforme que buscábamos.

\spart

Vamos a usar una propiedad de la convolución, y es la siguiente: dadas $f,g$, entonces \[ \od{f*g}{x} = \od{f}{x} * g = f* \od{g}{x}\]

Esto nos permitirá sacar la derivada de la convolución aprovechando que η es derivable. Calculamos su derivada: \[ η_t'(x) = t^2 η'(tx) \], lo que nos deja que \[ f_t' (x) = f * η_t'(x) = \int_{ℝ} f(x-y) t^2 η'(tx) \dif y \]

Ahora vamos a tratar de acotar esta integral acotando las dos funciones que tenemos. $η'$ está acotada por ser $η$ derivable, y $f$ también está acotada por ser continua en un compacto. Llamaremos $K_f, K_η$ a las respectivas cotas para $f$ y $η'$.

El siguiente paso es ver que, dado que tanto $f$ como η (y por lo tanto $η_t'$) tienen soporte compacto, no tenemos que evaluar la integral en todo $ℝ$ sino sólo donde alguna de las dos funciones sea distinta de cero. Podemos suponer que siempre hay un soporte contenido en el otro (si no, la intersección será más pequeña que cualquiera de los dos y la cota será menor).

Empezaremos viendo qué ocurre cuando $\sop η_t' ⊆ \sop f(x-y)$. En este caso, tendremos que \[ \int_{ℝ} f(x-y) t^2 η'(tx) \dif y = \int\limits_{\sop η_t'} f(x-y) t^2 η'(tx) \dif y ≤ t^2 K_f K_η \int\limits_{\sop η_t'} \dif y \]

Sabemos que $\int_{\sop η_t'} \dif y = m(\sop η_t')$, es decir, es la medida del soporte. Sin embargo, es fácil ver que $m(\sop η_t') ≤ \frac{m(\sop η)}{t}$ (el soporte de la derivada está contenido en el soporte de la función, y al multiplicar por $t$ el parámetro estamos contrayendo el soporte). Como η tiene soporte compacto, su medida es finita y lo que nos queda es que \[ K_f K_η t^2 \int\limits_{\sop η_t'} \dif y = K_f K_η t · m(\sop η_t') ≤ C_1 t \]

Nos falta ver el caso contrario: qué ocurre cuando  $\sop f(x-y) ⊆ \sop η_t'$. De manera análoga, nos quedaría que \[ \int\limits_{\sop f(x-y)} f(x-y) t^2 η'(tx) \dif y ≤ K_f K_η t^2 · m(\sop f(x-y)) \]

Ahora bien, este caso ($\sop f(x-y) ⊆ \sop η_t'$) ocurre sólo cuando $t$ es pequeño (para $t$ grande el soporte de $η_t$ se hace tan pequeño como queramos y acabará estando contenido en el de $f(x-y)$). Luego en este caso tendremos $t$ acotado y por lo tanto podremos decir que \[ K_f K_η t^2 · m(\sop f(x-y)) ≤ C_2 \] con $C_2 ∈ ℝ$ independiente de $t$. Sumando ambas cotas, tenemos que $f_t' ≤ C_1 t + C_2$, y tomando $C = \max \set{C_1, C_2}$ nos queda que $f_t' ≤ C t + C = C(1+t)$.

\end{problem}

\section{Hoja 2}

\begin{problem} Sea $(X, \norm{·})$ un espacio vectorial normado. Demuestra que

\ppart  Dado $\mathcal{U} ⊂ X$ abierto, el conjunto $x + \mathcal{U} = \set{ x+ y \tq y ∈ \mathcal{U}}$ es abierto, así como $r\mathcal{U} = \set{r · x \tq x ∈ \mathcal{U}}$ para $r > 0$. En otras palabras, demuestra que la topología de un espacio normado es compatible con la estructura de espacio vectorial.

\ppart La norma es una función Lipschitz continua de constante $1$: \[ \abs{\norm{x} - \norm{y}} ≤ \norm{x-y} \quad ∀x,y ∈ X \]

\ppart Si $x_n \to x$ en $X$, entonces $\norm{x_n} \to \norm{x}$.

\ppart Dado $E ⊂ X$ definamos $\dst(x,E) = \inf \set{\norm{x-y} \tq y ∈ E}$. Entonces $\dst(x,E)$ es también Lipschitz continua de constante $1$.

\solution

\spart \label{ej:Hoja2:1A}

Sea $\mathcal{U} ⊂ X$. Ya hemos visto que $x + \mathcal{U}$ es un abierto para un $x ∈ V ⊂ \mathcal{U}$. Si tomamos entonces $z ∈ x + \mathcal{U}$, existirá un $y ∈ \mathcal{U}$ con $z = x + y$. Como $\mathcal{U}$ es abierto, podemos tomar una bola $\bola_δ(y) ⊂ \mathcal{U}$ y entonces $x + \bola_δ(x) ⊂ x + \mathcal{U}$, luego efectivamente $x + \mathcal{U}$ es abierto.

Un argumento similar valdrá para demostrar que $r\mathcal{U}$ es abierto. No merece demasiado la pena pararse en ello.

\spart

Sumando y restando, podemos ver que \[ \norm{x} = \norm{(x+y) - y} ≤ \norm{x-y} + \norm{y} \], luego $\norm{x} - \norm{y} ≤ \norm{x-y}$.

Eso sí, no hemos conseguido el valor absoluto. Sin embargo, si repetimos el mismo proceso desde la norma de $y$ tenemos que \[\norm{y} = \norm{(y-x) + x} ≤ \underbracket{\norm{y-x}}_{= \norm{x-y}} + \norm{x} \], luego $\norm{x} - \norm{y} ≥ - \norm{x-y}$. Juntando ambas conclusiones, \[ \abs{\norm{x} - \norm{y}} ≤ \norm{x-y} \] tal y como nos pedían.

\spart

Se resuelve simplemente por continuidad de la norma.

\spart

Si $\dst(x,E)$ es Lipschitz de constante 1, entonces lo que queremos ver es que \[ \abs{\dst(x,E) - \dst(x',E)} ≤ \abs{x - x'} \]

Esto se puede entender como una generalización del apartado B, si en ese caso tomábamos $E = \set{0}$.

Para demostrarlo, tomamos $x,x' ∈ V$ y $ε > 0$. Entonces existen $y, y' ∈ E$ con \begin{gather*}
 \dst(x,E) ≤ \norm{x-y} ≤ \dst(x,E) + ε\\
 \dst(x',E) ≤ \norm{x'-y'} ≤ \dst(x',E) + ε\\
\end{gather*}, de tal forma que nos acercan todo lo que queramos a esa distancia\footnote{No tiene por que ser igual porque está definido como el ínfimo, no tiene por qué alcanzarse.}. Además, podemos hacer dos estimaciones adicionales por ser la distancia el ínfimo: \[ \dst(x', E) ≤ \norm{x' - y} \qquad \dst(x,E) ≤ \norm{x-y'} \]

Supongamos además sin pérdida de generalidad que $\dst(x,E) ≥ \dst(x',E)$. Entonces \begin{multline*}
\abs{\dst(x,E) - \dst(x',E)} =
	\underbracket{\dst(x,E)}_{≤\norm{x-y'}} - \underbracket{\dst(x', E)}_{≥ \norm{x' - y'} - ε} ≤ \norm{x + y'} - \norm{x'-y'} + ε ≤ \\ ≤ \abs{ \norm{x + y'} - \norm{x'-y'} } + ε
	≤ \norm{(x-y') - (x'-y')} + ε = \norm{x-x'} + ε
\end{multline*}, y haciendo tender $ε \to 0$ ya tenemos lo que buscábamos.

\end{problem}

\begin{problem} \label{ej:Hoja2:2} Sea $V$ un espacio vectorial sobre $\kbb = ℝ$ ó $ℂ$. Diremos que $A ⊂ V$ es convexo si $∀x,y ∈ A$ y $∀t ∈ [0, 1]$, entonces $(1-t)x + ty ∈ A$ (en otros términos, el segmento $[x,y] ⊂ A$ si $x,y ∈ A$).

\ppart Demuestra que \[ A + A = \set{x + y \tq x,y ∈ A} = 2A = \set{2x \tq x ∈ V} \]

\ppart Si $W$ es otro espacio vectorial sobre \kbb y $\appl{T}{V}{W}$ es lineal, entonces $T(A)$ es convexo en $W$.

\ppart Si $(V, \norm{\cdot})$ es normado, entonces dados $x ∈ V$ y  $r > 0$, los conjuntos \begin{align*}
\bola_r (x) &= \set{x ∈ V \tq \norm{x} < r} \\
\adh{\bola_r}(x) &= \set{x ∈ V \tq \norm{x} ≤ r}
\end{align*} son, respectivamente, la bola abierta y la cerrada, centradas en $x ∈ V$ y de radio $r > 0$. Demuestra que ambas son convexas.

\ppart Si $(V,\norm{\cdot})$ es normado y $A ⊂ V$ es convexo, entonces $\adh{A}$ es convexo.

\solution

\spart

Está claro que $2 A = \set{x + x \tq x ∈ A} ⊂ A + A$, y para esto no necesitamos para nada la convexidad. La inclusión no será demasiado complicada.

Sea $z ∈ A + A$. Entonces $z = x + y$ con $x,y ∈ A$. Dividiendo por dos, \[ \frac{1}{2} z = \frac{1}{2} x + \frac{1}{2} y\], que está en $A$ por ser convexo. Luego si $\frac{1}{2} z ∈ A$, sólo queda ver que $z ∈ 2A$.

\spart

Tomamos $T$ en $(1-t)x + ty ∈ A$ y listos, podemos hacerlo por linealidad.

\spart

Empezamos primero con la bola cerrada. Tomamos $p,q ∈ \adh{\bola_r}(x)$ y $ 0 ≤ t ≤ 1$. Entonces \begin{multline*} \norm{(1-t)p + t q - x} = \norm{(1-t)(p - x) + t(q - x)} ≤ \\ ≤ (1-t)\norm{p - x} +(t) \norm{q - x} ≤ (1-t) r + t (r) = r \end{multline*}, así que como $\norm{((1-t)p + t q) - x} ≤ r$, tenemos que $(1-t)p + t q ∈ \adh{\bola_r}(x)$.

Para la bola abierta se hace igual, sólo que aparece un menor estricto en $\norm{p-x} < r,\norm{q-x} < r$ y nos sale.

\spart

Sean $x,y ∈ \adh{A}$ y $t ∈ [0,1]$. Por ser $\adh{A}$ cerrado, existirán sendas sucesiones $\set{x_n}, \set{y_n} ⊂ A$  que convergen respectivamente a $x$ e $y$. Consideramos ahora los puntos $(1-t)x_n + ty_n ∈ A$ por ser $A$ convexo. Como $\adh{A}$ es cerrado, la sucesión de esos puntos tenderá a $(1-t)x + t y$ que está en $\adh{A}$.

\end{problem}

\begin{problem} Sea $X = (C[0,1], \norm{·}_∞)$. Entonces, si $f ∈ L^1[0,1]$ y \[ λ_f(g) ≝ \int_0^1 f(x) g(x) \dif x\], entonces $λ_f$ define un funcional acotado en $X$ cuya norma es exactamente $\norm{f}_1$. Demuéstralo siguiendo los siguientes pasos:

\ppart La desigualdad $\norm{λ_f}_{X^*} ≤ \norm{f}_1$ es trivial (justificarlo).
\ppart Si $f ∈ C[0,1]$ probar la desigualdad opuesta; junto con el apartado anterior esto prueba igualdad en este caso.
\ppart Si $f ∈ L^1[0,1]$ es cualquiera, usar que hay una sucesión de funciones continuas $f_n ∈ C[0,1]$ con $f_n \to f$ en $L^1[0,1]$

\solution

\spart

Podemos hacer la siguiente estimación: \[
\abs{λ_f(g)} = \abs{\int_0^1 f(x) g(x) \dif x} ≤ \int_0^1 \abs{f(x)} \abs{g(x)} \dif x ≤ \norm{g}_∞ \int_0^1 \abs{f(x)} \dif x = \norm{f}_1 \norm{g}_∞
\], luego $λ_f ∈ X^*$.

\spart

Si $f ∈ C[0,1]$, entonces $\norm{λ_f}_{X^*} ≥ \norm{f}_1$. Supongamos que $f ∈ C[0,1]$ y $f \not\equiv 0$. Consideramos \[ g (x) = \sign f(x) = \begin{cases} \frac{\abs{f(x)}}{f(x)} & f(x) ≠ 0 \\ 0 & f(x) = 0 \end{cases} \]

Entonces \[ λ_f(g) = \int_0^1 f(x) \sign f(x) \dif x = \int_0^1 \abs{f(x)} \dif x = \norm{f}_1 \]

Además, como $f \not \equiv 0$, $g(x) ≠ 0$ y $\abs{g(x)} = 1$, si $g(x) ≠ 0$, por lo que $\norm{g}_∞ = 1$ y entonces \[ \frac{\abs{λ_f (g)}}{\norm{g}_∞} = \frac{\norm{f}_1}{1} = \norm{f}_1 \implies \norm{λ_f}_{X^*} ≥ \norm{f}_1 \]

Ahora bien, tenemos un problema con esto, y es que $g$ no es continua y por lo tanto $g ∉ X$. Para arreglar el argumento vamos a regularizar $g$ con $\set{θ_t}_{t>0}$ una serie de  \textit{mollifiers}\footnote{\nref{def:AproxIdentidad}}. Para construirlos, fijamos $θ ∈ C_c(ℝ)$ par con $\sop θ ⊆ [-1, 1]$, $\norm{θ}_1 = 1$ y $θ ≥ 0$; y entonces \[ θ_t(x) = \frac{θ(\inv{t} x)}{t} \]

Tenemos que ampliar la definición de $g$ a $ℝ$, así que lo que haremos será extenderla con ceros a todo $ℝ$. Definimos entonces ahora \[ g_t = g * θ_t = \int_{ℝ} g(y) θ_t(x-y) \dif x\]

Supongamos adicionalmente que $\sop f ⊆ [ε, 1-ε]$ para cierto $0 < ε < \sfrac{1}{2}$, y que entonces tendremos $\sop g = \sop f$. Ahora vemos que $\sop g_t = \sop g + \sop θ_t = [ε - t, 1 - ε +t] ⊂ [0,1]$ si $0 < t ≤ ε$.

Calculamos ahora $λ_f(g_t)$ con $t ∈ (0,ε]$, ya que esta $g_t$ siempre será continua para $t > 0$. \begin{align*}
λ_f(g_t) &= \int_0^1 f(x) (g * θ_t)(x) \dif x = \\
	&= \int_{ℝ} f(x) (g * θ_t)(x) \dif x = \\
	&= \int_ℝ f(x) \left(\int_{ℝ} g(y) θ_t(x-y) \dif y \right) \dif x = \\
	&= \int_{ℝ} g(y) \left(\int_{ℝ} f(x) \underbracket{θ_t (x-y)}_{ = θ_t(y-x)} \dif x \right) \dif y = \\
	&= \int_{ℝ} g(y) (f*θ_t)(y) \dif y
\end{align*}

Como $f ∈ C[0, 1]$, entonces $f * θ_t \convs[L^∞][t][0] f$, luego podemos la integral converge igualmente: \[ \int_{ℝ} g(y) (f*θ_t)(y) \dif y \convs[][t][0] \int_{ℝ} g(y) f(y) \dif y \], así  que $λ_f(g_t) \convs[][t][0] λ_f (g) = \norm{f}_1$. Entonces \[ \norm{λ_f}_{X^*} ≥ \frac{\abs{λ_f(g_t)}}{\norm{g_t}_∞} \quad ∀t > 0\], ya que la definición del supremo era que  \[ \norm{λ_f}_{X^*} = \sup_{g ∈ X \setminus\set{0}} \frac{\abs{λ_f(g)}}{\norm{g}_X} \]

Lo único que nos falta ver es que $\norm{g_t}_∞ \convs[][t][0] 1$. Sabemos que $\norm{g_t}_∞ ≤ \norm{g}_∞ = 1$, como consecuecnia de que $θ ≥ 0$ y $\norm{θ}_1 = 1$. Pero si $f \not \equiv 0$, entonces $g$ es de módulo $1$ y continua en cualquier abierto donde $f ≠ 0$, y entonces $g_t(x) \convs[][t][0] g(x)$. Para verlo, si $g$ es continua en $x$ pues sale, y si no el módulo es exactamente $1$, y juntándolo tenemos que la norma infinito converge hacia uno cuando $t$ se hace pequeñito. Y juntando de nuevo esto con otra cosa, juraría que lo de la norma de antes, entonces nos queda que \[ \norm{λ_f}_{X^*} ≥ \liminf_{t \to 0} \frac{\abs{λ_f(g_t)}}{\norm{g_t}_∞} = \frac{\liminf_{t \to 0} \abs{λ_f(g_t)}}{\liminf_{t \to 0} \norm{g_t}_∞} = \norm{f}_1 \]

Y ya casi lo hemos probado si $\sop f ∈ [ε, 1-ε]$. Nos queda probarlo para el último caso de $f ∈ C[0,1]$. Sea $ε > 0$. Consideramos $f_ε $ como una versión truncada, formalmente \[ f_ε(x) = \begin{cases}
f(x) & x ∈ [ε, 1-ε] \\
f(ε)\left(1 - 2\frac{ε-x}{ε}\right) & x ∈ [\sfrac{ε}{2}, ε] \\
f(1-ε)\left(1 - 2\frac{1-ε-x}{1-ε}\right) & x ∈ [1- ε, 1- \sfrac{ε}{2}] \\
0 & x ∈ [0, \sfrac{ε}{2}] ∪ [ 1- \sfrac{ε}{2}, 1]
\end{cases}
\]

Vamos a ver entonces que \[ \norm{f-f_ε}_1 ≤ \dotsb ≤ 2ε\norm{f}_1 \], y entonces podemos escribir \[ λ_f(g) = λ_{f_ε}(g) + λ_{f-f_ε}(g) \], y como $\abs{λ_{f-f_ε}(g)} ≤ \norm{f-f_ε}\norm{g}_1 \convs[][ε][0] 0$, y luego \[\norm{λ_f}_{X^*} = \norm{λ_{f-f_ε} + λ_{f_ε}} ≥ \norm{λf_ε}_{X^*} - \norm{λ_{f-f_ε}}_{X^*} = \norm{f_ε}_1 - \norm{f - f_ε}_1 \convs[][ε][0] \norm{f} \], y esto ya sí termina la prueba cuando $f$ es continua. La idea sería la misma si tuviésemos $f ∈ L^1[0,1]$, que es lo que queríamos probar, usando una aproximación por continuas.


\end{problem}

\begin{problem}[5] Sea $X$ normado y $\dim X < ∞$, y sea $Y$ otro espacio normado. Sea $A$ una aplicación lineal $\appl{A}{X}{Y}$. Demuestra que $A$ es continua.

\solution

Como la dimensión de $X$ es finita, fijando $\mathcal{B} = \set{\ve_1, \dotsc, \ve_n}$ base de $X$, cogemos la base dual algebraica $\mathcal{B}^* = \set{\ve_1^*, \dotsc, \ve_n^*}$ y entonces todo $x ∈X$ se puede expresar como \[ x = \sum_{j = 1}^n \ve_j^*(x) \ve_j \]

En ese caso se pueden hacer peraciones y sale.

\end{problem}
