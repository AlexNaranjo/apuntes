%
% Soluciones a los ejercicios de Probabilidad II.
%
% Curso 2014 - 2015 2º cuatrimestre
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Todos los ejercicios que así se muestran han sido realizados en primera mano por los alumnos y posteriormente corregidos con las soluciones dadas en clase (Salvo que se indique lo contrario).

Se ha dado prioridad a las soluciones realizadas por el profesor y en algunas ocasiones se han puesto dos soluciones, indicando cuál es la copiada de clase.

Eso no implica que estén exentos de errores :D.

\section{Hoja 1}

\textcolor{blue}{CORREGIDA Y CONTRASTADA CON LAS SOLUCIONES DEL PROFESOR}

Se asume siempre que estamos trabajando en un espacio de probabilidad $(\Omega, \mathcal{A}, P)$, y que  $\mathcal{B}\subset \mathcal{A}$ es una sub-$\sigma$-\'algebra.

%%%%%%%%%%%%%%%%%%  PROBLEMA 1.1  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[1]De una urna con 10 bolas blancas y 10 bolas negras se extraen simultaneamente 3 bolas. 
Calcular la probabilidad de que exactamente dos de ellas sean blancas. Responder a la misma
pregunta si las bolas se extraen de manera sucesiva.
\solution

\begin{expla}
En este caso no hay reposición de las bolas extraídas. Por tanto no hay diferencia a la hora de calcular la probabilidad entre la extracción simultánea y la sucesiva.

Para resolver esos problemas, la forma más intuitiva es buscar la fracción:
\[
\frac{\text{Numero de casos favorables}}{\text{Numero de casos posibles}}
\]
\end{expla}
A = Extracción simultánea de 3 bolas blancas

b = blanca

n = negra
\[
P(A)=\underbrace{\frac{10}{20}}_{b}\underbrace{\frac{9}{19}}_{b}\underbrace{\frac{10}{18}}_{n}+\underbrace{\frac{10}{20}}_{b}\underbrace{\frac{10}{19}}_{n}\underbrace{\frac{9}{18}}_{b}+\underbrace{\frac{10}{20}}_{n}\underbrace{\frac{10}{19}}_{b}\underbrace{\frac{9}{18}}_{b} = 3\frac{900}{6840}=\frac{2700}{6840}=0.39
\]

\end{problem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%  PROBLEMA 1.2  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[2]Disponemos de dos urnas,
$U_1$, que contiene 6 bolas azules y 8 bolas blancas, y
$U_2$,
 que contiene
3 bolas azules y 9 bolas blancas. Se sortea con un dado equilibrado de 4 caras la elecci\'on de una
urna, escogiendose
$U_1$,
si salen 1,
2 o 3, y
$U_2$,
si sale 4. Posteriormente se extrae al azar una bola de
esa urna.

\ppart ?` Cual es la probabilidad de que la bola extraida sea azul?
Sugerencia: usar la regla de la probabilidad total. Respuesta: $43/112$.

\ppart  Si la bola extraida resulta ser blanca  ?`cual es la probabilidad de que proceda de la
urna
$U_1$?

Sugerencia: usar  Bayes o el apartado anterior. Respuesta:
$16/23 = 1 -  43/112$.
\solution
\begin{expla}

$U_1 \rightarrow$ 6a, 8b  (1,2,3)

$U_2 \rightarrow$ 3a, 9b  (4)
\end{expla}
\spart
A = bola extraída azul

$U_1$ = Extraemos de la urna 1

$U_2$ = Extraemos de la urna 2
\[
P(A) = P(A\cap U_1)+P(A\cap U_2) = P(A|U_1)P(U_1)+P(A|U_2)P(U_2)=
\]
\[
=\frac{3}{4}\cdot\frac{6}{14}+\frac{1}{4}\cdot\frac{3}{12}=\frac{18}{56}+\frac{3}{48}=\frac{18}{56}+\frac{1}{16}=\frac{36}{112}+\frac{7}{112}=\frac{43}{112}
\]

\spart
B = bola extraida blanca.

$U_1$ = Extraemos de la urna 1.

\[
P(U_1|B)= \frac{P(U_1 \cap B)}{P(B)} = \frac{P(B|U_1)P(U_1)}{P(B|U_1)P(U_1)+P(B|U_2)P(U_2)}=
\]
\[
=\frac{\frac{8}{14}\cdot\frac{3}{4}}{\frac{8}{14}\cdot\frac{3}{4}+\frac{9}{12}\cdot\frac{1}{4}}=\frac{\frac{24}{56}}{\frac{24}{56}+\frac{9}{48}}=\frac{\frac{24}{56}}{\frac{48}{112}+\frac{21}{112}}=\frac{\frac{24}{56}}{\frac{69}{112}}=\frac{48}{69}=\frac{16}{23}
\]

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 1.3  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[3]Enfermedades raras. Ning\'un test biol\'ogico es 100 $\%$ preciso. Supongamos que un test para
determinar si cierta infecci\'on se ha producido, da falsos positivos en un 1 $\%$ de los casos, y falsos
negativos en un 2 $\%$  de los casos. Si una de cada 100 000 personas entre la poblaci\'on general est\'a
infectada, determinar la probabilidad de que una persona escogida al azar est\'e infectada, sabiendo
que el test ha dado positivo.
\solution

\begin{expla}

\begin{center}
\includegraphics[scale=0.75]{img/Dvenn5.png}
\end{center}


\begin{itemize}

\item Las personas o están infectadas o no están infectadas. 1=P(A1)+P(A2)

\item Los tests o dan positivo o dan negativo: 1 = P(+)+P(-)

\item $A_1$ = persona infectada $\rightarrow P(A1)=10^{-5}$

\item $A_2$ = persona no infectada $\rightarrow P(A_2)=1-10^{-5}$

\item + = test positivo

\item - = test negativo

\item Falso positivo (probabilidad de que el test de positivo estando la persona sin infectar) = $1\% = P(+|A_2)=0.01$

\item $P(+|A_2)+P(-|A_2)=1 \rightarrow P(-|A_2)=0.99$

\item Falso negativo (probabilidad de que el test de negativo sabiendo que la persona esta infectada) = $2\% = P(-|A_1)=0.02$

\item $P(-|A_1)+P(+|A_1)=1 \rightarrow P(+|A_1)=0.98$
\end{itemize}

\end{expla}



\[
P(A_1|+)=\frac{P(A_1\cap +)}{P(+)}=\frac{P(+|A_1)P(A_1)}{P(+|A_1)P(A_1)+P(+|A_2)P(A_2)}=
\]
\[
=\frac{0.98\cdot10^{-5}}{0.98\cdot10^{-5}+0.01\cdot\frac{99999}{10^5}}=0.000979
\]

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 1.4  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[4]Angel y Benito tienen sendas barajas espa\~nolas (40 cartas). Cada uno saca de su
baraja una carta al azar (es decir, con iguales probabilidades, e independientemente). Hallar:

\ppart La probabilidad de obtener al menos un as. 

\ppart La probabilidad de obtener dos cartas del mismo palo. 

\ppart La probabilidad de no obtener ning\'un as.

\ppart La probabilidad de no obtener ni una copa ni una espada.
\solution

\begin{expla}

\end{expla}

\spart
A = obtener al menos un AS

B = no obtener ningun AS
\[
P(A)=\frac{4}{40}\cdot\frac{4}{40}+\frac{4}{40}\cdot\frac{36}{40}+\frac{36}{40}\cdot\frac{4}{40} = 0.19
\]

Otra forma
\[
P(A)=1-P(B)=1-\frac{36}{40}\cdot\frac{36}{40} = 1 - 0.81 = 0.19
\]

\spart
C = dos cartas del mismo palo

Da igual de que palo sea la primera carta, la cosa es que la segunda sea del mismo.

Pensando de otra forma tenemos el siguiente espacio muestral:

$\Omega = \{(c, o),(c, e),(c, b),(c, c),(o, o),(o, c),(o, e),(o, b),(e, c),(e, o),(e, b),(e, e),(b, c),(b, e)\\,(b, o),(b, b)\}$

\[
p(C)=\frac{1}{4}
\]

\spart
B = no obtener ningun AS

\[
P(B)=\frac{36}{40}\cdot\frac{36}{40} = 0.81
\]

\spart
D = no obtener ni una copa ni una espada

\[
P(D) = \frac{20}{40}\cdot\frac{20}{40}=\frac{1}{4}
\]


\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 1.5  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[5] Ana y Bea eligen cada una un n\'umero al azar, entre 0 y 2. Sean $A, B, C, D,$ los siguientes
eventos: 

\  $A$: La diferencia entre ambos n\'umeros es al menos 1/3.

\ $B$:   Al menos uno de los n\'umeros es mayor que 1/3.

\ $C$: Los dos n\'umeros son iguales.

\ $D$: El n\'umero de Bea es mayor que 1/3.

Hallar $P(B)$, $P(C)$ y $P(A\cup D)$.
\solution

\begin{expla}
Suponemos que el número elegido es natural, es decir, pertenece al conjunto $\{0,1,2\}$.
\end{expla}

\spart
E = Ningún número es mayor que 1/3.

\[
P(B)=1 - P(E)= 1 - \frac{1}{3}\cdot\frac{1}{3} = \frac{8}{9}
\]

\spart

Da igual qué numero escojas el primero, el que importa es el segundo.

Pensando de otra forma tenemos el siguiente espacio muestral:

$\Omega=\{(0,0),(0,1),(0,2),(1,0),(1,1),(1,2),(2,0),(2,1),(2,2)\}$

\[
P(C)=\frac{1}{3}
\]

\spart
Dado el espacio muestral $\Omega$ definido en el apartado anterior, vemos que:

$P(A)=\frac{2}{3}$ 

Ya que los elementos que cumplen A son: \{(0,1),(0,2),(1,0),(1,2),(2,0),(2,1)\}

$P(D)=\frac{2}{3}$

Ya que los elementos que cumplen D son: \{(0,1),(0,2),(1,1),(1,2),(2,1),(2,2)\}

Por tanto, los elementos que cumplen $A\cup D$ serán la unión de los elementos que cumplen A y los que cumplen D.

\[
P(A \cup D) = \frac{8}{9}
\]

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 1.6  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[6]Con 12 chicas y 4 chicos se forman al azar 4 grupos de 4 personas.
Calcular la probabididad de que haya un chico en cada grupo. Sugerencia:
usar la regla del producto.
 
\solution

\begin{expla}

$A_n$ = Un chico en el grupo n

B = Un chico en cada grupo
\end{expla}

$P(A_1)=4\cdot\frac{4}{16}\cdot\frac{12}{15}\cdot\frac{11}{14}\cdot\frac{10}{13}=0.4835$

$P(A_2|A_1)=4\cdot\frac{3}{12}\cdot\frac{9}{11}\cdot\frac{8}{10}\cdot\frac{7}{9}=0.509$

$P(A_3|A_1\cap A_2)=4\cdot\frac{2}{8}\cdot\frac{6}{7}\cdot\frac{5}{6}\cdot\frac{4}{5}=0.5714$

$P(A_4|A_1\cap A_2\cap A_3)=4\cdot\frac{1}{4}\cdot\frac{3}{3}\cdot\frac{2}{2}\cdot\frac{1}{1}=1$


\[
P(B)=\bigcap_{n=1}^4P(A_n)=P(A_1)P(A_2|A_1)P(A_3|A_2\cap A_1)P(A_4|A_1\cap A_2\cap A_3)=0.14
\]




\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 1.7  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[7]Benito tiene un dado  trucado, con 6 caras  numeradas del 1 al 6. 
La probabilidad de las
distintas caras es proporcional al n\'umero de puntos inscritos en
ellas. Hallar la probabilidad de que Benito obtenga con ese dado un n\'umero
par.
\solution

\begin{expla}

Teniendo en cuenta que la suma de los números del dado es 21

\end{expla}

\[
P(par)=P(2)+P(4)+P(6)=\frac{2}{21}+\frac{4}{21}+\frac{6}{21}=\frac{12}{21}=0.57
\]

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 1.8  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[8] En el esquema que aparece a continuaci\'on, el agua fluye  desde $A$ hacia
$B$. Hay, como se indica en el dibujo, ocho compuertas.
Independientemente unas de otras, cada compuerta est\'{a} abierta con
probabilidad $p$, $0 <p <1$. Calcular la probabilidad de que el agua llegue
 de $A$ a $B$. Calcular dicha probabilidad cuando $p = 1/3$.
% Drawing generated by LaTeX-CAD 1.8a - requires latexcad.sty
% (c) 1996 John Leis leis@usq.edu.au
$$\xymatrix{    &   & \circ\ar @{.}[dr]!U||&  &   \circ  \ar @{.}[dr]!U|| &  &  \\
A \ar[r]  & \circ  \ar @{.}[ru]!U||   \ar @{.}[rd]!U||   & &  \circ \ar @{.}[ru]!U||  \ar @{.}[rd]!U||  & & \circ \ar[r]  & B\\  
  &   & \circ \ar @{.}[ru]!U||  &  &   \circ  \ar @{.}[ru]!U||   &  &  }$$ 

Respuestas: $p^8 - 4 p^6 + 4 p^4, 289/6561$.
\solution

\begin{expla}

Se entiende que en las intersecciones el agua va en todas las direcciones.

Llamamos C al punto intermedio (a la intersección entre los dos rombos).

Para llegar hasta el punto C, tenemos que considerar la existencia de 4 puertas: $P_1, P_2, P_3, P_4$, enumeradas de izquierda a derecha y de arriba hacia abajo. Por tanto, para que el agua llegue a C, deben estar abiertas al menos $P_1$ y $P_2$, o $P_3$ y $P_4$.

Si consideramos el siguiente espacio muestral:

$\Omega = \{(0000),(0001),(0010),(0011),(0100),(0101),...,(1110),(1111)\}$

Formado por 16 elementos, en los que un 1 en la posición n indica que la puerta $P_n$ está abierta, tenemos que con esas 16 combinaciones el agua NO llegaría a C en los elementos con 4 0's (1), los elementos con 3 0,s (4) y los elementos 1010, 0101, 1001, 0110 (4). Por tanto solo nos sirven 7 combinaciones. La de todo 1's, las de 3 1's (4), 1100 y 0011.

$P(1100)=P(0011)=p^2(1-p)^2$

$P(1110)=P(1101)=P(1011)=P(0111)=p^3(1-p)$

$P(1111)=p^4$

Por tanto la probabilidad de llegar a C desde A es:

\[
P(AC)=2p^2(1-p)^2+4p^3(1-p)+p^4
\]

La probabilidad de llegar al punto B, desde el punto C es exactamente la misma que la de ir desde A hasta C.
\end{expla}
Por tanto, según la regla del producto quedaría:
\[
P(AB)=P(AC)P(CB|AC)=P(AC)^2=(2p^2(1-p)^2+4p^3(1-p)+p^4)^2=
\]
\[
=(2p^2(p^2+1-2p)+4p^3-4p^4+p^4)^2 = (2p^4 + 2p^2-4p^3-3p^4+4p^3)^2 =
\]
\[
=(-p^4+2p^2)^2=p^8-4p^6+4p^4
\]

Para $p=\frac{1}{3}$, nos queda:

\[
P(AB)=\frac{1}{3^8}-4\cdot\frac{1}{3^6}+4\cdot\frac{1}{3^4}=\frac{289}{6561}
\]



\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 1.9  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[9]En una reuni\'on hay 25 personas. Calcular la
probabilidad de que celebren su cumplea\~{n}os el mismo d\'ia del
a\~{n}o al menos dos personas. Observación: con frecuencia es m\'as f\'acil
calcular intersecciones que uniones. Sugerencia: calcular la probabilidad
del evento complementario.
\solution


A = cumpleaños mismo día al menos dos personas.

$A^c$ = cumpleaños distinto día todas las personas.

\[
P(A)=1-P(A^c)=1-\frac{365}{365}\cdot\frac{364}{365}\cdot\frac{363}{365}\cdot...\cdot\frac{341}{365}=1-0.43=0.57
\]

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 1.10  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[10]Inclusi\'on-Exclusi\'on: Probar que 
$
P(\cup_{i=1}^n A_i)= \sum_{k=1}^n \sum_{I\subset \{1, \dots, n\}, |I| = k} (-1)^{k-1}
P(\cap_{i\in I} A_i).
$
\solution

\begin{expla}

Tenemos que probar:

\[
P(\bigcup_{i=1}^nA_i)=\sum_{k=1}^{n}\sum_{I\subset \{1,...,n\}:|I|=k}(-1)^{k-1}P(\bigcap_{i\in I}A_i)
\]

Vamos a ver lo que significa para una colección de 3 subconjuntos $\{A_1,A_2,A_3\}$; de forma que sea fácil de ver. El término que puede llevar a la duda es el segundo sumatorio, sólo dice que escojamos todas las subcolecciones posibles de tamaño k dentro de nuestra colección $\{A_1,A_2,A_3\}$.

\begin{center}
\includegraphics[scale=0.75]{img/Dvenn4.png}
\end{center}

\[
P(A_1\cup A_2 \cup A_3)=\underbrace{(-1)^0P(A_1)+(-1)^0P(A_2)+(-1)^0P(A_3)}_{k=1}+
\]
\[
+\underbrace{(-1)^1P(A_1\cap A_2)+(-1)^1P(A_1\cap A_3)+(-1)^1P(A_2\cap A_3)}_{k=2}+\underbrace{(-1)^2P(A_1\cap A_2\cap A_3)}_{k=3}=
\]
\[
=\underbrace{P(A_1)+P(A_2)+P(A_3)}_{k=1}+\underbrace{(-P(A_1\cap A_2)-P(A_1\cap A_3)-P(A_2\cap A_3))}_{k=2}+\underbrace{P(A_1\cap A_2\cap A_3)}_{k=3}
\]

\end{expla}

\begin{itemize}
        \item Hipótesis:

        Suponemos que \[
        P(\bigcup_{i=1}^nA_i)=\sum_{k=1}^{n}\sum_{I\subset
\{1,...,n\}:|I|=k}(-1)^{k-1}P(\bigcap_{i\in I}A_i)
        \]
        \item Base de inducción: n=2

        Para n = 2 se tiene que $$P(A_1\cup A_2)=P(A_1)+P(A_2)-P(A_1\cap
A_2)$$ Vemos que esto es cierto para cualquier conjunto.

        \item Inducción:
        Supongamos la hipótesis cierta para n. Vamos a demostrar que es
válida para n+1.
        $${P\big(\bigcup_{i=1}^{n+1}A_i\big)=P\big((\bigcup_{i=1}^nA_i)\cup A_{n+1}\big)=P\big(\bigcup_{i=1}^nA_i\big)+P(A_{n+1})-P\big((\bigcup_{i=1}^nA_i)\cap
A_{n+1}\big)}$$
        A esto hemos llegado aplicando la fórmula para n = 2.

        Por hipótesis inductiva sabemos lo que vale $P\big(\bigcup_{i=1}^nA_i\big)$.
        Ahora escribimos $P\big((\bigcup_{i=1}^nA_i)\cap
A_{n+1}\big)=P\big(\bigcup_{i=1}^n(A_i\cap A_{n+1})\big)$

        $$P(\bigcup_{i=1}^n(A_i\cap A_{n+1}))=\sum_{1\leq i_1\leq
n}P(A_{i_1}\cap A_{n+1})\quad -\sum_{1\leq i_1<i_2\leq
n}P\big((A_{i_1}\cap A_{n+1})\cap (A_{i_2}\cap A_{n+1})\big)\quad
$$$$+\sum_{1\leq i_1<i_2<i_3\leq n}P\big((A_{i_1}\cap A_{n+1})\cap
(A_{i_2}\cap A_{n+1})\cap (A_{i_3}\cap
A_{n+1})\big)\;+\;\cdots$$$$+\;(-1)^{n-1}\sum_{1\leq
i_1<\cdots<i_n\leq n}P\big((A_{i_1}\cap A_{n+1})\cap \cdots \cap
(A_{i_n}\cap A_{n+1})\big)$$

        Además tenemos que:
        $$ P\big((A_{i_1}\cap A_{n+1})\cap(A_{i_2}\cap A_{n+1})\cap \cdots
\cap (A_{i_n}\cap A_{n+1})\big)=P(A_{i_1}\cap A_{i_2}\cap\cdots\cap
A_{i_n}\cap A_{n+1})$$
        Al final nos queda que :
        $$P(\bigcup_{i=1}^{n+1}A_i) = \sum_{k=1}^{n}\sum_{I\subset
\{1,...,n\}:|I|=k}(-1)^{k-1}P(\bigcap_{i\in I}A_i) + P(A_{n+1})$$$$ -
\sum_{k=1}^{n}\sum_{I\subset \{1,...,n\}:|I|=k}(-1)^{k-1}P(A_{i_1}
\cap A_{i_2} \cap .... \cap A_{i_k} \cap A_{n+1})$$
        Y esto es igual a:
        $$\sum_{k=1}^{n+1}\sum_{I\subset
\{1,...,n+1\}:|I|=k}(-1)^{k-1}P(\bigcap_{i\in I}A_i)$$
\end{itemize}

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 1.11  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[11] Emparejamientos al azar: tenemos $n$ cartas, que colocamos al azar en $n$ sobres
(en vez de cuidadosamente poner cada carta en su sobre).
Calcular la probabilidad de que alguna carta est\'a en el sobre correcto
(es decir, al menos una carta). Estimar dicha probabilidad cuando
$n\to\infty$. Sugerencia: usar Inclusi\'on-Exclusi\'on.
Observar que la probabilidad de que todas las cartas de 1 a $k$ esten en el sobre correcto
es $(n-k)!/n!$. Respuesta en el l\'{\i}mite: $1 - e^{-1}$.
\solution

\begin{expla}

Para hacernos una idea del problema consideramos n=3 y el siguiente espacio de probabilidad:

$\Omega_3=\{(123),(132),(213),(231),(312),(321)\}$

Que representa todas las posibles combinaciones (numero de carta-numero sobre, es decir, 3!) que puede haber con 3 cartas y 3 sobres. 

Según esto la probabilidad de que la carta 1 este en el sobre 1 sería: $\frac{2}{6} = \frac{2}{3\cdot 2}=\frac{1}{3}$

Sea: $A_i$ = carta n-esima en el sobre correcto.

Se observa fácilmente que: $P(A_i) = \frac{(n-1)!}{n!}$, siendo n el número de cartas totales.

Sea A = todas las cartas en el sobre correcto. Simplemente hay que observar que si la primera carta esta en el sobre correcto, para poner la segunda en su sobre tenemos una carta y un sobre menos donde elegir.

\[
P(A)=P(\bigcap_{j=1}^nA_j)=P(A_1)P(A_2|A_1)P(A_3|A_2\cap A_1)...P(A_j|\bigcap_{i=1}^{n-1}A_i)=
\]
\[
\frac{(n-1)!}{n!}\cdot\frac{(n-2)!}{(n-1)!}\cdot\frac{(n-3)!}{(n-2)!}\cdot...\cdot\frac{(n-n+1)!}{(n-n+2)!}\cdot\frac{(n-n)!}{(n-n+1)!}=\frac{1}{n!}
\]

Sea $B_k$ = todas las cartas de la 1 a la k en el sobre correcto.

\[
P(B_k)=P(\bigcap_{n=1}^kA_n)=P(A_1)P(A_2|A_1)P(A_3|A_2\cap A_1)...P(A_n|\bigcap_{i=1}^{k-1}A_i)=
\]
\[
\frac{(n-1)!}{n!}\cdot\frac{(n-2)!}{(n-1)!}\cdot\frac{(n-3)!}{(n-2)!}\cdot...\cdot\frac{(n-k+1)!}{(n-k+2)!}\cdot\frac{(n-k)!}{(n-k+1)!}=\frac{(n-k)!}{n!}
\]

Esta probabilidad es la misma cojamos las k primeras cartas o k cartas en diferentes posiciones.
\end{expla}

Vamos a usar Inclusión-Exclusión:
\[
P(\bigcup_{i=1}^nA_i)=\sum_{k=1}^{n}\sum_{I\subset \{1,...,n\}:|I|=k}(-1)^{k-1}P(\bigcap_{i\in I}A_i)
\]

C = Al menos una carta en el sobre correcto: 

\[
P(C)=P(\bigcup_{i=1}^nA_i)=\sum_{k=1}^{n}\sum_{I\subset \{1,...,n\}:|I|=k}(-1)^{k-1}\frac{(n-k)!}{n!}=\sum_{k=1}^{n}\left( (-1)^{k-1}\frac{(n-k)!}{n!}\binom{n}{k}\right)=
\]
\[
= \sum_{k=1}^{n}\left( (-1)^{k-1}\frac{(n-k)!}{n!}\cdot \frac{n!}{(n-k)!\cdot k!}\right)= \sum_{k=1}^{n}\left( (-1)^{k-1}\frac{1}{k!}\right) \stackrel{n \rightarrow \infty}{\rightarrow} 1-e^{-1}
\]
Ahora vamos a demostrar que:

\[
\sum_{k=1}^{n}\left( (-1)^{k-1}\frac{1}{k!}\right) \stackrel{n \rightarrow \infty}{\rightarrow} 1-e^{-1}
\]

\begin{proof}
\[
\sum_{k=1}^{n}\left( (-1)^{k-1}\frac{1}{k!}\right) = 1 + \sum_{k=0}^{n}\left((-1)^{k-1}\frac{1}{k!}\right) = 1 - \sum_{k=0}^{n}\left((-1)^{k}\frac{1}{k!}\right)
\]

Por otro lado, aplicando Taylor, sabemos que:

\[
f(x)=e^x=f(0)+\frac{f'(0)}{1!}(x-0)+\frac{f''(0)}{2!}(x-0)^2...
\]

Y por tanto:

\[
f(-1)=e^{-1}=1-\frac{1}{1!}+\frac{1}{2!}-\frac{1}{3!}... = \sum_{k=0}^{n}\left((-1)^{k}\frac{1}{k!}\right)
\]
\end{proof}



\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 1.12  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[12] Media o esperanza. Con los datos del problema anterior, calcular el n\'umero esperado de
emparejamientos al azar, es decir, cuantas cartas esperamos que est\'an en el sobre correcto.
Comentario: este problema es muy f\'acil.
\solution

\begin{expla}
\[
\mathbb{E}(X) = \sum_{w \in \Omega} X(w)P(w)
\]

En nuestro caso $\Omega$ esta formado por n! eventos (tal y como vimos en el ejercicio anterior), X es una variable aleatoria que vale "m", siendo m el número de cartas que están bien emparejadas con su sobre.

Así no se debe hacer: 
Llamamos $P(B_k)$ a la probabilidad de k cartas bien emparejadas. 

\[
\mathbb{E}(X) = \sum_{w \in \Omega} X(w)P(w) = \sum_{k=0}^n kP(B_k) = \sum_{k=0}^n k\cdot\frac{(n-k)!}{n!}
\]

Pero esto no vale porque $P(B_k)$ no te específica si el resto de cartas están o no emparejadas.
\end{expla}
Llamamos $A$ a la variable aleatoria que dice cuántas cartas están en el sobre correcto.

Consideramos $A_i$ a la variable aleatoria que vale 1 si la carta i está en el sobre i-ésimo y 0 en caso contrario. Y tenemos que $P(A_i=1)=\frac{1}{n}$.

Por tanto:
\[
\mathbb{E}(A)=\mathbb{E}({\sum_{i=1}^{n} A_i}) = \sum_{i=1}^n \mathbb{E}(A_i) = \sum_{i=1}^n 1 \cdot \frac{1}{n} = 1
\]




\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 1.13  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[13] Dado $C$ con $P(C) > 0$, decimos que $A$ y $B$ son condicionalmente independientes
con respecto a $C$ si $P(A\cap B|C) =P(A|C) P(B|C)$. Probar que si  $P(B\cap C) > 0$, 
$P(A\cap B|C) =P(A|C) P(B|C)$ es equivalente a $P(A|C) = P(A|B \cap C)$. 


\solution

\begin{expla}
Partimos de: $P(A\cap B|C)=P(A|C)P(B|C)$  

Despejamos $P(A|C)$ y operamos:
\end{expla}

\[
P(A|C)=\frac{P(A\cap B|C)}{P(B|C)}=\frac{\frac{P(A\cap B\cap C)}{P(C)}}{\frac{P(B \cap C)}{P(C)}}=\frac{P(A \cap B\cap C)}{P(B\cap C)}=P(A|B\cap C)
\]

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 1.14  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[14] Estudiar para $ \alpha>0 $ la convergencia en
media cuadr\'atica (es decir, en $L^2$) de la sucesi\'on $\{X_n\}_{n=1}^\infty$, sabiendo que

\[  P(X_n=n)=\frac{1}{n^\alpha}, \hspace{5mm} P(X_n=0)=1-\frac{1}{n^\alpha}.
  \]
\solution

\begin{expla}
Dado un espacio de probabilidad $(\Omega, \algb{M}, P)$, y una variable aleatoria $X_n$, tenemos que $X_n$ es también una función medible que cumple:

\[
X_n \stackrel{L_2 (n\rightarrow \infty)}{\rightarrow} X \Leftrightarrow \norm{X_n -X}_2  \rightarrow 0 \Leftrightarrow \int{\abs{X_n -X}^2dP} \rightarrow 0
\]

Tenemos que cuando $n \rightarrow \infty$, se cumple que:
\begin{itemize}
\item $P(X_n = n) \rightarrow 0$
\item $P(X_n = 0) \rightarrow 1$
\end{itemize}

Así definimos X como una variable aleatoria que cumple:

\begin{itemize}
\item $P(X = n) = 0$
\item $P(X = 0) = 1$
\end{itemize}

\end{expla}

\[
\int{\abs{X_n -X}^2dP} = \int_{\Omega}{\abs{X_n(w) -X(w)}^2dP(w)}
\]

Como X toma el valor 0 con probabilidad 1, tenemos:

\[
\int_{\Omega}\abs{X_n(w) -X(w)}^2dP(w)=\int_{\Omega}\abs{X_n(w)}^2dP(w)=\int_{\Omega}(X_n(w))^2dP(w) =
\]
\[
=\mathbb{E}((X_n)^2)=n^2\cdot\frac{1}{n^{\alpha}}+0^2\cdot(1-\frac{1}{n^{\alpha}})=\frac{n^2}{n^{\alpha}} \stackrel{n \rightarrow \infty}{\rightarrow} 0 \text{ si } \alpha>2 
\]

Por tanto la sucesión $\{X_n\}_{n=1}^\infty$ converge cuadráticamente si $\alpha > 2$.



\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





\newpage
\section{Hoja 2}

\textcolor{blue}{CORREGIDA Y CONTRASTADA CON LAS SOLUCIONES DEL PROFESOR}

Se asume siempre que estamos trabajando en un espacio de probabilidad $(\Omega, \mathcal{A}, P)$,
y que  $\mathcal{B}\subset \mathcal{A}$ es una sub-$\sigma$-\'algebra.


%%%%%%%%%%%%%%%%%%  PROBLEMA 2.1  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[1]Sea $X$ una v.a. con distribuci\'on $N(0,1)$ (normal con media 0 y varianza 1). 
Calcular el tercer momento $E(X^3)$.
\solution

\begin{expla}
Sea una variable aleatoria X, a esta se le asigna una función de distribución ($F_X(t)$) y otra de densidad ($f_X(t)$) que cumplen:

\[
F_X(t)=\int_{-\infty}^{t}f_x(t)dt
\]

En el caso de distribución N(0,1) tenemos:
\[
f(x)=\frac{e^{-t^2/2}}{\sqrt{2\pi}}
\]

Sabiendo que la esperanza se calcula como:
\[
\mathbb{E}(X)=\int_{-\infty}^{\infty}t\cdot f(t)dt
\]

Escribimos:
\end{expla}

\[
\mathbb{E}(X^3)=\int_{-\infty}^{\infty}t^3\cdot \frac{e^{-t^2/2}}{\sqrt{2\pi}}dt = \frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}t^3\cdot e^{-t^2/2}dt
\]

Hacemos un cambio de variable y llamamos $w=\frac{t^2}{2}$ por tanto: $\frac{dw}{dt}=t$

\[
\frac{1}{\sqrt{2\pi}}\int t^3\cdot e^{-t^2/2}dt = \frac{1}{\sqrt{2\pi}}\int 2tw\cdot e^{-w}\frac{dw}{t} = \frac{2}{\sqrt{2\pi}}\int w\cdot e^{-w}dw
\]

Ahora vamos a integrar por partes

$u=w$

$du=1$

$v=-e^{-w}$

$dv=e^{-w}$

\[
\frac{2}{\sqrt{2\pi}}\int w\cdot e^{-w}dw = \frac{2}{\sqrt{2\pi}}\left( -we^{-w}+\int e^{-w}dw \right) =\frac{2}{\sqrt{2\pi}}\left( -we^{-w}-e^{-w} \right)=
\]
\[
= \frac{-2e^{-w}}{\sqrt{2\pi}}(w+1)
\]

Y nos queda que:
\[
\mathbb{E}(X^3)=\int_{-\infty}^{\infty}t^3\cdot \frac{e^{-t^2/2}}{\sqrt{2\pi}}dt = \left.\frac{-2e^{-t^2/2}}{\sqrt{2\pi}}(t^2/2+1)\right|_{-\infty}^{\infty}=0
\]




\end{problem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%  PROBLEMA 2.2  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[2]Sea $\mathcal{B} \subset \mathcal{A}$ la sub-$\sigma$-\'algebra generada por una partici\'on
$\{A_1, \dots,A_n\}$ de $\Omega$, donde todos los conjuntos de la partici\'on tienen probabilidad positiva.
Dada una v.a. $X$, demostrar que si la variable aleatoria $T(X)$ satisface

1) para todo $B\in \mathcal{B}$, $\int_B T(X) dP = \int_B X dP$, y

2) $T(X)$ es  $\mathcal{B}$-medible, 

entonces cuando $w\in A_i$, $T(X)(w)= \frac{1}{P(A_i)}\int_{A_i} X dP$.

Probar que si  $T(X)(w)$ est\'a definida mediante  $T(X)(w):= \frac{1}{P(A_i)}\int_{A_i} X dP$ cuando  
 $w\in A_i$, entonces $T(X)$ satisface las condiciones 1) y 2) enunciadas arriba.

Este ejercicio demuestra que las condiciones 1) y 2) determinan de modo \'unico la esperanza condicional,
cuando la  sub-$\sigma$-\'algebra est\'a generada por una partici\'on (el resultado tambi\'en es cierto
para  sub-$\sigma$-\'algebras arbitrarias, pero entonces es necesario definir la esperanza condicional
mediante un procedimiento distinto).
\solution

\begin{expla}

Tenemos que demostrar la doble implicación, empezamos de izquierda a derecha y luego lo demostraremos de derecha a izquierda.
\end{expla}

\begin{itemize}
\item $\Rightarrow)$ Sabemos que:
\begin{enumerate}
\item $\forall B \in \algb{B}, \int_{B}T(X)dP = \int_{B}XdP$
\item $T(X)$ es $\algb{B}-medible$
\end{enumerate}

Tenemos que llegar a: $T(X)(w)= \frac{1}{P(A_i)}\int_{A_i} X dP$ con $w \in A_i$.

Partimos del punto 1, sabemos que esa propiedad se cumple $\forall B \in \algb{B}$, por tanto, se cumplirá para todos los elementos $A_i$ de la partición:
\[
\int_{A_i}T(X)dP = \int_{A_i}XdP  \Leftrightarrow  \frac{1}{P(A_i)}\cdot\int_{A_i}T(X)dP = \frac{1}{P(A_i)}\cdot\int_{A_i}XdP
\]

la parte derecha ya es idéntica a la parte derecha de la fórmula a la que queremos llegar, por tanto, sólo nos queda demostrar que: $\frac{1}{P(A_i)}\cdot\int_{A_i}T(X)dP = T(X)(w)$

Observamos que esto es cierto si la integral no depende de T(X) y se puede sacar como constante, ya que de darse eso tendríamos:

\[
\frac{1}{P(A_i)}\cdot\int_{A_i}T(X)dP = \frac{1}{P(A_i)}\cdot T(X)\cdot\int_{A_i}1dP = \frac{P(A_i) \cdot T(X)}{P(A_i)} = T(X)
\]

(Ver definición de integral sobre una medida)

Es aquí cuando usamos que T(X) es $\algb{B}-medible$. Eso quiere decir, que la antiimagen de un medible pertenece a $\algb{B}$. Como los elementos de $\algb{B}$ son los conjuntos $A_i$, quiere decir que T(X) (que es lo mismo que T(X(w))) tiene que tener valor constante para todos los elementos $w \in A_i$. 

¿Por qué?, bueno, piensa que en $A_2$ tenemos 2 elementos: $w_1$ y $w_2$, si $T(X(w_1))=2$ y $T(X(w_2))=4$, entonces, $T^{-1}(1,3)=w_1$ que es un elemento que NO pertenece a $\algb{B}$. Si no ha quedado claro con esta explicación, se puede ver algún ejemplo más tras la demostración de 'Ley de X' de estos apuntes.

Por tanto, definimos T(X)(w)=T(X(w)) y concluimos que T(X)(w) es constante para los w pertenecientes a cada partición $A_i$. Por tanto queda demostrada la implicación primera ya que:

\[
\frac{1}{P(A_i)}\cdot\int_{A_i}T(X)(w)dP = \frac{1}{P(A_i)}\cdot T(X)(w)\cdot\int_{A_i}1dP = \frac{P(A_i) \cdot T(X)(w)}{P(A_i)} =
\]
\[
=T(X)(w)  \text{ para todo } w \in A_i
\]

\item $\Leftarrow)$ Sabemos que:

$T(X)(w)= \frac{1}{P(A_i)}\int_{A_i} X dP$ con $w \in A_i$.

Tenemos que llegar a: 
\begin{enumerate}
\item $\forall B \in \algb{B}, \int_{B}T(X)dP = \int_{B}XdP$
\item $T(X)$ es $\algb{B}$-medible
\end{enumerate}

Empezamos demostrando 2: Lo hacemos por reducción al absurdo. Suponemos que no es $\algb{B}$-medible, entonces existe al menos un conjunto $A_j$ con al menos dos elementos $w_1$ y $w_2$ tal que $T(X(w_1))=c_1$ y $T(X(w_2))=c_2$ con $c_1 \neq c_2$. Por tanto:

\[
c_1=T(X)(w_1)=\frac{1}{P(A_j)}\int_{A_j} X dP \neq \frac{1}{P(A_j)}\int_{A_j} X dP=T(X)(w_2)=c_2
\]

Lo cual es contradictorio, porque como podemos ver, T(X)(w) es constante para elementos pertenecientes al mismo $A_i$, es decir, a la misma clase de equivalencia.

Ahora demostramos 1:

\[
T(X)(w)= \frac{1}{P(A_i)}\int_{A_i}X dP \Leftrightarrow T(X)(w)\cdot P(A_i)= \int_{A_i}X dP
\]

Como T(X) es $\algb{B}$-medible, entonces es constante para todos los w pertenecientes a una misma clase de equivalencia $A_i$, por tanto podemos introducir T(X) dentro de la integral quedando:
\[
\int_{A_i}T(X)dP = \int_{A_i}XdP
\]

Sea $B \in \algb{B}$ de modo que $B=A_{i_1} \cup A_{i_2} \cup ... \cup A_{i_k}$, entonces llegamos a la propiedad 1:

\[
\forall B \in \algb{B}, \int_{B}T(X)dP = \int_{A_{i_1}}T(X)dP + \int_{A_{i_2}}T(X)dP + ... + \int_{A_{i_k}}T(X)dP =
\]
\[
= \int_{A_{i_1}}XdP + \int_{A_{i_2}}XdP + ... + \int_{A_{i_k}}XdP = \int_{B}XdP
\]



\end{itemize}




\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 2.3  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[3]En un examen tipo test se plantean 5 preguntas para responder verdadero o falso. 
 Los puntos asignados a las preguntas V o F son: respuesta correcta,
1 punto, respuesta incorrecta,
- 1 punto, en blanco, 0 puntos. Valor m\'{\i}nimo del  problema: 0 puntos. Es decir, si la puntuaci\'on es
negativa se asigna un cero al problema. 

a)  Calcular la nota esperada de un alumno que responda a las 5 perguntas
de manera aleatoria, por ejemplo lanzando una moneda equilibrada 5 veces.

b) Sabiendo que el  alumno ha respondido correctamente a  la
primera pregunta, calcular la nota esperada.
\solution

\textcolor{green}{ESTA SOLUCIONADO EN EL EXAMEN FINAL DEL CURSO 2013/2014}

\begin{expla}
Se nos podría ocurrir plantear el problema de la siguiente manera:

Sea $X_i$ v.a. independientes que cumplen : $P(X_i(w)=1)=1/2=P(X_i(w)=-1) \forall i$

Por tanto, tenemos que:
\[
\mathbb{E}(X_i)=\frac{1}{2}\cdot(1)+\frac{1}{2}\cdot(-1)=0
\] 

Y si consideramos que i=5:
\[
\mathbb{E}\left(\sum_{i=1}^{5}X_i\right)=\sum_{i=1}^{5}\mathbb{E}(X_i)=0
\]

Pero esto no lo podemos plantear así ya que la nota mínima del problema es de 0. Y eso implica que las variables aleatorias $X_i$ no se pueden sumar ya que son dependientes y no expresarían la solución del problema.

Por tanto lo planteamos de otra manera: Sea el siguiente espacio muestral:

$\Omega = \{(00000), (00001), (00010),..., (11111)\}$, hay un total de $2^5=32$ combinaciones diferentes de posibles respuestas. Teniendo en cuenta que las preguntas no se dejan en blanco. Consideramos la variable aleatoria Z, asociada a ese espacio muestral.

\end{expla}

\spart
\begin{itemize}
\item 5 mal: nota del problema = 0
\item 1 bien y 4 mal: nota del problema = 0
\item 2 bien y 3 mal: nota del problema = 0
\item 3 bien y 2 mal: nota del problema = 1
\item 4 bien y 1 mal: nota del problema = 3
\item 5 bien: nota del problema = 5
\end{itemize}

\[
\mathbb{E}(Z)=\frac{\binom{5}{0}}{32}\cdot0+\frac{\binom{5}{1}}{32}\cdot0+\frac{\binom{5}{2}}{32}\cdot0+\frac{\binom{5}{3}}{32}\cdot1+\frac{\binom{5}{4}}{32}\cdot3+\frac{\binom{5}{5}}{32}\cdot5=
\]
\[
=\frac{\binom{5}{3}}{32}\cdot1+\frac{\binom{5}{4}}{32}\cdot3+\frac{\binom{5}{5}}{32}\cdot5=\frac{\frac{5!}{3!\cdot 2!}}{32}\cdot1+\frac{\frac{5!}{4!\cdot 1!}}{32}\cdot3+\frac{1}{32}\cdot5=\frac{10}{32}\cdot1+\frac{5}{32}\cdot3+\frac{1}{32}\cdot5=\frac{10}{32}+\frac{15}{32}+\frac{5}{32}=\frac{30}{32}=0.9375
\]

\spart
En esta ocasión:

A = posibles respuestas sabiendo que la primera respuesta es correcta =$\{(10000), (10001), (10010),..., (11111)\}$.


Estudiamos las posibles dentro del conjunto A, mirando los 4 digitos de mas a la derecha:
\begin{itemize}
\item 4 mal: nota del problema = 0
\item 1 bien y 3 mal: nota del problema = 0
\item 2 bien y 2 mal: nota del problema = 1
\item 3 bien y 1 mal: nota del problema = 3
\item 4 bien: nota del problema = 5
\end{itemize}

\[
\mathbb{E}(Z|A)=\frac{1}{P(A)}\left( \frac{\binom{4}{0}}{32}\cdot0+\frac{\binom{4}{1}}{32}\cdot0+\frac{\binom{4}{2}}{32}\cdot1+\frac{\binom{4}{3}}{32}\cdot3+\frac{\binom{4}{4}}{32}\cdot5 \right)=
\]

\[
= \frac{1}{\frac{1}{2}}\left( \frac{\binom{4}{2}}{32}\cdot1+\frac{\binom{4}{3}}{32}\cdot3+\frac{\binom{4}{4}}{32}\cdot5\right)=
2\cdot \left( \frac{\frac{4!}{2!\cdot 2!}}{32}\cdot1+\frac{\frac{4!}{3!\cdot 1!}}{32}\cdot3+\frac{1}{32}\cdot5 \right) =
\]
\[
= 2\cdot \left(\frac{6}{32}\cdot1+\frac{4}{32}\cdot3+\frac{1}{32}\cdot5 \right) = 2 \cdot \frac{23}{32}=\frac{46}{32}=1.4375
\]
%TODO se ha hecho por la cuenta la vieja


\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 2.4  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[4]Lanzamos un dado equilibrado de 4 caras dos veces. Sea $W$ la variable aleatoria que toma como valor
el m\'aximo de los dos lanzamientos. A continuaci\'on lanzamos una moneda equilibrada $W$ veces,  usando
$S$ para denotar el n\'umero de caras obtenido. Todos los lanzamientos son independientes.

a) Hallar $E(S|W)$.
 
b)  Hallar $E(S)$.
\solution

\begin{expla}
Por definición:

\[
E_{P_B}(X)=\frac{1}{P(B)}\int_{B}X(w)dP(w)=E(X|B)
\]

W = máximo de lanzar un dado de 4 caras 2 veces.

Consideramos el siguiente espacio muestral:

$\Omega=\{(1,1),(1,2),(1,3),(1,4),(2,1),(2,2),...,(4,1),(4,2),(4,3),(4,4)\}$

Donde $W(a,b)=max(a,b)$

Sea $P(X_i)=1/2$ probabilidad de que salga cara en el lanzamiento i-ésimo de una moneda. $X(w)=1$ si sale cara y 0 si sale cruz.

Por tanto: definimos $Y_n \sim Binomial(n,\frac{1}{2})$. $Y_n$ corresponde a las caras obtenidas al lanzar una moneda n veces.

\end{expla}


\spart
De manera intuitiva, vemos que el numero de caras que pueden salir, condicionando el numero de tiradas que vamos a hacer (W), es exactamente:

$\mathbb{E}(S|W)=P(cara)\cdot(\text{número de tiradas})=\frac{W}{2}$

Y que responde a una $Binomial(W,\frac{1}{2})$, quedando:

\[
\mathbb{E}(S|W)=\sum_{i=1}^{W}\mathbb{E}(X_i)=\mathbb{E}(Y_W)=\frac{W}{2}
\] 


SOLUCIÓN DEL PROFESOR:
$X_i=1$ si en el lanzamieno i-ésimo sale cara, $X_i=0$ si no. $P(X_i=1)=\frac{1}{2}$

$S_n=X_i + X_2 +...+X_n$

Definimos $S_w$ con $w=max{Y_1,Y_2}$ siendo $Y_1=$ primer lanzamiento e $Y_2=$segundo lanzamiento.

Sabemos que $P(Y_n=m)=\frac{1}{4}$ para $m=1,2,3,4$. Por tanto $P(Y_1=i_1,Y_2=i_2)=\frac{1}{16}$ para $1\leq i_1, i_2 \leq 4$

$P(W=1)=\frac{1}{16}$, $P(W=2)=\frac{3}{16}$, $P(W=3)=\frac{5}{16}$ y $P(W=4)=\frac{7}{16}$

Por tanto:

$$
\mathbb{E}(S|W)(w) =
  \left\lbrace
  \begin{array}{l}
     \mathbb{E}(S|W=1) = \frac{1}{2} \text{ si } w \in W^{-1}(1) \\
     \mathbb{E}(S|W=2) = 1 \text{ si } w \in W^{-1}(2) \\
     \mathbb{E}(S|W=3) = \frac{3}{2} \text{ si } w \in W^{-1}(3) \\
     \mathbb{E}(S|W=4) = 2 \text{ si } w \in W^{-1}(4) \\
  \end{array}
  \right.
$$


%Vamos a intentar aplicar la fórmula que tenemos para la definición de esperanza condicionada, nos quedaría:


%\[
%\mathbb{E}(S|W)=\frac{1}{P(W)}\sum_{w\in W}X(w)P(w)
%\]

%Habría que definir el espacio muestral S, que contemplaría el número de veces que se tira la moneda y las posibilidades de obtener un número de caras para esas veces, es bastante lioso, por lo tanto lo que hacemos es declarar una variable aleatoria $S_{i,w}$ que calcula la probabilidad de obtener cara en el lanzamiento i-ésimo, sabiendo que vamos a tirar w veces.

%\[
%= \frac{1}{P(W)}\sum_{w \in W}(1\cdot P(w=cara)\cdot P(w\in W)+0\cdot P(w=cruz)\cdot P(w\in W)) = 
%\]

%\[
%= \frac{1}{P(W)}\sum_{w \in W}1\cdot P(w=cara)\cdot P(w\in W)= \frac{1}{P(W)}\sum_{w \in W}1\cdot \frac{1}{2}\cdot P(W) =  W\cdot \frac{1}{2}
%\]

%Lo que hemos querido poner es que hay que mirar también la probabilidad de que se realizan W tiradas, ya que el sumatorio se realiza sobre elementos que pertenecen al espacio muestral donde se tiene en cuenta el número de tiradas que se realizan, y que viene determinado por W que es otra variable aleatoria.

\spart
$P(W=1)=\frac{1}{16}$, $P(W=2)=\frac{3}{16}$, $P(W=3)=\frac{5}{16}$ y $P(W=4)=\frac{7}{16}$

%TODO : este ejercicio esta resuelto sin aplicar ninguna fórmula dada en teoría.

\[
\mathbb{E}(S)=\mathbb{E}(\mathbb{E}(S|W))=\sum_{i=1}^{4}P(W=i)\cdot\mathbb{E}(S|W=i)=\sum_{i=1}^{4}\left( P(W=i)\cdot i \cdot \frac{1}{2}\right)= 
\]

\[
=\frac{1}{16}\cdot1\cdot\frac{1}{2}+\frac{2}{16}\cdot2\cdot\frac{1}{2}+\frac{6}{16}\cdot3\cdot\frac{1}{2}+\frac{7}{16}\cdot4\cdot\frac{1}{2}=\frac{1}{32}+\frac{1}{8}+\frac{18}{32}+\frac{28}{32}=\frac{51}{32}
\]
\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newpage
\section{Hoja 3}

\textcolor{blue}{CORREGIDA Y CONTRASTADA CON LAS SOLUCIONES DEL PROFESOR}


Salvo afirmaci\'on expresa en sentido
contrario se asume siempre que estamos trabajando en un espacio de probabilidad $(\Omega, \mathcal{A}, P)$,
que  $\mathcal{B}\subset \mathcal{A}$ es una sub-$\sigma$-\'algebra, que las funciones son medibles, etc..

Recordatorio: si $1\le p < \infty$, $\|f\|_p := \left(\int|f|^p\right)^{1/p}$, mientras que
$\|f\|_\infty$ denota el supremo esencial de $|f|$. De hecho, la definici\'on
 $\|f\|_p := \left(\int|f|^p\right)^{1/p}$ tiene sentido para cualquier $p > 0$ finito, pero puede demostrarse que si $p < 1$ esta expresi\'on no define una norma.


%%%%%%%%%%%%%%%%%%  PROBLEMA 3.1  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[1] La desigualdad aritm\'etico-geom\'etrica 
dice que la media  aritm\'etica de un conjunto de n\'umeros no negativos es al menos tan grande
como su media  geom\'etrica, es decir, si $a_1, \dots ,a_n > 0$ satisfacen  $a_1 + \cdots  + a_n = 1$,  y  
 $x_1, \dots ,x_n \ge 0$, entonces $\Pi_{i=1}^n x_i^{a_i} \le \sum_{i=1}^n a_i x_i$.
Demostrar dicha desigualdad. Sugerencia: usar la concavidad
de log, o la convexidad de exp.
\solution

\begin{expla}
Una función convexa cumple que: $f(tx+(1-t)y) \leq tf(x)+(1-t)f(y)$, para todo $x\leq y$. 't' y '1-t' se puede cambiar por cualquier numero finito de coeficientes que sumen 1,(la cosa es que evaluemos la función en un punto intermedio entre x e y).
Una función f es cóncava $\Leftrightarrow$ -f es convexa.
\end{expla}
Vemos que la igualdad se cumple también con el logaritmo.

\[
\prod_{i=1}^n x_i^{a_i} \leq \sum_{i=1}^{n}a_i x_i \Leftrightarrow \log\left(\prod_{i=1}^n x_i^{a_i}\right) \leq \log \left(\sum_{i=1}^{n}a_i x_i\right)
\]

Por tanto, definimos la función convexa: $f(x) = -\log(x)$, entonces, por convexidad tenemos que:
\[
-\log \left(\sum_{i=1}^{n}a_i x_i\right) \leq \sum_{i=1}^{n} a_i\left(-\log(x_i)\right) = \sum_{i=1}^{n} -\log\left((x_i)^{a_i}\right) 
\]
 
Multiplicamos por '-1' a ambos lados y nos queda:
\[
\log \left(\sum_{i=1}^{n}a_i x_i\right) \geq \sum_{i=1}^{n} \log\left((x_i)^{a_i}\right) = \log\left(\prod_{i=1}^n x_i^{a_i}\right) 
\]


\end{problem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%  PROBLEMA 3.2  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[2] Demostrar la siguiente desigualdad de Young: para $t, u \ge 0$, y $p,q > 1$ tales que
$1/p + 1/q =1$, tenemos $tu \le t^p/ p + u^q/ q$. Observaci\'on: \'esta es otra forma de
escribir la  desigualdad aritm\'etico-geom\'etrica para $n=2$, mediante un cambio obvio de variables.
\solution

\begin{expla}
Utilizamos $t=\sqrt[p]{w}$ y $u=\sqrt[q]{z}$
\end{expla}
Nos queda:
\[
tu\leq\frac{t^p}{p}+\frac{u^q}{q} \Leftrightarrow \sqrt[p]{w}\sqrt[q]{z} \leq \frac{\sqrt[p]{w}^p}{p} + \frac{\sqrt[q]{z}^q}{q} \Leftrightarrow w^{\frac{1}{p}} z^{\frac{1}{q}}\leq\frac{w}{p}+\frac{z}{q}
\]

Que es equivalente a la situación dele ejercicio anterior con: $a_1=\frac{1}{p}$, $a_2=\frac{1}{q}$. 

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 3.3  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[3] Sean $p,q \ge 1$ exponentes conjugados, es decir, $p$ y $q$ satisfacen
$1/p + 1/q =1$ (si $p=1$, entonces $q = \infty$, y viceversa).
Dado un espacio de medida arbitrario $(\Omega, \mathcal{A}, \mu)$, demostrar la desigualdad de
H\"older: si $f\in L^p$ y $g\in L^q$, entonces $fg\in L^1$, y $\|fg\|_1 \le \|f\|_p\|g\|_q$.
Sugerencias: El caso $p=1, q=\infty$ sale directamente de las definiciones.
Para $p >1$, suponemos que  $\|f\|_p, \|g\|_q\ne 0$, y reemplazamos 
$f$ y $g$ con 
 $f/\|f\|_p$ y $g/\|g\|_q$.  Despu\'es usamos la desigualdad de Young (punto a
punto) e integramos.
\solution

\begin{expla}
Utilizamos la desigualdad de Young, que hemos demostrado en el ejercicio anterior:
\[
tu\leq\frac{t^p}{p}+\frac{u^q}{q}
\]

Cogemos:
\[
t=\frac{\abs{f}}{\norm{f}_p}
\]
\[
u=\frac{\abs{g}}{\norm{g}_q}
\]

Además usamos que:
\[
\norm{f}_p=\left( \int \abs{f}^p  \right)^\frac{1}{p}  \Leftrightarrow  \norm{f}_p^p= \int \abs{f}^p \Leftrightarrow 1 = \frac{\int \abs{f}^p}{\norm{f}_p^p}
\]
\end{expla}

Sustituyendo en la Desigualdad de Young:
\[
\frac{\abs{f}}{\norm{f}_p} \cdot \frac{\abs{g}}{\norm{g}_q} \leq \frac{1}{p}\left( \frac{\abs{f}}{\norm{f}_p} \right)^p + \frac{1}{q}\left( \frac{\abs{g}}{\norm{g}_q} \right)^q 
\]

Ahora integramos a ambos lados, usamos lo que hemos puesto en la explicación, y nos queda:

\[
\int \frac{\abs{f}\abs{g}}{{\norm{f}_p}{\norm{g}_q}} d\mu =
\frac{\int \abs{fg}d\mu}{{\norm{f}_p}{\norm{g}_q}} = 
\frac{{\norm{fg}_1}}{{\norm{f}_p}{\norm{g}_q}} \leq \int \frac{1}{p}\left( \frac{\abs{f}}{\norm{f}_p} \right)^p + \int \frac{1}{q}\left( \frac{\abs{g}}{\norm{g}_q} \right)^q  = \frac{1}{p} +  \frac{1}{q} = 1 
\]

Y nos queda:
\[
\norm{fg}_1 \leq \norm{f}_p \norm{g}_q
\]

Para ver que $fg \in L^1$ es obvio teniendo la desigualdad anterior:
\[
\int \abs{fg} = \norm{fg}_1 \leq \norm{f}_p \norm{g}_q < \infty
\]

Para el caso p=1 y q=$\infty$. Suponemos $f\cdot g \geq 0$.
\[
\int f\cdot g \leq \int f \cdot \norm{g}_{\infty}= \norm{g}_{\infty} \int f = \norm{g}_{\infty} \norm{f}_{1}
\]

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 3.4  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[4] Dada $f:\Omega\to \mathbb{R}$ en $L^p$,  $1 < p <  \infty$, escoger $g\in L^q$ tal que
$\int fg = \|fg\|_1 = \|f\|_p\|g\|_q$. Concluir que 
$\|f\|_p= \operatorname{sup}_{\{g\in L^q: \|g\|_q=1\}} \int fg $.
\solution

Para buscar la g, empezamos pensando que ocurre cuando p=q, que esto sucede si p=q=2, entonces, es fácil ver que si cojo $g=f$, tenemos que:

\[
\int fg = \int f^2
\]

\[
\norm{fg}_1 = \int \abs{f^2} = \int f^2
\]

\[
\norm{f}_2 \norm{g}_2 = \left( \int \abs{f}^2 \right)^{\frac{1}{2}} \left( \int \abs{f}^2 \right)^{\frac{1}{2}} =  \int \abs{f}^2 = \int f^2
\]

Entonces, la g la buscamos de forma que dependa de f y  que pertenezca a $L^q$, es decir, de forma que al hacer la integral del valor absoluto elevado a q, sea menor que infinito, para ello, sabemos que f pertenece a $L^p$, por tanto, es fácil ver que si definimos $signo(f)=1$ si $f\geq0$ y $signo(f)=-1$ si $f<0$, y consideramos $g=signo(f)\abs{f}^\frac{p}{q}$; entonces esta g es válida:

\begin{itemize}
\item $g \in L^q$, si, ya que: $\int \abs{g}^q = \int\left( \abs{f}^{\frac{p}{q}}\right)^q = \int \abs{f}^p < \infty$
\item $\int fg = \int \abs{f} \abs{f}^{\frac{p}{q}} = \int \abs{f}^{1+\frac{p}{q}} = \int \abs{f}^p$
\item $\norm{fg}_1 = \int \abs{fg} = \int fg = \int \abs{f}^p$
\item $$\norm{f}_p\norm{g}_q = \left( \int \abs{f}^p \right)^\frac{1}{p} \left( \int \abs{g}^q \right)^\frac{1}{q} = \left( \int \abs{f}^p \right)^\frac{1}{p} \left( \int (\abs{f}^\frac{p}{q})^q \right)^\frac{1}{q} = $$
$$= \left( \int \abs{f}^p \right)^\frac{1}{p} \left( \int \abs{f}^p \right)^\frac{1}{q} = \left( \int \abs{f}^{p}\right)^{\frac{1}{p}+\frac{1}{q}} = \int \abs{f}^p$$
\end{itemize}

Por tanto se cumple que $\int fg = \|fg\|_1 = \|f\|_p\|g\|_q$.

Para concluir vemos que:
\[
\int fg = \norm{f}_p \norm{g}_q  \Leftrightarrow \int \frac{fg}{\norm{g}_q} = \norm{f}_p 
\]

Por tanto, si tenemos una h tal que $\norm{h}_q = 1$, por Hölder tendríamos que $\int fh \leq \norm{f}_p \norm{h}_q = \norm{f}_p$. Esto implica que:
\[
\norm{f}_p =  \operatorname{sup}_{\{g\in L^q: \|g\|_q=1\}} \int fg
\]

¿Por qué? Sabiendo que para un h que cumpla $\norm{h}_q = 1$, entonces $\int fh \leq \norm{f}_p$, solo falta ver que lo siguiente es falso:
\[
 \operatorname{sup}_{\{\in L^q: \|h\|_q=1\}} \int fh < \norm{f}_p
\]

Es decir, que existe un h al que eso no es menor, sino que es igual. y efectivamente existe ese h, que es:
\[
h = \frac{signo(f)\abs{f}^\frac{p}{q}}{\norm{signo(f)\abs{f}^\frac{p}{q}}_q}
\]
Tiene modulo 1 y cumple los mismos 4 puntos que cumple la g (la norma sale  de las integrales como una constante).

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 3.5  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[5] Usar $\|f\|_p= \operatorname{sup}_{\{g\in L^q: \|g\|_q=1\}} \int fg $ cuando $1 < p <  \infty$
para obtener la desigualdad triangular o de Minkowski: $\|f + g\|_p \le \|f\|_p + \|g\|_p$.
Probar tambi\'en los casos (bastante obvios) $p=1$  y  $p = \infty$.
\solution

\begin{expla}

\end{expla}
Para $1<p<\infty$:

\[
\norm{f+g}_p = sup_{\{h\in L^q: \norm{h}_q=1\}}\int (f+g)h =  sup_{\{h\in L^q: \norm{h}_q=1\}}\left(\int fh + \int gh\right) \leq 
\]
\[
\leq sup_{\{h\in L^q: \norm{h}_q=1\}}\int fh +  sup_{\{h\in L^q: \norm{h}_q=1\}} \int gh = \norm{f}_p + \norm{g}_p   
\]

Para p=1:

$\norm{f+g}_1 = \int \abs{f+g} \leq \int (\abs{f} + \abs{g})=  \int \abs{f} + \int \abs{g}$

Para $p=\infty$:

$\norm{f+g}_{\infty} = supEsencial\abs{f+g} \leq supEsencial\abs{f} + supEsencial\abs{g} = \norm{f}_{\infty} + \norm{g}_{\infty}$
\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 3.6  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[6] Probar que $\|\cdot\|_p$ es una norma en $L^p$ (considerando que dos funciones
son iguales
cuando son iguales en casi todo punto).
\solution

\begin{expla}
Para comprobar que es una norma, tenemos que comprobar que cumple las tres propiedades de una norma:
\begin{enumerate}
\item Positividad: $\norm{f}\geq 0$ y $\norm{f} = 0 \Leftrightarrow f=0$.
\item Proporcionalidad: $\norm{\lambda f} = \abs{\lambda}\norm{f}$.
\item Desigualdad triangular: $\norm{u+v} \leq \norm{u}+\norm{v}$     
\end{enumerate}

Los elementos $f \in L^p$ cumplen que $\int \abs{f}^p < \infty$. 

Además: $\norm{f}_p = \left( \int \abs{f}^p\right)^\frac{1}{p}$.
\end{expla}
\begin{enumerate}
\item  $\norm{f}_p = 0 \Leftrightarrow f=0$. 

$\Leftarrow)$ es obvia.

$\Rightarrow)$ Si $f \neq 0$ en un conjunto de medida positiva, entonces $\int \abs{f}^p >0$ para $1<p<\infty$. (si p<1 es una quasinorma, y si $p=\infty$ la norma es el supremo esencial de f.)
\item $\norm{\lambda f}_p=\abs{\lambda}\norm{f}_p$, lo demostramos: 
\[
\norm{\lambda f}_p =\left( \int \abs{\lambda f}^p \right)^{\frac{1}{p}} = \left( \int \abs{\lambda}^p \abs{f}^p\right)^{\frac{1}{p}} = (\abs{\lambda}^p)^{\frac{1}{p}} \left(\int \abs{f}^p\right)^{\frac{1}{p}} = \abs{\lambda} \norm{f}_p  
\]
\item $\norm{f+g}_p \leq \norm{f}_p+\norm{g}_p$.
Demostrado en el ejercicio anterior.
\end{enumerate}



\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 3.7  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[7] Probar la desigualdad de Jensen: si $X:\Omega\to I\subset \mathbb{R}$, donde
$I$ es un intervalo en $\mathbb{R}$, y $X$ tiene media finita, entonces para toda funci\'on
convexa $g:I\to \mathbb{R}$, $g(EX) \le E(g(X))$. 

Sugerencia: n\'otese que al trabajar en un espacio de probabiildad, si $L(t) := a t + b$ es una recta, entonces conmuta con la integraci\'on,
es decir,
$L(\int X(\omega) dP (\omega)) = \int L (X(\omega)) dP (\omega)$. La desigualdad de Jensen
es consecuencia de esta observaci\'on, junto con el hecho de que las funciones convexas
est\'an por encima de todas sus rectas soporte.
\solution

\begin{expla}

\end{expla}
Dado que g es convexa, podemos encontrar una función lineal $f(x)=ax+b$ tal que $f(\mathbb{E}(X)) = g(\mathbb{E}(X))$ y $f(x)\leq g(x)$ para todo $x \in \mathbb{R}$.

Es decir, existe una recta que siempre queda por debajo de g:
\begin{figure}[h]
\centering
\includegraphics[page=1,scale=0.745]{img/jensen.png}
\end{figure} 

Utilizamos la linealidad y tenemos que:
\[
\mathbb{E}(g(X)) \geq \mathbb{E}(f(X)) = \mathbb{E}(aX+b)=a\mathbb{E}(X)+b=f(\mathbb{E}(X))=g(\mathbb{E}(X))
\]

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 3.8  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[8] Probar que si $0 < r\le s \le \infty$,
 entonces $\|f\|_{L^r(\Omega, \mathcal{A}, P)}\le \|f\|_{L^s(\Omega, \mathcal{A}, P)}$, luego $L^s(\Omega, \mathcal{A}, P) \subset L^r(\Omega, \mathcal{A}, P)$ (sugerencia, usar Jensen o H\"older). Demostrar que si el espacio tiene medida infinita, esta
inclusi\'on puede fallar.
\solution

\begin{expla}
Tenemos que ver que para $0 <r\leq s \leq \infty$ se cumple que $\norm{f}_r \leq \norm{f}_s$, es decir:

\[
\left( \int \abs{f}^r dP \right)^{\frac{1}{r}} \leq \left( \int \abs{f}^s dP \right)^{\frac{1}{s}}
\]

%Por Hölder tenemos que: $\|fg\|_1 \leq \|f\|_p\|g\|_q$, que en nuestro caso sería: $\|f^2\|_1 \leq \|f\|_p\|f\|_q$


\end{expla}
Dos formas:
\begin{enumerate}
\item Usamos Jensen:

Jensen dice que para X variable aleatoria y $\phi$ función convexa pertenecientes a $L^1$, se tiene que:
\[
\phi(\mathbb{E}(X)) \leq \mathbb{E}(\phi(X))
\]


Tenemos que:

\[
\norm{f}_{L^r(\Omega, \algb{M}, P)} = \norm{f}_r= \left( \int_{\Omega} \abs{f}^r dP \right)^{\frac{1}{r}}
\]

Para simplificar la notación, omitiremos a partir de ahora $\Omega$ y dP. Usamos como función convexa $g(t)=\abs{t}^{\frac{s}{r}}$.

Por tanto tenemos que:
\[
g(\mathbb{E}(\abs{f}^r)) = \left(\int \abs{f}^r\right)^{\frac{s}{r}} \leq\int \left( \abs{f}^r \right)^{\frac{s}{r}} = \mathbb{E}(g(\abs{f}^r)) 
\]

Elevamos a $\frac{1}{s}$ a izquierda y derecha y obtenemos (desarrollamos desde el centro azul, hacia la izquierda y hacia la derecha):
\[
\norm{f}_r = \left(\int \abs{f}^r\right)^{\frac{1}{r}} = \textcolor{blue}{\left(\int \abs{f}^r\right)^{\frac{s}{rs}} \leq \left(\int \abs{f}^{\frac{rs}{r}}\right)^{\frac{1}{s}}} = \left(\int \abs{f}^s\right)^{\frac{1}{s}} = \norm{f}_s
\]



\item Usando Hölder:
si $f\in L^p$ y $g\in L^q$, entonces $fg\in L^1$, y $\|fg\|_1 \le \|f\|_p\|g\|_q$

Definimos la función $g=1$, que pertenece a $L^q$. Y cogemos $0<r<s<\infty$. Suponemos que $f \in L^s$ y $f\geq 0$, entonces tenemos que:
\[
\abs{f}=\abs{f}\cdot 1  \Leftrightarrow f^r = f^r \cdot 1
\]

Entonces queremos acotar $\int f^r \cdot 1$. Vemos que $f^r \in L^{\frac{s}{r}}$ y que $1 \in L^{\frac{s}{r}'}$  siendo $\frac{s}{r}'$ el conjugado de $\frac{s}{r}$, es decir cumple que ($\frac{1}{r/s}+\frac{1}{s/r} = 1$). Vamos a demostrar que  $f^r \in L^{\frac{s}{r}}$:
\[
 \int \abs{f^r}^{\frac{s}{r}} =  \int \abs f^s < \infty \text{ porque } f\in L^s
\]

Y ahora sí usamos Hölder:

\[
\int f^r = \int f^r \cdot 1 = \norm{f^r \cdot 1}_1 \leq \norm{f^r}_{\frac{s}{r}}\norm{1}_{\frac{s}{r}'}=\left( \int(f^r)^{\frac{s}{r}} \right)^{\frac{r}{s}} \cdot 1 = \left( \int f^s \right)^{\frac{r}{s}}
\]

Elevamos a izquierda y derecha por $\frac{1}{r}$ y obtenemos que:
\[
\left( \int f^r \right)^{\frac{1}{r}} \leq \left( \int f^s \right)^{\frac{1}{s}}
\]

Que es lo que queríamos demostrar.
\end{enumerate}



\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\newpage
\section{Hoja 4}

\textcolor{blue}{CORREGIDA Y CONTRASTADA CON LAS SOLUCIONES DEL PROFESOR SALVO SI SE INDICA LO CONTRARIO}

Salvo afirmaci\'on expresa en sentido
contrario se asume siempre que estamos trabajando en un espacio de probabilidad $(\Omega, \mathcal{A}, P)$,
que  $\mathcal{B}\subset \mathcal{A}$ es una sub-$\sigma$-\'algebra, que las funciones son medibles, etc..

Recordatorio: si $0 < p < \infty$, $\|f\|_p := \left(\int|f|^p\right)^{1/p}$, mientras que
$\|f\|_\infty$ denota el supremo esencial de $|f|$. 

%%%%%%%%%%%%%%%%%%  PROBLEMA 4.1  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[1] Ejemplos sobre la f\'ormula del cambio de variables.

\vskip .3 cm

a) Sea $X: [0,1)\to \mathbb{R}$ la variable aleatoria $X(w) := 2 w (\operatorname{mod}(1)$. Es decir,
si $w\in [0, 1/2)$, entonces  $X(w) = 2 w$, mientras que si  $w\in [1/2, 1)$, entonces  $X(w) = 2 w - 1$.
En  $[0,1)$ tenemos la probabilidad uniforme con los conjuntos de Borel (o Lebesgue, para este problema
da igual). Hallar la funci\'on de distribuci\'on $F_X$,  y  $P_X$, la probabilidad imagen o ley de $X$.
 Decidir razonadamente si $X$ es una v.a. continua, y en caso de respuesta afirmativa, hallar su funci\'on
de densidad $f_X$.

\vskip .3 cm

b) Sea $Y: [0,1]^2\to \mathbb{R}$ la variable aleatoria $Y(x,y) := y$.
En  $[0,1]^2$ tenemos la probabilidad uniforme con los conjuntos de Borel (o Lebesgue, para este problema
da igual). Hallar la funci\'on de distribuci\'on $F_Y$,  y  $P_Y$, la probabilidad imagen o ley de $Y$.
 Decidir razonadamente si $Y$ es una v.a. continua, y en caso de respuesta afirmativa, hallar su funci\'on
de densidad $f_Y$.

\vskip .3 cm


c) Decidir razonadamente si para toda funci\'on de Borel acotada $g:\mathbb{R}\to \mathbb{R}$,
$$\int_{ [0,1)} g(X(w)) dw = \int_{ [0,1]^2} g(Y(x,y)) dx dy.$$

\solution

\begin{expla}

\end{expla}

\spart 
Vemos que la variable aleatoria X(w) es una función que crece con pendiente 2 en el intervalo $[0,1/2)$ y en $[1/2,1)$:

\begin{figure}[h]
\centering
\includegraphics[page=1,scale=0.745]{img/f.png}
\end{figure} 

Por tanto, se ve que tiene la misma probabilidad de valer cualquier valor entre 0 y 1, la función de distribución será simplemente:

$$
F_x(x) =
  \left\lbrace
  \begin{array}{l}
     0 \text{ si } x < 0\\
     x \text{ si } x \in [0,1) \\
     1 \text{ si } x \geq 1 \\
  \end{array}
  \right.
$$

Por su parte, la ley de x, o $P_x$, recordemos que es:
$P_x(B) = P(X^{-1}(B)) = P(\{w \in \Omega : X(w) \in B\})=P(X \in B)$. En este caso coincide con la medida de Lebesgue de B.

Por ejemplo, si B es el intervalo $(1/2, 3/4)$, vemos que $X(w)$ toma esos valores en un exactamente $3/4-1/2=1/4$ de los valores de w.

Por tanto: $P_x(B)=\mu(B)$ (siendo $\mu$ la medida de Lebesgue).

Ahora queremos ver que X es una variable aleatoria continua. Nos remitimos a la definición de variable aleatoria continua:

X es una variable aleatoria continua $\Leftrightarrow$ $P_x$ es absolutamente continua con respecto a la medida de Lebesgue.

Recordemos lo que significa que una medida $\mu$ sea absolutamente continua con respecto a otra $\nu$:  $\mu << \nu$ si $\mu(A)=0 \Rightarrow \nu(A)=0$

Tenemos que ver que $P_x$ es absolutamente continua con respecto a la medida de Lebesgue, lo que es obvio ya que $P_x(B)=\mu(B)$. La llamaremos $P_{x1}$ para diferenciarla del apartado B.

Por último definimos la función de densidad $f_x(x)$.

$$
f_x(x) =
  \left\lbrace
  \begin{array}{l}
     0 \text{ si } x < 0\\
     1 \text{ si } x \in [0,1) \\
     0 \text{ si } x \geq 1 \\
  \end{array}
  \right.
$$
\spart
Obtenemos el mismo resultado que en el caso anterior para $F_x$ y para $f(x)$.

También para $P_x$, que lo llamaremos $P_{x2}(B)=\mu(B)$.


\spart 

Tenemos que demostrar que:

$$\int_{ [0,1)} g(X(w)) dw = \int_{ [0,1]^2} g(Y(x,y)) dx dy.$$

Vamos a utilizar la fórmula del cambio de variable:
\[
\int_{\Omega}g(X(w))dP(w)=\int_{\mathbb{R}}g(x)dP_X(x)
\]

Para aplicar la fórmula del cambio de variable, lo único que tenemos que hacer es cambiar los límites de integración por los valores en los que se mueven $X(w)$ e $Y(x,y)$, y cambiar la medida sobre la que se integra.

Empezamos por el primer término, y le aplicamos la fórmula del cambio de variable:
$$\int_{ [0,1)} g(X(w)) dw = \int_{ [0,1)} g(2w \mod{1}) dw= \int_{ [0,1)} g(x) dP_{x1}(x)$$

Ahora cogemos el segundo término:
$$\int_{ [0,1]^2} g(Y(x,y)) dx dy = \int_{ [0,1)} g(y) dP_{x2}(y)$$

Que son iguales ya que $P_{x2}(x) = P_{x1}(x)$.

\textcolor{blue}{Solucion del profesor al apartado c:}
\[
\int_0^1 g(X(w)) dw \stackrel{FCV}{=} \int_{\mathbb{R}}g(x)dP_x(x)= \int_{-\infty}^{\infty}g(x)dx
\]

\[
\int_0^1 \int_0^1 g(Y(x,y))dxdy \stackrel{FCV}{=} \int_{\mathbb{R}}g(y)dP_y(y)= \int_{-\infty}^{\infty}g(y)dy
\]

\end{problem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%  PROBLEMA 4.2  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[2] Demostrar la desigualdad de Jensen condicional: si $X:\Omega\to I\subset \mathbb{R}$, donde
$I$ es un intervalo en $\mathbb{R}$, $g:I\to \mathbb{R}$ es convexa, y $X, g(X) \in L^1$, entonces
 $g(E(X|\mathcal{B})) \le E(g(X)|\mathcal{B})$. 

\solution

\begin{expla}
Razonamiento parecido que en el ejercicio 7 de la hoja 3.
\end{expla}
Dada una función convexa $\phi$. Vemos que la función $\phi=\sup_{L} {L \leq \phi}$. Siendo L rectas que quedan por debajo de $\phi$ o que coinciden tangencialmente a ella.

Además: $L(t)=a+bt$. Lo simplificamos por continuidad para valores $a,b \in \mathbb{Q}^2$.

Por tanto tenemos de momento que $L \leq \phi$, y por tanto que $L(X) \leq \phi(X)$. De esto obtenemos lo siguiente:
\[
\mathbb{E}(\phi(X)|\algb{B}) \stackrel{positividad}{\geq} \mathbb{E}(L(X)|\algb{B}) = \mathbb{E}(a+bX|\algb{B}) = a+b\mathbb{E}(X|\algb{B}) = L(\mathbb{E}(X|\algb{B}))
\]

Y para terminar:
\[
\mathbb{E}(\phi(X)|\algb{B}) \geq \sup L(\mathbb{E}(X|\algb{B}))=\phi(\mathbb{E}(X|\algb{B}))
\]


\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 4.3  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[3] Sean $p,q \ge 1$ exponentes conjugados, es decir, $p$ y $q$ satisfacen
$1/p + 1/q =1$ (si $p=1$, entonces $q = \infty$, y viceversa).
Dado un espacio de probabilidad  $(\Omega, \mathcal{A}, P)$, enunciar y probar la desigualdad condicional de 
H\"older. Sugerencia: usar la desigualdad de H\"older.
\solution
\textcolor{red}{ESTE EJERCICIO ESTA MAAAAAL, COMO EL CULO VAYA, Y NO TENEMOS LA SOLUCION, DIJO QUE LO MIRARAMOS EN INTERNET}

\begin{expla}
Tenemos que demostrar que:
\[
\mathbb{E}(\abs{XY}|\algb{B}) \leq \mathbb{E}(\abs{X}^p|\algb{B})^{\frac{1}{p}} \mathbb{E}(\abs{Y}^q|\algb{B})^{\frac{1}{q}}
\]

Por ser esperanzas condicionadas, tenemos que cumplen que son $\algb{B}$-medibles y que:
$\forall B \in \algb{B}$, $\int_{B}\mathbb{E}(X|\algb{B})dP=\int_{B}XdP$. Además estas dos propiedad definen la esperanza condicionada de modo único.

\end{expla}
Empezamos aplicando dicha propiedad sobre $\mathbb{E}(\abs{XY}|\algb{B})$:
\[
\int_{\Omega}\mathbb{E}(\abs{XY}|\algb{B}) = \int_{\Omega}\abs{XY} = \norm{XY} \leq \norm{X}_p \norm{Y}_q = \left( \int_{\Omega} \abs{X}^p \right)^{\frac{1}{p}} \left( \int_{\Omega} \abs{X}^q \right)^{\frac{1}{q}} =  
\]

\[
= \left( \int_{\Omega} \mathbb{E}(\abs{X}^p|\algb{B}) \right)^{\frac{1}{p}} \left( \int_{\Omega} \mathbb{E}(\abs{X}^q|\algb{B}) \right)^{\frac{1}{q}}
\]

Por tanto, como esta propiedad define la esperanza de modo único, vemos que:

\[
\mathbb{E}(\abs{XY}|\algb{B}) \leq \mathbb{E}(\abs{X}^p|\algb{B})^{\frac{1}{p}} \mathbb{E}(\abs{Y}^q|\algb{B})^{\frac{1}{q}}
\]

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 4.4  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[4] Probar que si $X := \{X_n\}_{n=0}^{\infty}$  es una martingala adaptada a la filtraci\'on
$\{\mathcal{A}_n\}_{n=0}^{\infty}$, entonces para todo $n>0$ tenemos 
$EX_n = EX_0$. 
Enunciar los resultados an\'alogos para sub y supermartingalas.
\solution

\begin{expla}
Nos basamos en dos propiedades:
\begin{enumerate}
\item Por parte de la martingala usamos que:
$$\forall_n, \mathbb{E}(X_{n+1}|\algb{M}_n)=X_n$$
\item Por parte de la probabilidad condicionada usamos que:
$$\forall B \in \algb{B}, \int_{B}\mathbb{E}(X|\algb{B})dP=\int_{B}XdP$$
\end{enumerate}
\end{expla}
Aplicamos 1) sobre $X_n$ en el primer paso, 2) en el tercero (combinado con que $\Omega$ pertenece a $\algb{A}_n$), y definición de esperanza en el segundo y en el cuarto.
\[
\mathbb{E}(X_n) =\mathbb{E}(\mathbb{E}(X_{n+1}|\algb{A}_n)) = \int_{\Omega} \mathbb{E}(X_{n+1}|\algb{A}_n) dP = \int_{\Omega} X_{n+1}dP = \mathbb{E}(X_{n+1})
\]

Siguiendo este procedimiento n veces de derecha a izquierda llegamos a lo que nos piden: $\mathbb{E}(X_n) = \mathbb{E}(X_0)$.

Para las submartingalas y supermartingalas es lo mismo solo que poniendo $\geq$ y $\leq$ respectivamente.

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 4.5  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[5] Probar que si $X := \{X_n\}_{n=0}^{\infty}$  es una martingala adaptada a la filtraci\'on
$\{\mathcal{A}_n\}_{n=0}^{\infty}$, entonces para todo $m, n\ge 0$ tenemos
$E(X_{n + m}|\mathcal{A}_n) = X_n$. Enunciar los resultados an\'alogos para sub y supermartingalas.
\solution

\begin{expla}
Nos basamos en dos propiedades:
\begin{enumerate}
\item Por parte de la martingala usamos que:
$$\forall_n, \mathbb{E}(X_{n+1}|\algb{M}_n)=X_n$$
\item Propiedad de la torre (la 7) de la probabilidad condicionada:
Sean $\algb{C} \subset \algb{B} \subset \algb{M}$ $\salgb$s, si $X \in L^ 1(\algb{M})$ entonces:
\begin{itemize}
\item $\mathbb{E}(\mathbb{E}(X|\algb{C})|\algb{B}) = \mathbb{E}(X|\algb{C})$.
\item $\mathbb{E}(\mathbb{E}(X|\algb{B})|\algb{C}) = \mathbb{E}(X|\algb{C})$.
\end{itemize}
\end{enumerate}
\end{expla}
En el caso de las martingalas tenemos que:
$\algb{A}_n \subset \algb{A}_{n+1} \subset \algb{A}_{n+2}...$

\[
X_n = \mathbb{E}(X_{n+1}|\algb{A}_n) = \mathbb{E}(\mathbb{E}(X_{n+2}|\algb{A}_{n+1})|\algb{A}_n) = \mathbb{E}(X_{n+2}|\algb{A}_n) =
\]

\[
= \mathbb{E}(\mathbb{E}(X_{n+3}|\algb{A}_{n+2})|\algb{A}_n) =
\mathbb{E}(X_{n+3}|\algb{A}_n) = ... = \mathbb{E}(X_{n+m}|\algb{A}_n)
\]

Para las submartingalas y supermartingalas es lo mismo solo que poniendo $\geq$ y $\leq$ respectivamente.
\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 4.6  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[6] C\'omo ganar un euro con probabilidad 1, en un juego justo: la estrat\'egia del doble o nada.
Apostamos un euro a que sale cara. Si sale cara nos plantamos, si sale cruz apostamos
2 euros a que sale cara. Si sale cara nos plantamos, si sale cruz apostamos
4 euros a que sale cara. Etc.. Demostrar que la  estrat\'egia anterior gana un euro
 con probabilidad 1.
\solution

\begin{expla}

\end{expla}
Llamamos $X_n=$ ganancias acumuladas después de n partidas. Los eventos w será una sucesión de n dígitos que serán 1 si sale cara y -1 si sale cruz.

Vemos que existe T tal que $\mathbb{E}(X_0) = 0 < 1 = \mathbb{E}(X_t)$. Esto quiere decir que no se cumple ninguna de las 3 condiciones del teorema de parada opcional de Doob.

Por ejemplo dado $w_0=(1,-1,1,...)$ entonces $T(w_0)=1$. Dado $w_1=(-1,-1,1,...)$ entonces $T(w_1)=3$. Dado $w_2=(-1,-1,-1,1,...)$ entonces $T(w_0)=4$. 

Además vemos que P(no salga cara)=$\lim_{n \rightarrow \infty} \frac{1}{2^n} = 0$. Y $P(T=n)=\frac{1}{2^n}$.

Vamos a ver una a una las condiciones de parada opcional de Doob:
\begin{enumerate}
\item $\exists M >0$ tal que $T \leq M$. 

Esta condición falla ya que $T \notin L^{\infty}$ (no está acotada, no tiene máximo su gráfica vaya)
\item $\forall n \geq 0$ y casi todo $w \in \Omega$, $\abs{X_n(w)} \leq M$ con $M >0$ fijo.

También falla ya que se puede cumplir que para cierto w:$\abs{X_n(w)}=\abs{1-2^n} \stackrel{n \rightarrow \infty}{\rightarrow} \infty$.

Por tanto este valor no esta acotado por ningún M fijo.

\item $\mathbb{E}(T)<\infty$ y $\exists M > 0$ tal que para casi todo w y todo n $\abs{X_{n+1}(w) - X_n(w)} \leq M$.

$\mathbb{E}(T)=\sum_{n=1}^{\infty} n \cdot P(T=n) = \sum_{n=1}^{\infty} n \cdot \frac{1}{2^n} < \infty$,de hecho es 2, ya que:

\[
\sum_{n=0}^{\infty} t^n = \frac{1}{1-t} \text{ para } \abs{t}<1
\]

Y tenemos que:
\[
\frac{d}{dt}(\frac{1}{1-t})=\frac{1}{(1-t)^2}
\]

Por tanto:
\[
\sum_{n=0}^{\infty} nt^{n-1} = \frac{1}{(1-t)^2} \text{ para } \abs{t}<1
\]

Y concluimos que con $t=\frac{1}{2}$:
\[
t \sum_{n=1}^{\infty}nt^{n-1}=\frac{t}{(1-t)^2} \rightarrow \sum_{n=1}^{\infty}nt^{n-1} = \frac{\frac{1}{2}}{(1-\frac{1}{2})^2}=2
\]

Pero $\abs{X_{n+1}(w)-X_n(w)} = 2^{n-1}  \stackrel{n \rightarrow \infty}{\rightarrow} \infty$

Por tanto tampoco existe un M fijo y tampoco se cumple la propiedad.
\end{enumerate}

Concluimos que no se cumple ninguna de las 3 propiedades, por tanto, sí ganamos un euro con probabilidad 1.
\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\section{Hoja 5}

\textcolor{blue}{CORREGIDA Y CONTRASTADA CON LAS SOLUCIONES DEL PROFESOR SALVO SI SE INDICA LO CONTRARIO}

Salvo afirmaci\'on expresa en sentido
contrario se asume siempre que estamos trabajando en un espacio de probabilidad $(\Omega, \mathcal{A}, P)$,
que  $\mathcal{B}\subset \mathcal{A}$ es una sub-$\sigma$-\'algebra, que las funciones son medibles, etc.


Recordatorio: si $1\le p < \infty$, $\|f\|_p := \left(\int|f|^p\right)^{1/p}$, mientras que
$\|f\|_\infty$ denota el supremo esencial de $|f|$. De hecho, la definici\'on
 $\|f\|_p := \left(\int|f|^p\right)^{1/p}$ tiene sentido para cualquier $p > 0$ finito, pero puede
demostrarse que si $p < 1$ esta expresi\'on no define una norma.

%%%%%%%%%%%%%%%%%%  PROBLEMA 5.1  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[1] Sea $X: [0,1) \to [0,1)$ la identidad $X(w) = w$, donde a $[0,1)$ se le asigna la
probabilidad uniforme (en este caso, la medida de Lebesgue). Sea  $\mathcal{A}_n := \sigma([0,1/2^n), [1/2^n,2/2^n) , \dots,
[ (2^n - 1)/2^n, 1))$ la $ \sigma$-\'algebra generada por los intervalos di\'adicos $ [j/2^n,(j + 1)/2^n)$, $j = 1\dots, n-1$.
Calcular $E(X|\mathcal{A}_n)$. Decidir razonadamente si la sucesi\'on de v.a. $ \{E(X|\mathcal{A}_n)\}_{n=0}^{\infty}$
converge (y en caso de respuesta afirmativa, determinar a qu\'e) en alguno de los siguientes sentidos:
a) uniformemtente, b) en $L^p$, determinando para que valores de $p$ hay convergencia, c) en casi todo punto,
d) en medida.
\solution

\begin{expla}
Vemos que $\algb{M}_n$ esta generada a partir de una partición, de elementos disjuntos, y por tanto:
\[
\mathbb{E}(X|\algb{M}_n)(w)=\frac{1}{P(A_i)} \int_{A_i}X(w)dP(w)
\]

Definimos $\mathbb{E}(X|\algb{M}_1)(w)$

$$
\mathbb{E}(X|\algb{M}_1)(w) =
  \left\lbrace
  \begin{array}{l}
      \frac{1}{P([0, 1/2))}\int_{[0,1/2)}wdP(w)= \left.2 \frac{w^2}{2}\right|_0^{1/2} = 1/4  \\
     ... 3/4  \\
  \end{array}
  \right.
$$

Y $\mathbb{E}(X|\algb{M}_2)(w)$:
$$
\mathbb{E}(X|\algb{M}_2)(w) =
  \left\lbrace
  \begin{array}{l}
      \frac{1}{8}\\
      \frac{3}{8}\\
      \frac{5}{8}\\
      \frac{7}{8}\\
  \end{array}
  \right.
$$

\end{expla}

Obtenemos que:
$$
\mathbb{E}(X|\algb{M}_n)(w) =
  \left\lbrace
  \begin{array}{l}
      \frac{1}{2^{n+1}}\\
      \frac{3}{2^{n+1}}\\
      ...\\
      \frac{2^{n+1}-1}{2^{n+1}}\\
  \end{array}
  \right.
$$

Por tanto, observamos, que  $\mathbb{E}(X|\algb{M}_n)(w)$ converge a X(w)=w.

\begin{enumerate}
\item Convergencia uniforme:  una sucesión $f_n:I\rightarrow \mathbb{R}$ converge uniformemente a $f:I\rightarrow \mathbb{R}$ si: $\forall \epsilon >0$,  $\exists N_{\epsilon}$ tal que $\forall n \geq N_{\epsilon}$ se cumple que $\abs{f_n(x)-f(x)}<\epsilon$ $\forall x \in I$. Es decir si:
\[
\lim_{n \rightarrow \infty} \norm{f_n - f}_{\infty} = 0
\]

O equivalentemente si:
\[
\lim_{n \rightarrow \infty} \sup_{x \in I}\{\abs{f_n(x)-f(x)} \}=0
\]

En este caso sabemos que converge uniformemente ya que $\frac{1}{2^{n+1}}$ es una cota independiente de w.

La convergencia uniforme implica todas las demás. Aún así, vamos a repasarlas.

\item Convergencia en $L^p$:
\[
f_n \stackrel{L_p (n\rightarrow \infty)}{\rightarrow} g \Leftrightarrow \norm{f_n -g}_p  \rightarrow 0 \Leftrightarrow \int{\abs{f_n -g}^pd\mu} \rightarrow 0
\]

Por tanto, tenemos que ver que:
\[
\int \abs{\mathbb{E}(X|\algb{M}_n)(w)-X(w)| }^p \rightarrow 0
\]

Pero sabiendo que converge uniformemente no nos complicamos la vida y lo usamos, para $0<r<\infty$ tenemos que:
\[
0 \leq \lim_{n \rightarrow \infty} \norm{X-X_n}_{r} \leq \lim_{n \rightarrow \infty} \norm{X-X_n}_{\infty} = 0 \text{ por converger uniformemente}
\]


\item Convergencia en casi todo punto o puntual: una sucesión $f_n:I\rightarrow \mathbb{R}$ converge puntualmente a $f:I\rightarrow \mathbb{R}$ si: $\forall \epsilon >0$ y $\forall x \in I$, $\exists N_{\epsilon, x}$ tal que $\forall n \geq N_{\epsilon, x}$ se cumple que $\abs{f_n(x)-f(x)}<\epsilon$. Es decir si:
\[
\lim_{n \rightarrow \infty} f_n(x)-f(x)=0
\]

En este caso sabemos que converge puntualmente. Nos sirve la misma cota que hemos usado para la convergencia uniforme. Además, convergencia uniforme implica convergencia puntual como ya hemos dicho.

\item Convergencia en medida o en probabilidad:
Si $\forall \epsilon >0$, $\lim_{n \rightarrow \infty}P(\abs{X-X_n}\geq \epsilon)=0$.

Por ser convergente uniformemente, converge en medida.



%Pero por definición de esperanza condicionada y por ser X $\algb{M}_n$-medible, tenemos que:
%\[
%\mathbb{E}(X|\algb{M}_n) = X
%\]

%Elevamos a p e integramos y ya tenemos la igualdad que buscamos
\end{enumerate}

\end{problem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%  PROBLEMA 5.2  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[2]  Probar que si $X := \{X_n\}_{n=0}^{\infty}$  es una submartingala, y
$0  <  r\le s\le \infty$, entonces $\|X\|_r  \le \|X\|_s$, donde 
$\|X\|_p := \sup_{n\in \mathbb{N}} \|X_n\|_{p}$. 
\solution

\begin{expla}
utilizamos el ejercicio 3.8, es decir, que si $0<r\leq s \leq \infty$, entonces $\norm{f}_r \leq \norm{f}_s$.

La norma de una martingala se define:
\[
\norm{X}_p = \sup_n \norm{X_n}_p
\]
\end{expla}

Tenemos que ver que:
\[
\sup_n \norm{X_n}_r \leq \sup_n \norm{X_n}_s
\]

Entonces, existirá un p tal que:

\[
\sup_n \norm{X_n}_r  = \norm{X_p}_r \leq \norm{X_p}_s \leq \sup_n \norm{X_n}_s
\]

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 5.3  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[3] Probar que si $X := \{X_n\}_{n=0}^{\infty}$  es una martingala, y $ \|X\|_s <\infty$, donde 
$1\le s < \infty$, entonces  $Y := \{|X_n|^s\}_{n=0}^{\infty}$  es una submartingala
en $L^1$. 
\solution

\begin{expla}
Sabemos que:
\begin{enumerate}
\item $X_n = \mathbb{E}(X_{n+1}|\algb{A}_n)$
\item $\norm{X_n}_s = (\int \abs{X_n}^s)^{\frac{1}{s}} < \infty$
\end{enumerate} 
\end{expla}
Comprobamos las 3 condiciones que debe cumplir una martingala
\begin{enumerate}
\item Hay que ver que $\sup \norm{\abs{X_n}^s} < \infty$:
\[
\sup \int_{\Omega} \abs{X_n}^s dP =  \sup \norm{X_n}_s^s < \infty \text{ por 2.}
\]
\item La composición de funciones medibles es medible. \textcolor{red}{¿Qué estamos componiendo?}
\item Tenemos que probar que: $\mathbb{E}(\abs{X_{n+1}}^s|\algb{A}_n) \geq \abs{X_n}^s$:
\[
\mathbb{E}(X_{n+1}|\algb{A}_n) = X_n \Leftrightarrow \abs{X_n}^s  = \abs{\mathbb{E}(X_{n+1}|\algb{A}_n)}^s \stackrel{JensenCondicional}{\leq} \mathbb{E}(\abs{X_{n+1}}^s|\algb{A}_n)
\]
\end{enumerate}


\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 5.4  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[4]  Sea $\{X_n\}_{n=0}^{\infty}$ una sucesi\'on de v.a.i.i.d., tales que 
$P(X_i = 1)= P(X_i = -1) = 1/2$, y sea $S_n := \sum_{i=0}^n X_i$.
Vimos en clase que $S := \{S_n\}_{n=0}^{\infty}$  es una martingala adaptada a la filtraci\'on
$\{\mathcal{A}_n\}_{n=0}^{\infty}$, donde $\mathcal{A}_n := \sigma(X_0, \dots, X_n)$.
Sea $c > 0$. 
Calcular $\lim_{n\to \infty} P( -c < S_n < c)$. Sugerencia: se puede usar el Teorema del L\'{\i}mite Central,
visto en Probabilidad I.
 Decidir razonadamente si la martingala $S = \{S_n\}_{n=0}^{\infty}$ 
 est\'a en $L^1$, es decir, si $\sup_n \|S_n\|_1 < \infty$. Decidir razonadamente
si $S_n$ converge en casi todo punto a una funci\'on $S_ \infty$ tal que
$P(|S_\infty| = \infty) = 0$. 

\solution
\textcolor{red}{ESTE EJERCICIO ESTA MAL, A LA ESPERA DE UNA RESPUESTA}

\begin{expla}
El teorema central del límite dice que:
\[
\lim_{n \rightarrow \infty} P(\sqrt{n}(S_n - \mu) \leq \sigma t) = N(0,1)(t)
\]

Que para valores grandes de n, es equivalente a decir:
\[
\lim_{n \rightarrow \infty} P(\sqrt{n}(S_n - \mu) \leq t) = N(0,1)(\frac{t}{\sigma})
\]

Sabemos que $\bar{X} = \frac{1}{n} \sum_{i=1}^{n}X_i$

\end{expla}

En nuestro caso tenemos que:

\[
\bar{S_n} = \frac{1}{n} \sum_{i=1}^{n}S_n
\]

\[
\mu = \mathbb{E}(S_n)= 0
\]

\[
\sigma^2 = V(S_n) = \mathbb{E}(S_n^2)-\mathbb{E}(S_n)^2 = 1 - 0 = \frac{1}{n} 
\]

\[
\sigma = \frac{1}{\sqrt{n}}
\]


Vamos a normalizar la probabilidad, y dejarlo de la forma:
\[
P(\frac{\sqrt{n}(\bar{S_n}-\mu)}{\sigma} \leq z)
\]

En nuestro caso, nos debería quedar:
\[
P(n\bar{S_n}\leq z)
\]


Así, usamos que $S_n = n\bar{S_n}$, y nos queda que:
\[
P(-c<S_n<c)=P(-c<n\bar{S_n}<c)=P(n\bar{S_n}>-c)-P(n\bar{S_n}>c)=
\]

\[
=1-2P(n\bar{S_n}>c) \rightarrow 1-N(0,1)(c)
\]



\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newpage
\section{Hoja 6}

\textcolor{blue}{CORREGIDA Y CONTRASTADA CON LAS SOLUCIONES DEL PROFESOR}

Salvo afirmaci\'on expresa en sentido
contrario se asume siempre que estamos trabajando en un espacio de probabilidad $(\Omega, \mathcal{A}, P)$,
que  $\mathcal{B}\subset \mathcal{A}$ es una sub-$\sigma$-\'algebra, que las funciones son medibles, etc.

Recordatorio: si $1\le p < \infty$, $\|f\|_p := \left(\int|f|^p\right)^{1/p}$, mientras que
$\|f\|_\infty$ denota el supremo esencial de $|f|$. De hecho, la definici\'on
 $\|f\|_p := \left(\int|f|^p\right)^{1/p}$ tiene sentido para cualquier $p > 0$ finito, pero puede
demostrarse que si $p < 1$ esta expresi\'on no define una norma.


%%%%%%%%%%%%%%%%%%  PROBLEMA 6.1  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[1] Sea $\{\mathcal{A}_n\}_{n=0}^{\infty}$ una filtraci\'on de sub-$\sigma$-\'algebras de $\mathcal{A}$,
y sea $Y\in L^1(\Omega, \mathcal{A}, P)$. Demostrar que $X_n:=  E(Y|\mathcal{A}_n)$ define una
martingala en $L^1$. Decidir razonadamente si $X_n$ converge a $Y$ en $L^1$.

\solution

\begin{expla}
Vamos a ver que $X_n:=  E(Y|\mathcal{A}_n)$ cumple las tres propiedades de una martingala en $L^1$:
\begin{enumerate}
\item $\forall n$, $X_n \in L^1(\Omega, \algb{A}. P)$.
\item $\forall n$, $X_n$ es $\algb{A}_n$-medible.
\item $\forall_n$, $\mathbb{E}(X_{n+1}|\algb{A}_n)=X_n$ c.s.

\end{enumerate}

\end{expla}
Sabemos que X es una martingala en $L^p$ con $1\leq p \leq \infty$, si X es una martingala y $\norm{X}_p = \sup_{n}\norm{X_n}_p < \infty$. (Por una definición vista en la teoría en martingalas)

En este caso: $X_n = \mathbb{E}(Y|\algb{A}_n)$. Por la propiedad 4 de la esperanza condicionada, tenemos que esta es una contracción en $L^1$, es decir: $\norm{Y}_1 \geq \norm{\mathbb{E}(Y|\algb{A}_n)}_1$. Y nos queda:

\[
\sup_{n}\norm{X_n}_1 = \sup_{n}\norm{\mathbb{E}(Y|\algb{A}_n)}_1 \leq \norm{Y}_1 < \infty
\]

Por tanto, ya hemos demostrado la segunda parte de la definición.

Ahora vamos a ver que cumple las tres propiedades de una martingala:

\begin{enumerate}
\item Obvio por lo que acabamos de hacer. $X_n = \mathbb{E}(Y |\algb{A}_n) \in L^1$ ya que el supremo de esos $X_n$ pertenece a $L^1$.

Otra opción:
Tenemos que ver que $\mathbb{E}(Y|\mathcal{A}_n) \in L^1$, es decir, que $\int_{\Omega} \abs{\mathbb{E}(Y|\mathcal{A}_n)} dP < \infty$. 

Tenemos una propiedad de las esperanzas condicionadas (la tercera) que dice que: $\abs{\mathbb{E}(X|\algb{B})} \leq \mathbb{E}(\abs{X}|\algb{B})$.

En nuestro caso, aplicando esta propiedad y la segunda propiedad de la definición de esperanza condicionada ($\forall B \in \algb{B}$, $\int_{B}\mathbb{E}(X|\algb{B})dP=\int_{B}XdP$), nos queda que:

\[
\int_{\Omega} \abs{\mathbb{E}(Y|\mathcal{A}_n)} \leq  \int_{\Omega} \mathbb{E}(\abs{Y}|\mathcal{A}_n) = \int_{\Omega} \abs{Y} < \infty
\]


\item Tenemos que ver que $\mathbb{E}(Y|\mathcal{A}_n)$ es $\algb{A}_n$-medible. Lo cual es cierto por definición de esperanza condicionada.

\item Tenemos que ver que $\mathbb{E}(\mathbb{E}(Y|\mathcal{A}_{n+1})|\mathcal{A}_n) = \mathbb{E}(Y|\mathcal{A}_n)$

Aplicamos la regla de la torre (séptima propiedad de una esperanza condicionada), puesto que $\algb{A}_n \subset \algb{A}_{n+1}$, y sale directamente:

\[
\mathbb{E}(\mathbb{E}(Y|\mathcal{A}_{n+1})|\mathcal{A}_n) = \mathbb{E}(Y|\mathcal{A}_n)
\]

\end{enumerate}
Ahora vamos a ver razonadamente si $X_n$ converge a Y en $L^1$. Por definición de convergencia tenemos que ver que:
\[
X_n \stackrel{L_1 (n\rightarrow \infty)}{\rightarrow} Y \Leftrightarrow \norm{X_n -Y}_1  \rightarrow 0 \Leftrightarrow \int{\abs{X_n -Y}dP} \rightarrow 0 \Leftrightarrow \int{\abs{\mathbb{E}(Y|\mathcal{A}_n) -Y}dP} \rightarrow 0
\]


Buscamos un contraejemplo:
Sea $(\Omega, \algb{A}, P)=([0,1], Borel, \lambda)$ y $\algb{A}_n = \algb{A}_0 = \{\emptyset, [0,1] \}$. Entonces dada la variable aleatoria $Y(w)=w$, esta define una martingala en $L^1$. Sin embargo:


\[
\mathbb{E}(Y|\algb{A}_n)=\mathbb{E}(Y) = \frac{1}{2} \rightarrow \frac{1}{2}=X_{\infty}\neq Y
\]

Por tanto no converge.





\end{problem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%  PROBLEMA 6.2  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[2] Sea $X := \{X_n\}_{n=0}^{\infty}$   una martingala en $L^2$. Probar que los incrementos 
son ortogonales, donde los incrementos se definen como $Y_0 := X_0$, y 
para $n > 0$, $Y_n := X_n - X_{n-1}$. Es decir, demostrar que si $j\ne k$, entonces
$(Y_j, Y_k) = \int Y_j Y_k = 0$. 
\solution

\begin{expla}

\obs Si $k>j$:
\begin{itemize}
\item $\mathbb{E}(X_k|\algb{A}_j) = X_j$

Viene del ejercicio 4.5: $\mathbb{E}(X_{n+m}|\algb{A}_n)=X_n$

\item Si $j,i < k$, entonces: $\mathbb{E}(X_j-X_i|\algb{A}_k)=X_j-X_i$  (por lo anterior)

\item $\mathbb{E}(X_j|\algb{A}_k) = X_j$

Como $\algb{A}_j \subset \algb{A}_k$, y $X_j$ es $\algb{A}_j$-medible, entonces también es $\algb{A}_k$-medible. Y ahora solo hay que aplicar la observación de la propiedad 6 de la esperanza condicionada (Si X es $\algb{B}$-medible, entonces $\mathbb{E}(X|\algb{B})=X$). 

\item Si $m,n > j$, entonces: $\mathbb{E}(X_m-X_n|\algb{A}_j)=X_j-X_j=0$  (por lo anterior)
\end{itemize}
\end{expla}



Si $k>j$ y $j=0$ (se explican los pasos debajo):
\[
\mathbb{E}\left[(X_k-X_{k-1})X_0\right] = \mathbb{E}\left( \mathbb{E}\left[(X_k-X_{k-1})X_0|\algb{A}_0\right]\right)= \mathbb{E}\left(X_0 \cdot \mathbb{E}\left[(X_k-X_{k-1})|\algb{A}_0\right]\right) =
\]
\[
= \mathbb{E}\left(X_0 \cdot (X_0-X_0)\right)  = 0
\]

El primer paso, es por definición de esperanza condicionada, sea $\mathbb{E}(X|\algb{B})$, se cumple que para todo $B \in \algb{B}$ y en particular para $\Omega$:
\[
\int_{\Omega}\mathbb{E}(X|\algb{B}) = \int_{\Omega}X \Leftrightarrow \mathbb{E}(\mathbb{E}(X|\algb{B})) = \mathbb{E}(X)
\]

El segundo paso es por la propiedad 6 de esperanza condicionada, ya que $X_0$ es $\algb{A}_0$-medible.

El tercer paso es por la linealidad y la primera y segunda observación que hemos escrito.


De manera general con $k>j$:
\[
\mathbb{E}\left[(X_k-X_{k-1})(X_j-X_{j-1})\right] = \mathbb{E}\left( \mathbb{E}\left[(X_k-X_{k-1})(X_j-X_{j-1})|\algb{A}_j\right]\right) =
\]
\[
= \mathbb{E}\left((X_j-X_{j-1}) \cdot \mathbb{E}\left[(X_k-X_{k-1})|\algb{A}_j\right]\right)= \mathbb{E}\left((X_j-X_{j-1}) \cdot (X_j-X_j)\right)  = 0
\]



\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 6.3  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[3] Probar el Teorema de Pit\'agoras generalizado: Si $u_1, \dots, u_n$ son vectores ortogonales
en un espacio vectorial con un producto interno (escalar o herm\'{\i}tico), entonces
$\|\sum_1^n u_i\|^2 = \sum_1^n \| u_i\|^2$, donde  $\| w\|^2:= (w,w)$.

\solution

\begin{expla}
Lo resolvemos por inducción.
\end{expla}

\begin{itemize}
\item Paso base: n=2, tenemos que $u \bot v \Rightarrow <u,v>=0$.

\[
\norm{u+v}^2=<u+v,u+v>=<u,u>+<u,v>+<v,u>+<v,v> = \norm{u}^2 + \norm{v}^2
\]

\item Paso inductivo: Supongo que $\norm{\sum_{i=1}^n u_i}^2 = \sum_{i=1}^{n} \norm{u_i}^2 $ con $u_i \bot u_j$ para $i\neq j$, por tanto $<u_i,u_j>=0$ para $i \neq j$.

\[
\norm{\sum_{i=1}^{n+1} u_i}^2 = <\sum_{i=1}^{n+1} u_i, \sum_{i=1}^{n+1} u_i> = <u_{n+1}+\sum_{i=1}^n u_i,u_{n+1}+\sum_{i=1}^n u_i> =
\]
\[
= <u_{n+1},u_{n+1}>+<u_{n+1},\sum_{i=1}^n u_i> + <\sum_{i=1}^n u_i, u_{n+1}>+<\sum_{i=1}^n u_i,\sum_{i=1}^n u_i>=
\]
\[
=\norm{u_{n+1}}^2 + \norm{\sum_{i=1}^{n} u_i} \Rightarrow \|\sum_1^n u_i\|^2 = \sum_1^n \| u_i\|^2
\]
\end{itemize}



\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newpage
\section{Hoja 7}
\textcolor{red}{A LA ESPERA DE SER CORREGIDA ESTA FATAL Y CREO QUE SEGUIRÁ ESTANDO FATAL PORQUE ME DA PEREZA PONERLA Y CREO QUE ES POCO IMPORTANTE}

Salvo afirmaci\'on expresa en sentido
contrario se asume siempre que estamos trabajando en un espacio de probabilidad $(\Omega, \mathcal{A}, P)$,
que  $\mathcal{B}\subset \mathcal{A}$ es una sub-$\sigma$-\'algebra, que las funciones son medibles, etc.

Recordatorio: si $1\le p < \infty$, $\|f\|_p := \left(\int|f|^p\right)^{1/p}$, mientras que
$\|f\|_\infty$ denota el supremo esencial de $|f|$. De hecho, la definici\'on
 $\|f\|_p := \left(\int|f|^p\right)^{1/p}$ tiene sentido para cualquier $p > 0$ finito, pero puede
demostrarse que si $p < 1$ esta expresi\'on no define una norma.


%%%%%%%%%%%%%%%%%%  PROBLEMA 7.1  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[1] Sea  $Y\in L^1(\Omega, \mathcal{A}, \mu)$, donde $\mu$ es una medida. Probar que para
todo $\varepsilon > 0$, existe una   $\delta > 0$ tal que para todo conjunto $A$ con $\mu(A) \le \delta$,
$\int_A |Y| d\mu <  \varepsilon.$

\vskip .15 cm

Sugerencia: aplicar Convergencia Mon\'otona a $Y_n := |Y| \mathbf{1}_{\{|Y| \le n\}}$.

\solution

\begin{expla}

\end{expla}

\[
\int_A \abs{Y} d\mu = \int_A \lim_{n \rightarrow \infty} \abs{Y} \ind_{\{\abs{Y} \leq n\}} d\mu = \lim_{n \rightarrow \infty} \int_A \abs{Y} \ind_{\{\abs{Y} \leq n\}} d\mu \leq  \lim_{n \rightarrow \infty} \int_A n \ind_{\{\abs{Y} \leq n\}} d\mu
\]

En la línea anterior, hemos podido intercambiar la integral con el límite a la derecha del segundo igual porque estábamos bajo las hipótesis del Teorema de Convergencia Monótona: $\{\mathcal{Y}_n\}_{n=0}^{\infty}$ es una sucesión creciente de funciones positivas en $L^+$. 

%Sabemos que es creciente puesto que partiendo de $\mathcal{Y}_n$ y tomando  $\mathcal{Y}_{n+1}$ se observa que ésta se mantiene igual en los puntos cuya imagen es menor que $n-1$ mientras que toma valores mayores que 0 en los puntos cuya imagen se encuentra entre $n-1$ y $n$.

Ahora escogemos un A con $\mu(A)<\frac{\epsilon}{n}$ y nos queda:

\[
\lim_{n \rightarrow \infty} \int_A n \ind_{\{\abs{Y} \leq n\}} d\mu = \lim_{n \rightarrow \infty} n \int_A \ind_{\{\abs{Y} \leq n\}} d\mu = \lim_{n \rightarrow \infty} n \int \ind_{\{A \cap \{\abs{Y} \leq n\}\}} d\mu \leq
\]

\[
 \leq \lim_{n \rightarrow \infty} n \int \ind_{A} d\mu =\lim_{n \rightarrow \infty} n \cdot \mu(A) < \lim_{n \rightarrow \infty} n \frac{\epsilon}{n} = \epsilon
\]

\end{problem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%  PROBLEMA 7.2  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[2] Sea $\{\mathcal{A}_n\}_{n=0}^{\infty}$ una filtraci\'on de sub-$\sigma$-\'algebras de $\mathcal{A}$,
y sea $Y\in L^1(\Omega, \mathcal{A}, P)$. Como $X_n:=  E(Y|\mathcal{A}_n)$ define una
martingala en $L^1$, el l\'{\i}mite  $X_\infty:= \lim_n X_n$ existe casi seguro. 
Probar que $X_n$ converge a $X_\infty$ en $L^1$. Concluir que para todo $n \ge 0$,
$X_n =  E(X_\infty|\mathcal{A}_n)$. Hallar la relaci\'on entre $Y$ y $X_\infty$.
Decidir razonadamente si $X_n =  E(Y|\mathcal{A}_n)$.
\vskip .15 cm

Sugerencia: usar el ejercicio anterior.

\solution

\begin{expla}

\end{expla}
\spart Vamos a probar que $X_n$ converge a $X_{\infty}$ en $L^1$.

Es decir, tenemos que ver que:
\[
0=\lim_{n\rightarrow \infty} \int_{\Omega} \abs{X_n - X_{\infty}} \geq \lim_{n\rightarrow \infty} \abs {\int_{\Omega} X_n - \int_{\Omega} X_{\infty}} \geq 0 \Leftrightarrow \lim_{n\rightarrow \infty} \int_{\Omega} X_n = \int_{\Omega} X_{\infty}
\]

Por tanto vamos a tratar de demostrar que $\lim_{n \rightarrow \infty} \int_{\Omega} X_n = \int_{\Omega} X_{\infty}$

Para ello vamos a usar lo que sabemos:
\[
\lim_{n \rightarrow \infty} X_n = X_{\infty}  \Rightarrow \int_{\Omega} \lim_{n \rightarrow \infty} X_n = \int_{\Omega} X_{\infty} 
\]




\textcolor{red}{Esto no pertenece al ejercicio, es solo una contradicción con el ejercicio 6.2}

Aplicando lo primero y que, $X_n = \mathbb{E}(Y|\algb{A}_n)$ que implica que $\forall B \in \algb{A}_n$:
\[
\int_B  \mathbb{E}(Y|\algb{A}_n) = \int_B Y \Rightarrow \lim_{n \rightarrow \infty} \int_{\Omega}  \mathbb{E}(Y|\algb{A}_n) = \lim_{n \rightarrow \infty} \int_{\Omega}  X_n = \lim_{n \rightarrow \infty} \int_{\Omega}  Y = \int_{\Omega} Y
\]

Tenemos que $X_n$ converge a Y en $L^1$.


\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 7.3  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[3] Probar que existe una submartingala $X := \{S_n\}_{n=0}^{\infty}$  tal que 
  $Y := \{|S_n|\}_{n=0}^{\infty}$  no es una submartingala. 

\vskip .15 cm

Sugerencia: Para construir dicha submartingala, considerar
 la fortuna obtenida en tiempo $n$ cuando ganamos un euro con probabilidad $p$, para alg\'un 
$p\in (1/2, 1)$, y perdemos un euro con probabilidad $1-p$.

\solution

\begin{expla}

\end{expla}
Definimos la submartingala $X=\{S_n\}_{n=0}^{\infty}$. Siendo $S_n$ las ganancias al tirar n veces una moneda, ganando 1 euro con probabilidad 1/2 y perdiendo 1 euro con probabilidad 1/2.

Definimos la submartingala $Y=\{\abs {S_n}\}_{n=0}^{\infty}$. Siendo $S_n$ lo mismo que antes.

Así por ejemplo:
\begin{itemize}
\item $w_1=(1,1,-1,-1,1) \rightarrow X(w_1) = 1 \text{ y } Y(w_1)=1$
\item $w_2=(1,1,1,1,1) \rightarrow X(w_2) = 5 \text{ y } Y(w_2)=5$
\item $w_3=(-1,-1,-1,-1,-1) \rightarrow X(w_3) = -5 \text{ y } Y(w_3)=5$
\item $w_4=(-1,-1,-1,-1,1) \rightarrow X(w_4) = -3 \text{ y } Y(w_4)=3$
\end{itemize}

Vamos a comprobar que X es una martingala:
\begin{enumerate}
\item $S_n \in L^1$, correcto ya que $\int \abs{S_n}dP = n$
\item $S_n$ es $\algb{A}_n$-medible. Lo cual es cierto, definiendo la filtracion de $\algb{A}_n = \sigma(S_n)$.
\item $\mathbb{E}(S_{n+1}|\algb{A}_n)\geq S_n$, Esto es cierto ya que: $S_{n+1}=S_n + Z$, siendo Z una variable aleatoria que vale 1,o -1 con probabilidad 1/2. Por lo que nos queda que:
\[
\mathbb{E}(S_{n+1}|\algb{A}_n) = \mathbb{E}(S_{n}+Z|\algb{A}_n) = \mathbb{E}(S_{n}|\algb{A}_n) + \mathbb{E}(Z|\algb{A}_n) = S_n + 0 = S_n
\]
\end{enumerate}

Ahora vamos a ver que ocurre con Y. Las dos primeras propiedad las cumple igual de forma obvia, vemos que ocurre con la tercera:
\[
\mathbb{E}(\abs{S_{n+1}}|\algb{A}_n) = \mathbb{E}(\abs{S_{n}+Z}|\algb{A}_n) = \mathbb{E}(\abs{S_{n}+Z}|\algb{A}_n) \leq \mathbb{E}(\abs{S_{n}}+\abs{Z}|\algb{A}_n) =
\]
\[
=\mathbb{E}(\abs{S_{n}}|\algb{A}_n)+\mathbb{E}\abs{Z}|\algb{A}_n) = \abs{S_n}+1    
\]


\textcolor{red}{elena solucion}
\begin{expla}
Usaremos
\begin{itemize}
\item Sea $Y_{n}:=$ variable aleatoria que toma el valor $1$ si ganamos $1$ euro en la partida $n$ y -1 si lo perdemos.
\item $P(Y_{n}=1)=p$ y $P(Y_{n}=-1)=1-p$
\item Sea $S_{n}:=$ fortuna acumulada hasta la partida $n$.
\item $S_{n}=\sum_{k=1}^{n} Y_k$
\item Sabemos que $X=\{\{S_n\}_{n=0}^{\infty},  \{\sigma(Y_1,...Y_n)\}_{n=0}^{\infty}\}$ define una martingala (visto en clase).
\end{itemize}
\end{expla}
Ahora definimos  $Y=\{\{|S_n|\}_{n=0}^{\infty},  \{\sigma(Y_1,...Y_n)\}_{n=0}^{\infty}\}$ y demostraremos que no es una submartingala. En particular, veremos que las propiedades 1 y 2 de submartingalas son trivialmente ciertas para $Y$ pero la propiedad 3 no se cumple:

\begin {enumerate}
\item Dado que X es una martingala, sabemos que $S_{n}$ está en $L^1$, y por tanto,  $|S_{n}|$ también.
\item Dado que X es una martingala, sabemos que  $S_{n}$ es $\sigma(Y_1,...Y_n)$-medible, de modo que, tomando valores absolutos, se tiene que las  $|S_{n}|$ también son $\sigma(Y_1,...Y_n)$-medibles.

\item Llamemos $\algb{A}_{n}:=\sigma(Y_1,...Y_n)$ para facilitar la notación. Se tiene que:
\[
 \mathbb{E}(|S_{n+1}||\algb{A}_{n}) = \mathbb{E}(|\sum_{k=1}^{n+1} Y_k||\algb{A}_{n}) \leq  \mathbb{E}(|\sum_{k=1}^{n} Y_k|+ |Y_{n+1}||\algb{A}_{n})  =
\]
\[
  \mathbb{E}(|\sum_{k=1}^{n} Y_k||\algb{A}_{n})  +  \mathbb{E}(| |Y_{n+1}||\algb{A}_{n}) = |S_{n}| + 0 = |S_{n}|.
\]
Como no se verifica que $ \mathbb{E}(|S_{n+1}||\algb{A}_{n}) \geq |S_n|$, si no que se demuestra todo lo contrario,  $Y$ no es una submartingala.
\end{enumerate}


\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\section{Hoja 8}
\textcolor{blue}{Corregida y contrastada con las soluciones del profesor salvo que se diga lo contrario}

Salvo afirmaci\'on expresa en sentido
contrario se asume siempre que estamos trabajando en un espacio de probabilidad $(\Omega, \mathcal{A}, P)$,
que  $\mathcal{B}\subset \mathcal{A}$ es una sub-$\sigma$-\'algebra, que las funciones son medibles, etc..


Recordatorio: si $0 < p < \infty$, $\|f\|_p := \left(\int|f|^p\right)^{1/p}$, mientras que
$\|f\|_\infty$ denota el supremo esencial de $|f|$. 

%%%%%%%%%%%%%%%%%%  PROBLEMA 8.1  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[1]Decidir razonadamente si la independencia de los sucesos $A$ y $B$ es equivalente a la independencia de $A$ y $B^c$.

\solution
SOLUCIÓN 1:

Si A y B son independientes entonces $P(A \cap B) = P(A)P(B)$.

Por tanto queremos ver que $P(A \cap B^c) = P(A)P(B^c)$:
\[
P(A \cap B^c) = P(A \cap (\Omega \setminus B)) = P((A \cap \Omega) \setminus (A \cap B)) = P(A \setminus (A\cap B))
\]

Como $(A\cap B)$ esta contenido en A:
\[
P(A \setminus (A \cap B))= P(A)-P(A\cap B) = P(A)-P(A)P(B)=P(A)(1-P(B))=P(A)P(B^c)
\]

Por tanto A y $B^c$ también son independientes.

SOLUCIÓN ALTERNATIVA:
\[
P(A)=P(A|B)P(B)+P(A|B^c)P(B^c)
\]

Sabiendo que $P(A|B)=P(A)$, entonces:
\[
P(A) = P(A)P(B)+P(A|B^c)P(B^c)  \Leftrightarrow  P(A)-P(A)P(B)=P(A|B^c)P(B^c) \Leftrightarrow P(A)(1-P(B))=P(A|B^c)P(B^c)
\]

Pero $1-P(B)=P(B^c)$. Y nos queda:
\[
P(A)P(B^c)=P(A|B^c)P(B^c)  \Leftrightarrow P(A)=P(A|B^c)
\]

y queda demostrado que A y $B^c$ son independientes.
\end{problem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%  PROBLEMA 8.2  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[2] Hallar la relaci\'on entre la independencia de los sucesos $A$ y $B$, y la independencia de las variables aleatorias $\mathbf{1}_A$ y $\mathbf{1}_B$.

\solution

\textcolor{red}{Sin revisar}

Si A y B son independientes entonces $P(A \cap B) = P(A)P(B)$.

En esta ocasión vamos a probar que para toda colección $C_1,C_2$ de conjuntos de Borel en $\mathbb{R}$, tenemos que: $P(\ind_A \in C_1 \cap \ind_B \in C_2)=P(\ind_A \in C_1)P(\ind_B \in C_2)$. 

Notación: $P(\ind_A \in C_1 \cap \ind_B \in C_2) = P(w \in \Omega : \ind_A \in C_1 \cap \ind_B \in C_2)$

Tenemos que al tratarse de una función indicatriz, los conjuntos de Borel $C_1$ o $C_2$ que nos interesan son aquellos que contienen al 0 o al 1, distinguimos los siguientes casos:

\begin{enumerate}
\item Si $C_1$ o $C_2$ no contienen al 0 o al 1:
\[
P(\ind_A \in C_1 \cap \ind_B \in C_2) = P(\emptyset \cap \emptyset) = 0 = P(\emptyset) P(\emptyset) =P(\ind_A \in C_1)P(\ind_B \in C_2)
\]

\item Si tanto $C_1$ como $C_2$ contienen al 0 y al 1:
\[
P(\ind_A \in C_1 \cap \ind_B \in C_2) = P(\Omega \cap \Omega) = 1 = P(\Omega) P(\Omega) = P(\ind_A \in C_1)P(\ind_B \in C_2)
\]

\item Si $C_1$ contiene al 1 y al 0 y $C_2$ contienen solo al 1:
\[
P(\ind_A \in C_1 \cap \ind_B \in C_2) = P(\Omega \cap B) = P(B) = P(\Omega) P(B) = P(\ind_A \in C_1)P(\ind_B \in C_2)
\]


\item Si $C_1$ contiene al 1 y al 0 y $C_2$ contienen solo al 0:
\[
P(\ind_A \in C_1 \cap \ind_B \in C_2) = P(\Omega \cap B^c) = P(B^c) = P(\Omega) P(B^c) = P(\ind_A \in C_1)P(\ind_B \in C_2)
\]

\item Si $C_1$ y $C_2$ contienen solo al 1, usamos la independencia de A y B:
\[
P(\ind_A \in C_1 \cap \ind_B \in C_2) = P(A \cap B) = P(A)P(B) = P(\ind_A \in C_1)P(\ind_B \in C_2)
\]

\item Si $C_1$ contiene solo al 1 y $C_2$ contienen solo al 0, usamos el ejercicio anterior:
\[
P(\ind_A \in C_1 \cap \ind_B \in C_2) = P(A \cap B^c) = P(A)P(B^c) = P(\ind_A \in C_1)P(\ind_B \in C_2)
\]

\item Si $C_1$ y $C_2$ contienen solo al 0, usamos la independencia de A y B y algo semejante al ejercicio anterior:
\[
P(\ind_A \in C_1 \cap \ind_B \in C_2) = P(A^c \cap B^c) = P(A^c\cap(\Omega \setminus B)) = P(A^c \setminus (A^c \cap B)) =
\]
\[
P(A^c)-P(A^c \cap B)=P(A^c)-P(A^c)P(B)= P(A^c)(1-P(B))=P(A^c)P(B^c) = P(\ind_A \in C_1)P(\ind_B \in C_2)
\]


\end{enumerate}


Por tanto $\ind_A$ y $\ind_B$ son variables aleatorias independientes.

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 8.3  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[3]Probar que si $X_n\to  X$ en $L^p$, donde  $0 < p \le \infty$, entonces $X_n\to  X$ en probabilidad.


\solution


Tenemos que:

\[
\lim_{n \rightarrow \infty} \norm{X_n -X}_p = 0 \text{ es decir } \lim_{n \rightarrow \infty} \int_{\Omega} \abs{X_n -X}^p dP = 0
\]

Y queremos probar que $\forall \epsilon > 0$: 
\[
\lim_{n \rightarrow \infty} P(\abs{X_n -X}\geq \epsilon) = 0
\]

Partimos de la definición de convergencia en probabilidad:
\[
P(\abs{X_n -X}\geq \epsilon) = \int \ind_{\{\abs{X_n - X}> \epsilon\}}dP = \int \ind_{\{\abs{X_n - X}^p> \epsilon^p\}}dP \leq \int \frac{\abs{X_n - X}^p}{\epsilon^p}
\]
\[ \ind_{\{\abs{X_n - X}^p> \epsilon^p\}}dP \leq \frac{1}{\epsilon^p}  \int \abs{X_n - X}^p dP
\]

Que por hipótesis tiende a 0 cuando la n tiende a infinito.



\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 8.4  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[4]Probar que si $X_n\to  X$ en  probabilidad, entonces $X_n\to  X$ en distribuci\'on. Decimos que
 $X_n\to  X$ en distribuci\'on si para todo punto $x$ de continuidad de $F_X$, $\lim_n F_{X_n} (x) = F_X (x)$.
\solution
\textcolor{blue}{Esta es una de las demostraciones que hay que aprenderse de memoria}


Queremos probar que para todo punto t de continuidad de $F_x$ y todo $\epsilon >0, \exists N=N(\epsilon)$, tal que $\forall n \geq N$ se tiene que:
\[
F_X(t)-2\epsilon \leq F_{X_n}(t) \leq F_X(t)+2\epsilon
\]

Fijamos $\epsilon>0$, como t es un punto de continuidad de $F_X$, existe un $\delta>0$ tal que si $\abs{s-t}\leq \delta$, entonces $\abs{F_X(s)-F_X(t)} \leq \epsilon$. Escogemos un $\delta$ con esa propiedad.

Además sabemos que como $X_n \stackrel{P}{\rightarrow} X$, $\exists N>0$ tal que $\forall n \geq N$, $P(\abs{X_n -X}>\delta)\leq \epsilon$. \textcolor{blue}{Por definición de convergencia en probabilidad}.

Probamos primero: $F_{X_n}(t) \leq F_X(t)+2\epsilon$:
\[
F_{X_n}(t) = P(X_n \leq t) \leq P(\{X \leq t+\delta\} \cup \{\abs{X_n -X} > \delta\}) \leq P(X \leq t+\delta)+ P(\abs{X_n -X} > \delta)
\]
\[
\leq F_X(t+\delta)+\epsilon \leq F_X(t)+\epsilon+\epsilon = F_{X_n}(t) + 2\epsilon
\]

El penúltimo signo $\leq$ se debe a la hipótesis de convergencia en probabilidad y el último a la continuidad de $F_X$.

\textcolor{blue}{Intento explicar el primer signo $\leq$ que aparece por si no se ve bien. Se supone que queremos ver como $X_n$ se va aproximando a X en probabilidad, se supone entonces que $P(X_n \leq t) \leq P(\{X \leq t+\delta\}$ se va a cumplir (es decir $F_{X_n}(t) \leq F_X(t+\delta)$, porque X se parece a $X_n$ y además estamos sumando un $\delta >0$. Pero puede que $F_{X_n}$ se este aproximando a $F_X$ por debajo, entonces por ello sumamos los $w \in \Omega$ en los que la diferencia entre X(w) y $X_n(w)$ sea mayor que $\delta$.}

Acabamos probando que $F_X(t)-2\epsilon \leq F_{X_n}(t)$:
\[
F_X(t)-2\epsilon \leq F_X(t-\delta)-\epsilon \leq P(X_n \leq t)+ P(\abs{X -X_n} \leq \delta) - \epsilon \leq F_{X_n}(t) + \epsilon - \epsilon  = F_{X_n}(t)
\]

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 8.5  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[5]Dar un ejemplo de dos variables aleatorias incorrelacionadas pero no independientes.
Sugerencia: Considerar dos v. a. no triviales con soportes disjuntos y media 0.


\solution

\textcolor{red}{Sin revisar}

Sea $\Omega = Borel(\mathbb{R})$.

$$
X =
  \left\lbrace
  \begin{array}{l}
      0 \text{ si } w < -1\\
      1 \text{ si } -1 \leq w < 0\\
      -1 \text{ si } 0 \leq w < 1\\
      0 \text{ si } w \geq 1\\
  \end{array}
  \right.
Y =
  \left\lbrace
  \begin{array}{l}
      0 \text{ si } w < -1\\
      1 \text{ si } -1 \leq w < 1\\
      -1 \text{ si } 1 \leq w < 3\\
      0 \text{ si } w \geq 3\\
  \end{array}
  \right.
$$

Entonces:
$$
XY = X =
  \left\lbrace
  \begin{array}{l}
  	  0 \text{ si } w < -1\\
  	  1 \text{ si } -1 \leq w < 0\\
  	  -1 \text{ si } 0 \leq w < 1\\
  	  0 \text{ si } w \geq 1\\
  \end{array}
  \right.
$$

\[
\mathbb{E}(XY)-\mathbb{E}(X)\mathbb{E}(Y) = 0 - 0*0 = 0
\]

Sin embargo no son independientes.



\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 8.6  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[6] Los l\'{\i}mites superior e inferior de una sucesi\'on de conjuntos  $\{A_n\}_{n=1}^{\infty}  $ se
definen respectivamente como $\limsup_n A_n := \cap_{n\ge 1 } \cup_{k\ge n } A_k$ y 
$\liminf_n A_n := \cup_{n\ge 1 } \cap_{k\ge n } A_k$. Determinar que conjunto es m\'as grande.
Hallar la relaci\'on entre  $\limsup_n A_n$ y  $\limsup_n \mathbf{1}_{ A_n}$. Hacer lo mismo con los
 l\'{\i}mites inferiores.



\solution


El limite superior es más grande o igual que el límite inferior.

El limite superior de $A_n$ es un conjunto, y el límite superior de $\ind_{A_n}$ es una función. Entonces se cumple que sea:
\[
A = \limsup_n A_n \text{ y } \ind_A = \limsup_n \mathbf{1}_{ A_n}
\]

Entonces:
\[
\ind_A =
  \left\lbrace
  \begin{array}{l}
  	  1 \text{ si } w \in A\\
  	  0 \text{ si } w \notin A\\
  \end{array}
  \right.
\]

Lo mismo ocurre con el limite inferior.

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 8.7  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[7] Dada la probabilidad uniforme en $[0,1)$,  para $n \ge 1$ y $0 \le k < n$ definimos 
$X_{n,k} :=\mathbf{1}_{[k/n, (k + 1)/n)}$. Con el orden
del diccionario, los pares $(n,k)$ est\'an ordenados linealmente. Hallar  $\limsup_{(n,k)\to \infty} X_{n,k}$
y  $\liminf_{(n,k)\to \infty} X_{n,k}$. Hallar $P(\limsup_{(n,k)\to \infty} X_{n,k} > 1/2\})$.
Decidir razonadamente si la sucesi\'on  $\{ X_{n,k}\}_{(n, k) \ge (1,0)} $
converge en probabilidad, en $L^p$,  $0 < p \le \infty$, y  en casi todo punto.


\solution
\textcolor{red}{sin revisar}

\[
\limsup_{(n,k)\to \infty} \mathbf{1}_{[k/n, (k + 1)/n)} = \liminf_{(n,k)\to \infty} \mathbf{1}_{[k/n, (k + 1)/n)} = \mathbf{1}_{[0,1)}
\]

Por tanto, la sucesión converge en casi todo punto a $\mathbf{1}_{[0,1)}$. Por tanto también converge en probabilidad.

\[
P(\limsup_{(n,k)\to \infty} X_{n,k} > 1/2\}) = P(\{\mathbf{1}_{[0,1)} > 1/2\}) = 1/2
\]

Sólo falta ver si converge en $L^p$, es decir:
\[
\lim_{(n,k) \rightarrow \infty} \norm{\mathbf{1}_{[k/n, (k + 1)/n)} -\mathbf{1}_{[0,1)}}_p = 0 \text{ es decir } \lim_{(n,k) \rightarrow \infty} \int_{\Omega} \abs{\mathbf{1}_{[k/n, (k + 1)/n)} -\mathbf{1}_{[0,1)}}^p dP = 0
\]

Desarrollamos la integral:
\[
\lim_{(n,k) \rightarrow \infty} \int_{\Omega} \abs{\mathbf{1}_{[0,k/n)\cup[(k + 1)/n,1)}}^p dP = \lim_{(n,k) \rightarrow \infty} \int_{\Omega} \abs{\mathbf{1}_{[0,k/n)\cup[(k + 1)/n,1)}} dP = 
\]
\[
\lim_{(n,k) \rightarrow \infty} \int_{\Omega} \mathbf{1}_{[0,k/n)\cup[(k + 1)/n,1)} dP = \lim_{(n,k) \rightarrow \infty} \int_{[0,k/n)\cup[(k + 1)/n,1)} 1 dP =
\]
\[
= \lim_{(n,k) \rightarrow \infty} \frac{k}{n}+\frac{n-k-1}{n} = \lim_{(n,k) \rightarrow \infty} \frac{n-1}{n} = 1
\]

Por tanto concluimos que no converge en $L^p$.


\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 8.8  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[8]Probar que si $\{X_n\}_{n=1}^{\infty}$  converge c. s. a $X$, entonces para todo $\epsilon > 0$,
$P(\limsup_n\{ |X_n - X| > \epsilon\}) = 0$.


\solution
\textcolor{blue}{Idea: Coger la demostración del teorema entero y parar hasta llegar a ese punto}

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 8.9  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[9]Probar que si para todo $\epsilon > 0$,
$P(\limsup_n\{ |X_n - X| > \epsilon\}) = 0$, entonces  $\{X_n\}_{n=1}^{\infty}$  converge casi seguro a $X$.
Sugerencia: Tomar $\epsilon = 1/k$, $k$ natural, y usar el hecho de que la uni\'on numerable de conjuntos
de probabilidad cero tiene probabilidad cero.


\solution
\textcolor{red}{sin revisar}

Sabemos que:
\[
P(\limsup_n\{ |X_n - X| > \epsilon\}) = 0
\]

Que es lo mismo que decir que, tomando $\epsilon = 1/n$ para n natural:
\[
P(\cap_{n=1}^{\infty} \cup_{k\geq n} |X_k - X| > \frac{1}{k}) = 0
\]

Queremos ver que también es igual al límite inferior...
\[
P(\cup_{n=1}^{\infty} \cap_{k\geq n} |X_k - X| > \frac{1}{k}) = 0
\]

... y poder decir así que existe el límite lo cual es obvio ya que:
\[
D=\{\cap_{k\geq 1} |X_k - X| > \frac{1}{k}\} \leq \{\cap_{n=1}^{\infty} \cup_{k\geq n} |X_k - X| > \frac{1}{k}\} = C
\]

Y sabemos que la $0\leq P(D) \leq P(C)=0$ por lo que hemos visto antes. Eso, unido a que la unión numerable de conjuntos de probabilidad 0...
\[
\cup_{n=1}^{\infty} \{\cap_{k\geq 1} |X_k - X| > \frac{1}{k}\} \geq \{\cup_{n=1}^{\infty} \cap_{k\geq n} |X_k - X| > \frac{1}{k}\}
\]

... tiene probabilidad 0, determina que existe la probabilidad del límite inferior y que es 0, y por tanto:
\[
P(\lim_n\{ |X_n - X| > \epsilon\}) = 0
\]

Esto quiere decir que:
\[
P(\lim_n\{ |X_n - X| \leq \epsilon\}) = 1
\]

Y por tanto que $\forall \epsilon > 0$:
\[
\lim_n\{ |X_n - X| \leq \epsilon
\]

Es decir, que $\forall \epsilon > 0, \exists N: \forall n \geq N$, se tiene que:
\[
\abs{X_n -X} \leq \epsilon
\]

Por tanto, se demuestra la convergencia casi segura.

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 8.10  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[10]Probar que la convergencia casi seguro implica la convergencia en probabilidad. Sugerencia: Usar alguno de los
problemas anteriores.


\solution
\textcolor{red}{sin revisar}

Sabemos que $\forall \epsilon > 0, \exists N: \forall n \geq N$, se tiene que:
\[
\abs{X_n -X} < \epsilon
\]

Es decir $\lim_{n \rightarrow \infty} \abs{X_n-X}=0$.

Y por los ejercicios anteriores, esto implica que:
\[
P(\lim_n\{ |X_n - X| > \epsilon\})= 0
\]

Desarrollamos:
\[
0 \leq \lim_n P(\{ |X_n - X| > \epsilon\})=\lim_{n \rightarrow \infty} \int  \ind_{\{\abs{X_n-X}>\epsilon\}} dP \stackrel{TCD}{=} \int \lim_{n \rightarrow \infty}  \ind_{\{\abs{X_n-X}>\epsilon\}} dP =
\]
\[
= P(\lim_n\{ |X_n - X| > \epsilon\})= 0
\]


\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Hoja 9}
\textcolor{red}{Esta hecha en su paǵina en el curso de año pasado, la estoy pasando aquí pero tengo algunas dudas de lo que hace en algunos ejercicios}

Para el Jueves 23/4/2015. Salvo afirmaci\'on expresa en sentido
contrario se asume siempre que estamos trabajando en un espacio de probabilidad $(\Omega, \mathcal{A}, P)$,
que  $\mathcal{B}\subset \mathcal{A}$ es una sub-$\sigma$-\'algebra, que las funciones son medibles, etc.. 

%%%%%%%%%%%%%%%%%%  PROBLEMA 9.1  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[1]Lanzamos una moneda lastrada, con probabilidad de sacar cara
igual a $3/5$. Si sale cara lanzamos un dado equilibrado con cuatro
caras numeradas del 1 al 4, y si sale cruz lanzamos un dado
equilibrado con seis caras numeradas del 1 al 6. Sea $Y$ el n\'umero
obtenido. Denotando $X=1$ si sale cara, $X=0$ si sale cruz, hallar
a) $E(Y|X)$, y  b) $E(Y)$.

\solution

En el primer caso se trata de una función:
\[
\mathbb{E}(Y|X) =
  \left\lbrace
  \begin{array}{l}
  	  \frac{10}{4} \text{ si } Y(w) = cara\\
  	  \frac{21}{6} \text{ si } Y(w) = cruz\\
  \end{array}
  \right.
\]

Tras obtener esa función es fácil sacar la esperanza de Y:
\[
\mathbb{E}(Y)=\frac{3}{5}\cdot\frac{10}{4} + \frac{2}{5}\cdot\frac{21}{6} = 2.9
\]

\end{problem}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%  PROBLEMA 9.2  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[2] Tenemos un dado equilibrado con 6 caras numeradas del 1 al 6. Lanzamos la  primera vez y 
apuntamos el n\'umero $x$ obtenido. A continuaci\'on, 
lanzamos el dado $x$ veces y sumamos los valores $y_i$ obtenidos: $s_x = y_1 + \cdots  + y_x$.
Hallar $E(X)$, $E(Y_i)$, y $E(S_X)$, donde $X$ es la variable aleatoria ``resultado del primer lanzamiento", $Y_i$ el resultado del lanzamiento
$ 1 + i$, y $S_X = \sum_{k = 1}^X Y_k$. Se asume que todas
las tiradas son independientes. 
Determinar la relaci\'on entre las tres medias.

\solution
\begin{enumerate}
\item Comenzando hallando la esperanza de X:
\[
\mathbb{E}[X] = \frac{21}{6}
\]

\item En segundo lugar hallamos la esperanza de $Y_i$, que es la misma que de X:
\[
\mathbb{E}[Y_i] = \frac{21}{6}
\]
\item Concluimos hallando la esperanza la esperanza de $S_x$. Pero el valor de $S_x$ depende de la variable aleatoria X, por tanto, vamos a definir la función $\mathbb{E}(S_x | X)$:
\[
\mathbb{E}(S_x | X) =
  \left\lbrace
  \begin{array}{l}
  	  \frac{21}{6} \text{ si } X(w) = 1\\
  	  \frac{21}{6}\cdot 2 \text{ si } X(w) = 2\\
  	  \frac{21}{6}\cdot 3 \text{ si } X(w) = 3\\
  	  \frac{21}{6}\cdot 4 \text{ si } X(w) = 4\\
  	  \frac{21}{6}\cdot 5 \text{ si } X(w) = 5\\
  	  \frac{21}{6}\cdot 6 \text{ si } X(w) = 6\\
  \end{array}
  \right.
\]

Por tanto:
\[
\mathbb{E}(S_x)=\frac{1}{6}\cdot\frac{21}{6}+\frac{1}{6}\cdot\frac{21}{6}\cdot2+\frac{1}{6}\cdot\frac{21}{6}\cdot3+\frac{1}{6}\cdot\frac{21}{6}\cdot4+\frac{1}{6}\cdot\frac{21}{6}\cdot5+\frac{1}{6}\cdot\frac{21}{6}\cdot6 = 12.25 = (\frac{21}{6})^2
\]
\end{enumerate}

Se tiene que:
\[
\mathbb{E}(X)=\mathbb{E}(Y_i)=\frac{21}{6}
\]

Y que:
\[
\mathbb{E}(S_x)=\mathbb{E}(X)\mathbb{E}(Y_1)
\]

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 9.3  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[3]a) Enunciar la Ley Fuerte de Los Grandes N\'umeros.

b) Consideramos $(0,1)$ con la medida de Lebesgue, Escribimos $w\in (0,1)$ usando
la expansi\'on decimal habitual. Definimos $X_n(w)$ como el n\'umero de sietes que aparecen
en las $n$ primeras posiciones de la expansi\'on decimal de $w$ (por ejemplo, $X_3(0.777) = 3,
 x_3(0.12345) = 0$.  Decidir razonadamente si $\lim_n n^{-1}X_n$ converge casi seguro, y
 en caso de respuesta afirmativa, hallar el l\'{\i}mite.
 
 c) Decidir razonadamente si 
 $$
 \lim_n \frac{X_n - 10^{-1}n}{n^{2/3}}
 $$ 
 converge casi seguro, y
 en caso de respuesta afirmativa, hallar el l\'{\i}mite.


\solution

\spart
La ley fuerte de los grandes números de Kolgomorov dice:

Sea $\{X_n\}_{n \geq 1}$ una sucesión de v.a.i.i.d en $L^1$, entonces:
\[
\frac{1}{n}\sum_{j=1}^{n} \stackrel{c.s.}{\rightarrow} \mathbb{E}(X_1)
\]

\spart
Por la ley fuerte de Kolgomorov: \textcolor{red}{¿Qué es exactamente que estén identicamente distribuidas?Por que en este caso lo están?}
\[
\frac{1}{n}\sum_{j=1}^{n} \stackrel{c.s.}{\rightarrow} \mathbb{E}(X_1) = \frac{1}{10}
\]

\spart

\[
\lim_n \frac{X_n-\frac{n}{10}}{n^{2/3}}
\]

Podemos usar el siguiente teorema:
Sean $\{X_n\}_{n\geq 1}$ v.a.i.i.d con $X_1 \in L^p$, $p \in [1,2)$. Entonces:
\[
\frac{1}{n^{1/p}}\sum_{j=1}^{n}X_j \stackrel{c.s.}{\rightarrow} 0
\]
Sea $Y_j \sim Bernoulli(1/10)$. $Y_j \in L^p$ $\forall p \in [1, \infty]$, en particular $Y_j \in L^{\frac{3}{2}}$, luego por la tasa de convergencia anterior para v.a en $L^p$ con $p \in [1,2)$, tenemos que:
\[
\lim_n \frac{X_n-\frac{n}{10}}{n^{2/3}} = \lim_n \frac{\sum_{i=1}^{n} (Y_i -\frac{1}{10})}{n^{2/3}} = 0
\]

Con $X_i = Y_i - \frac{1}{10}$. \textcolor{red}{Valdría sin poner el $\frac{1}{10}$ no?}

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 9.4  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[4]Calcular las funciones generatrices de probabilidad, de momentos y las funciones caracter\'{\i}sticas de $X$ cuando $X$ es
a) Bernoulli$(p)$,
b) Binomial$(n,p)$,
c) Poisson$(\lambda)$. Usar unicidad y la propiedad multiplicativa para conclu\'{\i}r que la suma de v.a. independientes
con distribuci\'on Poisson$(\lambda_i)$, $i = 1, \dots, n$, es Poisson$(\sum_1^n \lambda_i)$.
\solution
Llamamos $g$ a la función generatriz, $\psi$ a la función generadora (o generatriz) de momentos y $\varphi$ a la función característica.


\begin{enumerate}
\item Para $X~Bernoulli(P)$:
\[
g_X(t)=\mathbb{E}(t^X)=t^0(1-p)+tp=1+(t-1)p
\]
\[
\psi_X(t)=\mathbb{E}(e^{tX})=1-p+pe^t=1+(e^t-1)p
\]
\[
\varphi_X(t)=\mathbb{E}(e^{itX})=1+(e^{it}-1)p
\]

\item $S_n~B(n,p)$. Usamos q=1-p
\[
g_{S_n}(t)=\mathbb{E}(t^{S_n})=(q+tp)^n
\]
\[
\psi_{S_n}(t)=\mathbb{E}(e^{tS_n})=(q+e^tp)^n
\]
\[
\varphi_{S_n}(t)=\mathbb{E}(e^{itS_n})=(q+e^{it}p)^n
\]

\item Para $Y~Poisson(\lambda)$:
\[
g_Y(t)=\mathbb{E}(t^Y)=\sum_{n=0}^{\infty}t^ne^{-\lambda}\frac{\lambda^n}{n!}=e^{\lambda t}e^{-\lambda}=e^{\lambda(t-1)}
\]
\[
\psi_Y(t)=\mathbb{E}(e^{tY})=\sum_{n=0}^{\infty}e^{tn}e^{-\lambda}\frac{\lambda^n}{n!}=e^{\lambda(e^t-1)}
\]
\[
\varphi_Y(t)=\mathbb{E}(e^{itY})=e^{\lambda(e^{it}-1)}
\]
\end{enumerate}



\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 9.5  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[5]Calcular la funci\'on generatriz de momentos y la funci\'on caracter\'{\i}stica de $X\sim \operatorname{Uniforme}(0,1)$. 


\solution
Función característica:
\[
\varphi_X(t)=\mathbb{E}(e^{itX})=\int_{0}^{1}e^{itX}dx=\frac{e^{it}-1}{it}
\]

Función generatriz de momentos.
\[
\psi_X(t)=\frac{e^t-1}{t}
\]




\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 9.6  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[6]Calcular la funci\'on generatriz de momentos y la funci\'on caracter\'{\i}stica de $Z\sim N(0,1)$. 

\solution

Sea $X~N(0,1)$. Su función de densidad es $f_X(t)=e^{\frac{-x^2}{2}}$ Completando el cuadrado y usando el hecho de que la integral de una densidad es 1, tenemos que la función generatriz:

\[
\psi_X(t)=\mathbb{E}(e^{tX})=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{tX}e^{-\frac{x^2}{2}}dx=e^{\frac{t^2}{2}}\frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty}e^{-\frac{(x-t)^2}{2}} dx = e^{\frac{t^2}{2}}
\]

Y la función característica:
\[
\varphi_X(t)=\mathbb{E}(e^{itX})=e^{\frac{(it)^2}{2}}=e^{-\frac{t^2}{2}}
\]



\end{problem} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 9.7  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[7]Probar que la funci\'on caracter\'{\i}stica de $X$ con media 0 y varianza $1$
satisface
$$
\phi_X(t) = 1 - \frac{1}{2} t^2   + o(t^2)
$$ 
cuando $t\to 0$. Sugerencia: usar la expansi\'on de Taylor de $e^{ir}$, donde $r$ es real.


\solution


\[
\varphi_X(t)=\mathbb{E}(e^{itX})=\mathbb{E}(1+itX-\frac{t^2}{2}X^2+o(t^2X^2))=1-\frac{t^2}{2}\mathbb{E}(X^2)+o(t^2\mathbb{E}(X^2)) \stackrel{t \rightarrow 0}{=} 1-\frac{t^2}{2}+o(t^2)
\]

Recordar que $\mathbb{E}(X)=0$ y que $0=Var(X)=\mathbb{E}(X-\mathbb{E}(X))^2=\mathbb{E}(X-0)^2=\mathbb{E}(X^2)$

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 9.8  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[8] Demostrar el TCL usando los ejercicios anteriores, unicidad y la propiedad multiplicativa de las funciones
caracter\'{\i}sticas, y el Teorema de Continuidad de L\'evy-Cram\'er.


\solution
El TCL para v.a.i.i.d en $L^2$. Suponemos sin pérdida de generalidad que $\mathbb{E}(X_1)=0$ y $Var(X_1)=1$ (Si no, en vez de $X_i$ usamos $Y_i =\frac{X_i -\mathbb{E}(X_i)}{\sqrt{Var(X_i)}}$).

Cogemos la función característica siguiente:
\[
\phi_{\frac{\sum_{i=1}^{n}X_i}{\sqrt{n}}}(t) =
\]

Por la propiedad multiplicativa, es igual a:
\[
=\prod_{i=1}^{n} \phi_{\frac{X_i}{\sqrt{n}}}(t)= \prod_{i=1}^{n} \mathbb{E}(e^{it(\frac{X_i}{\sqrt{n}})})=
\prod_{i=1}^{n} \mathbb{E}(e^{iX_i(\frac{t}{\sqrt{n}})})= 
\]

Usando el ejercicio 7, es decir, que para una variable aleatoria con media 0 y varianza 1, se tiene que: \textcolor{red}{El paso de rojo no pillo de donde sale}
\[
=\left(1-\frac{1}{2}\left(\frac{t}{\sqrt{n}}\right)^2+o(\frac{t^2}{n})\right)^n = \textcolor{red}{\left(e^{\frac{-t^2}{2n}}\right)^n=e^{\frac{-t^2}{2}}}
\]

Ahora recordamos Levi-Cramer:
\[
\forall t \in \mathbb{R}, \lim_{n \rightarrow \infty} \phi_{X_n}(t)=\phi_X(t) \Leftrightarrow X_n \stackrel{D}{\rightarrow} X
\]

Y usamos Levi-Cramer y el ejercicio 6 para decir que $\phi_X(t)=e^{-t^2/2}$ para $X\sim N(0,1)$:
\[
\forall t \in \mathbb{R}, \lim_{n \rightarrow \infty} \phi_{}(t)=\phi_{\frac{\sum_{i=1}^{n}X_i}{\sqrt{n}}}(t)=e^{\frac{-t^2}{2}} = \phi_X(t) \Leftrightarrow {\frac{\sum_{i=1}^{n}X_i}{\sqrt{n}}} \stackrel{D}{\rightarrow} X\sim N(0,1)
\]

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 9.9  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[9] Para $n=1, 2, 3, \dots$, sea  $\{X_{n,m}\}_{m=1}^{n}$  una configuraci\'on triangular de v.a., tales que las variables en cada fila son independientes, sea $S_{n} := \sum_{i=1}^n X_{n,i}$, y sean $0 < a < b < 1$. Supongamos que las
variables en la fila $n$ son Bernoulli($p_n$), es decir, $P(X_n = 1) = p_n$, $P(X_n = 0) = 1 - p_n$.
Usar el Teorema de Berry-Esseen para demostrar que si $a \le p_n \le b$ para todo $n$,
entonces $(S_n - n p_n)/\sqrt{n p_n (1 - p_n)}$ converge en distribuci\'on a $Z\sim N(0,1)$.
\solution

EL teorema de Berry-Esseen dice lo siguiente:
Sean $\{X_n\}_{n\geq 1}$ v.a.i.i.d con $\mathbb{E}(X_1)=\mu$, $Var(X_1)=\sigma^2$ y $\mathbb{E}(\abs{X_1}^3)<\infty$; entonces $\forall x \in \mathbb{R}$:
\[
\abs{F_{\frac{(\sum_{j=1}^{n}X_j)-n\mu}{\sigma \sqrt{n}}}(x)-F_Z(x)} \leq \frac{c\cdot \mathbb{E}(\abs{X_1}^3)}{\sigma^3\sqrt{n}} \text{ , donde $Z\sim N(0,1)$ y $c\in (0.4,0.5)$}
\]

En este caso tenemos $S_n = \sum_{i=1}^{n}X_{n,i}$ con $X_{i,n} \sim Bernoulli(p_n)$ por tanto: $\mathbb{E}(X_{i,n})=p=\mu$ y $Var(X_{i,n})=p(1-p)=\sigma^2$.

Por tanto, $\forall n$ y por Berry-Jensen, se tiene que $
\forall x$:
\[
\frac{(S_n - n p_n)}{\sqrt{n p_n (1 - p_n)}} = \frac{(\sum_{j=1}^{n}X_{n,j})-n\mu}{\sigma \sqrt{n}} \Rightarrow \abs{F_{\frac{(\sum_{j=1}^{n}X_{n,j})-n\mu}{\sigma \sqrt{n}}}(x)-F_Z(x)} \leq \frac{c\cdot \mathbb{E}(\abs{X_1}^3)}{\sigma^3\sqrt{n}}
\]

Pero $\sigma$ es mayor que $C_1=\min(\sqrt{a-a^2},\sqrt{b-b^2}) < \sigma$ y por tanto $\sigma^3 > C_1^3$ constante > 0. Además $\norm{X_{n,m}}_{\infty}=1$ por tanto $C_2=\norm{X_{n,m}}_3=\mathbb{E}(\abs{X_{n,m}}^3) \leq 1$.

Por tanto nos queda que:
\[
\abs{F_{\frac{(\sum_{j=1}^{n}X_{n,j})-n\mu}{\sigma \sqrt{n}}}(x)-F_Z(x)} \leq \frac{c\cdot \mathbb{E}(\abs{X_1}^3)}{\sigma^3\sqrt{n}} \leq  \frac{c\cdot C_2}{C_1\sqrt{n}} \stackrel{n \rightarrow \infty}{\rightarrow} 0
\]

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 9.10  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[10]Probar que si $\{X_n\}_{n=0}^{\infty}$  es una sucesi\'on de v.a.i.i.d. en $L^2$, se cumple
la condici\'on de Lindeberg. Sugerencia, usar el Teorema de la Convergencia Dominada de
Lebesgue.

\solution
La condición de Lindeberg es que:
$\forall \epsilon>0$, $\lim_{n \rightarrow \infty} L(n, \epsilon)=0$.

Siendo:
\[
L(n, \epsilon)= \frac{1}{S^2_n}\sum_{i=1}^{n}\mathbb{E}\left( \abs{X_i-\mu_i}^2 \ind_{\{\abs{X_i - \mu_i} \geq \epsilon S_n\}} \right)
\]

En nuestro caso tenemos $S_n^2=n \sigma^2$, y nos queda:
\[
\frac{1}{ \sigma^2}\sum_{i=1}^{n}\mathbb{E}\left( \abs{X_i-\mu_i}^2 \ind_{\{\abs{X_i - \mu_i} \geq \epsilon \sigma \sqrt{n}\}} \right) = \frac{n}{ \sigma^2}\mathbb{E}\left( \abs{X_i-\mu_i}^2 \ind_{\{\abs{X_i - \mu_i} \geq \epsilon \sigma \sqrt{n}\}} \right)
\]

Como:
\[
0 \leq \abs{X_1 - \mu}^2\ind_{\{\abs{X_i - \mu_i} \geq \epsilon \sigma \sqrt{n}\}} \leq \abs{X_1 - \mu}^2
\]

Que converge c.s a 0, y \textcolor{red}{no entiendo por que}

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 9.11  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[11]Para $n=1, 2, 3, \dots$, sea  $\{X_{n, m}\}_{m=1}^{n}$  una configuraci\'on triangular de v.a., tales que las variables en cada fila son independientes, y sea $S_{n} := \sum_{i=1}^n X_{n, i}$.

a) Enunciar la condici\'on de Lindeberg.

b) Suponiendo que las variables en la fila $n$ son Bernoulli($p_n$), con
$p_n = n^{- 1/2}$,
decidir razonadamente
si 
$(S_n - n p_n)/\sqrt{n p_n (1 - p_n)}$ converge en distribuci\'on a $Z\sim N(0,1)$.

c) Sea $s \in (0,1)$. Decidir razonadamente qu\'e sucede si en el apartado anterior, en
vez de  $p_n = n^{ - 1/2}$ tenemos $p_n = n^{ - s}$.

d) Decidir razonadamente qu\'e sucede si en el apartado b), en
vez de  $p_n = n^{- 1/2}$ tenemos $p_n = n^{ - 1}$.
Sugerencia: recordar la Ley de los N\'umeros Peque\~nos.
\solution
\spart La condición de Lindeberg es que:
$\forall \epsilon>0$, $\lim_{n \rightarrow \infty} L(n, \epsilon)=0$.

Siendo:
\[
L(n, \epsilon)= \frac{1}{S^2_n}\sum_{i=1}^{n}\mathbb{E}\left( \abs{X_i-\mu_i}^2 \ind_{\{\abs{X_i - \mu_i} \geq \epsilon S_n\}} \right)
\]

\spart 
\textcolor{red}{No entiendo porque se hace 0 lo de dentro de la integral}

\textcolor{red}{Se puede aplicar berry jensen para demostrarlo?}

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%  PROBLEMA 9.12  %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[12]Probar que la condici\'on de Lyapunov implica la condici\'on de Lindeberg. 
La condici\'on de Lyapunov nos dice que existe un $\delta > 0$ tal que
$\lim_{n\to \infty} Lyap(n, \delta) = 0$, donde 
$$
Lyap(n, \delta) := \frac{1}{s_n^{2 + \delta}} \sum_{k=1}^n  E|X_k - \mu_k|^{2 + \delta} .
$$



\solution




\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

