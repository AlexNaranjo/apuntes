\chapter{Métodos Runge Kutta}

\section{Tablero de Butcher}
Como ya vimos en la sección \ref{sec:runge-kurra} los métodos Runge Kutta se caracterizan por la necesidad de calcular unos valores $K_i$ como sigue:
\[K_i = f\left(x_n+c_ih, y_n+h \sum_{j=1}^ia_{ij}K_j\right)\]
para escribir después el método en si mismo:
\[y_{n+1} = y_n+ h \sum_{i=1}^s b_i K_i\]

La idea de este método se apoya en la aproximación de la integral como nos muestra la siguiente ecuación.
\[y(x_{n+1}) = y(x_n) + \int_{x_n}^{x_{n+1}} f(x,y(x))\dif x \approx y(x_n)+h \sum_{j}^{s} b_j f(x_n+hc_j, y(x_n+hc_j))\]

Las diferentes formas de aproximar la función $y(x_n+hc_j)$ nos dan las diferentes $K_i$ de los distintos métodos Runge Kutta.

Una manera de representar de forma compacta el método es mediante el \concept{Tablero de Butcher}. Este tablero nos permite representar de forma cómoda los valores de las constantes que intervienen en el método. Tiene la siguiente estructura:

\begin{center}
\begin{tabular}{c|cccccc}
$c_1$ & $a_{11}$ & $a_{12}$ & $a_{13}$ & $\cdots$ & $a_{1,s}$\\
$c_2$ & $a_{21}$ & $a_{22}$ & $a_{23}$ & $\cdots$ & $a_{2,s}$\\
$\vdots$ & $\cdots$ & $\cdots$ & $\cdots$ & $\ddots$ & $\vdots$ \\
$c_s$ & $a_{s1}$ & $a_{s2}$ & $a_{s3}$ & $\cdots$ & $a_{ss}$\\
\hline
 & $b_1$ & $b_2$ & $b_3$ & $\cdots$ & $b_s$\\
\end{tabular}
\end{center}

Si recordamos la definición, decimos que un método de Runge Kutta es explícito si cada $K_i$ puede calcularse explícitamente a partir de los valores anteriormente calculados. Si atendemos al tablero de Butcher, esto se traduce en que la matriz $A = a_{ij}$ sea triangular inferior estricta.

Evidentemente el método será implícito siempre que la matriz no sea triangular inferior estricta.

No obstante, existe un subconjunto dentro de los métodos de Runge Kutta que hay que destacar. Se trata de los métodos semi-implícitos que se caracterizan por tener la matriz $A$ triangular inferior pero con elementos en la diagonal.

Estos métodos son algo más complejos que los explícitos, pues tenemos que hacer aproximaciones y emplear métodos como el del punto fijo para calcular algunas $K_i$, pero siguen siendo mejores que los métodos con matrices que tengan elementos no nulos por encima de la diagonal pues nos obliga a resolver un sistema de ecuaciones implícito.

Aunque el tablero de Butcher lo estamos estudiando dentro del contexto de los métodos Runge-Kutta, puede utilizarse con cualquier método como muestra el siguiente ejemplo.

\begin{example}
Si consideramos el método de Euler modificado tenemos el siguiente tablero:

\begin{center}
\begin{tabular}{c|cc}
$0$ & & \\
$\frac{1}{2}$ & $\frac{1}{2}$ & \\
\hline
 & $0$ & $1$\\
\end{tabular}
\end{center}
\end{example}

\begin{example}
Si consideramos el método RK-Raday IA de dos etapas, el tablero queda de la forma:

\begin{center}
\begin{tabular}{c|cc}
$0$ & $1/4$ & $-1/4$\\
$2/3$ & $1/4 $ & $5/12$ \\
\hline
 & $1/4$ & $3/4$\\
\end{tabular}
\end{center}
\end{example}

\begin{example}
Si consideramos el método RK-Lobatto IIIA de dos etapas, el tablero queda de la forma:

\begin{center}
\begin{tabular}{c|cc}
$0$ & $0$ & $0$\\
$1$ & $1/2 $ & $1/2$ \\
\hline
 & $1/2$ & $1/2$\\
\end{tabular}
\end{center}
\end{example}

\obs En general es frecuente encontrarse con tableros que tienen huecos vacíos en la matriz $A$. Estos huecos representan 0s.

\section{Hipótesis de los métodos numéricos}

Los métodos de Runge-Kutta son métodos de un sólo paso con función de incremento:
\[\phi_f(x_n,y_n;h) = \sum_{i=1}^sb_ik_i(x_n,y_n;h)\]

Puesto que las etapas $k_i$ son evaluaciones de la función $f$, no es difícil convencerse de que $\phi_f$ satisface la condición de Lipschitz con respecto a su segunda variable si $f$ también lo hace.

\begin{example}
Vamos a comprobar que la función incremento es Lipschitz en la segunda variable en el método de Euler modificado.

Debemos recordar que este método se basa en el uso de la siguiente recurrencia:
\[y_{n+1}=y_n+hf\left(x_n+\frac{1}{2}h,y_n+\frac{h}{2}K_1\right)=y_n+hf\left(x_n+\frac{1}{2}h,y_n+\frac{h}{2}f(x_n,y_n)\right)\]
\[\norm{k_1(x_n,y_n;h)-k_1(x_n,\tilde{y}_n;h)} \leq L\norm{y_n-\tilde{y}_n}\]
\[\norm{k_2(x_n,y_n;h)-k_2(x_n,\tilde{y}_n;h)} \leq L\left( \norm{y_n-\tilde{y}_n}+\frac{h}{2}\norm{k_1(x_n,y_n;h)-k_1(x_n,\tilde{y}_n;h)}\right) \leq \atop L\left(1+\frac{h}{2}\right) \norm{y_n-\tilde{y}_n}\]
de donde se obtiene inmediatamente la condición de Lipschitz deseada observando que en este caso $\phi_f(x_n,y_n;h)=k_2(x_n,y_n;h)$
\end{example}

\obs En general todo método Runge-Kutta satisface las hipótesis para métodos numéricos si tomamos un $h$ suficientemente pequeño.

Podemos ver que si tomamos $h=0$ tenemos
\[k_i(x_n,y_n;0)=f(x_n,y_n)\]
lo que nos permite escribir:
\[\phi_f(x,y;0)=\sum_{i=1}^sb_if(x,y)\]
Aplicando el criterio de consistencia \ref{theorem:consist_iif_relaciones} vemos que los métodos Runge-Kutta son consistentes si y sólo si
\[\sum_{i=1}^sb_i = 1\]

Por otro lado podemos comprobar que el criterio de la raíz \ref{theorem:criterio_raiz} siempre se satisface para los métodos Runge-Kutta lo que nos garantiza que siempre serán 0-estables.

\section{Orden del método}
Considerando un método Runge-Kutta general podemos escribir el residuo como:
\[R_n = y(x_{n+1})-y(x_n)-h \sum_{i=1}^sb_iK_i(x_n,y(x_n);h)\]

Para escribirlo más comodamente definimos dos funciones:
\[\begin{array}{l}
F(h) = y(x_{n+1}) = y(x_n+h)\\
G(h) = y(x_n)+h \sum_{i=1}^sb_iK_i(x_n,y(x_n),;h)
\end{array}\]

de modo que el residuo queda escrito como
\[R_n=F(h)-G(h)\]

Consideramos también un problema de valor inicial genérico de la forma:
\[\begin{array}{l}
y'=f(x,y(x))\\
y(0)=y^0
\end{array}\]

Escribiendo el desarrollo de Taylor de la función $F(h)$ en torno a $x_n$ tenemos:
\[F(h)=y(x_n)+hy'(x_n)+\frac{h}{2}y''(x_n)+\frac{h^3}{3!}y'''(ε_n) = \atop y(x_n)+hf+\frac{h^2}{2}\left(f_x+f_y\cdot f\right)+\frac{h^3}{3!}y'''(ε_n)\]

donde tanto la función $f$ como sus derivadas están evaluadas en $(x_n,y(x_n))$.

Por definición de $K_i$ tenemos
\[K_i(x_n,y(x_n);h) = f\left(x_n+c_ih,y_n + h\sum_{j=1}^sa_{ij}K_j(x_j,y(x_j);h)\right)\]
y derivando con respecto a $h$ tenemos:
\[\frac{\partial K_i}{\partial h}(x_n,y(x_n);h) =c_if_x\left(x_n+c_ih,y(x_n) + h\sum_{j=1}^sa_{ij}K_j(x_j,y(x_j);h\right)+\atop f_y\left(x_n+c_ih,y(x_n)+h\sum_{j=1}^sa_{ij}K_j(x_j,y(x_j);h\right)\sum_{j=1}^sa_{ij}K_j(x_j,y(x_j);h)\]

Si evaluamos ahora en $h=0$ tenemos
\[\frac{\partial K_i}{\partial h}(x_n,y(x_n);0) = c_if_x\left(x_n,y(x_n)\right) + \sum_{j=1}^s a_{ij}f_y(x_n,y(x_n))f(x_n,y(x_n))\]

Apoyándonos en esto es relativamente sencillo calcular el desarrollo de Taylor de $G(h)$\footnote{Desarrollamos sólamente el sumatorio, que es una suma de funciones $f$ evaluadas en distintos puntos}, con lo que obtenemos:
\[G(h)=y(x_n)+h\sum_{i=1}^sb_if() + h^2\sum_{i=1}^sb_ic_if_x()+h^2\sum_{i,j=1}^sb_ic_{ij}f_y()+\varphi(x_n,y(x_n),h)h^3\]
siendo $f() = f(x_n,y(x_n))$.

Nuestro objetivo es calcular el residuo y estimarlo como $O(h^p)$ siendo así $p$ el orden del método. Como ya hicimos al principio del curso, vamos a restar los desarrollos de Taylor con lo que obtenemos:

\[R_n=h\left(f-\sum_{i=1}^sb_if\right)+h^2\left(\frac{1}{2}f_x+\frac{1}{2}f_y\cdot f - \sum_{i=1}^sb_ic_if_x()-\sum_{i,j=1}^sb_ic_{ij}f_y()\right)+O(h^3)\]

Si queremos que el residuo sea de orden 3 (y por tanto el método tenga orden de convergencia $2$) necesitamos que todos los términos distintos de $O(h^3)$ se cancelen en la fórmula del residuo.

Así vemos que necesitamos:
\begin{itemize}
\item $\sum_{i=1}^sb_i = 1$, que ya nos garantizaba la consistencia
\item $\sum_{i=1}^sb_ic_i = \frac{1}{2}$
\item $\sum_{i,j=1}^sb_ic_{ij} = \frac{1}{2}$
\end{itemize}

Por otra parte, si consideramos el problema $y'(x)=1, \ x\in [0,b], \ y(0)=0$ cuya solución es la función identidad se tiene que
\[R_n=h\left( 1 - \sum_{i=1}^sb_i\right)\]
Por consiguiente, si $\sum_{i=1}^sb_i\neq 1$ el método no tiene orden de consistencia 1.

De manera análoga se demuestra que las condiciones que acabamos de ver son necesarias para tener orden de consistencia de 2.

Una vez llegados a este punto el lector puede convencerse de que calcular condiciones de orden para un método de Runge-Kutta general es una tarea ardua. Así que vamos a simplificar un poco las cosas.

Cualquier problema de valor inicial puede escribirse en \concept{forma autónoma} a costa de aumentar la dimensión del espacio. En efecto, si definimos
\[\overline{y}(x)=\left(\begin{array}{c} x \\ y(x)\end{array}\right), \ \ \tilde{y}^0 = \left(\begin{array}{c} 0 \\ y(0)\end{array}\right), \ \ \overline{f}(\overline{y}(x)) = \left(\begin{array}{c} 1 \\ f(x,y(x))\end{array}\right)\]
el problema original es equivalente al problema autónomo:
\[\overline{y}'(x)=\overline{f}(\overline{y}(x)), \ \ \overline{y}(a)=\tilde{y}^0\]

\begin{problem}[ Ejercicio de examen]Si un sistema cumple $c_i = \sum_{j=1}^sa_{ij}$ entonces la solución del sistema y la del automatizado coinciden

\solution

\doneby{Pedro}

Si tomamos la definición genérica de un método Runge-Kutta y la aplicamos sobre el PVI genérico:
\[\begin{array}{l}
y'=f(x,y(x))\\
y(0)=y^0
\end{array}\]
obtendremos
\[y_{n+1} = y_n+ h \sum_{i=1}^s b_i K_i \text{ siendo } K_i = f\left(x_n+c_ih, y_n+h \sum_{j=1}^ia_{ij}K_j\right)\]

Por otro lado, si aplicamos la misma definición del método al problema autónomo tendremos:
\[\overline{y}_{n+1}=\overline{y}_n+h\sum_{i=i}^sb_i\overline{K}_i\ \text{ siendo } \overline{K}_i = \overline{f}\left(\overline{y}_n+h\sum_{j=1}^ia_{ij}\overline{K}_j\right)\]

Con las relaciones entre las funciones originales y las del sistema autónomo podemos reescribir las dos ecuaciones anteriores (la de la $\overline{K}_i$ y la de $\overline{y}_n$) de la siguiente forma:
\small
\[\overline{K}_i = \left( \begin{array}{c} r_i \\ K_i \end{array}\right) = \left(\begin{array}{c} 1 \\ f\left(x_n+h\sum a_{ij}r_j, y_n+h \sum_{j=1}^ia_{ij}K_j\right)\end{array}\right) \implies \]
\[\implies \left( \begin{array}{c} x_{n+1} \\ y_{n+1}\end{array}\right) = \left( \begin{array}{c} x_n \\ y_n\end{array}\right) + h\sum_{i=1}^sb_i\left( \begin{array}{c} r_i \\ K_i\end{array}\right)=  \left(\begin{array}{c} x_n+h\sum_{i=1}^sb_i \\ y_n+h\sum_{i=1}^sb_if\left(x_n+h\sum a_{ij}r_j, y_n+h \sum_{j=1}^ia_{ij}K_j\right)\end{array}\right)=\]
\normalsize

\obs La primera igualdad procede de la idea feliz de que escribiendo $\bar{K}_i$ de esa forma simplificamos la notación\footnote{Supongo que será parte de la definición de los métodos Runge-Kutta sobre sistemas}. Una vez tenemos eso, podemos deducir la segunda igualdad de la siguiente forma:
\[\overline{K}_i = \overline{f}\left(\overline{y}_n+h\sum_{j=1}^ia_{ij}\overline{K}_j\right) = \left(\begin{array}{c} 1 \\ f(\overline{y}_n(x)+h\sum_{j=1}^ia_{ij}\overline{K}_j(x))  \end{array} \right) =\]
\[= \left(\begin{array}{c} 1 \\ f\left(\left(\begin{array}{c} x_n \\ y_n\end{array}\right)+h\sum_{j=1}^ia_{ij}\left(\begin{array}{c} r_j \\ k_j\end{array}\right)\right)  \end{array} \right) = \left(\begin{array}{c} 1 \\ f\left(\left(\begin{array}{c} x_n + h\sum_{j=1}^ia_{ij}r_j \\ y_n + h \sum_{j=1}^ia_{ij}k_j\end{array}\right)\right)  \end{array} \right)\]

Sabemos que cualquier método RK satisface $\sum b_j=1$. Por otro lado, por la construcción que hemos realizado, podemos ver que $r_j=1$.

Por otro lado, si queremos que con las segundas coordenadas formemos la misma ecuación que al resolver el PVI original necesitamos que
\[c_i = \sum_{j=1}^s a_{ij}\]

Es evidente entonces que cuando no se de esta condición el problema autonomizado tendrá una solución distinta al original.


\end{problem}

\subsection{Barreras de Butcher}
\begin{theorem}
Un método de Runge-Kutta explícito de $s$ etapas no puede tener orden mayor que $s$
\end{theorem}
\begin{proof}
Consideramos el método PVI:
\[\left\{\begin{array}{l}
y'=y \text{ con } y\in [0,b]\\
y(0)=1
\end{array}\right.\text{ cuya solución es } y=e^x \text{ con } x\in [0,\log(b)]\]

Como siempre, si aplicamos un método de Runge-Kutta genérico tendremos que el residuo es:
\[R_n=y(x_{n+1})-y(x_n)-h\sum_{i=1}^sb_iK_i(x_n,y(x_n);h)\]

Nuevamente definimos las funciones $F$ y $G$:
\[\begin{array}{l}
F(h) = y(x_n+h)\\
G(h) = y(x_n)+h \sum_{i=1}^sb_iK_i(x_n,y(x_n);h)
\end{array}\]

de modo que el residuo queda escrito como
\[R_n=F(h)-G(h)\]

Por Taylor podemos ver que
\[F(h)=F(0)+\sum_{j=1}^p\frac{\partial^j F}{\partial h^j}(0)\frac{h^j}{j!} + O(h^{p+1})\]
Pero sabemos que
\[F(h)=e^{x_n+h}=e^{x_n}e^h \implies \frac{\partial^p F}{\partial h^p}(0)=e^{x_n}=y(x_n)\]

Atendiendo a esta propiedad que acabamos de mencionar podemos escribir:
\[F(h)=y(x_n)+\sum_{j=1}^p \frac{h^j}{j!}y(x_n) + O(h^{p+1})\]

Por otro lado tenemos
\[\left.\frac{\partial^p G}{\partial h^p}\right|_{h=0} = \left. p\sum_{j_1=1}^sb_{j_1}K_{j_1}^{(p-1)}\right|_{h=0}\]
siendo
\[K_{j_1}^{(p-1)}=\left.(p-1)\sum_{j_2=1}a_{j_1j_2}K_{j_2}^{(p-2)}\right|_{h=0} = \left.(p-1)!\sum_{j_1h_2...j_p}^2a_{j_1j_2}\cdots a_{j_{p-1}j_p} K_{j_p}^{(0)}\right|_{h=0}\]
Una vez conocemos la fórmula de las derivadas, podemos escribir el desarrollo de Taylor de $G(h)$ que nos queda:
\[G(h)=y(x_n)+y(x_n)\sum_{j=1}^p\frac{h^j}{j!}\sum_{j_1,...,j_p=1}^sb_{ij}a_{j_1j_2}\cdots a_{j_{p-1}j_p}K_{j_p}^{(0)} + O(h^{p+1})\]

Una vez tenemos los desarrollos de Taylor de $G(h)$ y de $F(h)$ podemos calcular la diferenia entre ambos, lo que nos permitirá conocer el residuo.

Si tuviéramos que el residuo tiene orden mayor que $p$, por ejemplo $p+1$ tendríamos
\[F(h)-g(h) = O(h^{p+1})\implies p!\sum_{j_1,...,j_p=1}a_{j_1j_2}\cdots a_{j_{p-1}j_p} = 1\]

Para que sea $1$, como poco debemos evitar que los $a_{j_i,j_{i+1}}$ sean nulos.

Es obvio entonces que uno de los sumandos ha de ser distinto de 0. Para ese sumando, puesto que la matriz de las $a_{ij}$ es triangular inferior, necesitamos que
\[j_1>j_2>...>j_p\]
Pero la matriz tiene tamaño $s$. Es evidente por tanto que si llegamos a tener un $p$ mayor que $s$ será imposible satisfacer la relación, con lo que todos los términos del sumatorio serán nulos y por tanto el sumatorio también.

\end{proof}


\begin{prop}
El teorema anterior nos pone una cota muy amplia que, a menudo, no se alcanza. En general se tiene que:

\begin{enumerate}
\item Con $s=1,2,3,4$ se puede conseguir orden $1,2,3,4$ respectivamente.
\item Con $p=5$ no existe un método RK explícito con orden $p$
\item Con $p\geq 7$ no existe un método RK explícito con orden $p$ con $p=s+1$
\item Con $p\geq 8$ no existe un método RK explíito de orden $p$ con $s=p+1$.
\end{enumerate}
\end{prop}

\section{A-estabilidad}
Vamos a ver cómo se aplica la definición de A-estabilidad a los métodos Runge-Kutta.

Si vamos a resolver un problema del tipo $y'=λy$ con $λ\in \cplex$ empleando un método Runge-Kutta, tendremos:
\[K_i=f\left(x_n+c_ih,y_n+h\sum_{j=1}^sa_{ij}K_j\right)\]
\[y_{n+1}=y_n+h\sum_{i=1}^sb_iK_i\]

que es equivalente a escribir:
\[Y_i=y_n+h\sum_{j=1}^sa_{ij}f(x_n+c_jh,Y_j)\]
\[y_{n+1}=y_n + h \sum_{i=1}^sb_if(x_n+c_ih,Y_i)\]

Trabajando con vectores definidos en $\real^s$ podemos escribir:
\[\left(\begin{array}{c}
Y_1 \\
Y_2 \\
\vdots \\
Y_s
\end{array}\right) = \left(\begin{array}{c}
y_n \\
y_n \\
\vdots \\
y_n
\end{array}\right) +h\cdot \left(\begin{array}{ccccc}
a_{11} & a_{12} & a_{13} & \cdots & a_{1,s}\\
a_{21} & a_{22} & a_{23} & \cdots & a_{2,s}\\
\vdots & \cdots & \cdots & \ddots & \vdots \\
a_{s1} & a_{s2} & a_{s3} & \cdots & a_{ss}
\end{array}\right) \cdot \left( \begin{array}{c} f(x_n+c_jh, Y_1) \\ f(x_n+c_jh, Y_2) \\ \vdots \\ f(x_n+c_jh, Y_s)\end{array}\right)\]

\[y_{n+1}=y_n+h\left(b_1,b_2, \cdots, b_s \right)\left(\begin{array}{c}
Y_1 \\
Y_2 \\
\vdots \\
Y_s
\end{array}\right)\]

Recordando que estamos considerando $y'=f(x,y)=λy$ podemos simplificar la primera ecuación obteniendo:
\[\left(\begin{array}{c}
Y_1 \\
Y_2 \\
\vdots \\
Y_s
\end{array}\right) = \left(\begin{array}{c}
y_n \\
y_n \\
\vdots \\
y_n
\end{array}\right) +h\cdot \left(\begin{array}{ccccc}
a_{11} & a_{12} & a_{13} & \cdots & a_{1,s}\\
a_{21} & a_{22} & a_{23} & \cdots & a_{2,s}\\
\vdots & \cdots & \cdots & \ddots & \vdots \\
a_{s1} & a_{s2} & a_{s3} & \cdots & a_{ss}
\end{array}\right) \cdot λ \cdot \left(\begin{array}{c}
Y_1 \\
Y_2 \\
\vdots \\
Y_s
\end{array}\right)\]

Por definición, teníamos que el dominio de estabilidad lineal es:
\[D = \{z=hλ \tq |R(z)|<1\}\]

En nuestro caso tendremos
\[R(λh) = 1+λh\left(b_1,b_2,\cdots ,b_s \right)(1-λhA)^{-1}e, \text{ siendo } e \text{ vector columna de } 1s\]

\obs Para entender esta fórmula basta con despejar el vector columna $\overline{Y}=(Y_1,...,Y_n)^T$ de la fórmula que lo define.

No obstante, para los métodos Runge-Kutta esta definición se acompaña de cierta precisión, que recoge el siguiente lemma:
\begin{lemma}
Para todo método Runge-Kutta se tiene que $R(z) \in \mathbb{P}_{S/S}$ y si el método es explícito, entonces $R(z) \in \mathbb{P}_S$.

Aquí $\mathbb{P}_S$ es el conjunto de los poliniomios de grado menor o igual que $S$ y $\mathbb{P}_{S/S}$ el conjunto de funciones racionales $\frac{p}{q}$ con $p,q \in \mathbb{P}_S$
\end{lemma}
\begin{proof}
Recordando la definición de matriz inversa tenemos:
\[(1-zA)^{-1}=\frac{\text{adj}(1-zA)^T}{det(1-zA)}\]

Los elementos de $(1-zA)$ son lineales. El adjunto pertenece a $\mathbb{P}_{S-1}$ por lo que pertenece a $\mathbb{P}_S$.

Si el método es explícito, la matriz $(1-zA)$ será triangular inferior con 1s en la diagonal por lo que su determinante será 1..
\end{proof}
\begin{corol}
Ningún RK explícito es A-estable.
\end{corol}

\begin{lemma}\label{lemma:a-estable}
Sea $R(z)$ una función racional no constante entonces
\[|R(z)| < 1 \ \forall z \in \cplex^- \iff \text{ los polos de R tienen parte real positiva y } |R(it)| < 1 \ \forall t \in \real\]
\end{lemma}
%\begin{proof}
%Si $|R(z)|<1$ en $\cplex^-$ los polos sólo pueden estar en $\cplex^+$ y, por continuidad, sabemos que $|R(z)|\leq1$ en $\overline{\cplex}$ lo que implica que $|R(t)|\leq 1 \ \forall t \in \real$.

%Si todos los polos tienen parte real positiva, entonces el máximo y el mínimo se alcanzan en la frontera.

%TODO completar la ultima afirmación que no la veo clara.
%\end{proof}

\section{Pares encajados}

En general es habitual encontrarnos con que el método aproxima bien la función en algunos intervalos pero no en otros. En estas ocasiones lo que nos interesa es tomar un $h$ más pequeño en aquellas regiones en las que la función es singular, invirtiendo mucho esfuerzo en ello y relajarnos tomando $h$ mayor en regiones donde la función es más estable.

Tomemos como ejemplo el \textbf{método de Euler} que, recordemos, venía definido por la recursión:
\[y_{n+1} = y_n + h f(x_n,y_n)\]

El error se calcula como:
\[\text{error} = y''(x_n)\frac{h_n^2}{2}+O(h^3)\]

Cuando estudiemos el intervalo $x_n \to x_{n+1}$ tendremos el error:
\[\text{error} = \left(f_x(x_n,y_n)+f_y(x_n,y_n)f(x_n,y_n) \right)\frac{h_n^2}{2}\]

Si el error es menor, en valor absoluto, que una cierta toleracia, aceptamos el valor $y_{n+1}$ calculado. En caso contrario, repetimos estas cuentas reduciendo $h$ a la mitad.

\obs Este procedimiento corre el riesgo de entrar en bucle infinito si la tolerancia que tomamos es demasiado pequeña. En la práctica acostumbra a establecerse un máximo de iteraciones tras las cuales dejamos de intentar mejorar el error.

Una pega importante de este procedimiento es que para cada cálculo del error tenemos que hacer dos evaluaciones de funciones derivadas (que suelen ser más complicadas que la función $f$ original). Computacionalmente este trabajo es muy costoso y para evitarlo se emplean los pares encajados.

El \textbf{par encajado} consiste en el empleo de dos métodos Runge-Kutta:
\begin{itemize}
\item \textbf{Avanzador}
Este método tendrá orden $p$ y su tablero de Butcher será:

\begin{center}
\begin{tabular}{c|c}
c & A \\
\hline
 & b\\
\end{tabular}
\end{center}


\item \textbf{Estimador}
Este método tendrá orden $\tilde{p}>p$ y su tablero de Butcher será:

\begin{center}
\begin{tabular}{c|c}
c & A \\
\hline
 & $\tilde{b}$\\
\end{tabular}
\end{center}
\end{itemize}

La forma de proceder consiste en suponer que el estimador nos da la solución exacta de forma que el error será:
\[\text{error} = h_n \sum_{i=1}^s(b_i-\tilde{b}_i)K_i\]

Veamos un ejemplo
\begin{example}
Consideramos los métodos:
\begin{itemize}
\item \textbf{Avanzador}
\begin{center}
\begin{tabular}{c|cc}
0 & 0 & 0 \\
1 & 1 & 0\\
\hline
 & 1 & 0\\
\end{tabular}
\end{center}

\item \textbf{Estimador}

\begin{center}
\begin{tabular}{c|cc}
0 & 0 & 0 \\
1 & 1 & 0 \\
\hline
 & 1/2 & 1/2\\
\end{tabular}
\end{center}
\end{itemize}

Ambos métodos emplearán las mismas $K_i$, pues se definen igual. Una vez vemos esto podemos escribir:

\[\left\{ \begin{array}{l}
K_1=f(x_n,y_n)\\
K_2=f(x_n+h,y_n+hK_1)

\end{array} \implies  \begin{array}{l}
y_{n+1} = y_n+hb_1K_1+hb_2K_2\\
\tilde{y}_{n+1} = \tilde{y}_n + h\tilde{b}_1K_1 + h\tilde{b}_2K_2
\end{array}\right.\]

Aunque puede parecer que tenemos que calcular dos $K$ en cada paso, la realidad es que $K_2$ de un paso coincide con $K_1$ del paso siguiente.

El método estimado se ha empleado en cada caso para estimar el error $y_{n+1}$.
\end{example}

En general, todos los métodos en los que la primera $K$ que calculamos en cada paso coincide con la última $K$ calculada en el paso anterior, se denominan \concept{FSAL} y vienen caracterizados por la propiedad:
\[a_{s_i}=b_i \text{ con } i=1...2 \text{ y } c_i = \sum_{j=1}^sa_{ij}\]