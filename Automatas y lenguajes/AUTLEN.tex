\documentclass{apuntes}

% Paquetes adicionales

% --------------------



\title{Autómatas y lenguajes}
\author{Pedro Valero y Alberto Parramón}

\date{2014/2015}
\usepackage{forest}

\begin{document}
\pagestyle{plain}

\maketitle
\tableofcontents
\newpage

\printindex

\chapter{Introducción}
Vamos a trabajar con tres elementos fundamentales:
\begin{itemize}
\item \textbf{Máquinas/Autómata}
\begin{itemize}
\item Autómatas finitos $\Rightarrow$ Expresiones regulares
\item Autómatas de pila $\Rightarrow$ Lenguajes independientes del contexto
\end{itemize}
\item \textbf{Problemas} ¿Qué se puede computar?. Conjeturas que se creen ciertas pero cuya veracidad, por ahora, no se ha demostrado.

\item \textbf{Lenguajes/Gramática}
\end{itemize}

Nuestro objetivo es ver qué relación existe entre estos tres elementos. Para ello, primero debemos establecer algunas definiciones.

\section{Lenguaje}
\begin{defn}[Símbolo]
``Letra", elemento de un conjunto
\end{defn}

\begin{defn}[Alfabeto]
Conjunto finito de símbolos no vacío.
\end{defn}

\begin{defn}[Palabra (Cadena)]
Secuencia finita de símbolos tomados de un alfabeto.
La palabra vacía tiene 0 símbolos y se representa por $\lambda$.
\end{defn}

Será conveniente acostumbrarnos a usar el término ``cadena'' en lugar del término ``palabra'' ya que representa mejor el concepto que queremos representar.


\begin{defn}[Longitud de cadena]
Número de símbolos que contiene.
\end{defn}

\begin{defn}[Lenguaje]
Conjunto de palabras, cualquier subconjunto de $\Sigma^*$.
\end{defn}

Hay algunos casos particulares de lenguajes:
\subsection{Lenguajes particulares}
\begin{defn}[Lenguaje universal (sobre $\sum$)]
Denotado por $\sum^*$ representa el conjunto de todas las palabras que se pueden formar con los símbolos de $\Sigma$, incluido $\lambda$.
\end{defn}

\begin{defn}[Lenguaje de un autómata]
Conjunto de palabras que acaban en un estado final del autómata y, por tanto, son aceptadas por el mismo.
\end{defn}

\begin{defn}[Lenguaje vacío]
Lenguaje que no contiene ningún elemento: $\phi$.
\end{defn}

\begin{defn}[Lenguaje $\lbrace \lambda \rbrace$]
Lenguaje que sólo contiene $\lambda$.
\end{defn}
El lenguaje $\lbrace \lambda \rbrace$ es distinto del lenguaje vacío aunque $\lambda$ sea la palabra vacía. En particular $|\{\lambda\}|=1$ y $|\phi|=0$.

\begin{defn}[Lenguaje $\Sigma^+$ ]
\[ \Sigma^+ = \Sigma \setminus \lbrace \lambda \rbrace \]
\end{defn}

\begin{defn}[Lenguajes Regulares]
Son lenguajes que pueden ser admitidos por autómatas finitos.
\end{defn}

\subsection{Operadores de utilidad}
Tenemos dos operadores importantes:
\begin{enumerate}
\item \begin{defn}[Cierre estrella (star-closure)]
El operador estrella se corresponde con la suma infinita:
\[a^* = \lambda + a + a^2 + a^3 + ...\]

\end{defn}
\item \begin{defn}[Cierre positivo (positive-closure)]
Este operador se corresponde con la suma infinita:
\[a^+ = a + a^2 + a^3 + ...\]
\end{defn}
\end{enumerate}
En otras palabras, estos conceptos nos sirven para entender por qué $\Sigma ^*$  representa todas las palabras posibles. Para poder entender esto debemos entender la multiplicación como una concatenación y la suma como un \textit{OR}. Así el ``elemento neutro del producto'' sería la cadena vacía.

Tomemos ahora el alfabeto binario $\Sigma = \lbrace 0, 1 \rbrace$
Entonces:
\[\Sigma ^* = (0+1)^* = \lambda + (0+1)+(0+1)^2+(0+1)^3+... = \]
\[= \lambda + 0 + 1 + 00 + 01 +10 +11 +000+001+010+011+100+101+110+111+...\]
Y con esto vemos cómo se forman todas las posibles combinaciones de bits de diferente longitud. Si en lugar de este alfabeto hubiésemos tomado nuestro alfabeto castellano, habríamos obtenido todas las posibles combinaciones de letras.

Si cada conjunto de símbolos representa una cadena, es lógico pensar que no tiene sentido la operación suma como la hemos conocido siempre. La forma de entenderlo es que si yo tengo la palabra \textit{abc} y también tengo la palabra \textit{cda}, en total tengo \textit{abc}+\textit{cda}, es decir, tengo ambas palabras (unión).

\section{Expresiones regulares}

\begin{defn}[Expresión regular]
Forma alternativa de representar un lenguaje regular.
\end{defn}

Dado un alfabeto $\Sigma$ existen tres tipos de expresiones regulares primitivas:
\begin{enumerate}
\item $\emptyset$

L($\emptyset$) = $\emptyset$
\item $\lambda$

 L($\lambda$)=$\lbrace \lambda \rbrace$
\item $a\in \Sigma$

L($a$) = $\lbrace a \rbrace$
\end{enumerate}

A partir de estas expresiones regulares primitivas podemos construir expresiones regulares compuestas aplicando la siguiente regla:

Siendo $\alpha, \beta$ dos expresiones regulares primitivas o compuestas sobre $\Sigma$ también lo son:
\begin{enumerate}
\item $\alpha + \beta$ (Unión de lenguajes)

L($\alpha + \beta$) = L($\alpha $) $\cup$ L($\beta$)
\item $\alpha . \beta$ (Concatenación de lenguajes)

L($\alpha . \beta$) = L($\alpha $). L($\beta$)
\item $\alpha^*$ (Cierre)

L($\alpha^*$) = L($\alpha$)$^*$

L($\beta^*$) = L($\beta$)$^*$

El cierre es la repetición de cero o más veces de las expresiones regulares a las que aplica.
\end{enumerate}

Orden de precedencia de los operadores (de más a menos):
\begin{enumerate}
\item *
\item .
\item +
\end{enumerate}
Cuando la precedencia no esté clara o se quiera alterar, se pueden (y deben) usar paréntesis.

\begin{example}
Encontrar los lenguajes definidos por las siguientes expresiones regulares:
\begin{enumerate}
\item$a.(b+a).b$\\
 Cadenas de tres símbolos que empiezan por 'a' y acaban por 'b' y el símbolo central es una 'a' o una 'b': \{abb,aab\}
\item $(a+b)$\\
Cadenas de un solo símbolo, que es o 'a' o 'b': \{a,b\}
\item $(a+b)*$\\
Todas las cadenas posibles formadas por los símbolos a y b (incluso la cadena vacía)
\item $(a+b).(a+b)*$ \\
Todas las cadenas posibles formadas por los símbolos a y b. Pero no incluye la cadena vacía ya que por $(a+b)$ necesariamente deben contener una 'a' o una 'b'.
\item $(aa+bb)*$ \\
Todas las cadenas posibles formadas por 'a' y 'b' con la condición de que siempre aparezcan los símbolos consecutivos un número par de veces. Es decir, cadenas del tipo 'aaaabbaabbbbbb', (no valdría 'aaabb') (incluyendo la cadena vacía).

\end{enumerate}

\end{example}


\chapter{Gramática}
\begin{defn}[Gramática]
Hay varias definiciones para este término. No son muy precisas pero nos dan una idea de su significado:
\begin{enumerate}
\item Mecanismo para formalizar matemáticamente un lenguaje.
\item Conjunto de reglas que determinan cómo formar las cadenas de un lenguaje.
\end{enumerate}
\end{defn}

\begin{example}
Tomemos las reglas:
\begin{enumerate}
\item ORACIÓN $\rightarrow$ SUJETO PREDICADO
\item SUJETO $\rightarrow$ ARTÍCULO NOMBRE
\item PREDICADO $\rightarrow$ VERBO
\item ARTÍCULO $\rightarrow$ el | un
\item NOMBRE $\rightarrow$ coche | perro
\item VERBO $\rightarrow$ come | corre
\end{enumerate}
Estas reglas constituyen una gramática que nos permite generar un lenguaje. En este caso el lenguaje estaría por formado todas las cadenas que se pueden construir a partir de estas reglas. Empezando siempre por el axioma (en este caso: ``ORACIÓN'').
%TODO Ejemplo de árbol de derivación con estas reglas
\end{example}

Una gramática está compuesta por una serie de elementos que definiremos a continuación.

\begin{defn}[Símbolos terminales (T)]
Conjunto de símbolos que pueden aparecer en la cadena final (o sentencia). En el ejemplo anterior serían elementos terminales aquellos escritos en minúscula. Para ellos no existe ninguna regla que indique cómo se derivan.
\end{defn}

\newpage
\begin{defn}[Símbolos no terminales (N)]
Conjunto de símbolos que no pueden aparecer en la cadena final. Simplemente son usados para definir las reglas de derivación.
\end{defn}

\noindent Los conjuntos T y N deben ser disjuntos, es decir $T \cap N = \emptyset$. Utilizaremos el símbolo $\Sigma$ para referirnos a la unión de ambos, $\Sigma = T \cup N$.

\begin{defn}[Reglas de producción (P)]
Explican cómo se transforma un símbolo no terminal en un conjunto de símbolos terminales y/o no terminales.
\end{defn}

\begin{defn}[Símbolo inicial / Axioma (S)]
Indica dónde empieza a construirse la cadena. En el ejemplo anterior, el axioma sería el símbolo ORACIÓN. Una gramática sólo puede tener un único axioma.
\end{defn}

\begin{defn}[Gramática (G)]
Cuádrupla formada por T, N, P y S.

\[ G = ( T, N, P, S) \]
\end{defn}

Una gramática permite generar cadenas para un lenguaje. Por ejemplo, para la gramática anterior:

ORACIÓN $\rightarrow$ (derivamos ORACIÓN:)

SUJETO PREDICADO $\rightarrow$ (derivamos SUJETO:)

ARTÍCULO NOMBRE PREDICADO $\rightarrow$ (derivamos ARTÍCULO:)

el NOMBRE PREDICADO $\rightarrow$ (derivamos NOMBRE;)

el coche PREDICADO $\rightarrow$ (derivamos PREDICADO:)

el coche VERBO $\rightarrow$ (derivamos VERBO:)

el coche corre (acabamos, pues no hay más que derivar)

Este proceso se llama derivación. Cada una de las cadenas en una derivación se denomina {\em forma sentencial}. La última de ellas es una cadena válida del lenguaje generado por la gramática, y se denomina {\em sentencia}. Está formada únicamente por símbolos terminales de la gramática. El lenguaje generado por una gramática, $L(G)$, es el conjunto de todas las sentencias posibles, es decir, el conjunto de todas las cadenas de símbolos no terminales que pueden derivarse a partir del axioma.

Vamos a ver algunos ejemplos de gramáticas y los lenguajes que generan:
\begin{example}
Tomemos la gramática generada por las reglas:
\begin{enumerate}
\item S $\rightarrow$ aSb
\item S $\rightarrow$ $\lambda$
\end{enumerate}

\begin{gather*}
T = \lbrace a, b \rbrace \\
N = \lbrace S \rbrace
\end{gather*}

Y su axioma es S.

El lenguaje generado por esta gramática serían todas las palabras de la forma: $a^i b^i$ con $ i=0,1,... \infty$
\end{example}

\begin{example}
Ahora vamos a tratar de construir la gramática que define un lenguaje dado:
L=$\lbrace (ab)^na, n \geq 0 \rbrace$

La gramática que define este lenguaje es:
\begin{enumerate}
\item S $\rightarrow$ abS
\item S $\rightarrow$ a
\end{enumerate}

El autómata finito asociado a este lenguaje sería:
\begin{center}
\includegraphics[scale=0.75]{automata1.png}
\end{center}
\end{example}


\begin{defn}[Gramáticas independientes del contexto]
Son aquellas cuyas reglas tienen un único símbolo no terminal en el lado izquierdo.
\end{defn}


\begin{example}[Gramática dependiente de contexto]
\begin{itemize}
\item aSb $\rightarrow$ abb
\item cSd $\rightarrow$ cdd
\end{itemize}
S puede derivarse dependiendo de lo que la rodee, es decir, de su contexto.
\end{example}

\begin{example}[Gramática independiente de contexto (regular)]
\begin{itemize}
\item A $\rightarrow$ aA
\item A $\rightarrow$ a
\end{itemize}
A la derecha tenemos únicamente símbolos terminales o bien símbolos terminales acompañados de un único símbolo no terminal.
Si el elemento no terminal está a la izquierda se denomina gramática lineal por la izquierda. En caso contrario, gramática lineal por la derecha.
\end{example}

\newpage
{\bf Nota a lo anterior:} Una gramática es {\em lineal por la derecha} (right-linear) si todas sus reglas de producción son de una de las dos formas siguientes:

\begin{itemize}
\item B $\rightarrow$ aA
\item B $\rightarrow$ a
\end{itemize}

\noindent con $A, B \in N$ y $a \in T^{*}$. Una gramática es {\em lineal por la izquierda} (left-linear) si todas sus reglas de producción son de una de las dos formas siguientes:

\begin{itemize}
\item B $\rightarrow$ Aa
\item B $\rightarrow$ a
\end{itemize}

\noindent con $A, B \in N$ y $a \in T^{*}$. Una gramática es {\em regular} si es lineal por la izquierda o lineal por la derecha. Todas las gramáticas regulares son independientes del contexto.

\begin{defn}[Equivalencia de gramáticas]
Dos gramáticas son equivalentes si generan el mismo lenguaje
\end{defn}

\section{Gramáticas independientes del contexto}
Como ya vimos una gramática puede representarse como una cuádrupla G=(N,T,S,P), es decir, consta de símbolos no terminales, símbolos terminales, un axioma y unas reglas de producción.

En el caso de una gramática independiente del contexto las reglas de P son de la forma:
\begin{itemize}
\item $A \rightarrow x$

Con $A\in N$, $x \in \Sigma^*$ con $\Sigma=T\cup N$
\end{itemize}

\begin{example}
El lenguaje:
\[L = \lbrace ww^R : w \in (a+b)^*\rbrace\]
es independiente del contexto puesto que puede representarse por medio de una gramática G independiente del contexto, que sería:
\begin{itemize}
\item $S \rightarrow aSa$
\item $S \rightarrow bSb$
\item $S \rightarrow \lambda$
\end{itemize}

\newpage
Para demostrar que el lenguaje generado por la gramática, L(G), es el mismo que L, habría que hacer dos cosas:

\begin{enumerate}
\item Probar que cualquier palabra de $L(G)$ está en $L$.
\item Probar que cualquier palabra de $L$ está en $L(G)$.
\end{enumerate}

El punto 1 es fácil de ver, pues está claro que cualquier cadena generada por $G$ es simétrica (siempre que añadimos una $a$ al principio añadimos otra al final, y lo mismo para $b$).

Para demostrar el punto 2 vamos a probar que si $w \in L$ entonces $w \in L(G)$ usando inducción:

\begin{itemize}
\item Todas las cadenas de $L$ tienen un número par de símbolos: $|w| = 2n$  $\forall w \in L$, $n=0,1,2,...$

\item Para $n=0$ tenemos $w = \lambda \in L$ y se cumple que $w \in L(G)$. Tomamos este caso como base de la inducción.

\item Supongamos que se cumple la hipótesis para $n$: si $w \in L$ con $|w| = 2n$ entonces $w \in L(G)$.

\item Demostremos que se cumple la hipótesis para $n+1$:

Sea $w \in L$ con $|w| = 2n+2$. Entonces $w$ debe ser de la forma $ava$ o $bvb$ con $v \in L$ y $|v| = 2n$. Por hipótesis tenemos que $v \in L(G)$, es decir se puede generar a partir del axioma $S$. Finalmente si $w = ava$ podemos generarla a partir del axioma con la regla $S \rightarrow aSa$, y si $w = bvb$ podemos generarla a partir del axioma con la regla $S \rightarrow bSb$. Por tanto también $w \in L(G)$.

\end{itemize}

\end{example}

\begin{defn}[Derivación directa]
Dada una gramática independiente del contexto G=(N,T,S,P) y sean $v$, $w$ dos formas sentenciales, decimos que w es derivación directa de v:
\[v \rightarrow w\ \equiv v=xZy \wedge w=x\alpha y \wedge \exists \ regla \ en \ P \tq Z \rightarrow \alpha\]
\end{defn}

\noindent con $Z \in N$ y $\alpha \in \Sigma^{*}$.

\begin{defn}[Derivación]
Dada una gramática G=(N,T,S,P) y sean $v$, $w$ dos formas sentenciales, decimos que w es derivación de v, y lo escribimos $v \rightarrow^{+} w$, si existe una cadena de formas sentenciales $a_{0}$, $a_{1}$, $a_{2}$,... $a_{n}$, tales que:
\[v = a_0 \rightarrow a_1 \rightarrow a_2 \rightarrow ... \rightarrow a_n = w\]
\end{defn}

\newpage
\begin{defn}[Lenguaje generado por G]
Dada una gramática G=(N,T,S,P) definimos el lenguaje generado por ella como:
\[L(G) = \lbrace w \in T^*: S \rightarrow^{+} w \rbrace\]
\end{defn}


Veamos algunos ejemplos:

\begin{example}
Dado el lenguaje:
\[L = \lbrace a^n b^n : n\geq 0 \rbrace\]

La gramática que genera este lenguaje es:
\begin{itemize}
\item $S \rightarrow \lambda$
\item $S \rightarrow aSb$
\end{itemize}
\end{example}

\begin{example}
Dado el lenguaje:
\[L = \lbrace w \in (a+b)^* \tq n_a(w)=n_b(w)\rbrace\]

La gramática que genera este lenguaje es:
\begin{itemize}
\item $S \rightarrow aSb | bSa | \lambda$
\item $S \rightarrow SS$
\end{itemize}

Para construirla nos hemos fijado en que una palabra w puede ser de 4 formas:
\[w = \left\{ \begin{array}{lcc}
             aw_0b &   con  & w_0 \in L \\
             \\ bw_0a &  con & w_0 \in L \\
             \\ aw_0a  \Rightarrow  w = w_1w_2  & con  & w_1,w_2 \in L\\
             \\ bw_0b  \Rightarrow  w = w_1w_2  &  con &  w_1,w_2 \in L
             \end{array}
   \right.\]
\end{example}

Una sentencia puede ser derivada de diferentes formas a partir de una misma gramática. Para estos casos vamos a definir derivaciones ``leftmost'' y ``rightmost'':

\begin{defn}[Leftmost]
Consiste en derivar, en cada paso, el elemento no terminal colocado más a la izquierda. Se deja para el lector el arduo trabajo de deducir que significa una derivación ``rightmost''.
\end{defn}

Esto nos lleva a definir un nuevo concepto:

\begin{defn}[Ambigüedad]
Una gramática se define como ambigua si existen dos o más \textbf{árboles de derivación distintos} para la misma sentencia.

Otra forma de definirlo sería considerar ambiguas aquellas gramáticas para las que existen dos \textbf{derivaciones leftmost (o rightmost)} distintas para la misma sentencia.
\end{defn}

\begin{example}
Consideremos la gramática dada por las reglas:
\begin{itemize}
\item $E \rightarrow E + E | E \times E | I$
\item $I \rightarrow a | b | c$
\end{itemize}

Se trata de una gramática ambigua ya que la sentencia $a+b\times c$ tiene dos derivaciones distintas leftmost.
\begin{enumerate}
\item $E \rightarrow E + E \rightarrow I + E \rightarrow a + E \rightarrow a + E \times E \rightarrow a + I \times E \rightarrow a + b \times E \rightarrow a + b \times I \rightarrow a + b \times c$
\item $E \rightarrow E \times E \rightarrow E + E \times E \rightarrow I + E \times E \rightarrow a + E \times E \rightarrow a + I \times E \rightarrow a+b \times E \rightarrow a + b \times I \rightarrow a + b \times c$
\end{enumerate}
\end{example}

Ya vimos que dos gramáticas son equivalentes si generan el mismo lenguaje pero vamos a recalcar que hay \textbf{infinitas} gramáticas que generan el mismo lenguaje.

Dada una gramática G=(T,N,S P) para obtener otra equivalente basta con hacer otra:
\[G' =(T, N \cup \lbrace Z \rbrace, Z, P \cup \lbrace Z \rightarrow E \rbrace)\]

\newpage

\chapter{Autómatas finitos: deterministas y no deterministas}
\begin{defn}[Autómata finito determinista]
Se representa como:
\[ A=(Q, \Sigma, \delta, q_0, F)\]
 donde:
\begin{itemize}
\item Q = conjunto de estados
\item $\Sigma$ =  alfabeto de entrada
\item $\delta$ = función de transición: $\delta : Q\times \Sigma \rightarrow Q$
\item $q_0$ = estado inicial $\in$ Q
\item F = conjunto de estados finales, $F \subset Q$
\end{itemize}
\end{defn}

% La función de transición no es inyectiva, dos elementos del dominio pueden tener la misma imagen (desde dos estados distintos se puede pasar al mismo estado).
%\begin{defn}[Autómata Finito Determinista. AFD\IS]
%Implica que la función de transición es inyectiva. Dado un estado y una entrada sólo hay un estado al que podamos pasar.
%\[\delta: Q \times \Sigma \rightarrow Q\]
%\end{defn}

\begin{defn}[Función de transición extendida\IS]
Consiste en una extensión de la función de transición a cadenas. Se representa como $\delta ^*$:
\[\delta^*(q, w)=q_1\]
Siendo $w\in \Sigma ^*$, $q$ el estado en el que comenzamos y $q_1$ el estado al que llegamos tras procesar toda la palabra. Recordemos que $\Sigma$ es el alfabeto de entrada (conjunto de símbolos) y $\Sigma^*$ es el conjunto formado por todas las posibles cadenas de símbolos que puedes crear con dicho alfabeto.
\end{defn}

\begin{defn}[Lenguaje aceptado\IS por un AFD]
El lenguaje aceptado por un autómata finito determinista, A, es el conjunto de palabras que llevan al autómata a un estado final.
\[L(A) = \lbrace w \in \Sigma^* \ : \ \delta^*(q_0, w) \in F \rbrace\]
\end{defn}

Veamos algún ejemplo:
\begin{example}
Queremos un autómata que reconozca el lenguaje: L=$\lbrace$101,110$\rbrace$

El autómata resultado es:
\begin{center}
\includegraphics[scale=0.75]{automata2.png}
\end{center}
\obs: Un autómata determinista tiene que tener todas las transiciones definidas, se sobreentiende que las transiciones que no están dibujadas van a un sexto estado en el cual se quedan colgadas.

Este autómata es determinista y su transición de estados dada una entrada "101" sería:

 \begin{tabbing}
   \hspace*{2cm} \= \hspace*{2cm} \= \hspace*{2cm} \= \hspace*{2cm} \= \kill
  entrada:\> 1   \> 0   \> 1   \\
 q0 \> q1 \> q3 \> q4  \\
 \end{tabbing}

Por tanto "101" forma parte del lenguaje del autómata.

Pero podríamos representar el mismo lenguaje con un autómata no determinista:
\begin{center}
\includegraphics[scale=0.75]{automata3.png}
\end{center}
Y su transición de estados dada una entrada "101" sería:

 \begin{tabbing}
   \hspace*{2cm} \= \hspace*{2cm} \= \hspace*{2cm} \= \hspace*{2cm} \= \kill
  entrada: \> 1   \> 0   \> 1  \\
 q0 \> q1 \> q4 \> q5\\
  \> q2 \> - \> - \\

 \end{tabbing}

Como al menos uno de los caminos lleva a un estado final, la cadena "101" forma parte del lenguaje del autómata (los guiones indican transición no definida, o a un estado vacío).

La ventaja de un autómata no determinista es que podemos explorar varias ramas en paralelo.
\end{example}


\begin{defn}[Transición $\lambda$]
Transición que puede ocurrir sin consumir ningún valor de entrada. Un autómata finito que tenga transiciones de este tipo se considera no determinista.
\end{defn}

\begin{defn}[Autómata finito no determinista. AFN\IS]
Autómata con función de transición de la forma:
\[\delta: Q\times (\Sigma \cup \lbrace \lambda \rbrace) \rightarrow 2^Q\]
Es decir, dado un estado y una entrada (posiblemente vacía) salta a un conjunto de estados.
\end{defn}

\begin{defn}[Función de transición de un AFN extendida\IS]
Consiste en una extensión de la función de transición a cadenas. Se representa como $\delta ^*$:
\[\delta^*(q, w) = E\]
Siendo $w\in \Sigma ^*$, $q$ el estado en el que empezamos y $E$ el conjunto de estados a los que llegamos tras procesar toda la palabra.
\end{defn}

\begin{defn}[Lenguaje aceptado\IS por un AFN]
El lenguaje aceptado por un autómata finito no determinista, A, es el conjunto de palabras que llevan al autómata a un estado final.
\[L(A) = \lbrace w \in \Sigma^* \ : \ \delta^*(q_0, w)\cap F \neq \emptyset \rbrace\]
\end{defn}

\begin{example}
Diseñar un autómata para el siguiente lenguaje

Tenemos el alfabeto: $\Sigma = \lbrace 0,1,2,3,4,5,6,7,8,9,+,-,\cdot \rbrace$ con las siguientes restricciones:
\begin{enumerate}
\item Signo puede o no aparecer
\item Parte decimal puede aparecer o no
\item Parte entera puede o no aparecer
\item Debe haber al menos parte entera o decimal.
\end{enumerate}

El autómata queda:
\begin{center}
\includegraphics[scale=0.75]{automata4.png}
\end{center}
\end{example}

\newpage

\begin{defn}[Fuentes de indeterminismo de un AF]
Un autómata finito no determinista se caracteriza por tener alguna de las siguientes propiedades:
\begin{enumerate}
\item Con la misma entrada, varias transiciones posibles para un mismo estado.
\item Hay transiciones lambda
\item Se puede transitar a $\emptyset$ (transiciones no definidas)
\end{enumerate}
\end{defn}

\begin{example}
Otro ejemplo de autómata no determinista podría ser:
\begin{center}
\includegraphics[scale=0.75]{automata3a.png}
\end{center}
Cuya transición de estados dada una entrada "101" sería:

 \begin{tabbing}
   \hspace*{2cm} \= \hspace*{2cm} \= \hspace*{2cm} \= \hspace*{2cm} \= \hspace*{2cm} \kill
 Estados \> 1   \> 0   \> 1   \\
 q0 \> - \> - \> -  \\
 q1 \> q2 \> q4 \> q5\\
 \end{tabbing}

En este autómata, podemos avanzar al estado 'q1'  a través de la transición $\lambda$, aunque el primer símbolo de la entrada sea un '1', (segunda línea de la tabla). Sin embargo en el camino en el que nos mantenemos en el estado 'q0'  (primera línea de la tabla), no tenemos la transición definida con entrada '1' y vamos al estado vacío. Llegamos a un estado final, por tanto la palabra "101" pertenece al lenguaje de este autómata.

\end{example}

\begin{theorem}
Los autómatas finitos no deterministas y deterministas son equivalentes.
\end{theorem}

Esto significa que dado un autómata finito determinista que acepta un determinado lenguaje, existe otro no determinista que acepta el mismo lenguaje, y viceversa. Se utilizan los AFN por comodidad, porque las demostraciones formales son más sencillas.

\section{Equivalencia entre AF y ER}
Vamos a ver cuál es la relación existente entre los autómatas finitos y las expresiones regulares.

Como ya vimos una gramática puede expresarse como una cuádrupla G=(N,T,S,R), es decir, consta de símbolos no terminales, símbolos terminales, un símbolo inicial o axioma y unas reglas de producción.

Recordamos también que una gramática regular es aquella que es lineal por la derecha o lineal por la izquierda

Hay 4 formas de representar un lenguaje regular, y en ellas reside la equivalencia entre autómatas finitos y expresiones regulares. Las 4 formas de representar un lenguaje regular son:
\begin{enumerate}
\item Describiendo todos sus componentes
\item Con una gramática regular
\item Con una expresión regular
\item Mediante un AFN/AFD
\end{enumerate}

\newpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\chapter{Autómatas a pila}
Dado el lenguaje:
\[L=\lbrace ww^R \tq w \in (0+1)^*\rbrace\]
que representa palabras capicúas sobre $\lbrace 0, 1\rbrace$ con un número par de símbolos, vamos a intentar construir un autómata finito para él.

Esto no es posible ya que siempre necesito llegar hasta la mitad de la palabra ``almacenando'' lo que hemos leído para después comprobar que lo leemos al revés. Puesto que la palabra puede tener longitud arbitraria, necesitaremos una cantidad de memoria arbitraria y esto no es viable (sería necesario un autómata con un número infinito de estados).

Es aquí donde surgen los autómatas a pila. Estos autómatas se caracterizan por que en cada salto indicamos una entrada, saca el símbolo de la cima de la pila y añade una cadena a la pila.

Para el lenguaje dado, el autómata a pila que lo representa sería:

\begin{center}
\includegraphics[scale=0.75]{automata5.png}
\end{center}
En teoría, las etiquetas de los arcos del autómata son de la forma:
\[A_b^c\]
donde $A$ sería la entrada leída, $b$ el elemento que extraemos del tope de la pila y $c$ la cadena que insertamos en la pila. Por tanto una transición solo se puede dar si aparte de tener la entrada correspondiente, la cima de la pila coincide con lo que vamos a extraer de ella. Introducir $az$ implica que la 'a' se queda como cima de la pila, y la 'z' detrás.

En algunos ejercicios usaremos la notación $\{b,A,c\}$ con idéntico significado.

\newpage
\begin{defn}[Autómata a pila]
Un autómata a pila se representa como:
\[A=(Q, \Sigma, \delta, q_0, F, \topl, A_0)\]
siendo:
\begin{itemize}
\item $Q$: conjunto de estados
\item $\Sigma$: alfabeto de entrada
\item $\delta$: función de transición: $\delta:Q\times(\Sigma \cup \{\lambda\})\times \topl \rightarrow 2^{Q\times \topl^*}$
\item $q_0$: estado inicial $\in Q$
\item $F$: conjunto de estados finales
\item $\topl$: alfabeto de la pila
\item $A_0$: símbolo inicial de la pila $\in \tau$
\end{itemize}

\noindent {\bf Nota:} En realidad el codominio de $\delta$ es el conjunto de subconjuntos finitos de $Q\times \topl^*$.

Básicamente consiste en un autómata, como los vistos hasta ahora, acompañado de una pila en la que realizaremos inserciones y extracciones en cada transición.
\end{defn}

Vamos a ver un pequeño ejemplo que explica cómo entender la función de transición:
\begin{example}
La evaluación de la función de transición:
\[\delta(q,a,x)=\{(p,y)\}\]

\noindent con $q, p \in Q$, $a \in \Sigma$, $x \in \topl$ y $y \in \topl^{*}$.
Implica que, estando en el estado 'q', ante una entrada 'a', habiendo en la cima de la pila un 'x' pasamos al estado 'p', insertando en la pila 'y', y sacando la 'x'.
\end{example}

En estos autómatas los conceptos de determinismo o no determinismo se mantienen. Es decir, un autómata a pila será no determinista si dada una entrada y un elemento a extraer de la cima de la pila tiene varias acciones que puede llevar a cabo (varios pares ('estado','inserción en pila')). Profundizaremos más adelante en este concepto.

\obs Aunque un autómata finito determinista es equivalente a uno no determinista, con autómatas a pila no ocurre lo mismo.

\begin{defn}[Autómata a pila determinista]
Un autómata a pila será determinista si cumple las siguientes condiciones:
\begin{itemize}
\item No hay transiciones $\lambda$ o, si las hay, no hay ninguna otra transición con un símbolo diferente. Es decir, si desde un estado p y con a en la pila pudieras pasar a otro q por una transición lambda, no habría otra posible transición desde el estado p y con a en la pila.
\item Dada una situación (estado p, símbolo de entrada x y símbolo a en la cima de la pila) $\delta (p,x,a)$ contiene como mucho un elemento $(q,b)$.
\end{itemize}
\end{defn}

\begin{defn}[Descripción instantánea]
Se trata de una representación de la situación actual del autómata. (q,X,A) es una descripción instantánea donde:\\
\begin{enumerate}
\item q = Nodo/estado en el que nos encontramos
\item X = Entrada que falta por leer, $X \in \Sigma^{*}$
\item A = Contenido de la pila, $A \in \topl^{*}$
\end{enumerate}

Dada una descripción instantánea podemos continuar el procesamiento de la cadena sin perder información.
\end{defn}

\begin{defn}[Precedencia entre descripciones instantáneas]
Decimos que una descripción instantánea precede a otra:
\[(q,xX,aA) \vdash (p, X, bA)\]
si:
\[(p,b) \in \delta(q, x, a)\]
Siendo 'p','q' nodos del autómata, 'x' el siguiente símbolo de entrada que se va a leer, 'X' el resto de la cadena de entrada; 'a' el carácter que hay en la cima de la pila, 'A' el resto del contenido de la pila y,'b' una cadena de símbolos que insertamos en la pila. En este transición se lee 'x', se saca 'a' de la cima de la pila y se introduce 'b'.

Es decir, una descripción instantánea precede a otra si hay una transición que nos lleva de una a otra.

\end{defn}

\begin{defn}[Precedencia *]
Decimos que hay precedencia * entre dos descripciones instantáneas:
\[(q,X,A) \vdash^* (p, Y, B)\]
cuando hay una secuencia $d_0,d_1,..., d_n$ de descripciones instantáneas tales que:
\[(q,X,A) = d_0 \vdash d_1 \vdash ... \vdash d_n = (p, Y, B)\]
\end{defn}

\noindent con $p, q \in Q$, $X, Y \in \Sigma^{*}$ y $A, B \in \topl^{*}$.

Basándonos en estas definiciones, podemos representar el lenguaje aceptado por un autómata a pila como:
\[L(A) = \lbrace w \in \Sigma^* \tq (q_0, w, A_0) \vdash^* (p, \lambda, X)\rbrace\]
con $p\in F, X \in \topl^*$

\begin{example}
Queremos encontrar un autómata que represente el lenguaje:
\[L = \lbrace a^nb^n, n \geq 0\rbrace\]

El autómata de pila que representa este lenguaje es:

\begin{center}
\includegraphics[scale=0.75]{automata6.png}
\end{center}
\end{example}

\newpage
\chapter{Gramáticas de atributos}
Un compilador se encarga de generar el código máquina


En primer lugar debemos saber que un compilador realiza tres tareas:
\begin{enumerate}
\item \textbf{Análisis morfológico}. A estas alturas consideramos que ya nos viene hecho y no nos preocupamos por ello.
\item \textbf{Análisis sintáctico}. Consiste en construir el árbol de derivación. Es decir, se comprueba que la sentencia obtenida sale de derivar el axioma.
\item \textbf{Análisis semántico}. Consiste en recorrer el árbol de derivación calculando atributos y realizando comprobaciones semánticas.
\end{enumerate}

Vamos a centrarnos ahora en análisis semántico.
\section{Análisis semántico}

Vamos a explicar brevemente la idea que hay detrás de esto:

Partiendo de una gramática independiente del contexto, y derivando el axioma hasta obtener una sentencia, queremos obtener alguna función específica de nuestra gramática.

Es entonces cuando surgen las gramáticas de atributos, que añaden lo siguiente a la gramática independiente del contexto:

-Incorporan atributos a los nodos no terminales.

-Incorporan funciones sobre esos atributos en las reglas de derivación. (Además veremos que estas funciones se podrán realizar en instantes concretos).

-Incorporan una \textbf{información global} a la gramática. La información global está formada por un conjunto de variables que son accesibles desde cualquier nodo del árbol.

Por tanto, una vez obtenido el árbol de derivación, el análisis semántico recorrerá dicho árbol utilizando el recorrido en  \textbf{profundidad por la izquierda y con vuelta atrás} (lo explicamos más adelante) corroborando el buen funcionamiento de estas nuevas funciones.

El paso de información de un nodo a otro del árbol se podrá realizar mediante tres métodos:

-Síntesis: la información se propaga del nodo inferior al nodo superior.

-Herencia: la información se propaga del nodo superior al nodo inferior.

-Usando la información global.

Vamos a ver un ejemplo con detalle, explicando sobre él algunos de los conceptos:

Como hemos dicho, la idea es que dada una gramática independiente del contexto, podemos conseguir que esta realice funciones específicas. Lo conseguimos añadiendo atributos y especificaciones a las reglas de derivación.

\begin{example}
Consideremos la gramática:
\[G=\{\{(,)\}, \{L,I\}, L, P\}\]
Con las reglas de derivación P:
\begin{itemize}
\item $L \rightarrow (I)$
\item $I \rightarrow (I)$
\item $I \rightarrow II$
\item $I \rightarrow λ$
\end{itemize}

En primer lugar vamos a derivar el axioma y obtener una sentencia, nos queda (por ejemplo) el siguiente árbol de derivación:

\begin{forest}
for tree={
  draw,
  minimum height=1cm,
  anchor=north,
  align=center,
  child anchor=north
},
[{L}, align=center, name=SS
  [$($, tier=word]
  [{I}, name=PDC
    [{I}, name=MS
      [{$($}, tier=word]
      [{I}
        [{λ}, tier=word]]
      [{$)$}, tier=word]]
    [{I}
      [{$($}, tier=word]
      [{I}
        [{$($}, tier=word]
        [{I}
          [{λ}, tier=word]]
        [{$)$}, tier=word]]
      [{$)$}, tier=word]]
    ]
    [{$)$}, tier=word]
 ]
\end{forest}

Vemos que semánticamente es correcto ya que se obtiene de derivar el axioma $L$.

Queremos hacer que esta gramática nos devuelva la profundidad de una expresión, es decir, el máximo número de paréntesis contenidos unos en otros.

Para ello, vamos a utilizar gramática de atributos, en primer lugar debemos añadir a los elementos L e I un atributo profundidad.

Vamos a intentar hacerlo propagando el atributo profundidad hacia arriba (es decir, usando síntesis). Por ello, un nodo terminal como sería λ debería tener profundidad 0. Por lo que la cuarta regla queda de la forma:
\[I \rightarrow λ \{I.prof=0\}\]

En las dos primeras reglas, debemos realizar una propagación hacia arriba, aumentando en uno la profundidad. Así estas dos primeras reglas quedan de la forma:
\[L \rightarrow (I) \{L.prof = I.prof+1\}\]
\[I_1 \rightarrow (I_2) \{I_1.prof=I_2.prof+1\}\]

Para la tercera regla, podemos ver que la profundidad que asciende será el máximo de las profundidades de cada uno de los símbolos en los que deriva. La tercera regla quedaría:
\[I_1 \rightarrow I_2I_3 \{I_1.prof = max(I_2.prof, I_3.prof)\}\]

Si ahora analizamos los instantes en que se realizaría la función de cada regla y agrupamos todo lo indicado anteriormente llegamos a:
\[G=\{\{(,)\}, \{L(prof),I(prof)\}, L, P, \emptyset \}\]
Con P:
\begin{itemize}
\item $L \rightarrow (I) \{2: \ L.prof = I.prof+1\}$
\item $I_1 \rightarrow (I_2) \{2: \ I_1.prof=I_2.prof+1\}$
\item $I_1 \rightarrow I_2I_3 \{2: \ I_1.prof = max(I_2.prof, I_3.prof)\}$
\item $I \rightarrow λ \{0: \ I.prof=0\}$
\end{itemize}

Así, hemos conseguido una gramática de atributos que realiza una función específica (obtener la profundidad). Lo hemos logrado a partir de una gramática independiente del contexto añadiendo atributos y el instante en el que debo realizar cada asignación en las reglas de derivación.

Además, una gramática de atributos contiene una información global, en este caso no la vamos a usar por tanto la declaramos vacía (es el último símbolo que se ha añadido a G).

También debemos saber que el árbol se recorre en \textbf{profundidad por la izquierda y con vuelta atrás}. Esta técnica consiste en recorrer siempre el hijo más a la izquierda, cuando no queden hijos a la izquierda, continuamos con el siguiente hijo más a la izquierda. Cuando no queden hijos volvemos al nodo superior y así sucesivamente.

Si suponemos un caso simple en el que un nodo raíz tiene n hijos inmediatos, podemos ver que al aplicar este procedimiento recorremos el nodo raíz n+1 veces. Por tanto el nodo raíz tendrá n+1 instantes.

Vamos a dejar claro a qué nos referimos con el instante en el que realizo la función de cada regla. Para ello, debemos saber que al realizar el análisis semántico cada nodo sabe qué regla de derivación se ha usado sobre él, y por tanto conocerá el número de hijos inmediatos que tiene. Así, por ejemplo un nodo I al que le aplicamos la regla $I_1 \rightarrow I_2 I_3$, tiene 3 instantes:

-El instante 0, en el que se visita el nodo $I_1$.

-El instante 1, en el que se vuelve al nodo $I_1$ después de haber visitado el nodo $I_2$.

-El instante 2, en el que se vuelve al nodo $I_1$ después de haber visitado el nodo $I_3$.

Veamos ahora como queda el árbol realizando el recorrido en profundidad por la izquierda y con vuelta atrás y aplicando las distintas funciones:

\begin{forest}
for tree={
  draw,
  minimum height=1cm,
  anchor=north,
  align=center,
  child anchor=north
},
[{L\\0:nada\\}, align=center, name=SS
  [$($, tier=word]
  [{$I^1$\\p=2 en instante 2}, name=PDC
    [{$I^2$\\p=1 en instante 2}, name=MS
      [{$($}, tier=word]
      [{$I^4$\\p=0 en instante 0}
        [{λ}, tier=word]]
      [{$)$}, tier=word]]
    [{$I^3$\\p=2 en instante 2}
      [{$($}, tier=word]
      [{$I^5$\\p=1 en instante 2}
        [{$($}, tier=word]
        [{$I^6$\\p=0 en instante 0}
          [{λ}, tier=word]]
        [{$)$}, tier=word]]
      [{$)$}, tier=word]]
    ]
    [{$)$}, tier=word]
 ]
\end{forest}

Hemos puesto superíndices a las I para poder referirnos ahora a ellas:

El recorrido en profundidad de esté árbol quedaría:

Empezamos en L, estamos en el instante 0 de L... bajamos a $($, como es terminal no hacemos nada... subimos a L, instante 1 de L... bajamos a $I^1$, instante 0 de $I^1$...bajamos a $I^2$, instante 0 de $I^2$... bajamos a $($ como es terminal no hacemos nada... subimos a $I^2$, instante 1 de $I^2$...bajamos a $I^4$, instante 0 de $I^4$, que como es un nodo que deriva con la regla $I\rightarrow \lambda$ le aplicamos la función $I.prof=0$... subimos a $I^2$, instante 2 de $I^2$, que como es un nodo que deriva con la regla $I_1\rightarrow (I_2)$, le aplicamos la función $I_1.prof=I_2.prof+1$, por tanto tenemos ahora mismo $I^2.prof=1$... bajamos a $)$, como es un nodo terminal no hacemos nada... subimos a $I^2$, instante 3 de $I^2$... subimos a $I^1$, instante 1 de $I^1$... bajamos a $I^3$, instante 0 de $I^3$... etc etc etc.

\end{example}



\begin{example}
Vamos a apoyarnos en la gramática definida en el ejemplo anterior y a modificar simplemente las acciones, con el fin de obtener un programa que cuente el número de listas (parejas de paréntesis).

Vamos a realizarlo de tres formas distintas: con síntesis, herencia y con información global. Estas diferentes formas dan lugar a diferentes acciones, pero el árbol de derivación sigue siendo el mismo.

Cada una de las reglas de derivación serían:

\begin{itemize}
\item \textbf{Síntesis}
\[G=\{\{(,)\}, \{L(n),I(n)\}, L, P\}\]
\begin{itemize}
\item $L \rightarrow (I) \{2: \ L.n = I.n+1\}$
\item $I_1 \rightarrow (I_2) \{2: \ I_1.n=I_2.n+1\}$
\item $I_1 \rightarrow I_2I_3 \{2: \ I_1.n = I_2.n + I_3.n)\}$
\item $I \rightarrow λ \{0: \ I.n=0\}$
\end{itemize}

\item \textbf{Herencia}
\[G=\{\{(,)\}, \{L(n),I(antes, despues)\}, L, P\}\]
\begin{itemize}
\item $L \rightarrow (I) \{0: \ I.antes=1, \ 2: \ L.n=I.despues\}$
\item $I_1 \rightarrow (I_2) \{0: I_2.antes = I_1.antes, \ 2: \ I_2.despues=I_1.despues\}$
\item $I_1 \rightarrow I_2I_3 \{0: \ I_2.antes = I_1.antes, \ 1: \ I_3.antes=I_2.despues, \ 2: \ I_1.despues=I_3.despues\}$
\item $I \rightarrow λ \{0: \ I.despues=I.antes\}$
\end{itemize}
Hemos usado alguna acción que se sale de un esquema puro de herencia por ser imposible realizarlo de otra forma.

\item \textbf{Información global}
\[G=\{\{(,)\}, \{L,I\}, L, P\}\]
\begin{itemize}
\item $L \rightarrow (I) \{0: \ n=0; \ 1: \ n = n+1\}$
\item $I_1 \rightarrow (I_2) \{0: \ n=n+1\}$
\item $I_1 \rightarrow I_2I_3$
\item $I \rightarrow λ $
\end{itemize}
Este caso es bastante más sencillo, pues en cada derivación tienes acceso al contador global y basta con incrementarlo, sin preocuparte por los sucesores.

Además, usando información global es realmente importante indicar los instantes en que se realiza cada acción puesto que de lo contrario habría lugar a confusión. (En los casos anteriores los atributos imponían un orden sin ambigüedad).

\end{itemize}
\end{example}

En las transparencias podéis ver estos tres ejemplos para diferenciar la herencia, la síntesis y la información global (llamada tabla de símbolos):

\includepdf[frame=true, noautoscale=true, delta=10 10, nup=2x4,pages={2-9}, scale=0.72]{pdf/GramaticaAtributos.pdf}

\includepdf[frame=true, noautoscale=true, delta=10 10, nup=2x4,pages={10-17}, scale=0.72]{pdf/GramaticaAtributos.pdf}

\includepdf[frame=true, noautoscale=true, delta=10 10, nup=2x4,pages={18-25}, scale=0.72]{pdf/GramaticaAtributos.pdf}

\includepdf[frame=true, noautoscale=true, delta=10 10, nup=2x4,pages={26-27}, scale=0.72]{pdf/GramaticaAtributos.pdf}

\begin{example}
Para ver claramente las diferencia entre análisis semántico y sintáctico nos podemos fijar en lo siguiente, según el ejemplo 1:

int $\_x$,$\_y$

$\_x=S$

Esta bien semántica y sintácticamente.

int $\_x, \_y$

$\_z=7$

Esta bien sintácticamente pero no semánticamente.
\end{example}

\begin{example}
Vamos a tomar un caso sencillo con dos reglas sacadas del último ejemplo mostrado en las transparencias. La semántica es:
\begin{itemize}
\item \[L \rightarrow i \{st(i.nombre,L.tipo\}\]
\item \[A \rightarrow i = E \{in TS (i.nombre)==true; gt(i.nombre)==E.tipo; sv(i.nombre, E.valor)\}\]
\end{itemize}

Para la primera regla, el orden en que se recorrería el árbol de derivación es:
\begin{verbatim}
- En el instnate 0 bajamos del nodo L al nodo i
- En el instante i volvemos al nodo L
\end{verbatim}

Como es lógico, la operación de setType representada en la gramática no puede realizarse hasta conocer el tipo de L y el nombre de i, por lo que se realiza en el instante 1.

Lo representaríamos como:
\[L \rightarrow i \{1:st(i.nombre,L.tipo\}\]

Vamos ahora con la segunda regla. El orden en el que se recorrería el árbol de derivación seria:
\begin{verbatim}
- En el instante 0 bajamos del nodo A al nodo i, en el instante 1 volvemos al A.
- En ese instante bajaríamos al nodo =, del que volveríamos en el instante 2.
- Por último bajaríamos al nodo E y volveríamos del mismo en el instante 3
\end{verbatim}

Siguiendo esta secuencia y analizando en qué momento tenemos disponibles los valores para cada operación la segunda regla de la gramática sería de la forma:
\[A \rightarrow i = E \{ 1: \ in TS (i.nombre)==true; 3: \ gt(i.nombre)==E.tipo; 3: \ sv(i.nombre, E.valor)\}\]

En este tercer ejemplo es importante añadir algunas comprobaciones más en las reglas, que no vienen en las trasparencias:

-En la regla $A \rightarrow i=E$ comprobar $inTS(i.nombre)==true$ (es decir, ¿está en la tabla de símbolos i.nombre?)

-En la regla $E_1 \rightarrow E_2 + E_3$ y $E_1 \rightarrow E_2 * E_3$ comprobar que $E_2.tipo=E_3.tipo$

-En la regla $E \rightarrow i$ comprobar $inTS(i.nombre)==true$ y $gt(i.nombre)==E.tipo$ con gt=getType (es decir, ¿es el parametro tipo de i.nombre igual al de E?).
\end{example}



\chapter{Analizador morfológico}
Empezamos este capítulo recordando la función de un compilador.

Dado un archivo $fuente$ este archivo pasará a través de un analizador de código, que realiza tres funciones, utilizando para ello una tabla de símbolos que el mismo genera:

-Análisis morfológico.

-Análisis sintáctico.

-Análisis semántico.\\

Tras ser analizado, se genera un árbol de derivación como los vistos hasta ahora.

Nos centramos ahora en el analizador morfológico (también llamado léxico o scanner). Este realiza principalmente la siguiente función:

-Obtención de unidades sintácticas. Son los llamados tokens. Estos pueden ser identificadores, constantes, palabras reservadas ($if$, $them$, $for$,...) u otros símbolos (simples: $+$, $=$,... o dobles: $:=$,...).\\

Pero también es importante en la consecución de las siguientes tareas secundarias:

1)Eliminar delimitadores (espacios en blanco, tabuladores, saltos de línea...).

2)Eliminación de comentarios en el código.

3)Detecta errores morfológicos. (Símbolos inválidos, constantes e identificadores mal construidos...)

4)Inicializa algunas tareas semánticas, es decir, calcula el valor de algunos atributos de constantes o identificadores.\\


Lo vemos con un ejemplo: tenemos un lenguaje que admite el siguiente tipo de objetos:

-Identificadores: letra seguida de cero o más dígitos y letras.

-Constantes: (números enteros).

-Palabras reservadas: $begin$, $end$, $int$, $print$.

-Símbolos: simples: $;$; dobles: $:=$.\\


Con este lenguaje, y dado el siguiente fichero de entrada $fuente$.

\begin{verbatim}
begin
	int A;
	A := 100;
	print A;
end
\end{verbatim}

El analizador morfológico generará un fichero de salida semejante al siguiente:

\begin{verbatim}
<"begin",TOK_BEGIN>
<"int",TOK_INT>
<"A",TOK_ID, valor='A'>
.
.
.
<"100",TOK_CONST, valor=100>
.
.
.
\end{verbatim}

La 4ª tarea secundaria, que tenía el analizador morfológico, es la que realiza la función de crear un atributo valor e inicializarlo.

\chapter{Analizador sintáctico}

\section{Tablas de análisis (ascendente y descendente)}
Las tablas de análisis consisten en analizar una sentencia, con el fin de saber si se trata de una sentencia válida para una gramática dada o no.

Veamos un ejemplo:
\begin{example}
Dada la gramática:
\begin{itemize}
\item 1) E $\rightarrow$ E + E
\item 2) E $\rightarrow$ E x E
\item 3) E $\rightarrow$ -E
\item 4) E $\rightarrow$ (E)
\item 5) E $\rightarrow$ id
\end{itemize}

Vamos a realizar un \textbf{análisis ascendente} partiendo de la siguiente sentencia:
\begin{center}
(id+id)xid
\end{center}

Para ello vamos a contar con la ayuda de una pila y nos basaremos en dos reglas básicas:
\begin{enumerate}
\item \textbf{Reducción}
Se aplicará siempre que sea posible. No hay elección.

Se lleva a cabo cuando los elementos de la cima de la pila coinciden con la parte derecha de alguna regla de nuestra gramática.

En este caso se extraen los elementos de la pila y se sustituyen por la parte izquierda de la regla indicada anteriormente.

\item \textbf{Desplazamiento}
Se aplica cuando no puede realizarse ninguna reducción.

Consiste en introducir en la cima de la pila el siguiente elemento de la sentencia que estamos analizando
\end{enumerate}

Aplicando estas reglas a nuestra sentencia obtenemos el siguiente resultado:
\begin{center}
\begin{tabular}{| c | c | c | c |}
\hline
Instante & Entrada & Pila & Acción \\
\hline
0 & (id+id)xid & - & Desplazamiento \\
\hline
1 & id+id)xid & ( & Desplazamiento \\
\hline
2 & +id)xid & (id & Reducción(5) \\
\hline
3 & +id)xid & (E & Desplazamiento \\
\hline
4 & id)xid & (E+ & Desplazamiento \\
\hline
5 & )xid & (E+id & Reducción(5)\\
\hline
6 & )xid & (E+E & Reducción(1) \\
\hline
7 & )xid & (E & Desplazamiento \\
\hline
8 & xid & (E) & Reducción(4) \\
\hline
9 & xid & E & Desplazamiento \\
\hline
10 & id & Ex & Desplazamiento \\
\hline
11 & - & Exid & Reducción(5) \\
\hline
12 & - & ExE & Reducción(2) \\
\hline
13 & - & E & Fin \\
\hline
\end{tabular}
\end{center}
Puesto que en la pila sólo queda el axioma, concluimos que la sentencia es válida para este lenguaje.
\end{example}

Tras este ejemplo podemos definir formalmente el algoritmo de análisis ascendente.

\begin{defn}[Análisis ascendente]
Este algoritmo consiste de dos pasos:
\begin{verbatim}
Mientras queden elementos en la entrada:
  desplazamos
  mientras podemos reducir:
      reducimos    

Si en la pila sólo está el axioma:
    OK. 
En caso contrario:
    ERROR.
\end{verbatim}
\end{defn}

Una vez visto el análisis ascendente el siguiente paso es estudiar el análisis descendente. 

En este caso, empezamos en el axioma y aplicamos derivaciones intentando llegar a la sentencia que queremos analizar.

Vamos a trabajar con gramáticas sencillas en las que sólo haya una derivación posible en cada paso. En caso de no ser así deberíamos avanzar todo lo posible para después hacer backtracking, comprobando que no dejamos caminos sin explorar.

Veámoslo con un ejemplo:
\begin{example}
Dada la gramática:
\begin{itemize}
\item E $\rightarrow$ TB
\item B $\rightarrow$ +TB | λ
\item T $\rightarrow$ FX
\item X $\rightarrow$ *FX | λ
\item F $\rightarrow$ i | (E)
\end{itemize}
Vamos a usar un \textbf{análisis descendente} para comprobar si es válida la sentencia:
\begin{center}
i*i
\end{center}
Para hacerlo apoyándonos en una pila, como en el ejemplo anterior, los pasos a seguir son los siguientes:

\begin{tabular}{| c | c | c |}
\hline
Instante & Pila  & Acción\\
\hline
1 & E & Derivamos\\
\hline 
2 & B | T & Derivamos \\
\hline
3 & B | X | F & Derivamos\\
\hline 
4 & B | X | i & Estraemos pues i está en la sentencia dada\\
\hline
5 & B | X & Derivamos \\
\hline
6 & B | X | F | *  & Extraemos pues i está en la sentencia dada \\
\hline
7 & B | X | F & Derivamos \\
\hline
8 & B | X | i & Estraemos pues i está en la sentencia dada \\
\hline
9 & B | X & Derivación λ\\
\hline
10 & B & Derivación λ \\
\hline
\end{tabular}

Si al finalizar nos queda la pila vacía concluimos que la sentencia estudiada es correcta.
\end{example}

\subsection{Tablas LR(n)}

\begin{defn}[Tabla LR(n)]
Tabla empleada para el procesamiento de entrada desde la izquierda (\textbf{L}eft) mediante derivaciones \textbf{R}ight-most con \textbf{n} símbolos de entrada.

\end{defn}

Veamos un ejemplo de como funciona esto
\begin{example}
Consideremos la gramática G=(T, N, E, P):
\begin{itemize}
\item E $\rightarrow$ T | E+T
\item T $\rightarrow$ i | (E)
\end{itemize}

El primer paso para realizar el análisis LR(O) consiste en añadir un axioma E' y la regla:
\begin{center}
E' $\rightarrow$ E\$
\end{center}

De forma que la gramática extendida quedaría:
\begin{itemize}
\item E' $\rightarrow$ E \$
\item E $\rightarrow$ T | E+T
\item E $\rightarrow$ E+T
\item T $\rightarrow$ i | (E)
\item T $\rightarrow$ (E)
\end{itemize}

Ahora lo que hacemos es añadir un '.' delante del símbolo que estemos analizando en cada paso.

Veamos con el siguiente ejemplo

\end{example}

\appendix
\chapter{Ejercicios}
\input{tex/Ejercicios.tex}

\chapter{Ejercicios 1ª Hoja}
\input{tex/ejerciciosHoja1/ejsAutlen.tex}

\chapter{Ejercicios 2ª Hoja}
\input{tex/ejerciciosHoja2/HOJA2.tex}

\printindex
\end{document}
