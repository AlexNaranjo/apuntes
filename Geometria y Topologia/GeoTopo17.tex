

\documentclass[palatino, bibnumbers]{apuntes}

\title{Geometría y Topología}
\author{Jose Antonio García del Saz}
\date{16/17 C2}
\usepackage{mathtools,tikz,lmodern, xparse}
\usetikzlibrary{%arrows, chains, matrix, 
	positioning, graphs,
	%shadows,
	shapes, shapes.callouts,
	%shapes.geometric,
	%shapes.misc
}
\usepackage{amsmath} %For align environement
\usepackage{color}% to define the next colors
\definecolor{bananayellow}{rgb}{1.0, 0.88, 0.21}
\definecolor{citrine}{rgb}{0.89, 0.82, 0.04}
\definecolor{darktangerine}{rgb}{1.0, 0.66, 0.07}

\newcommand{\Varrow}[3]{\begin{tikzpicture}[remember picture,overlay, ->, L/.style = {draw, #1}]
	\draw%[]
	(#2) edge[L] (#3); 
	\end{tikzpicture}       } 
% Paquetes adicionales
\usepackage{enumitem}
\usepackage{kpfonts}
\usepackage{tikztools}
\usepackage{fancysprefs}
\usepackage{tikz-3dplot}
\usepackage{physics}
\usepackage{xfrac}
\usepackage{wrapfig}
\usepackage{fastbuild}
\usepackage{soul}

%\usepackage{lipsum}
\usepackage{tikz-cd}
\usetikzlibrary{calc,patterns,angles,quotes}
\usetikzlibrary{arrows}
\usetikzlibrary{patterns}
\usetikzlibrary{intersections}
\usetikzlibrary{calc}
\usetikzlibrary{fadings}

\tikzset{
	snake/.style={
		rounded corners,
		to path={
			-- ([xshift=1em]\tikztostart.east)
			-- ([xshift=1em]\tikztostart.south east)
			-- ([xshift=-1em]\tikztotarget.north west)
			-- ([xshift=-1em]\tikztotarget.west)
			-- (\tikztotarget)
		}
	},
	snake up/.style={
		rounded corners,
		to path={
			-- ([xshift=-1em]\tikztostart.west)
			-- ([xshift=-1em]\tikztostart.north west)
			-- ([xshift=1em]\tikztotarget.south east)
			-- ([xshift=1em]\tikztotarget.east)
			-- (\tikztotarget)
		}
	}
}

\setlist{itemsep=1pt, topsep=5pt}
\bibliographystyle{alpha}
% --------------------

%\precompileTikz

\newcommand{\Id}{\mop{Id}}
\newcommand{\cln}{\colon\!}

\setcounter{tocdepth}{3}

\begin{document}
\pagestyle{plain}
\newcommand\tab[1][1cm]{\hspace*{#1}}
% http://tex.stackexchange.com/a/14243
\relpenalty=9999
\binoppenalty=9999
%\newcommand\restr[2]{\ensuremath{\left.#1\right|_{#2}}}
\begin{abstract}
Estos son los apuntes del curso de Geometría y Topología, del profesor Fernando Chamizo.
\end{abstract}

\maketitle

\tableofcontents
\newpage
% Contenido.

\chapter{Álgebra Tensorial}

\section{Tensores en  $ℝ^{n}$}
\subsection{Definiciones y ejemplos}
Estudiar los tensores en $ℝ^n$ es en realidad estudiar el Álgebra Lineal pero en varias variables. En primer curso (Álgebra I) estudiamos las aplicaciones, las cuales eran de la forma:
\begin{align*}
	\appl{π}{ℝ^{n}&}{ℝ^{m}} \\
	\overline{x} &\longmapsto[\overline{y}=A\cdot \overline{x}]
\end{align*}

\begin{defn}[Aplicación\IS lineal] Sea $f$ una aplicación entre dos espacios vectoriales $V$, $W$ sobre el mismo cuerpo $K$. Decimos que $f$ es una \textbf{aplicación lineal} si se cumplen las siguientes propiedades ($λ\in K$):
	\begin{enumerate}
		\item $f(λ\overline{x})=λ\cdot f(\overline{x})$ .
		\item $f(\overline{x_1}+\overline{x_2})=f(\overline{x_1})+f(\overline{x_2})$
	\end{enumerate}
\end{defn}

\begin{defn}[Aplicación\IS bilineal] Sea 
	\begin{align*}
	\appl{f}{ℝ^{n}×ℝ^{n}&}{ℝ} \\
	\overline{x},\overline{y} &\longmapsto{f(\overline{x},\overline{y})}
	\end{align*}
una aplicación, decimos que es \textbf{bilineal} si es una aplicación lineal en cada una de las dos variables, es decir:
\begin{enumerate}
	\item $f(λ\overline{x},\overline{y})=λ\cdot f(\overline{x},\overline{y})$; $f(\overline{x_1}+\overline{x_2},\overline{y})=f(\overline{x_1},\overline{y})+f(\overline{x_2},\overline{y})$
	\item $f(\overline{x},λ\overline{y})=λ\cdot f(\overline{x},\overline{y})$; $f(\overline{x},\overline{y_1}+\overline{y_2})=f(\overline{x},\overline{y_1})+f(\overline{x},\overline{y_2})$
\end{enumerate}
\end{defn}
\textbf{Observación:} todas las aplicaciones bilineales entre dos espacios se pueden escribir de la siguiente manera:
$$f(\overline{x},\overline{y})=\overline{x}^{T}A\overline{y}$$ con A una matriz n×n.
\newpage
\begin{defn}[Aplicación\IS multilineal] Decimos que una aplicación es \textbf{multilineal} si es lineal en cada una de sus variables. 
\end{defn}

\begin{defn}[Tensor\IS n veces covariante] Es cualquier aplicación multilineal
	$\appl{T}{\varprod_{i=1}^n V}{ℝ}$, siendo V un espacio vectorial de dimensión finita sobre $ℝ$ (que como sabemos de otros cursos son isomorfos a $ℝ^{n}$).
\end{defn}

\begin{example} Sea
	\begin{align*}
		\appl{T}{ℝ^{3}×ℝ^{3}&}{ℝ} \\
		T\left(\begin{pmatrix}x_1\\x_2\\x_3\end{pmatrix},\begin{pmatrix}y_1\\y_2\\y_3\end{pmatrix}\right) &\longmapsto{x_1\cdot y_3}
	\end{align*}
es obvio que T es multilineal, luego T es un tensor 2 veces covariante en $ℝ^{3}$.
\end{example}
\begin{example} Sea
	\begin{align*}
		\appl{T}{ℝ^{3}×ℝ^{3}&}{ℝ} \\
		T\left(\begin{pmatrix}x_1\\x_2\\x_3\end{pmatrix},\begin{pmatrix}y_1\\y_2\\y_3\end{pmatrix}\right) &\longmapsto{x_1\cdot x_3}
	\end{align*}
	se ve rápidamente que \underline{no} es una aplicación lineal respecto de la variable $\overline{x}$.
\end{example}
\begin{example} Sea
	\begin{align*}
		\appl{T}{ℝ^{3}×ℝ^{3}×ℝ^{3}&}{ℝ} \\
		T\left(\begin{pmatrix}x_1\\x_2\\x_3\end{pmatrix},\begin{pmatrix}y_1\\y_2\\y_3\end{pmatrix},\begin{pmatrix}z_1\\z_2\\z_3\end{pmatrix}\right) &\longmapsto{\begin{vmatrix}
			x_1 & y_1 &  z_1 \\ 
			x_2 & y_2 & z_2 \\ 
			x_3 & y_3 & z_3 \\ 
		\end{vmatrix}}
	\end{align*}
	la propiedad de linealidad del producto por un escalar es obvia por las propiedades de los determinantes. La propiedad de linealidad que conserva la adición se demuestra fácilmante desarrollando el determinante por adjuntos en la primera columna. Luego T es un tensor 3 veces covariante.
\end{example}
\newpage
En Álgebra Lineal estudiamos no sólo las aplicaciones $\appl{T}{ℝ^{n}}{ℝ}$ , sino también las de la forma $\appl{T}{ℝ^{n}}{ℝ^{m}}$. Ahora bien, estás últimas pueden convertirse al primer tipo mediante un elemento del \textbf{espacio dual}.

\begin{defn}[Espacio\IS dual] Sea V un espacio vectorial entonces se define el espacio dual y se denota con $V^{*}$ de la siguiente manera: \[ V^{*} = \{ \appl{f}{V}{ℝ} \tq \text{f es lineal} \}\]
\end{defn}

Con la definición anterior, un truco para pasar de unas aplicaciones a otras es que consideramos un elemento del espacio dual ($\phi\inℝ^{m}$) y uno del espacio vectorial original ($\overline{x}\inℝ^{n}$) y entonces tenemos una aplicación $$\hat{f}(\phi,\overline{x})=\phi(f(\overline{x}))\inℝ$$

De la misma manera, en lugar de considerar "tensores vectoriales" (esta expresión no es correcta, pero se da para ilustrar), los cuales serían aplicaciones multilineales $\appl{T}{\varprod_{1}^s V}{\varprod_{1}^r V}$, es más conveniente pensar en que el tensor va a depender también de los elementos del espacio dual. Luego vamos a considerar tensores de la forma $$\appl{T}{\underbrace{V^{*}×\cdots×V^{*}}_{\text{r veces}}×\underbrace{V×\cdots×V}_{\text{s veces}}}{ℝ}$$ que sean multilineales.

\begin{defn}[Tensor\IS r veces contravariante y s veces covariante] Es cualquier aplicación multilineal de la forma:
	$$\appl{T}{\underbrace{V^{*}×\cdots×V^{*}}_{\text{r veces}}×\underbrace{V×\cdots×V}_{\text{s veces}}}{ℝ}$$ siendo V un espacio vectorial de dimensión finita sobre $ℝ$ (isomorfo a $ℝ^{n}$) y $V^{*}$ su espacio dual. Diremos análogamente que T es un tensor de tipo (r,s).
\end{defn}

Como ya sabemos, si V es un espacio vectorial y tenemos una base de V (llamémosla $\base = \{ \overline{v_1},...,\overline{v_n} \}$), entonces existe una base natural de $V^{*}$
llamada \textbf{base dual} (denotada por $\base^{*}=\{\tilde{\phi}^{1},\cdots ,\tilde{\phi}^{n}\}$) y que está determinada por la propiedad: $$\tilde{\phi}^{i}(\overline{v_j})=\delta_j^{i} = \begin{cases} 0 & i ≠ j \\ 1 & i = j \end{cases}$$
\newpage
\begin{example} Daremos un ejemplo casi absurdo para ilustrar. Dar la base dual es muy fácil cuando tenemos una base ortonormal, por ejemplo la base canónica de $ℝ^{2}$:
$$\base = \{\overline{e_1},\overline{e_2}\}=\left\{\begin{pmatrix}1\\0\end{pmatrix},\begin{pmatrix}0\\1\end{pmatrix}\right\}$$En este caso obtenemos la base dual solamente girando los elementos de la base canónica, obteniendo: $$\base^{*} =\{\tilde{\phi^{1}},\tilde{\phi^{2}}\}=\left\{\begin{pmatrix}1&0\end{pmatrix},\begin{pmatrix}0&1\end{pmatrix}\right\}$$
\end{example}
\begin{example} Imaginemos que estamos en $ℝ^{2}$ pero en esta ocasión tenemos una cualquiera de sus bases $\base=\{\overline{v_1},\overline{v_2}\}$. Ahora consideramos la matriz $A=\begin{pmatrix}v_{11}&v_{21}\\v_{12}&v_{22}\\ \end{pmatrix}$, y entonces tenemos que $\begin{cases}\overline{v_1}=A\cdot \overline{e_1} \\\overline{v_2}=A\cdot \overline{e_2}\end{cases}$. Construímos entonces la base dual $\base^{*}=\{ \tilde{\phi^{1}},\tilde{\phi^{2}}\}$ de esta manera con la matriz de cambio de base: $\begin{cases} \tilde{\phi^{1}}(\overline{x})=\begin{pmatrix}0&1\end{pmatrix}\cdot A^{-1}\cdot \overline{x} \\\tilde{\phi^{2}}(\overline{x})=\begin{pmatrix}1&0\end{pmatrix}\cdot A^{-1}\cdot \overline{x}\end{cases}$
\end{example}
\begin{example} Si f es un \textit{endomorfismo}, digamos $\appl{f}{ℝ^{n}}{ℝ^{n}}$, se corresponde de manera unívoca con un tensor de tipo (1,1):
	\begin{align*}
	\appl{T}{(ℝ^{n})^{*}×ℝ^{n}&}{ℝ} \\
	T(\tilde{\phi},\overline{x}) &\longmapsto{\tilde{\phi}(f(\overline{x}))}
	\end{align*}
\end{example}
\begin{example} Cualquier vector de un espacio vectorial ($\overline{v}\in V$) se puede hacer corresponder con un tensor (1,0)  que se alimenta de elementos del dual y devuelve reales definido como:
	\begin{align*}
	\appl{T_{\overline{v}}}{V^{*}&}{ℝ} \\
	T_{\overline{v}}(\tilde{\phi}) &\longmapsto{\tilde{\phi}(\overline{v})}
	\end{align*}
\end{example}

Vamos a introducir notación para las definiciones y resultados venideros, y hay que hacerla nuestra ya que es el infierno de casi todos los estudiantes al abrir un libro de cálculo tensorial. De aquí en adelante (aunque se venga haciendo desde el inicio) se usarán \textbf{subíndices} para numerar vectores ($\overline{v_1},\overline{v_2},...,\overline{v_n}$) y \textbf{superíndices} para numerar elementos del espacio dual, aunque también sean vectores ($\tilde{\phi^{1}},\tilde{\phi^{2}},...,\tilde{\phi^{n}}$) .
\newpage
\begin{defn}[Componentes\IS de un tensor] Sea un tensor $T$ de tipo (r,s) y sean $\base=\{\overline{v_1},...,\overline{v_n}\}$, $\base^{*}=\{\tilde{\phi^{1}},...,\tilde{\phi^{n}}\}$ las bases de $V$ y $V^{*}$ respectivamente, se definen las componentes de $T$ en la base $\base$ como la siguiente colección de números:
	$$T_{j_1\space j_2\space \cdots \space j_s}^{i_1\space i_2\space \cdots \space i_r}=T(\tilde{\phi^{i_1}},\tilde{\phi^{i_2}},...,\tilde{\phi^{i_r}},\overline{v}_{j_1},\overline{v}_{j_2},...,\overline{v}_{j_s})(**)$$
\end{defn}
\begin{example} Calcular las componentes del tensor:
	\begin{align*}
	\appl{D}{ℝ^{2}×ℝ^{2}&}{ℝ} \\
	D\left(\begin{pmatrix}x_1\\x_2\end{pmatrix},\begin{pmatrix}y_1\\y_2\end{pmatrix}\right) &\longmapsto{\begin{vmatrix}
		x_1 & y_1 \\ 
		x_2 & y_2 \\ 
		\end{vmatrix}}
	\end{align*}
	Lo primero que hacemos es coger una base (cogemos la canónica porque será lo más usual este curso). Como es un tensor (0,2), no habrá superíndices en las componentes, sólo subindices. Las componentes son: $$D_{j_1\space j_2}=\begin{cases}
		D_{1\space1}=D(\overline{e_1},\overline{e_1})=\begin{vmatrix}1&1 \\ 0&0 \\ \end{vmatrix}=0\\
		D_{1\space2}=D(\overline{e_1},\overline{e_2})=1\\
		D_{2\space1}=-1\\
		D_{2\space2}=0\\
	\end{cases}$$
\end{example}
\subsection{Convenio de Einstein}
Se usa para ahorrar en escritura cuando se habla del álgebra tensorial. En resumen consiste en que cuando se repite un índice arriba y abajo entonces hay que suponer que sumamos en él.
\begin{example} En primer curso, la combinación lineal se escribía como: $λ_1\cdot \overline{v_1}+\cdots +λ_n\cdot \overline{v_n}$. Si usamos el convenio esto se escribiría simplemente como $λ^{i}\cdot \overline{v}_i$
\end{example}
\begin{example} Sea una aplicación lineal $f(\overline{x})=A\cdot \overline{x}$, se corresponde unívocamente con un tensor (1,1):
	\begin{align*}
		\appl{T}{(ℝ^{n})^{*}×ℝ^{n}&}{ℝ} \\
		T(\tilde{\phi},\overline{v}) &\longmapsto{\tilde{\phi}(\overline{v})}
	\end{align*}
Las componentes (en la base canónica) son las $a_j^{i}$, donde $i$ son las filas y $j$ las columnas.
$$f(\overline{x})=a_j^{i}\cdot x^j;\overline{x}=x^j\cdot \overline{e_j}$$
Las componentes $T_j^{i}\equiv T(\tilde{\phi}^{i},\overline{e_j})=\begin{pmatrix}0&\cdots & 1 & \cdots &0\end{pmatrix}\cdot A\cdot \begin{pmatrix}
0 \\ \cdots \\ 1 \\ \cdots\\0\end{pmatrix}=a_j^{i}$
\end{example}
\newpage
\begin{example} Un tensor (1,3) muy importante es el llamado tensor de Riemann $\appl{T}{R=V^{*}×V×V×V}{ℝ}$. En relatividad $dim V = 4$ y tiene $4\cdot 4\cdot 4\cdot 4=256$ componentes y, para aplicarlo a un elemento del dual, digamos con componentes $(a_1,a_2,a_3,a_4)$, y a tres vectores, con coordenadas$(b_1,b_2,b_3,b_4), (c_1,c_2,c_3,c_4), (d_1,d_2,d_3,d_4)$, debemos escribir:
$$\sum_{i=1}^{4}\sum_{j=1}^{4}\sum_{k=1}^{4}\sum_{l=1}^{4}R_{j\space k\space l}^{i}a_i\space b^{j}\space c^{k}\space d^{l}$$
Esta expresión tiene demasiados sumatorios, si la reescribimos con el criterio de Einstein queda $R_{j\space k\space l}^{i}a_i\space b^{j}\space c^{k}\space d^{l}$\newline
\end{example}
\begin{defn}[Producto\IS tensorial] Sea T un tensor de tipo (r,s) y sea S otro tensor de tipo (u,v), de define su producto tensorial $T\otimes S$ como un nuevo tensor de tipo (r+u,s+v): $$T\otimes S(\tilde{\phi^{1}},\cdots ,\tilde{\phi^{r+u}})=T(\tilde{\phi^{1}},\cdots ,\tilde{\phi^{r}},\overline{v}_1,\cdots,\overline{v}_s)\cdot S(\tilde{\phi^{r+1}},\cdots ,\tilde{\phi^{r+u}},\overline{v}_{s+1},\cdots,\overline{v}_{s+v})$$
\end{defn}
\begin{example}Tomaremos estos dos tensores de tipo (0,1) en $ℝ^{2}$: $$T(\overline{x})=2\cdot x^{1}+x^{2}; S(\overline{x})=5\cdot x^{1}$$ y hallaremos ahora las componentes de $T\otimes S$, que será por definición un nuevo tensor de tipo (0,2), de la forma $\appl{T\otimes S}{ℝ^{2}×ℝ^{2}}{ℝ}$, y le llamaremos P a partir de ahora. Entonces tenemos que: $$\begin{cases}
	P_{1\space1}=P(\overline{e}_1,\overline{e}_1)=10\\
	P_{1\space2}=0\\
	P_{2\space1}=P(\overline{e}_2,\overline{e}_1)=5\\
	P_{2\space2}=0\\
	\end{cases}$$
	
\end{example}
\begin{example}En física e ingeniería se consideran muchos tensores importantes. Por ejemplo, el \textbf{tensor de inercia} es un tensor de tipo (0,2) que mide (entre otras cosas) lo difícil que es girar un sólido rígido respecto a un eje dado (en términos físicos es difícil si necesito un gran \textbf{trabajo} para girarlo).\\
	$$
	\begin{tikzpicture}
	\draw (-1,0) arc (180:360:1cm and 0.5cm);
	\draw[dashed] (-1,0) arc (180:0:1cm and 0.5cm);
	\draw (0,0) circle (1cm);
	\shade[ball color=blue!10!white,opacity=0.20] (0,0) circle (1cm);
	\draw[thick,->] (0,-1.5,0) -- (0,1.5,0) node[above]{$eje$};
	\end{tikzpicture}
\tab
	\begin{tikzpicture}
	\draw (-1,0) arc (180:360:1cm and 0.5cm);
	\draw[dashed] (-1,0) arc (180:0:1cm and 0.5cm);
	
	\draw (0,0) circle (1cm);
	\shade[ball color=blue!10!white,opacity=0.20] (0,0) circle (1cm);
	\draw[thick,->] (-0.9,-1.5,0) -- (-0.9,1.5,0) node[above]{$eje$};
	\end{tikzpicture}
	$$
Por ejemplo, en la figura, suponiendo que es una esfera que pesa toneladas, la intuición nos dice sin hacer cálculos que costará menos rotar la esfera respecto al primer eje que rotarla respecto al segundo (y estamos en lo cierto).
\end{example}
\begin{example}Los tensores también aparecen en el \textbf{entrelazamiento cuántico}, dentro del campo de la mecánica cuántica.
Una partícula se corresponde con un vector, que como sabemos se corresponde con un tensor (1,0). Pues si tenemos dos partículas $\overline{v}$ y $\overline{w}$, entonces el sistema formado por ambas partículas es  $\overline{v}\otimes\overline{w}$.
\end{example}
\section{Repaso (intuitivo) de Geometría Diferencial}
Empezaremos con el objeto matemático al que llamamos \textbf{variedad}. Como idea, una variedad es un objeto geométrico que se puede parametrizar por abiertos de $ℝ^{n}$, pero el gran salto con respecto de la geometría de la asignatura de GCS, es que la variedad no está inmersa en $ℝ^{n}$, sino que está ahí (como el universo), sin preocuparnos lo que sea que la rodea.\newline
\indent La idea de variedad diferenciable n-dimensional es, por tanto, la de un objeto geométrico compuesto por parches que son similares a abiertos de $ℝ^{n}$. Partimos de un espacio topológico M al que exigimos que tenga la propiedad de Hausdorff y una base numerable (segundo axioma de numerabilidad). La primera propiedad es natural si queremos poder tratar separadamente los puntos, y las segunda va también en este sentido, porque permite asegurar la existencia de particiones de la unidad, que son totalmente necesarias para hacer el an análisis local típico de la geometría diferencial.
Una carta nos dice la manera de allanar un parche de M en $ℝ^{n}$ (es la inversa de la parametrización en ese abierto de $ℝ^{n}$).
$$\includegraphics[width=\linewidth]{img/GT17_Variedad_carta}$$
El número de parámetros que la parametrización requiere es llamado en geometría \textbf{dimensión} de la variedad (mientras en mecánica lo llamaríamos \textbf{grados de libertad}). Si llamamos $\phi$ a la inversa de la parametrización (la carta), entonces tendremos (mirar figura superior) algo como $\phi(x^{1},\cdots,x^{n})$, que nos lleva a $ℝ^{n}$ y donde $x^{i}$ son lo que llamamos \textbf{funciones coordenadas}. Vamos a formalizarlo rápidamente para no empezar a divagar.

\begin{defn}[Carta\IS n-dimensional] Una carta n-dimensional de M es un par $(\mathcal{U},\phi)$ donde $\mathcal{U}$ es un abierto
de M y $\phi$ es una función $\appl{\phi}{\mathcal{U}}{ℝ^{n}}$ que es homeomorfismo sobre su imagen.
\end{defn}

Como un punto puede estar tapado por varios parches, diferentes abiertos de cartas, debemos asegurarnos de que el análisis no se estropea bajando por una $\phi$ o por otra. Una de las cosas que se suelen pedir en Geometría Diferencial es que si existe otra parametrización $\psi$, entonces tanto $\psi\circ\phi$ como $\phi\circ\psi$ han de ser de clase $C^{\infty}$ (En el abierto intersección de $ℝ^{n}$), y en ese caso se dice que las cartas son \textbf{compatibles} 

\begin{defn}[Derivada parcial i-ésima en una variedad] Se dice que una función $\appl{f}{M}{ℝ}$ es $C^{\infty}$ si para cada carta $(\mathcal{U},\phi)$ la función $\appl{f\circ \phi^{-1}}{\phi(\mathcal{U})}{ℝ}$ lo es, y se define para cada $p \in \mathcal{U}$ la derivada parcial i-ésima en la variedad como $$\restr{\pdv{f}{x^{i}}}{p}=D_i(f\circ \phi^{-1})(\phi(p))$$ donde $D_i$ denota la derivada parcial usual respecto de la variable i-ésima.
\end{defn}

No daremos la definición rigurosa, pero sí diremos que a las funciones $C^{\infty}$ entre dos variedades $M$ y $N$ que tienen inversa $C^{\infty}$ se llaman \textbf{difeomorfismos}
\newpage
Un problema técnicamente más complejo es la definición del \textbf{espacio tangente}, que en el caso de subvariedades de $ℝ^{n}$ es muy fácil. No es una mera adaptación porque allí los vectores tangentes eran “pelos” orientados que se salían de la subvariedad, mientras que ahora concebimos las variedades como una entidad única, sin referencia a un posible “exterior”. Hay varias maneras de superar este obstáculo. Aquí mencionaremos las definiciones matemáticas que corresponden a ver los vectores tangentes como velocidades de curvas y como derivadas direccionales. La segunda es más abstracta, se hace introduciendo implícitamente el concepto de derivación.

\begin{defn}[Espacio\IS tangente de una variedad] Se llama espacio tangente de una variedad M en un punto p al conjunto cociente $T_p(M)=K_p(M) /\sim$ donde $K_p(M)=\{ $Funciones $\appl{c}{(-\epsilon,\epsilon)}{M}$ con $c(0)=p \}$ y $\sim$ identifica las funciones curvas tales que $(\phi\circ c_1)'(0)=(\phi\circ c_2)'(0)$ con $(\mathcal{U},\phi)$ una carta. Se llama \textbf{vector tangente} de M en p a cualquiera de sus elementos.
\end{defn}

\begin{defn}[Vector\IS tangente en una variedad] Se llama vector tangente de $M$ en $p$ a cualquier operador $ℝ$-lineal $\appl{v}{E_p(M)}{ℝ}$ que satisface $v(fg)=v(f)g(p)+f(p)v(g)$ para todo $f,g \in E_p(M)$, donde $E_p(M)$ es el anillo de funciones $M\longmapsto ℝ$ definidas en un entorno suficientemente pequeño de p. Se llama espacio tangente de $M$ en un punto $p$ al conjunto formado por todos los vectores tangentes.
\end{defn}

A partir de las curvas que corresponden a los ejes coordenados (una vez que bajamos a $ℝ^{n}$ por la carta) se obtienen unos vectores tangentes que denotaremos con el extraño nombre $\restr{\pdv{}{x^{i}}}{p}$. Para ser rigurosos, si $\{\overline{e}_1,\overline{e}_2,\cdots,\overline{e}_n\}$ es la base canónica, fijada una carta $(\mathcal{U},\phi=(x^1,\cdots,x^n))$ con la primera definición se tiene $$\restr{\pdv{}{x^{i}}}{p}=[c_i]\tab con \tab c_i(t)=\phi^{-1}(\phi(p)+t\overline{e}_i),\tab i=1,2,...,n $$

Denominar a estos vectores con el mismo símbolo que el de las derivadas parciales no es casual, pues con la segunda definición no son más que las derivadas parciales i-ésimas en la varidad, es decir 
\begin{equation}
\appl{\restr{\pdv{}{x^{i}}}{p}}{f}{\restr{\pdv{f}{x^{k}}}{p}}
\end{equation}
Por razones obvias se les suele denotar con la notación abreviada $\restr{\partial_i}{p}$, o incluso $\partial_i$ si el punto no se indica.
\begin{prop} El espacio tangente $T_p(M)$ tiene una estructura natural de espacio vectorial cuya dimensión es la de la variedad diferenciable M.
\end{prop}
\begin{prop} Para cada punto p de una variedad diferenciable n-dimensional $M$, el conjunto $\{\restr{\partial_1}{p},\restr{\partial_2}{p},\cdots,\restr{\partial_n}{p}\}$ es una base de $T_p(M)$.
\end{prop}
Hasta aquí , si aún no nos hemos quitado la vida con la notación, podemos proceder.
\newpage
Con $\appl{f}{M}{N}$ podemos pasar curvas en curvas lo cual induce una aplicación $T_p(M)\longmapsto T_{f(p)}(N)$. Aunque ésta es la idea intuitiva, es más sintético proceder tomando en cuenta la segunda definición de espacio tangente.

\begin{defn}[Aplicación\IS tangente] Sea $\appl{f}{M}{N}$. Se llama aplicación tangente de f en p y se denota con $\restr{\dv{f}}{p}$, a la aplicación lineal $T_p(M)\longmapsto T_{f(p)}(N)$ que aplica un elemento de $T_p(M)$ (considerado con la segunda definición), digamos $v(\cdot)$ en $v(\cdot\circ f)$
\end{defn}
\begin{prop} Sea $\appl{f}{M}{N}$ y sean $(\mathcal{U}(p),\phi)$ y $(\mathcal{V}(f(p)),\psi)$ cartas de M y N respectivamente en los puntos indicados. La matriz de la aplicación tangente $\restr{df}{p}$ en las bases $\{\restr{\pdv{}{x^1}}{p},\cdots,\restr{\pdv{}{x^m}}{p}\}$ y $\{\restr{\pdv{}{y^1}}{f(p)},\cdots,\restr{\pdv{}{y^n}}{f(p)}\}$ correspondientes a estas cartas es la matriz jacobiana de $\psi\circ f\circ\phi^{-1}$ en $\phi(p)$.
\end{prop}

Dada una carta $(\mathcal{U},\phi=(x^1,\cdots,x^n))$ de M tiene sentido considerar las aplicaciones tangentes de las funciones coordenadas $\restr{dx^{i}}{p}$ como funciones de $M$ en $ℝ$ con la estructura de variedad obvia. Usando las definiciones de vector tangente se puede probar que $$\restr{dx^{i}}{p}(\restr{\pdv{}{x^j}}{p})=\delta_j^i$$ o dicho de otra forma $$\{\restr{dx^1}{p},\restr{dx^2}{p},\cdots,\restr{dx^n}{p}\}\tab \text{es la base dual de} \tab\{\restr{\pdv{}{x^1}}{p},\restr{\pdv{}{x^2}}{p},\cdots,\restr{\pdv{}{x^n}}{p}\}$$

\begin{defn}[Espacio\IS cotangente] Dada una carta $(\mathcal{U},\phi=(x^1,\cdots,x^n))$ de $M$, al espacio vectorial sobre $ℝ$ generado por $\{\restr{dx^1}{p},\restr{dx^2}{p},\cdots,\restr{dx^n}{p}\}$ se denomina espacio cotangente de M en p y se denota con $T_p^{*}(M)$, por ser el dual de $T_p(M)$.
\end{defn}

\begin{defn}[Uno forma] Los elementos de $T_p^{*}(M)$ se llaman uno formas (o covectores).
\end{defn}

Como cabía esperar, en lo sucesivo descargaremos la notación para las aplicaciones tangentes y las bases introducidas de $T_p(M)$ y $T_p^{*}(M)$ omitiendo el punto cuando no sea relevante. Por ejemplo, escribiremos $dx^1$ en lugar de $\restr{dx^1}{p}$.

Una vez más insistimos en que todos los espacios vectoriales sobre $ℝ$ son lo mismo, y una vez fijadas las bases las operaciones se realizan coordenada a coordenada como nos enseñaron en primero cuando casi todo era con vectores de $ℝ^{n}$. Los elementos del dual no albergan nada nuevo y siguen funcionando como se indicó en la sección anterior (y en el curso de primero) por mucho que pongamos $d$ y $\partial$ por todos los lados. En un ejemplo: \\ $$(2dx^1+3dx^2)(2\pdv{}{x^1}-\pdv{}{x^2})=1 \tab\text{porque}\tab \begin{pmatrix}2&3\end{pmatrix}\begin{pmatrix}2\\-1\end{pmatrix}=1$$
\newpage
\section{Tensores en el espacio tangente}
Ya hemos visto tensores en $ℝ^{n}$, veamos que pasa cuando consideramos los tensores en las variedades en general. Podemos considerar tensores cuyo espacio vectorial subyacente sea el espacio tangente de la variedad en cada punto, y cuyo dual es el cotangente: $$V=T_p(M)\tab V^*=(T_p(M))^*$$
Será conveniente permitir variar el punto y tener entonces un \textbf{campo de tensores} (un tensor definido en cada punto de la variedad) y queremos que sea diferenciable (en algún sentido).\\ 
Para hacer la teoría más sintética, es conveniente introducir el \textbf{fibrado}, tanto tangente como cotangente: $$TM=\bigcup\limits_{p\in M} T_p(M)\tab T^*M=\bigcup\limits_{p\in M} (T_p(M))^* $$ con cierta estructura de variedad. En física a esto se le llama \textbf{espacio de fases} (partículas elementales).

\begin{defn}[Tensor\IS en una variedad] Sea $M$ una variedad. Un tensor (en rigor, campo tensorial) $C^{\infty}$ de tipo $(r,s)$ en $M$ es una aplicación que a cada punto $p$ le asigna un tensor de tipo $(r,s)$ con $V=T_p(M)$ y $V^*=(T_p(M))^*$ y tal que en cada carta las componentes sean funciones $C^{\infty}$.
\end{defn}

\begin{defn}[Métrica] Una métrica en una variedad es un tensor de tipo (0,2) en dicha variedad tal que si en una carta $(g_{i\space j})$ es la matriz de componentes, entonces esta matriz es simétrica y no singular.
\end{defn}

\begin{example}En $ℝ^{2}$ con la carta identidad tenemos $G=dx\otimes dx+dy\otimes dy$. Tomemos $\overline{v}=a^i\pdv{}{x^i}$ y $\overline{w}=b^i\pdv{}{x^i}$, ambos en $ℝ^{2}$, y entonces con la métrica G: $$G(\overline{v},\overline{w})=dx(\overline{v})\cdot dx(\overline{w})+dy(\overline{v})\cdot dy(\overline{w})=a^1+b^1\cdot a^2+b^2$$ que es el producto escalar usual.
\end{example}
\begin{example}\textbf{(Métrica de Poincaré)}\indent En $ℝ×ℝ^+$ se define la métrica:\space$G=y^{-2}(dx\otimes dx+dy\otimes dy)$. Sea $\overline{v}=3\restr{\pdv{}{x}}{p}+\restr{4\pdv{}{y}}{p}$; para $p=(0,1)$; entonces $G(\overline{v},\overline{v})=3^2+4^2=25$. Y si $\overline{w}=3\restr{\pdv{}{x}}{q}+\restr{4\pdv{}{y}}{q}$; para $q=(0,5)$; entonces $G(\overline{w},\overline{w})=1$.
\end{example}
\begin{example}\textbf{(Métrica alrededor de un agujero negro)}\indent Se define como $$G=-\left(1-\frac{r_0}{r}\right)dt\otimes dt+\left(1-\frac{r_0}{r}\right)^{-1}dr\otimes dr$$ donde r es la distancia al centro, t es el tiempo y $r_0$ una constante.
\end{example}
\newpage
\begin{example}\textbf{(Componentes de la métrica usual en $ℝ^2$ )}\indent Tenemos la métrica usual del producto escalar con componentes: $$\begin{cases}
	g_{1\space1}=1 \tab g_{1\space2}=0\\
	g_{2\space1}=0 \tab g_{2\space2}=1\\
	\end{cases}$$
	En general, una métrica es de la forma $G=g_{i\space j}dx^i\otimes dx^j$. En la matriz de componentes de la métrica usual nos encontramos con la identidad (simétrica y no singular).
\end{example}

\begin{example}\textbf{(Componentes de la métrica de Poincaré )}\indent Se tienen las componentes:$$G=(g_{i\space j})=\begin{pmatrix}y^{-2}&0\\0&y^{-2}\\ \end{pmatrix}$$ Lo mas importante de los tensores en variedades es saber como se comportan sus componentes por cambios de carta.
\end{example}

Si tenemos un tensor T en una variedad M para cierta carta, el tensor no depende de la carta pero las componentes si: $$\tilde{T}^{i_1,\cdots,i_r}_{j1,\cdots,j_s}=\pdv{y^{i_1}}{x^{k_1}}\cdots\pdv{y^{i_r}}{x^{k_r}}\pdv{x^{l_1}}{y}\cdots\pdv{x^{l_s}}{y^{j_s}}\cdot T^{k_1,\cdots,k_r}_{l_1,\cdots,l_s},\tab\text{para }\psi=(y^1,\cdots,y^n)$$
Esto se deduce por la regla de la cadena de la sigiente manera: $$\pdv{y^j}=\pdv{x^i}{y^j}\pdv{x^i};\tab dy^j=\pdv{y_j}{x^i}dx^i;$$
Por definición tendriamos que $\tilde{T}^{i_1\cdots i_r}_{j_1,\cdots,j_s}=T(dy^{i_1},\cdots,dy^{i_r},\pdv{y^{j_1}},\cdots,\pdv{y^{j_s}})$.
Si recordamos la definición de métrica como tensor $(0,2)$ en una variedad cuya matriz de componentes es simétrica y no singular nos preguntamos si podemos pasar los tensores de una variedad $M$ a tensores en una variedad $N$ por medio de una aplicación. Y la respuesta es que no es tan trivial como parece. 

\begin{defn}[Pull-back] Sea un tensor $T$ de tipo $(0,s)$ en la variedad N, se llama pull-back (o en español menos popularmente imagen recíproca) al tensor de tipo $(0,s)$ en $M$ definido por la siguiente expresión: $$f^*T(\overline{v}_1,\cdots,\overline{v}_s)=T(df(\overline{v}_1),\cdots,df(\overline{v}_s))$$
\end{defn}

El pull-back será muy interesante cuando estudiemos las formas diferenciables. Para los tensores $(r,s)$ no está bien definida por no saber como tratar con los elementos del dual, y para los de tipo $(r,0)$ hay otra cosa definida que se llama push-forward.
\newpage
\begin{defn}[Métrica\IS inducida] Sea $i$ una inclusión, y sea $T=G$ una métrica, se dice que $i^*G$ es la métrica inducida.
\end{defn}

\begin{example} Sean 
	\begin{tikzcd}
	\mathbb{S}^1 \arrow[r] \arrow[d, "\phi=ang."] & ℝ^2 \arrow[d, "\psi=id"] \\
		\theta & (x,y)
	\end{tikzcd} 
y la métrica usual en $ℝ^2$: $G=dx\otimes dx+dy\otimes dy$\\
En lugar de seguir la fórmula de la definición de pull-back recordamos que la aplicación tangente usa las derivadas parciales (el jacobiano). Tenemos pues:\\
$$\begin{cases}
x=cos (\theta)\\
y=sen(\theta)\\
\end{cases}\mapsto
\begin{cases}
dx=-sen (\theta)\\
dy=cos(\theta)\\
\end{cases}$$
Cabe decir que la llave izquierda no es un cambio de carta, es la inclusión $\psi\circ i\circ \phi^{-1}$. Concluyendo, tenemos la métrica inducida por la usual en $\mathbb{S}^1$:
$$i^*G=(-sen(\theta))^2d\theta\otimes d\theta + (cos^2(\theta))d\theta\otimes d\theta=d\theta\otimes d\theta$$ 
\end{example}
\begin{example}\textbf{(Típico de GCS: Superficie inmersa en $ℝ^3$ )}\\ 
	\begin{wrapfigure}{r}{0.5\textwidth}
		\begin{center}
			\includegraphics[width=0.40\textwidth]{img/GT17_parametric_surface}
		\end{center}
		\caption{Superficie inmersa en $ℝ^3$ }
	\end{wrapfigure}
Llamamos (u,v) a los parámetros de la superficie, que en la foto aparecen como $(x,y)$:\\
$\begin{cases}
x=f^1(u,v)\\
y=f^2(u,v)\\
z=f^3(u,v) \\
\end{cases}$
$\begin{cases}
dx=\pdv{f^1}{u}du+\pdv{f^1}{v}dv\\
dy=\pdv{f^2}{u}du+\pdv{f^2}{v}dv\\
dz=\pdv{f^3}{u}du+\pdv{f^3}{v}dv \\
\end{cases}$
\\La métrica inducida por la usual (que para confundirnos aún más recibe también el nombre de usual) es la suguiente:\\ \\
$i^*(dx\otimes dx+dy\otimes dy+dz\otimes dz)=$$$(\pdv{f^1}{u}du+\pdv{f^1}{v}dv)\otimes(\pdv{f^1}{u}du+\pdv{f^1}{v}dv)\cdots=$$\\$\left(\norm{\pdv{f}{u}}\right)^2du\otimes du+\cdots$
\end{example}

Como se ha visto en los ejemplos, en la práctica es mucho más cómodo aplicar las siguientes fórmulas que la ley general de transformación de tensores:
$\begin{cases}
\pdv{y^j}=\pdv{x^i}{y^j}\pdv{x^i}\\
dy^j=\pdv{y_j}{x^i}dx^i\\
\end{cases}$
\newpage
\begin{example}Vamos a cambiar la métrica usual de $ℝ^2$ a polares (y como venimos haciendo, pasamos de la fórmula del pull-back). Tenemos la métrica $G=dx\otimes dx+dy\otimes dy$ y el cambio de carta $\begin{cases}
	x=r\cdot cos (\theta)\\
	y=r\cdot sen(\theta)\\
	\end{cases}\mapsto
	\begin{cases}
	dx=cos(\theta)dr-sen(\theta)d\theta\\
	dy=sen(\theta)dr+cos(\theta)d\theta\\
	\end{cases}$\\
	Si sustituímos en la métrica usual tenemos: $$cos^2(\theta)dr\otimes dr+r^2sen^2(\theta)d\theta\otimes d\theta+sen^2(\theta)dr\otimes dr+r^2cos^2(\theta)d\theta\otimes d\theta=dr\otimes dr+r^2d\theta\otimes d\theta$$
\end{example}
\chapter{Geometría Riemanniana}
\section{Cálculo de variaciones y mecánica}
El cálculo de variaciones se dedica a hallar máximos y mínimos de \textbf{funcionales} (entendiendo esta palabra como aplicación que va de un espacio de funciones a $ℝ$). Lo más común es el ejercicio en el que hay que minimizar una integral (como hallar el "tobogán" más rapido entre dos puntos dados), aunque hay otros como intentar hallar el área máxima que podemos encerrar con una cuerda de longitud 1 (Demostrar que la solución es una circunferencia no es trivial sin el cálculo de variaciones). 
\begin{obs}Durante este capítulo, se usara la notación $\dot{f}\equiv\dv{f}{t}$
\end{obs}
\begin{example}\textbf{(Problema de la Braquistócrona I)}\\
\begin{wrapfigure}{r}{0.5\textwidth}
		\begin{center}
			\includegraphics[width=0.5\textwidth]{img/GT17_Brachistochrone}
		\end{center}
		\caption{Curva Braquistócrona}
\end{wrapfigure}
Una curva braquistócrona, o curva del descenso más rápido, es la curva entre dos puntos que es recorrida en menor tiempo (el "tobogán" del que hablábamos antes), por un cuerpo que comienza en el punto inicial con velocidad cero, y que debe desplazarse a lo largo de la curva hasta llegar al segundo punto, bajo acción de una fuerza de gravedad constante y suponiendo que no existe fricción.\\ \\ \indent Supongamos que dicha curva es la gráfica de una función $y=f(x)$ en $ℝ^2$ y entonces el problema consiste en minimizar el tiempo que se tarda en ir de $A$ a $B$. Dicho tiempo (llamémosle $T$) se corresponde con la integral: $$\int_{0}^{1}\sqrt{\frac{1+(\dot{f}(x))^2}{2g\cdot f(x)}}dx;\tab\text{donde g es la constante gravitacional.}$$ 
\end{example}

A continuación se enuncia la proposición básica del cálculo de variaciones.
\newpage
\begin{prop} Dados $a,b\inℝ$; $\overline{c},\overline{d}\in ℝ^n$ y un conjunto de funciones $$\mathcal{C}=\{F=(q^1,\cdots,q^n)\space:\space F(a)=\overline{c},\space F(b)=\overline{d}\}$$ Supongamos además que tenemos la integral $$\int_{a}^{b}L dt;\tab\text{con }L=L(t,F(t),\dot{F}(t))$$ y que esta alcanza un máximo o un mínimo en $\mathcal{C}$, entonces dicha F es solución de la versión discreta (la que se usa en mecánica clásica) de \textbf{Euler-Lagrange}:
\begin{equation}
\label{eq:Euler-Lagrange}
\dv{t}\left(\pdv{L}{\dot{q^i}}\right)=\pdv{L}{q^i};\tab i=1,\cdots,n
\end{equation}
\end{prop}

\begin{proof}Sea $f(\epsilon)=\int_{a}^{b}L(t,F_0(t)+\epsilon\alpha t,\dot{F}_0(t)+\epsilon\dot{\alpha}(t))dt$; siendo $F_0$ la función para la que se alcanza el extremo y siendo $\appl{\alpha}{[a,b]}{ℝ^n}$ arbitraria con $\alpha(a)=\alpha(b)=0$; $\appl{f}{I}{ℝ}$ alcanza un extremo en $\epsilon=0$, por tanto:
	$$\dot{f}(0)=0\longrightarrow0=\int_{a}^{b}\left(\pdv{L}{q^i}\alpha^i+\pdv{L}{\dot{q}^i} \dot{\alpha}^i \right)dt=\int_{a}^{b}\left(\pdv{L}{q^i}-\dv{t}\left(\pdv{L}{\dot{q}^i}\right)\dot{\alpha}^i \right)dt$$
	que implica las ecuaciones de Euler-Lagange.
\end{proof}

\begin{example}$$I=\int_{1}^{2}(1+t^2(\dot{q}(t))^2)dt;\tab q(1)=2, q(2)=3$$
	Hay que encontrar la $q=q(t)$ que minimiza I. ¿Cuál es? Pues atendiendo a la proposición llamamos $L$ al integrando y tenemos siguiendo la fórmula: $$L=1+t^2\dot{q}^2\longrightarrow\begin{cases}
	\pdv{L}{\dot{q}}=2t^2\dot{q};\\
	\pdv{L}{q}=0;
	\end{cases}\longrightarrow\dv{t}(2t^2\dot{q})=0;$$
	Luego nos queda la EDO $4t\dot{q}+2t^2\ddot{q}=0$ y solo hay que resolverla. Tenemos que $t^2\dot{q}=cte\rightarrow \dot{q}(t)=\frac{cte}{t^2}$. Luego ya podemos integrar para hallar la función deseada: $$q(t)=K_1+\frac{K_2}{t}\xrightarrow{q(1)=2, q(2)=3}q(t)=4-\frac{2}{t}$$
Con esta q tenemos que I=3. Por ejemplo, si hubieramos cogido $q(t)=t+1$, entonces tendríamos $I=\frac{10}{3}\textgreater3$.\\
\end{example}
Volvamos ahora al ejemplo de la Braquistócrona.
\begin{example}\textbf{(Problema de la Braquistócrona II)}\\
Teníamos que:
$$\int_{0}^{1}\sqrt{\frac{1+(\dot{f}(x))^2}{2g\cdot f(x)}}dx;\tab f(0)=1,f(1)=0$$ 
\newpage
Luego tenemos que nuestro integrando, al que venimos llamando $L$, es:
$$L=\sqrt{\frac{1+(\dot{q}(x))^2}{2g\cdot q(x)}}\longrightarrow\begin{cases}\pdv{L}{\dot{q}}=\frac{1}{\sqrt{2gq}}\frac{\dot{q}}{\sqrt{1+\dot{q}^2}}\\
\pdv{L}{q}=\sqrt{\frac{1+(\dot{q}(x))^2}{2g}}\cdot\frac{-1}{2}\cdot q^{\frac{-3}{2}}
\end{cases}$$
Y si ahora usamos las ecuaciones de Euler-Lagrange nos sale una EDO bastante complicada. Para ayudar damos una proposición nueva
\end{example}
\begin{prop}\textbf{(Ley de conservación de la energía)} Supongamos que $L$ no depende explícitamente de $t$. Por tanto, $L(t,q,\dot{q})=L(q,\dot{q})$ y entonces a lo largo de cualquier solución de las ecuaciones de Euler-Lagrange se cumple que:
\begin{equation}
\label{eq:ConservacionEnergia}
\dot{q}^i\pdv{L}{\dot{q}^i}-L=cte
\end{equation}
\end{prop}
\begin{proof}
	$\dv{t}\left(\dot{q}^i\pdv{L}{\dot{q}^i}-L\right)=\ddot{q}^i\pdv{L}{\dot{q}^i}$Por terminar...
\end{proof}
HASTA AQUÍ EL PRIMER EXAMEN PARCIAL\\
¿Por qué pueden interesarnos estos resultados del cálculo de variaciones en geometría de variedades? Imaginemos que tenemos el problema anterior, en el que intentabamos minimizar $\int_{1}^{2}(1+t^2(\dot{q}(t))^2)dt$, donde $L=1+t^2\dot{q}^2$ y el mínimo era 3.
Ahora, si cambiamos de nombre la función incógnita $q$, el mínimo debería seguir siendo el mismo. Entonces yo podría considerar (por ejemplo, yo,porque quiero) llamar a la $q=h^2$  (suponiendo que esto funciona bien: como $q$ es positiva se puede escribir como un cuadrado...) y el mínimo debería ser el mismo y alcanzarse para la misma función. El nuevo $\tilde{L}=1+t^2(2h\dot{h})^2$ y el mínimo debe salir el mismo salvo el cambio de nombre. Si antes $q(t)=4-\frac{2}{t}$ ahora $h(t)=\sqrt{4-\frac{2}{t}}$. Esto es sorprendente porque si cambio las coordenadas en las ecuaciones de Euler-Lagrange el problema parece compicarse, pero funciona perfectamente:
$$\dv{t}\left(\pdv{L}{\dot{q}}\right)=\pdv{L}{q};\tab\dv{t}\left(\pdv{\tilde{L}}{\dot{h}}\right)=\pdv{\tilde{L}}{h}$$
Luego es lícito hacer este cambio 
$$L\longmapsto\tilde{L}$$
$$q\longmapsto h$$
 De alguna manera, que las ecuaciones de Euler-Lagrange sean invariantes por cambios de coordenadas es atractivo, porque quiere decir que en los problemas de Geometría puedo cambiar la carta (las coordenadas) y haciendo el cambio de variable pertinente todo funcionará de lujo.

Esto tiene una aplicación en mecánica (s.XVIII y S.XIX) que es fantástica. En estos siglos surgió la idea de dejar de aplicar $\overline{F}=m\cdot\overline{a}$ en cada punto por resolver un problema de cálculo de variaciones: 
\newpage
\begin{prop}\textbf{(Principio de mínima acción o de acción estacionaria)}\\
	En mecánica se usa este langrangiano siempre $L=E_c-E_p$ que es la diferencia entre energía cinética (energía que aportan las partículas, que es igual a $E_c=\frac{1}{2}\sum_{i=1}m_i\norm{\dv{\overline{x}_i}{t}}^2$) y energía potencial (la energía que las partículas toman del campo donde se encuentren, como el gravitatorio donde vale $E_p=\sum_{i=1}m_igh_i$) y existe el \textbf{principio de mínima acción}, que dice que los sistemas mecánicos evolucionan entre dos tiempos de modo que $\int_{t1}^{t2}Ldt$ es mínima (aunque en rigor es estacionaria).
\end{prop}
\begin{example}
	\begin{wrapfigure}{r}{0.5\textwidth}
		\begin{center}
			\begin{tikzpicture}
			\pgfmathsetmacro{\Gvec}{1.5}
			\pgfmathsetmacro{\myAngle}{30}
			% calculate lengths of vector components
			\pgfmathsetmacro{\Gcos}{\Gvec*cos(\myAngle)}
			\pgfmathsetmacro{\Gsin}{\Gvec*sin(\myAngle)}
			
			\coordinate (centro) at (0,0);
			\draw[dashed,gray,-] (centro) -- ++ (0,-3.5) node (mary) [black,below]{$ $};
			\draw[thick] (centro) -- ++(270+\myAngle:3) coordinate (bob);
			\pic [draw, ->, "$\theta$", angle eccentricity=1.5] {angle = mary--centro--bob};
			\draw [blue,-stealth] (bob) -- ($(bob)!\Gcos cm!(centro)$);
			\draw [-stealth] (bob) -- ($(bob)!-\Gcos cm!(centro)$)
			coordinate (gcos)
			node[midway,above right] {$l\cos\theta$};
			\draw [-stealth] (bob) -- ($(bob)!\Gsin cm!90:(centro)$)
			coordinate (gsin)
			node[midway,above left] {$l\sin\theta$};
			\draw [-stealth] (bob) -- ++(0,-\Gvec)
			coordinate (g)
			node[near end,left] {$g$};
			\pic [draw, ->, "$\theta$", angle eccentricity=1.5] {angle = g--bob--gcos};
			\filldraw [fill=black!40,draw=black] (bob) circle[radius=0.1];
			\end{tikzpicture}
		\end{center}
		\caption{Péndulo en $ℝ^n$}
	\end{wrapfigure}
	
	Supongamos que queremos ver el movimiento de un péndulo. En lugar de imaginar fuerzas y tensiones ficticias, veámoslo matemáticamente. El punto extremo del péndulo tiene coordenadas $(x,y)$ y entonces $L=\frac{1}{2}m(\dot{x}^2+\dot{y}^2)-mgy$. Si nos fijamos, $(x,y)$ no son buenas coordenadas para este problema. Cojamos mejor la coordenada $\theta=$ángulo. Entonces$$ \begin{cases}
		x=l\cdot sen(\theta)\\
		y=-l\cdot cos(\theta)\\
	\end{cases}\longrightarrow \begin{cases}
	dx=l\cdot cos(\theta)d\theta\\
	dy=-l\cdot cos(\theta)d\theta\\
	\end{cases}$$
	y entonces $L=\frac{1}{2}m\cdot l^2\dot{\theta}^2+mgl\cdot cos(\theta)$. \\Ahora aplicando \refeq{eq:Euler-Lagrange} tenemos:
	$$\dv{t}\left(\pdv{L}{\dot{\theta}}\right)=\pdv{L}{\theta}\longrightarrow\dv{t}\left(ml^2\dot{\theta}\right)=-mgl\cdot sen(\theta)\longrightarrow\ddot{\theta}=-\frac{g}{l}sen(\theta)$$
	que es la ecuación del péndulo de toda la vida.
\end{example}
\\
\begin{obs}
	Este resultado de invarianza ante cambios de coordenadas  nos llevará como veremos a calcular geodésicas en variedades muy rápidamente si lo vemos como un problema del cálculo de variaciones minimizando las métricas en dicha variedad.
\end{obs}
\section{Geodésicas en variedades}
\begin{defn}[Variedad\IS semiriemanniana] Una variedad semiriemanniana es una variedad dotada de una métrica.
\end{defn}

\begin{defn}[Variedad\IS riemanniana] Una variedad riemanniana es una variedad semiriemanniana cuya métrica es definida positiva.
\end{defn}

Recuérdese que la condición de no degeneración que se pedía a una métrica G es que su matriz de componentes ($g_{ij}$) fuera no singular. Esto no impide que ocurran cosas raras como $G(∂1, ∂1) = 0$  o $G(∂1, ∂1) < 0$. Tal comportamiento estrafalario (¿vectores con longitudes nulas o imaginarias?) es conveniente en relatividad pero extraño a nuestras ideas geométricas, por ello es natural dar una denominación específica a las variedades con métricas definidas positivas, es decir, con matriz ($g_{ij}$) definida positiva en todo punto.\newline
Si en una carta  $U,φ = (x^1,...,x^n)$  una métrica tiene componentes $g_{ij}$, para cada par de campos de vectores $X = X^i∂_i, Y = Y^i∂_i$ se tiene, por definición $G(X,Y) = g_{ij}X^iY^j$. Si queremos referirnos a a métrica $G$ sin mencionar las componentes de los vectores a los que se aplica, en pura ortodoxia notacional se escribe $g_{ij} dx^i ⊗ dx^j$ ya que $dx^i(X) = X^i$ y $dx^i(Y ) = Y^i$, como covectores, y $⊗$ significa el producto del resultado de aplicar dos tensores. Esta operación se define en general para cualquier par de tensores.
\begin{example}En $ℝ^n$ con la carta trivial la métrica que corresponde al producto escalar usual es $g_{ij}dx^i ⊗dx^j$ con $g_{ij} = 1$ si $i = j$ y 0 en otro caso. Ésta es la métrica usual y la más empleada, pero hay una infinidad de formas de convertir $ℝ^n$ en una variedad semiriemanniana.
\end{example}

La forma clásica de expresar una métrica, a veces denostada por los matemáticos, reemplaza productos tensoriales por productos habituales en un sentido formal. Así la métrica usual en $ℝ^2$ se escribiría $dx^2+dy^2$ y la métrica $dx⊗dx+dx⊗dy+dy⊗dx+10dy⊗dy$ sería $dx^2+2dxdy+10dy^2$.También es habitual el nombre común $ds^2$ en vez de $G$,incluso si la métrica no es definida positiva.

Al igual que el lagrangiano $L = T$ (energía cinética) determina las ecuaciones de movimiento intrínsicamente, sin estar forzados a emplear un sistema de coordenadas externo; también una métrica determina unas curvas destacadas en una variedad riemanniana. En ambos casos, lo que se obtiene es una generalización de las rectas. En el primer caso por analogía con el principio de inercia (en ausencia de fuerzas el movimiento es rectilíneo y uniforme) y en el segundo por la propiedad minimizante de la distancia que estudiaremos mas adelante.

\begin{defn}[Geodésica]Dada una variedad semiriemanniana $M$, se dice que una curva parametrizada $\appl{c}{I}{M}$ es una geodésica si en cada carta $(U,φ)$ con $Im (c)\cap U \neq ∅$, las funciones $(φ\circ c)(t) = (x^1(t), . . . , x^n(t))$ satisfacen las ecuaciones de Euler-Lagrange (\refeq{eq:Euler-Lagrange})para $L = g_{ij}\dot{x}^i\dot{x}^j$ con $g_{ij}$ las componentes de la métrica de $M$.
\end{defn}

\begin{example}Sea $G$ la métrica usual en $ℝ^2$, entonces $L=g_{ij}\dot{q}^i\dot{q}^j = \dot{x}^2+\dot{y}^2$ con $q^1=x$, $q2=y$. Los cálculos para las ecuaciones de Euler-Lagrange (\refeq{eq:Euler-Lagrange})son:$$\pdv{L}{\dot{x}}=2\dot{x};\tab\dv{t}\left(\pdv{L}{\dot{x}}\right)=\dv{t}\left(2\dot{x}\right)=2\ddot{x}\tab\pdv{L}{x}=0$$ y lo mismo con $y$. Entonces las ecuaciones de Euler-Lagrange son: $$\ddot{x}=0\tab\ddot{y}=0$$ que se resuelven como $(x(t), y(t)) = (x_0, y_0) + t(a_0, b_0)$. Esto concuerda con el principio de inercia. En coordenadas polares, un cálculo prueba $L = \dot{r}2 + r^2\dot{\theta}^$2 y las ecuaciones de Euler-Lagrange son: $$\ddot{r}=r\dot{\theta}^2\tab r\ddot{\theta}+2\dot{r}\dot{\theta}=0$$ Estas ecuaciones tan complicadas todavía representan las mismas trayectorias rectilíneas. Por ejemplo, podemos comprobar que $r(t) = \frac{1}{sen(θ(t))}$ con $cot(θ(t)) = t$, correspondiente a la recta horizontal antes mencionada, es solución.
\end{example}

Lo último que habíamos visto es que si teníamos una geodésica entonces estaba parametrizada por arco cumpliendo $g_{ij}\dot{x}^i\dot{x}^j=cte$.
Los símbolos de Christoffel son algo que aparece de manera natural al estudiar la geometría de superficies de forma extrínseca (fijándose en el entorno). La curvatura se suele calcular usando el producto de curvaturas principales (extrínsecamente) o con formas diferenciables (intrínsecamente).

Sobre los símbolos de Christoffel: Si la matriz $(g_{ij})$ era definida positiva yo puedo reparametrizar la curva: $$x^i(t)\longrightarrow x^i(kt); \text{ luego tenemos que } \dv{t}(x^i(kt))=k\dot{x}^i(kt)\text{ y por tanto:}$$$$ g_{ij}\dot{x}^i\dot{x}^j=cte\longrightarrow  k^2g_{ij}\dot{x}^i\dot{x}^j=cte$$
Tomando $k^2=cte$ conseguiría $g_{ij}\dot{x}^i\dot{x}^j=1$. Esto es porque la constante es positiva (porque la matriz es definida positiva), en otro caso no puedo parametrizar por longitud de arco todas las geodésicas (no sería igual a 1).
\begin{obs}Recordemos que para poder parametrizar por longitud de arco tenemos que tener $G(\overline{v},\overline{v})=1$, que es lo mismo que se ve en GCS.
\end{obs}

Vamos a atar algunos cambios sueltos, como que no está claro que estén "bien" definidas las geodésicas. Sabemos que funcionan con cambio de carta (aunque no lo hemos visto con todo rigor), pero ¿funcionan como en GCS que dado un valor inicial y un vector inicial ya tengo definida una sola geodésica? ¿Nos salen las mismas geodésicas que en segundo? ¿minimizan las geodésicas la longitud en pequeños entornos?.\\ \indent Sabemos que algunas EDO tienen más de una solución así que \textbf{no está claro} que estén bien definidas (especialmente en los casos que no hay unicidad o existencia de la EDO asociada). \\ \indent Desde el punto de vista de la física la primera y la última pregunta se contestan fácil: Por ejemplo, dada una partícula si le pones una posición inicial y un vector inicial está definida si miramos el momento y el espacio donde está, luego la respuesta sería que sí, que está bien definida (por tramposo que parezca el razonamiento). Por otro lado, por el principio de mínima acción también nos haría intuir una respuesta afirmativa.\\
\begin{wrapfigure}{r}{0.3\textwidth}
	\begin{tikzpicture}[remember picture]   
	%horizantal
	\foreach \y in {0.6, 0.9, 1.2, 1.5}
	\draw[xshift=0.2cm , yshift=\y cm, gray] (-2,-1.8)..controls (-1.5,-0.6)..(0.5,-1.6)
	(0.5,-1.6)..controls (1.0,-1.8) and (1,-2)..(2,-1);
	\draw[xshift=0.2cm , yshift=0.1cm, gray] (-2,-1.8)..controls (-1.5,-0.6)..(0.5,-1.6)
	(0.5,-1.6)..controls (1.0,-1.8) and (1,-2)..(2,-1);
	%vertical                                                 
	\draw[xshift=0.1 cm , yshift=-0.0cm, gray]     (-1.9,-1.1)..controls (-1.8,-0.1)..(-1.3,0.8);                                
	\draw[xshift=0.24 cm , yshift=-0.0cm,gray]     (-1.9,-1.1)..controls (-1.8,-0.1)..(-1.3,0.8);                           
	\draw[xshift=0.4 cm , yshift=-0.0cm, gray]     (-1.9,-1.1)..controls (-1.8,0)..(-1.3,0.8);                                               
	\foreach \x in { 0.4, 0.7, 1.0, 1.3}             
	\draw[xshift=\x cm , yshift=-0.0cm, gray]     (-1.9,-1.1)..controls (-1.8,0)..(-1.3,0.8);
	\foreach \x in { 1.6, 1.9}             
	\draw[xshift=\x cm , yshift=-0.3cm,gray]      (-1.9,-0.9)..controls (-1.8,0.1)..(-1.3,0.6);
	\foreach \x in { 2.2, 2.5}             
	\draw[xshift=\x cm , yshift=-0.3cm, gray]     (-1.9,-1.1)..controls (-1.8,0)..(-1.3,0.4);
	%rectangle
	\draw [rectangle,fill=gray!50,rotate=-20,xshift=-1.13cm,yshift=-0.16cm](-0.13,-0.3) rectangle (0.13,0.3);
	\node[](nodeA) at (-1.1cm,0.4cm){};
	\node[black](nodeB) at (0.7cm,0.5cm) {\tiny{$T_p(M)$}};
	%coord syst
	\draw [->,thick,color=red,xshift=-1.1cm,yshift=0.3cm,rotate=-15](0,0) -- (xyz cs:x=0.3) ; 
	\node [black, above, xshift=-1cm,yshift=-0.6cm] at (-0.2,1.3) {\tiny{}};  
	%\draw [->,thick,color=red, xshift=-1.1cm,yshift=0.3cm,rotate=10](0,0) -- (xyz cs:y=0.5) ;
	\node [black, right, xshift=-1.8cm] at (0.9,0.2) {\tiny{$\overline{\Phi}_u$}};
	\draw [->,thick,color=red, xshift=-1.1cm,yshift=0.3cm,rotate=23](0,0) -- (xyz cs:z=1.0) ; 
	\node [black, above, xshift=-0.67cm,yshift=0.3cm] at (-0.6,-0.87) {\tiny{$\overline{\Phi}_v$}};     
	\end{tikzpicture}
	\Varrow{black}{nodeA}{nodeB}         
\end{wrapfigure}
\indent Vamos a dar demostraciones matemáticas para dejar de intuír. En Geometría de Curvas y Superficies, se trabaja con el siguiente sistema de referencia: $\{\overline{\Phi}_u,\overline{\Phi}_v,\overline{N}\}$ .\\ \indent Con este criterio cada vector de $\mathbb{R}^3$ tendrá unas componentes en esta base.Si no sabemos que la superficie está en $\mathbb{R}^3$ se usan los símbolos de Christoffel, dando las componentes en $\overline{\Phi}_u,\overline{\Phi}_v$ de sus derivadas.\\
\indent Cuando uno calcula la aceleración de una partícula en $\overline{\Phi}_u$ tenemos $$\pdv[2]{\overline{\Phi}}{u}=\overline{\Phi}_{uu}=\Gamma^1_{11}\overline{\Phi}_u+\Gamma^2_{11}\overline{\Phi}_v+e\overline{N}$$, donde e es el primer coeficiente de la 2ª forma fundamental. Lo mismo con las otras derivadas $\overline{\Phi}_{uv}$ y $\overline{\Phi}_{vv}$. Los símbolos de Christoffel son las $\Gamma$. Estos símbolos se usan porque las curvaturas en realidad sólo dependen de las métricas: la curvatura de Gauss se puede dar en función de $e,f,g$ (coeficientes de la $II FF$), pero haciendo magia negra podemos dejarla en función sólo de los símbolos de Christoffel, convirtiéndose esta en un invariante intrínseco de la variedad (\textbf{theorema egregium} de Gauss).
\begin{prop}
Las ecuaciones de Euler-Lagrange que definen las geodésicas son equivalentes a estas con una pinta más amable
\begin{equation}
\label{eq:Geodesicas-Christoffel}
\ddot{x}^k+\Gamma^k_{ij}\dot{x}^i\dot{x}^j=0
\end{equation}
donde los $\Gamma^k_{ij}$ son los simbolos de Christoffel dados por esta difícil expresión
\begin{equation}
\label{eq:Christoffel-metrica}
\Gamma^k_{ij}=\frac{1}{2}g^{mk}\left(\pdv{g_{im}}{x^j}+\pdv{g_{jm}}{x^i}-\pdv{g_{ij}}{x^m}\right)
\end{equation}
con $(g^{ij})$ la matriz inversa de $(g_{ij})$. En particular, existe una única geodésica una vez escogidos punto y vector inicial $x^i(0)$ y $\dot{x}^i(0)$. Si $(g_{ij})$ es una métrica como es no singular se puede calcular la inversa, y como las funciones serán $C^\infty$ todo se podrá calcular.
\end{prop}

\begin{obs} Esta proposición se usa para calcular los símbolos de Christoffel en casos sencillos (pocas dimensiones y métricas sencillas) sin usar la fórmula, simplemente hallando las ecuaciones diferenciales de las geodésicas y comparar coeficientes.
\end{obs}

\begin{example} Hallaremos los símbolos de Christoffel para $\mathbb{S}^2$ con la métrica inducida y la carta $(\theta,\phi)$.
Sabíamos que la métrica es $G=d\theta^2+sen^2(\theta)d\phi^2$, habíamos hallado las ecuaciones de Euler-Lagrange de las geodésicas y teníamos, con lagrangiano $L=\dot{\theta}^2+sen^2(\theta)\dot{\phi}^2$: $$\dv{t}\left(\pdv{L}{\dot{\theta}}\right)=\pdv{L}{\theta}\longrightarrow\ddot{\theta}=2sen(\theta)cos(\theta)\dot{\phi}^2\longrightarrow\ddot{\theta}-sen(\theta)cos(\theta)\dot{\phi}^2=0\longrightarrow\ddot{x}^1+\Gamma^1_{ij}\dot{x}^i\dot{x}^j=0$$
$$\Gamma^1_{22}=-sen(\theta)cos(\theta)$$ y el resto son 0. Por otro lado:
$$\dv{t}\left(\pdv{L}{\dot{\phi}}\right)=\pdv{L}{\phi}\longrightarrow\dv{t}(2sen^2(\theta)\dot{\phi})=0\longrightarrow4sen(\theta)cos(\theta)\dot{\theta}\dot{\phi}+2sen^2(\theta)\ddot{\phi}=0;$$
$$\ddot{\phi}+2\frac{cos(\theta)}{sen(\theta)}\dot{\theta}\dot{\phi}=0\longrightarrow\ddot{x}^2+\Gamma^2_{ij}\dot{x}^i\dot{x}^j=0\longrightarrow\begin{cases}
\Gamma^2_{12}=\Gamma^2_{21}=\frac{cos(\theta)}{sen(\theta)}\\
\Gamma^2_{ij}=0 \text{ en otro caso}\\
\end{cases}$$
\end{example}
\begin{prop}Las geodésicas en una variedad satisfacen las siguientes ecuaciones:
\begin{equation}
\ddot{x}^i+\Gamma^i_{jk}\dot{x}^i\dot{x}^j=0
\end{equation}
donde $\Gamma^i_{jk}$ don los símbolos de Christoffel.
\end{prop}
\begin{proof} A partir de la k-ésima ecuación de Euler-Lagrange tenemos: $$\dv{t}(2g_{kj}\dot{x}^j)=2\pdv{g_{ij}}{x^k}\dot{x}^i\dot{x}^j;$$
	$$2g_{kj}\ddot{x}^j-\pdv{g_{ij}}{x^k}\dot{x}^i\dot{x}^j=-2\pdv{g_{kj}}{x^i}\dot{x}^i\dot{x}^j=\left(-\pdv{g_{kj}}{x^i}-\pdv{g_{ki}}{x^j}\right)\dot{x}^i\dot{x}^j;$$$$g_{kj}\ddot{x}^j+\frac{1}{2}\left(\pdv{g_{kj}}{x^i}+\pdv{g_{ki}}{x^j}-\pdv{g_{ij}}{x^k}\right)\dot{x}^i\dot{x}^j=0;$$
	Sabemos que $g_{kj}\ddot{x}^j$ se corresponde a coger la k-ésima coordenada del producto matricial $\begin{pmatrix}g_{11}&\cdots&g_{1n}\\\vdots&\ddots&\vdots\\g_{n1}&\cdots&g_{nn}\\\end{pmatrix}\begin{pmatrix}\ddot{x}^1\\\vdots\\\ddot{x}^n\end{pmatrix}$ Si ahora multiplicamos por la matriz inversa se tiene $g^{lk}g_{kj}\ddot{x}^j=\delta^l_j\ddot{x}^j=\ddot{x}^l$ y ya sale la fórmula con los símbolos de Christoffel
\end{proof}
En las variedades no se tiene en cuenta el vector normal $\overline{N}$, de hecho para dimensiones grandes no existe la segunda forma fundamental, mientras que los símbolos de Christoffel existen en cualquier variedad.
\subsection{Relatividad General}
La teoría de la relatividad general, postulada por Albert Einstein, es en realidad bastante geométrica. Einstein postuló (a grandes rasgos) que el potencial gravitatorio (o fuerza de la gravedad) no existía (es decir, que toda la energía de las partículas era energía cinética), y a cambio de esto, la energía cinética viene reemplazada por una métrica (que él no calculó). Einstein sólo sabía de dicha métrica las condiciones complicadísimas que debía cumplir, (esencialmente que la curvatura fuese nula en algún sentido), pero no la formuló.

\indent Con esta filosofía, las trayetorias de las partículas materiales serán las geodésicas parametrizadas por longitud de arco ($G(\dot{\gamma},\dot{\gamma})=1$), siendo el parámetro de las geodésicas el tiempo medido por un observador en la partícula.
Para las trayectorias de las partículas de luz tenemos que serán las geodésicas que cumplen ($G(\dot{\gamma},\dot{\gamma})=0$).
\begin{example}Para movimientos radiales (como la caída de una piedra al centro de la tierra) bajo el efecto de una masa m con simetría esférica que no esté rotando (estática), la métrica fuera de ella es la \textbf{Métrica de Schwarzchild (radial)} (que ya apareció antes en estas notas como la métrica alrededor de un agujero negro):
	$$G=\left(1-\frac{r_0}{r}\right)dt^2-c^{-2}\left(1-\frac{r_0}{r}\right)^{-1}dr^2$$
donde c es la velocidad de la luz, r es la distancia al centro y $r_0$ una constante dada por $r_0=\frac{2gm}{c^2}$ donde g es la constante de gravitación y m como ya dijimos la masa.\\ \indent En $r=r_0$ la matriz de la métrica se vuelve singular (aparecen un coeficiente 0 en la coordenada temporal y un $\infty$ en la espacial) y el asunto se pone feo (aparece una especie de potencial gravitatorio infinito).\\ \indent En el caso de los planetas, estrellas, etc. el valor de $r_0$ es muy pequeño con respecto al radio del cuerpo en cuestión (en el caso de La Tierra tenemos $r_0\approx8mm$). Existen sin embargo objetos llamados agujeros negros, que son unas "estrellas raras" con radio menor que $r_0$, es decir, que la singularidad queda fuera.\\ \indent Vamos a calcular ahora las geodésicas más sencillas (correspondientes a un rayo de luz) para esta métrica. Para ello recordemos que se tenía que cumplir $G(\dot{\gamma},\dot{\gamma})=0$ y vamos a ver como solo usando \ref{eq:ConservacionEnergia} podemos ver la forma de las geodésicas:
$$\left(1-\frac{r_0}{r}\right)dt^2-c^{-2}\left(1-\frac{r_0}{r}\right)^{-1}dr^2=0\tab\begin{cases}t=t(\lambda);\\r=r(\lambda);\end{cases}\begin{cases}\dot{t}=\dv{t}{\lambda};\\\dot{r}=\dv{r}{\lambda};\end{cases}\frac{\dot{t}}{\dot{r}}=\frac{dt}{dr}$$ 
\indent Por la conservación de la energía, si la métrica es 0 en el instante inicial entonces es 0 siempre.
$$\left(\dv{t}{r}\right)^2=c^{-2}\left(1-\frac{r_0}{r}\right)^{-2}\xrightarrow{\textbf{quitamos} \sqrt{}}\dv{t}{r}=\pm c^{-1}\left(1-\frac{r_0}{r}\right)^{-1}$$
Ya tenemos la EDO que satisfacen las geodésicas, que encima es de variables separadas, así que tomando $\int_{r_o}^{r}$:
$$t=\pm c^{-1}\restr{(r+r_0\log(r-r_0))}{R_0}^R;$$
$$\pm ct=r-R_0+r_0\log(\frac{r-r_0}{R_0-r_0});$$
Luego si el rayo se acerca al agujero negro tenemos $R=R_0$ sale un $\infty$ (esto significa que, visto desde fuera, cuando el rayo de luz va acercándose al agujero negro, tarda un "tiempo infinito" en atravesarlo, es decir, lo vemos parado).
\end{example}
\section{Conexiones afines. Derivada covariante}
Nuestro objetivo en esta sección es derivar campos de vectores (y tensores, en general) en variedades. 
\begin{example}
Sea la variedad $ℝ^2$. Consideramos una base que varía con el tiempo $\mathcal{B}=\{\overline{u}_1,\overline{u}_2\}$ (supongamos por ejemplo que van girando). Supongamos ahora que tenemos un vector $\overline{v}=(a(t),b(t))$ con coordenadas en la base $\mathcal{B}$, es decir $\overline{v}=a(t)\overline{u}_1(t)+b(t)\overline{u}_2(t)$ y por tanto:
$$\dv{\overline{v}}{t}=a'(t)\overline{u}_1(t)+b'(t)\overline{u}_2(t)+a(t)\overline{u}_1'(t)+b(t)\overline{u}_2'(t)$$ Definiremos: $$\begin{cases}
\overline{u}_1'(t)=\Gamma_1^1\overline{u}_1(t)+\Gamma^2_1\overline{u}_2(t)\\\overline{u}_2'(t)=\Gamma_1^2\overline{u}_1(t)+\Gamma^2_2\overline{u}_2(t)
\end{cases}$$
Entonces $\dv{\overline{v}}{t}$ tiene coordenadas $(a',b')+(a\Gamma^1_1+b\Gamma^1_2,a\Gamma^2_1+b\Gamma^2_2)$. El primer sumando es la \textbf{variación de las coordenadas} y el segundo sumando es la \textbf{variación del sistema de referencia}.
\end{example}

El análogo en variedades sería que la derivada de un campo de vectores $v^i\pdv{x^i}$ en la dirección $\pdv{x^j}$ debería ser algo con coordenada i-ésima: $$\pdv{v^i}{x^j}+\Gamma^i_{jk}v^k$$
\begin{obs} En principio no está claro qué condiciones deben satisfacer las $\Gamma^i_{jk}$ para que no den contradicciones al cambiar de carta. En lugar de buscr estas condiciones se suele dar una definición a través de conexiones.
\end{obs}

\begin{defn}[Conexión\IS afín] Una conexión afín en una variedad $M$ es una aplicación que asigna a cada par de campos de vectores $X$ e $Y$ un tercero $\nabla_XY$ tal que: 
\begin{enumerate}
\item$\nabla_{fX+gY}Z=f\nabla_XZ+g\nabla_YZ$ .
\item$\nabla_X(Y+Z)=\nabla_XY+\nabla_XZ$
\item$\nabla_X(fY)=f\nabla_XY+X(f)Y$
\end{enumerate}
donde $\appl{f,g}{M}{ℝ}$ arbitrarias.
\end{defn}
\begin{prop} Si $\nabla$ es una conexión afín, entonces $$\nabla_XV=\left(\pdv{V^k}{x^i}+\Gamma^k_{ij}V^j\right)X^i\pdv{x^k};$$ donde $V=V^i\pdv{x^i}$ y $\Gamma^k_{ij}$ es la componente k-ésima de $\nabla_{\pdv{x^i}}\pdv{x^j}$.
\end{prop}
\begin{proof}$\nabla_XV=X^i\nabla_{\pdv{x^i}}\left(V^j\pdv{x^j}\right)$, aplicando la propiedad 1 de conexión afín (ya que $X=X^i\pdv{x^i}$). Ahora aplicamos la propiedad 3 y tenemos que $$\nabla_XV=X^i\left(V^j\nabla_{\pdv{x^i}}\pdv{x^j}+\pdv{V^j}{x^i}\pdv{x^j}\right);$$ que reordenando y sacando factor común se convierte en lo que queremos.
\end{proof}


Sea $∇$ una conexión afín y $V$ un campo de vectores fijado, entonces $X\longmapsto\nabla_XV$ es lineal y bien definida en el espacio tangente. Es lineal en el sentido de que va de $T_p(M)$ a $T_p(M)$ y nos da un tensor $(1,1)$.
En este sentido, una conexión afín asigna a un tensor $(1, 0)$ un tensor $(1, 1)$. Se puede extender (no es obvio) a una manera de asignar a un tensor $(r, s)$ un tensor $(r, s + 1)$.
En este sentido, una conexión afín define una "derivada covariante".

\begin{defn}[Derivada\IS covariante]Sea $V =V^i∂_i$ y $∇_XV= \left(\pdv{V^k}{x^i}+\Gamma^k_{ij}V^j\right)X^i∂_k$. La derivada covariante de V
sería el tensor de tipo $(1,1)$ con componentes $T^k_i$ dadas por $\pdv{V^k}{x^i}+\Gamma^k_{ij}V^j$.
\end{defn}

En muchos textos se reserva el nombre de derivada covariante a la particularización en los vectores tangentes de una curva. Sea $c(t)$ la curva, $x^i(t) =
(x^i\circ c)(t)$ y $V(t)=V$ en el punto $c(t)$ que corresponde a $(x^1(t),\cdots,x^n(t))$. En lugar de $X$ queremos poner $\dot{x}^i(t)\partial_i$. Sustituyendo la coordenada k-ésima es: $$\dv{V}{t}+\Gamma^k_{ij}V^j\dv{x^i}{t}$$ Es decir, la derivada covariante de $V$ a lo largo de $c(t)$ es $$\frac{DV}{dt}=\left(\dv{V^k}{t}+\Gamma^k_{ij}V^j\dv{x^i}{t}\right)\pdv{x^k}$$

Lo ultimo que habíamos visto es que si $\nabla$ es una conexión razonable (que satisfaciera las dos hipótesis) entonces los $\Gamma^i_{jk}$ eran los símbolos de Christoffel. Ahora bien, estas funciones no está claro que con un cambio de carta den lugar a una conexión bien definida. Lo que hay que ver por tanto es que no lleguemos a contradicciones tras un cambio de carta.
Vamos a ver la existencia. Vamos a comporbar que el candidato que tenemos con los $\Gamma^i_{jk}$ da lugar a una conexión. Hay dos pruebas, una "sin cartas" (más sintética) y otra con cartas.
\begin{proof}\textbf{Demostración con cartas} Usamos que sabemos las fórmulas de los símbolos de Christoffel y sabemos cómo se transforman los símbolos de Christoffel. Hay que probar que si yo hago la derivada covariante en una carta $(y^1,\cdots,y^n)$ puedo pasar a otra carta $(x^1,\cdots,\x^n)$ tensorialmente (si no fuese así no tendríamos una conexión bien definida). $$\underbrace{\nabla_jV'^i}_{\text{carta }(y^1,\cdots,y^n)}=\pdv{y^i}{x^k}\pdv{x^m}{y^j}\underbrace{\nabla_mV^k}_{\text{carta }(x^1,\cdots,x^n)}(*)$$ Por la forma que tiene esta expresión está claro que se cumplen las propiedades de conexión. La única diferencia es que la fórmula que nos garantiza que se cumplan era dada fijada una carta ($\nabla_jV^i$ no es intrínseco, depende de la base, ver la analogía abajo en la observación). 
	Lo primero que hacemos para salvar esto sería: $$\pdv{V'^i}{y^j}\Gamma'^i_{jk}V'^k\tab\Gamma',V'\text{ son los de la otra carta;}$$
	$$\pdv{V'^i}{y^j}\Gamma'^i_{jk}V'^k=\pdv{y^i}{x^k}\pdv{x^m}{y^j}\left(\pdv{V^k}{x^m}+\Gamma^k_{ml}V^l\right)$$
	$$V'^i=\pdv{y^i}{x^k}V^k$$ si yo aquí derivo (y sabiendo $\pdv{V^k}{y^j}=\pdv{x^m}{y^j}\pdv{V^k}{x^m}$): $$\pdv{V'i}{y^j}=\pdv{y^j}(\pdv{y^i}{x^k})V^k+\pdv{y^i}{x^k}\pdv{V^k}{y^j}=\underbrace{\pdv{y^i}{x^k}{x^m}\pdv{x^m}{y^j}V^k}_{\text{\parbox{2cm}{lo que debería desaparecer cuando se acumula con los símbolos de Christoffel}}}+\underbrace{\pdv{y^i}{x^k}\pdv{x^m}{y^j}}_{\text{lo que necesito}}\pdv{V^k}{x^m}$$
	Para que las partes con $V^r$ se cancelen se necesita $\Gamma'^i_{kj}\pdv{y^k}{x^l}=\Gamma^k_{lm}\pdv{y^i}{x^k}\pdv{x^m}{y^j}-\pdv{y^i}{x^k}{x^m}\pdv{x^m}{y^j}$ y esto es el problema número 9 de la hoja 3, luego... hemos terminado.
\end{proof}
\begin{obs}[Analogía con Álgebra lineal] Imaginemos que tenemos $Y=AX$, donde A es una matriz. Ahora voy a cambiar de base $CY=\tilde{A}CX$, y entonces la primera ecuación ya no se satisface, ahora tenemos que $A=C^{-1}\tilde{A}C$. El análogo en la ecuación es que $\pdv{y^i}{x^k}\pdv{x^m}{y^j}$ son el $C$ y el $C^{-1}$
O por ejemplo si nos dicen "imaginemos los vectores con coordenada primera igual a 7", eso no está bien definido porque la coordenada de un mismo vector varía con las bases). 
\end{obs}
\subsection{Curvatura}
 A partir de ahora sólo usaremos la conexión de Levi-Civita. La curvatura de una variedad Riemanniena se expresa a través del \textbf{tensor de Riemann} que habitualmente se considera de tipo $(1,3)$
 (FALTA DEFINIR $\nabla_k$ (tensor tipo $(1,1)$)y lo haremos el martes)
 
 \begin{defn}[Tensor\IS de Riemann] Se llama tensor de Riemann al tensor de tipo $(1,3)$ con componentes $R^i_{jkl}$ tales que para cualquier campo de vectores sea el conmutador de la derivada covariante, es decir: $$\nabla_k\nabla_lV^i-\nabla_l\nabla_kV^i=R^i_{jkl}V^j$$ La derivada covariante tenia la derivada de la V, así que algo tiene que pasar para que desaparezcan y esto tenga sentido.
 \end{defn}

\begin{obs}¿Por qué no aparacen derivadas de los $V^i$? $$\nabla_lV^i=\delta
	_lV^i+\Gamma^i_{lr}V^r;$$ $$\nabla_k\nabla_lV^i\longrightarrow\delta_k\delta_lV^i\tab\text{con las simetrías se cancela}$$
	Los términos con derivada primera vienen de derivar los términos $\Gamma^i_{lr}V^r$ $$\nabla_k\nabla_lV^i\longrightarrow\Gamma^i_{lr}\delta_kV^r$$
	\end{obs}
Al margen de lo difícil que puede llegar a ser comprender qué tiene que ver esto con una curvatura geométricamente, se presentan varios problemas en la definición:
\begin{enumerate}
	\item ¿Qué significa $\nabla_k\nabla_lV^i$? Sólo sabemos aplicar $\nabla$ a campos vectoriales y $\nabla_lV^i$ no es un campo vectorial. Aunque parezca que podemos intuir cómo se calcula seguramente nos equivocaríamos, ya que hay que tener en cuenta ciertos detalles (como que en otras variedades distintas de $ℝ^n$ alguna de las coordenadas no tiene por qué tener dirección constante).
	\item¿Por qué depende linealmente de $V^i$?¿Dónde están las derivadas de $V^i$ que deberían aparecer?
	\item¿Por qué es tensorial? Es decir, ¿por qué esto está bien definido tras cambios de carta?
\end{enumerate}
Para responder a la primera pregunta vamos a extender el concepto de derivada covariante para tensores en general. Queremos extender el significado de la derivada covariante $\nabla_X$ (basta extender $\nabla_j$). Para vectores (tensores $(1,0)$) queremos que salga la derivada usual $$\nabla_jV^k=\partial_jV^k+\Gamma^k_{jl}V^l$$ Para funciones (tensores $(0,0)$), por otro lado, queremos obtener la derivada parcial, que utilizábamos para generalizar $\partial_jf$ :$$\nabla_jf=\partial_jf$$ Y además queremos que respete contracciones: $$\nabla_l\left(T^{i_1\cdots i_r}_{j_1\cdots j_s}S^{j_1\cdots j_s}_{k_1\cdots k_t}\right)=\left(\nabla_lT^{i_1\cdots i_r}_{j_1\cdots j_s}\right)\left(S^{j_1\cdots j_s}_{k_1\cdots k_t}\right)+\left(T^{i_1\cdots i_r}_{j_1\cdots j_s}\right)\left(\nabla_lS^{j_1\cdots j_s}_{k_1\cdots k_t}\right)$$
Veamos que estas propiedades determinan la derivada covariante de las uno-formas (tensores de tipo $(0,1)$) de la forma $\omega=\omega_idx^i$. Nosotros sabemos derivar campos de vectores $V=V^i\pdv{x^i}$, pero como uno es el dual del otro, sea $f=\omega_iV^i$ (pasamos de un tensor $(0,1)$ y un tensor $(1,0)$ a otro $(0,0)$). Si hacemos su derivada covariante con respecto a $j$: $$\partial_jf=(\nabla_j)f=(\partial_j\omega_i)V^i+\omega_i(\partial_jV^i);$$
$$\underbrace{\partial_jf}_{\text{lo conozco}}=(\nabla_j)f=(\partial_j\omega_i)V^i+\underbrace{\omega_i(\nabla_jV^i)}_{\text{lo conozco}};$$ $$(\nabla_j\omega_i)V^i=\underbrace{\partial_j\omega_iV^i+\omega_i\partial_jV^i}_{\nabla_jf=\partial_jf}-\underbrace{\omega_i(\partial_jV^i+\Gamma^i_{jk}V^k)}_{\nabla_jV^i};$$
\begin{obs}Ahora podemos renombrar $\omega_i\Gamma^i_{jk}V^k$ a $\omega_k\Gamma^k_{ji}V^i$ y esto me permite sacar factor común.
\end{obs}
$$(\nabla_j\omega_i)V^i=(\partial_j\omega_i-\Gamma^k_{ji}\omega_k)V^i\longrightarrow\nabla_j\omega_i=\partial_j\omega_i-\Gamma^k_{ji}\omega_k$$ En general, si uno supiera derivar todos los tensores de tipo $(r,s)$ con $r+s\leq d$, podría definir de forma única la derivada covariante de otro tensor tipo $(r,s+1)$ o $(r+1,s)$, luego por inducción puedo derivar cualquiera del mismo modo que hemos visto (considerando $\nabla\left(T^{i_1\cdots i_r}_{j_1\cdots j_sj_{s+1}}V^{j_{s+1}}\right)$ que sí sabemos derivar y despejando).
\begin{prop}\textbf{Regla para tensores en general} Cada superíndice se deriva con $+\Gamma$ y cada subíndice se deriva con $-\Gamma$
\end{prop}
\begin{example}$$\nabla_jT^{ik}=\partial_jT^{ik}\underbrace{+\Gamma^i_{jl}T^lk}_{\text{derivamos "i"}}\underbrace{+\Gamma^k_{jl}T^{il}}_{\text{ahora "k"}}$$ (el superíndice de la $\Gamma$ es siempre lo que derivamos,  luego se cuadran los índices restantes para mantener los sumatorios)
\end{example}
\begin{example}$$\nabla_jT^i_k=\partial_jT^i_k+\Gamma^i_{jl}T^l_k-\Gamma^l_{jk}T^i_l$$
\end{example}

(***HASTA AQUI EL SEGUNDO PARCIAL*****)\\
% Apéndices (ejercicios, exámenes)
\appendix

\chapter{Ejercicios}
\input{tex/GeoTopo17_Ejs.tex}
\chapter{Exámenes}
\input{tex/GeoTopo17_Exs.tex}
\bibliography{../Apuntes}
\printindex
\end{document}
