\documentclass[nochap,palatino]{apuntes}

\title{Tratamiento de Señales Visuales}
\author{Guillermo Julián Moreno}
\date{15/16 C1}

% Paquetes adicionales
\usepackage{fancysprefs}
\usepackage{xfrac}

% --------------------

\begin{document}
\pagestyle{plain}

\begin{abstract}
Estos apuntes son un pequeño resumen, no exhaustivo, de la asignatura de Tratamiento de Señales Visuales de la EPS UAM. Aquí recogemos todos los conceptos de forma resumida, basándonos en los apuntes proporcionados por los profesores.
\end{abstract}

\maketitle

\section{Operadores puntuales}

Empezamos definiendo las imágenes como aplicaciones $\appl{ψ}{ℝ^2}{I}$, de tal forma que a cada punto $(x,y)$ le asignan un determinado valor de luminosidad. Para imágenes en escala de grises, $I$ será habitualmente $[0,255]$, aunque también podríamos considerar $I = [0,1]$.

Dada una imagen $ψ(x,y)$ de origen, aplicaremos operadores $\appl{T}{ℝ}{ℝ}$ para obtener imágenes de salida $θ(x,y)$. La convención será que la transformación llevará valores $r_k$ a otros $s_k$.

\subsection{Modelado de histograma}

Este tipo de operadores cambian la amplitud de luminosidad de la imagen y se pueden modelar según su efecto en el histograma de la distribución de posibles valores de luminosidad de los píxeles de la imagen. Se puede normalizar para obtener probabilidades entre 0 y 1. Hay tres operaciones principales con operadores puntuales.

\subsubsection{Ajuste de contraste}

El ajuste de contraste pretende expandir ciertas zonas del rango dinámico y comprimir otros para mejorar el contraste de la imagen. Se define como \[
s = T(r) = \begin{cases}
α r & 0 ≤ r < a \\
β(r-a) + s_a & a ≤ r < b \\
γ(r-b) + s_b & b ≤ r < L - 1
\end{cases}
\]

$α,β,γ$ se calculan según los parámetros $a,b,s_a,s_b$ para que $T$ sea continua. En general, la transformación se puede definir como una transformación lineal en tres intervalos: \begin{align*}
[0, a) &\to [0,s_a) \\
[a, b) &\to [s_a, s_b) \\
[b, L - 1] &\to [s_b, L - 1]
\end{align*}

Un posible puede ocurrir debido a la cuantización: al comprimir ciertos rangos, la aplicación puede dejar de ser inyectiva y entonces será imposible recuperar esa información al restaurar el contraste.

\subsubsection{Igualación de histograma}

La igualación de histograma pretende obtener un histograma lo más uniforme posible. La transformación se define como \[ s = T(r) = \sum_{i=0}^r P(I = i) = P(I <= r) \] donde $P(I = i)$ es la probabilidad de que un determinado píxel de la imagen de entrada tenga valor $i$. En otras palabras, se hace una transformación usando como ``tabla de referencia'' la CDF de luminancia.

Se pueden usar otras funciones como tabla siempre y cuando sean monótonas crecientes. Además, también se puede realizar la ecualización adaptativa realizando el proceso de ecualización en porciones de la imagen. Se puede usar ventana deslizante o bien considerar ventanas con solapamiento.

\subsubsection{Especificación de histograma}

La idea es transformar el histograma de una imagen para que se parezca al de otra imagen. Se hace una correspondencia por percentiles. Sea $F$ la CDF del histograma de la imagen a modificar y $G$ la de la imagen especificada. Entonces, se define la transformación $T$ como \[ s = T(r) = \inf \set{i ∈ [0, L-1] \tq G(i) ≥ F(r) }\]

\subsection{Modificación de niveles}

\subsubsection{Recorte y umbralización}

El recorte sirve para eliminar ruido si se sabe que los niveles de la imagen están en un rango $[a,b]$, llevándolos a un rango final $[s_a, s_b]$. La transformación se define como \[ s = T(r) = \begin{cases} s_a & 0 ≤ r < a \\
α(r - a) + s_a & a ≤ r < b \\
s_b & b ≤ r < L - 1
\end{cases} \] con $α = \frac{s_b - s_a}{b - a}$, esto es, el parámetro que hace que $T$ sea continua.

Cuando $a = b = u$, lo que se realiza es una umbralización, lo que nos permite separar objetos según su luminosidad. El umbral $u$ se puede obtener por un algoritmo iterativo. Dado un umbral $u_i$, se define el siguiente umbral como \[ u_{i + 1} = \frac{\avg{A_i} + \avg{B_i}}{2}\], donde $\avg{A_i}, \avg{B_i}$ son los conjuntos de valores por encima y por debajo respectivamente del umbral $u_i$. El proceso se para cuando la diferencia entre umbrales es menor que uno. El primer umbral $u_0$ se suele tomar como la mitad del rango del histograma, esto es, \[ u_0 = \frac{\max ψ - \min ψ}{2} \]

El umbral se puede seleccionar de forma adaptativa considerando una división de ventanas de la imagen.

Una posibilidad es considerar que la distribución del histograma es una normal bimodal. En ese caso, la PDF es \[ p(r) = P_a p_a(r) + P_b p_b(r) \], siendo $P_a, P_b$ coeficientes de ajuste con $P_a + P_b = 1$. El umbral se definiría como \[ u = \frac{μ_a + μ_b}{2} + \frac{σ_b^2}{μ_b - μ_a} \log \frac{P_a}{P_b} \]

\subsubsection{Negativo, seccionado y extracción de bits}

El negativo es simplemente invertir el histograma, con una transformación $s = T(r) = L - 1 - r$.

El seccionado pretende extraer una determinada banda de niveles sobre el resto. Puede ser extracción o resalte. Respectivamente: \[ s = T_E(r) =
\begin{cases}
s_a & r ∉ [a,b] \\
s_b & r ∈ [a,b]
\end{cases} \qquad
s=T_R(r) = \begin{cases}
r & r ∉ [a,b] \\
s_b & r ∈ [a,b]
\end{cases}
\]

La extracción de bits hace lo que uno podría pensar basándose en el nombre. Los valores cuyo bit $n$-ésimo vale $1$ se transforman a $s_b$, y el resto a $s_a$.

\subsubsection{Compresión logarítmica de rango}

La compresión logarítmica de rango viene dada por \[ s = T(r) = (L - 1) \left(\frac{r}{L - 1}\right)^{\frac{1}{γ}} \]

Si $γ > 1$, se trata de una corrección Gamma, y si $γ < 1$, es corrección Gamma inversa (la respuesta de un monitor CRT, por ejemplo).

\subsection{Aspectos operativos}

Al trabajar con valores discretos, tenemos los problemas habituales de redondeos y truncamientos. Pero además tenemos el problema de que hay que hacer $M × N$ cálculos y operaciones, a pesar de que sólo tenemos $L$ posibles valores de enntrada. En la práctica, se puede o bien modificar la VLT (Video Lookup Table, una tabla de $L$ entradas que relaciona un índice en la imagen con su valor real de colores) o montar una Lookup Table (LUT) con todos los posibles valores de entrada y salida y utilizarla para calcular los píxeles de salida de la imagen final sin tener que rehacer los cálculos.

\subsection{Operaciones aritméticas y lógicas}

Se trata de operaciones aritméticas simples sobre dos imágenes. La resta de imágenes permitirá ver errores, eliminar ruido, detectar movimiento o detectar objetos nuevos o desaparecidos. La suma se puede usar para eliminar ruido de media 0 con un promedio de $N$ imágenes, o también para gestionar transparencias

Las operaciones lógicas también se pueden usar, como por ejemplo, con el operador \textit{AND} para obtener regiones de interés de una imagen.

\section{Operadores LSI}

Los operadores lineales e invariantes con el espacio efectúan transformaciones sobre entornos de píxeles dados. Para eso, se usa una ventana deslizante. Este tipo de operaciones tienen un trasfondo de tratamiento de señal, y vamos a verlo formalmente. Dada una transformación $\appl{T}{M_{2a × 2b}}{[0,L]}$\footnote{$M_{i×j}$ es el espacio de matrices de tamaño $i × j$.} que considera entornos rectangulares $A$ de ancho $2a$ y alto $2b$, podemos considerar la respuesta a un impulso unidad $δ$ de tamaño $2a × 2b$, donde $δ[0,0] = 1$ y vale $0$ en el resto de puntos. Esa respuesta viene dada por $h = T(δ)$. La principal ventaja de esto es que al ser $T$ una transformación lineal, podemos definirla únicamente con esa respuesta, ya que cualquier entorno de la imagen origen será una combinación lineales de deltas desplazadas. Así, la transformación completa $\appl{τ}{M_{M×N}}{M_{M×N}}$ de toda la imagen será \[ θ = τ(ψ) = ψ \ast h \], donde $\ast$ es la convolución, que se define para cada píxel de la siguiente forma: \[ θ[n,m] = \sum_{k=-a}^a \sum_{l = -b}^b ψ[n + k, m + l] \cdot h[-k, -l] \]

Por comodidad, consideraremos $w[n,m] = h[-n, -m]$, invirtiendo la respuesta al impulso $h$ y evitándonos que hacer la inversión en cada cálculo.

Al usar la convolución, podemos irnos al dominio frecuencial. Como notación, las letras minúsculas se referirán al dominio espacial y en mayúsculas al dominio frecuencial, pasando de una a otra con la transformada discreta de Fourier. En el dominio frecuencial, la convolución es un producto y por lo tanto \[ Θ = ψ \cdot H \], bastante más fácil de computar.

El problema de este tipo de filtros es que nos comemos los bordes al no tener suficientes píxeles para valorar. Para ello, se puede realizar \textit{padding} aumentando la imagen original. Para rellenar esos píxeles adicionales, se pueden usar valores fijos, hacer una simetría, extender el bore con los mismos valores o hacer una réplica circular con el otro lado de la imagen.

La convolución también tiene la propiedad de separabilidad: si nuestro kernel $w$ se puede expresar como $w = x \otimes y$, siendo $\otimes$ el producto de Kronecker y $x,y$ vectores fila y columna respectivamente, entonces aplicar $w$ es lo mismo que aplicar $x$ y después $y$.

Normalmente, para no modificar el valor medio de la luminosidad de la imagen, ponderaremos $w$ para que la suma de sus elementos sea $1$.

\subsection{Tipos de filtros}

Un filtro simple es la correlación: si ponemos como máscara un objeto, la imagen final destacará las zonas donde ese mismo objeto aparezca. El resto son más complejos y merecen secciones aparte.

\subsubsection{Suavizado}

El suavizado es un filtro paso bajo en frecuencia. Sirve para suavizar o emborronar, y se distingue por tener todos los elementos positivos (realiza una media de los píxeles colindantes).

El promedio puede ser ponderado o no ponderado. Un caso especial del ponderado es el binomial, donde consideramos vectores $B_n$ con los coeficientes binomiales, esto es, con $B_n[i] = {n \choose i}$, y con el filtro $w_n = \frac{1}{2^n} B_n \otimes \trans{B_n}$.

El suavizado también se puede hacer con un filtro de mediana, que suele ser más efectivo filtrando ruido tipo sal y pimienta.

\subsubsection{Realce de contornos}

El realce de contornos se relaciona con la primera y segunda derivada direccionales, versión discreta:
\[ \dpa{f[n]}{n} = f[n+ 1] - f[n]\qquad \pd[2]{f[n]}{n} = f[n+1] + f[n-1] - 2f[n]\]

El estudio de la laplaciana (la suma de las derivadas segundas en las dos direcciones) \[ \nabla^2 ψ = \pd[2]{ψ}{x} + \pd[2]{ψ}{y} \] nos da dos posibles máscaras: la laplaciana $L'$ y la isótropa $I'$, que realzan los bordes:
\[
L' = \begin{pmatrix} 0 & 1 & 0 \\ 1 & -4 & 1 \\ 0 & 1 & 0 \end{pmatrix}
\qquad
I' = \begin{pmatrix} 1 & 1 & 1 \\ 1 & -8 & 1 \\ 1 & 1 & 1 \end{pmatrix}
\]

Normalmente, lo que se quiere hacer es obtener los bordes sin el resto de la imagen, así que las máscaras finales que hacen eso se pueden obtener restando esas máscaras a un único punto obteniendo lo siguiente:
\[
L = \begin{pmatrix} 0 & -1 & 0 \\ -1 & 5 & -1 \\ 0 & -1 & 0 \end{pmatrix}
\qquad
I = \begin{pmatrix} -1 & -1 & -1 \\ -1 & 9 & -1 \\ -1 & -1 & -1 \end{pmatrix}
\]

La Laplaciana nos da la segunda derivada. Para la primera (el gradiente) tenemos varios operadores posibles: Roberts, Prewitt, Sobel y diagonales:
\begin{gather*}
R_1 = \begin{pmatrix} - 1 & 0 \\ 0 & 1 \end{pmatrix} \qquad R_2 = \begin{pmatrix}0 & -1 \\ 1 & 0 \end{pmatrix} \\
P_v = \frac{1}{6} \begin{pmatrix} -1 & -1 & -1 \\ 0 & 0 & 0 \\ 1 & 1 & 1 \end{pmatrix};\,P_h = \trans{P_v}
\qquad
S_v = \frac{1}{8} \begin{pmatrix} -1 & -2 & -1 \\ 0 & 0 & 0 \\ 1 & 2 & 1 \end{pmatrix};\,S_h = \trans{P_v} \\
D^1_1 = \frac{1}{6} \begin{pmatrix} 0 & 1 & 1 \\ -1 & 0 & 1 \\ -1 & -1 & 0 \end{pmatrix};\,D^1_2 = \trans{D^1_1} \qquad
D^2_1 = \frac{1}{8} \begin{pmatrix} 0 & 1 & 2 \\ -1 & 0 & 1 \\ -2 & -1 & 0 \end{pmatrix};\,D^2_2 = \trans{D^2_1}
\end{gather*}

El realce también se puede hacer con filtros diseñados en frecuencia, más concretamente filtros paso alto.

\paragraph{Detección de bordes} La detección de bordes se puede hacer con el realce de contornos. El problema es que los operadores que hemos visto son muy sensibles al ruido, especialmente la laplaciana. Por eso, muchas veces se hace una gaussiana antes (Laplacian of Gaussian)

\subsection{Transformaciones geométricas}

Las transformaciones geométricas mueven los píxeles, normalmente sin cmabiar su valor. Tenemos las transformaciones afines usuales (rotación, escalado, desplazamiento e inclinación) dadas por las respectivas matrices \[
R = \begin{pmatrix} \cos φ & - \sin φ & 0 \\ \sin φ & \cos φ & 0 \end{pmatrix}\quad
E = \begin{pmatrix} ρ_x & 0 & 0 \\ 0 & ρ_y & 0 \end{pmatrix} \quad
D = \begin{pmatrix} 1 & 0 & d_x \\ 0 &  1 & d_y \end{pmatrix} \quad
I = \begin{pmatrix} 1 & -i_x & 0 \\ i_y &  1 & 0 \end{pmatrix} \quad
\]

Para hacer las transformaciones afines, normalmente añadimos un ``1'' al final del vector columna $(x y)$ para poder tener en cuenta los desplazamientos.

Las transformaciones nos permiten corregir distorsiones de la imagen, como la corrección radial o las perspectivas usando proyección geométrica.

\section{Operadores morfológicos}


Los operadores morfológicos implican un tipo de selección distinto al que hemos visto en los LSI. Consideramos dos operadores, supremo ($\vee$) e ínfimo ($\wedge$) y el punto $δ_L(n)$ que vale $0$ cuando $n = 0$ y $- ∞$ en otro caso. Así, una señal cualquiera $x$ se puede representar como \[ x[n] = \bigvee_{k=-∞}^∞ x[k] + δ_L[n-k]\]

\subsection{Dilatación y erosión}

Con el supremo y el ínfimo podemos definir dos operaciones similares a la convolución: la dilatación ($\oplus$) y la erosión ($\ominus$): dado un elemento estructurante $b$, definimos ambos operadores de la siguiente forma para cada punto $y[n]$: \begin{align*}
y[n] = x[n] \oplus b &= \bigvee_{k=-∞}^∞ (x[k] + b[n-k]) \\
y[n] = x[n] \ominus b &= \bigwedge_{k=-∞}^∞ (x[k] + b[n-k])
\end{align*}

\begin{figure}[hbtp]
\centering
\includegraphics[width=1\textwidth]{img/DilatacionErosion.png}
\caption{Efecto de los operadores dilatación y erosión}
\label{fig:DilatacionErosion}
\end{figure}

En la práctica, la dilatación asigna a un píxel el máximo de todos los píxeles de su entorno que coinciden con los ceros del elemento estructurante invertido. Análogamente, la erosión hace lo mismo pero poniendo el mínimo. El efecto de ambos operadores se puede ver en la \fref{fig:DilatacionErosion}.

La dilatación y la erosión son distributivas con respecto al ínfimo y al supremo:
\begin{align*}
(x \vee y) \oplus b &= (x\oplus b) \vee (y \oplus b) \\
(x \wedge y) \ominus b &= (x\ominus b) \wedge (y \ominus b)
\end{align*}

La composición funciona con la dilatación, pero no del todo bien con la erosión:
\begin{align*}
x \oplus a \oplus b &= x \oplus (a \oplus b) \\
x \ominus a \ominus b &= x \ominus (a \oplus b)
\end{align*}

Si el elemento estructurante incluye el origen, la dilatación es extensiva (los píxeles de salida tienen valor mayor o igual que la entrada) y la erosión antiextensiva (los píxeles de salida tienen valor menor o igual que la entrada).

La dilatación puede reparar objetos, cerrando bordes y quitando imperfecciones (mordidas). La erosión también puede quitar imperfecciones (salidas) y además separar objetos. Ambos operadores pueden ayudar a simplificar imágenes.

\subsubsection{Gradientes morfológicos}

Podemos sacar gradientes por dilatación ($x \oplus b - x$), por erosión ($x - x \ominus b$) o morfológicos ($x\oplus b - x \ominus b$). Si usamos imágenes binarias, deberíamos usar la operación \textit{XOR}.

\subsection{Apertura y cierre}

\begin{figure}[hbtp]
\centering
\includegraphics[width=\textwidth]{img/AperturaCierre.png}
\caption{Efecto de los operadores apertura y cierre.}
\label{fig:AperturaCierre}
\end{figure}

Podemos componer los dos operadores básicos para obtener la apertura $γ_b$ y el cierre $φ_b$: \begin{align*}
γ_b &= (x \ominus b) \oplus b \\
φ_b &= (x \oplus b) \ominus b
\end{align*}

El efecto de estos operadores se puede ver en la \fref{fig:AperturaCierre}. La apertura ``abre'' los huecos negros y el cierre ``cierra'' esos mismos agujeros. La apertura es antiextensiva y el cierre extensivo, independientemente de $b$. Además, son idempotentes: sucesivas aplicaciones dan el mismo resultado ($γ(γ(x)) = γ(x)$).

Estos operadores se pueden usar para eliminar objetos de acuerdo a su nivel de gris y de su estructura.

\subsubsection{Detección de objetos}

Se pueden buscar elementos que aparecen como máximos relativos con $x - γ(x)$  (top-hat) y mínimos relativos como $φ(x) - x$ (dual top-hat).

\subsubsection{Granulometría}

\begin{figure}[hbtp]
\centering
\includegraphics[width=0.9\textwidth]{img/Granulometria.png}
\caption{Granulometría: aplicación sucesiva de aperturas y cierres para detectar cambios en el área.}
\label{fig:Granulometria}
\end{figure}

Aplicando sucesivos bancos de aperturas y cierres (\fref{fig:Granulometria}) con elementos estructurantes crecientes podemos llegar a detectar el tamaño de los objetos.

\subsubsection{Filtrado por reconstrucción}

\begin{figure}[hbtp]
\centering
\includegraphics[width=0.9\textwidth]{img/AperturaReconstruccion.png}
\caption{Resultados de la apertura por reconstrucción.}
\label{fig:AperturaReconstruccion}
\end{figure}

Se usan para hacer simplificaciones preservando contornos. Se coge la imagen de entrada $x$, un marcador $y$ y un elemento estructurante que definirá la conectividad de los elementos que mantiene. Se define entonces la apertura y cierre por reconstrucción como un proceso iterativo\begin{align*}
γ^r_i &= (γ^r_{i-1} \otimes b) \wedge x \\
φ^r_i &= (φ^r_{i-1} \ominus b) \vee x
\end{align*} con $γ^r_0 = φ^r_0 = y$. Las operaciones completas se hará con $i \to ∞$, pero por comodidad denotaremos los operadores como $γ^r \equiv γ^r_{∞}$ y $φ^r \equiv φ^r_{∞}$

Tanto la apertura como cierre por reconstrucción tienen las mismas propiedades de extensividad e idempotencia que sus versiones sin reconstrucción.

Normalmente, para el marcador se usarán versiones modificadas de la imagen de entrada, de tal forma que podremos emular la dilatación y erosión sin perder bordes. La apertura mediante reconstrucción de la erosión eliminará ruido más claro que el resto de la imagen, por ejemplo.

También se puede usar este tipo de filtrado para rellenar agujeros, usando un marcador que sea igual a toda la imagen y un cierre por reconstrucción. Un top-hat con la apertura por reconstrucción ($x - γ^r (x;y)$) nos quitará objetos parcialmente visibles si usamos un marcador que sea 1 únicamente en los bordes.

La apertura por reconstrucción de la erosión facilitará el suavizado sin perder los contornos, y el cierre mediante reconstrucción de la dilatación nos permitirá eliminar ruido o elementos pequeños más oscuros sin perder los contornos originales.

\section{Extracción de características}

En este tema trataremos de extraer patrones y características de las imágenes. Una cosa que tendremos que tener en cuenta es la escala de lo que estamos detectando. Esto será lo que se llama el análisis multiescala.

\subsection{Primeras aproximaciones: quad-tree y pirámides}

\begin{figure}
\centering
\includegraphics[width=0.8\textwidth]{img/QuadTree.png}
\caption{Representación en quad-tree de una imagen usando criterios de homogeneidad.}
\label{fig:QuadTree}
\end{figure}

Una primera aproximación es la representación en quad-tree (\fref{fig:QuadTree}): seleccionamos un criterio de homogeneidad, un umbral de decisión y un tamaño mínimo de región; y vamos subdividiendo hasta llegar al tamaño mínimo construyendo un árbol por el camino. Salvo esas últimas regiones, el resto cumplen los criterios de homogeneidad.

La segunda aproximación es el uso de repreentaciones piramidales, donde vamos suavizando y diezmando la imagen original para reducir su tamaño. No es un esquema muy formal pero es rápido y computacionalmente barato.

Una ventaja de usar las pirámides es que podemos mandar una imagen en distintas resoluciones sin tener que enviar todos los datos: podemos enviar la imagen original y las diferencias entre niveles, que nos ocuparán menos píxeles.

Podemos usar pirámides paso bajo, donde convolucionamos con un kernel y después reducimos el tamaño a la mitad. El kernel tiene que ser positivo, simetrico, monomodal, de área unidad. Para $N=3$, sólo el dado por $c[n] = \set{\frac{1}{4}, \frac{1}{2}, \frac{1}{4}}$ lo cumple. Para $N=5$, son de la forma \[ c[n] = \set{\frac{1 - 2a}{4}, \frac{1}{4}, \frac{a}, \frac{1}{4}, \frac{1 - 2a}{4}} \]

El siguiente paso es usar las pirámides paso-banda (DoLP, \textit{difference of low-pass pyramids}) en general o, en el caso que más nos interesará, la pirámide laplaciana o DoG \textit{Difference of Gaussians} cuando la pirámide paso-bajo sea Gaussiana (la principal ventaja de usar kernels gaussianos es que minimizamos la aparición de estructuras por \textit{aliasing}). La idea de la pirámide laplaciana es que cada nivel es una aproximación al LoG de la imagen a esa escala.

\subsection{Espacio escala}

En el espacio escala, en lugar de reducir el tamaño de la imagen, lo que haremos será suavizar, y además podremos tomar la variable de suavizado continua. Veremos que usando el kernel gaussiano, tendremos una aproximación perfecta de la LoG. También veremos que es invariante a traslaciones. Por último, el espacio escala no introduce artefactos, estructuras que no existiesen en la imagen.

\begin{defn}[Representación\IS espacio escala] Dada una señal escalar de dimensión $N$ $\appl{ψ}{ℝ^N}{ℝ}$, su representación espacio escala es una aplicación $\appl{L}{ℝ^N × ℝ^+}{ℝ}$ dada por \begin{align*}
L(\vx, 0) &= ψ(\vx) \\
L(\vx, t) &= g(\vx, t) \ast ψ(\vx)
\end{align*} donde $g(\vx, t)$ es la gaussiana dada por \[ g(\vx, t) = \frac{1}{2πt}e^{-\frac{\norm{\vx}}{2t}}\]
\end{defn}

El kernel gaussiano es el único que hace que se cumpla, entre otras cosas, la siguiente ecuación: dados $t_2 > t_1$ entonces \( L(\vx, t_2) = g(\vx, t_2 - t_1) \ast L(\vx, t_1) \label{eq:ScaleSpaceGroup} \) ya que se cumple que \[ g(\vx, σ_1^2) \ast g(\vx, σ_2^2) = g(\vx, σ_1^2 + σ_2^2) \]

El tamaño de la gaussiana que se aplica determina el tamaño de muestreo: un tamaño más pequeño nos dará más precisión en la escala pero costará más calcular.

Otra propiedad útil del espacio escala es que, dadas dos imágenes concretas $I_1, I_2$ y dos filtros gausianos $G_1$, $G_2$ con desviaciones típicas $σ_1, σ_2$ respectivamente, entonces las imágenes $L\ast(G_1 \ast I_1)$ (la laplaciana de la gaussiana) se parece más a $G_1 \ast I_1 - G_2 \ast I_2$ cuanto más se parezcan $I_2$ e $I_1$ y más se parezcan las desviaciones típicas $σ_1,σ_2$.

Más concretamente, y con algo un poco más precisión porque lo de antes es lo menos matemático que he visto en años, si tenemos una imagen $I$ y un kernel gaussiano $G_1$; entonces $L\ast(G_1 \ast I) = (G_1 \ast I) - (G_1 \ast (G_1 \ast I))$ o, en otras palabras, la laplaciana de la gaussiana es igual (salvo un factor de escala) a la diferencia entre las imágenes resultantes de aplicar una y dos veces el kernel $G_1$ a la imagen $I$.

Este hecho nos permitirá obtener la representación en espacio escala de la imagen aplicando iterativamente convoluciones con un kernel fijo, mucho más barato computacionalmente que usando kernels cada vez más grande.

Un hecho importante del espacio escala cumple la ecuación del calor, dada por \[ ∂_t L = \frac{\nabla^2 L}{2} = \frac{1}{2} \sum_{i=1}^N \pd[2]{L}{x_i}\]

Esta propiedad permite inferir que esta representación no realza máximos ni mínimos, y además nos da una pseudodemostración de por qué la LoG y la DoG son equivalentes:
\[ \nabla^2 L(\vx,t) \simeq L(\vx,t + Δt) - L(\vx, t - Δt) \]

También podremos derivar haciendo una ligera perversión matemática: \[ ∂_{x^my^n} L(\vx, t) = ∂_{x^my^n} \left(g(\vx,t) * ψ(\vx)\right) = \left(∂_{x^my^n} g(\vx, t)\right) * ψ(\vx) \], cuya principal ventaja es que las propiedades del espacio escala se trasladan a sus derivadas, por ser la derivada de la gaussiana también una gaussiana.

Este hecho nos permitirá hacer detección inicial de características básicas (blobs), pudiendo detectar también en qué escala se producen. Hay tres métodos para esto: los basados en la LoG, en la DoG, y en el determinante del Hessiano (DoH).

\subsubsection{LoG - Laplacian of Gaussian}

Podemos considerar la laplaciana en las direcciones espaciales. Simplificando y tomando $L_{xx} = \pd[2]{L}{x}$, diremos que  \[ \nabla^2 L = L_{xx} + L_{yy}\], que tomará grandes valores negativos y positivos para bloques claros y oscuros de anchura $2 \sqrt{t}$. La respuesta, eso sí, dependerá de la escala y decrece según esta aumenta, así que para evitarlo se normaliza con respecto de la escala,  de tal forma que nos quedará \[ \nabla^2_{\text{norm}} = t(L_{xx} + L_{yy}) \]

En ese operador buscaremos máximos y mínimos locales, que nos darán puntos y escalas de las regiones más claras y oscuras, respectivamente, del espacio escala.

\subsubsection{DoG - Difference of Gaussians}

En este caso simplemente hacemos lo mismo que en el LoG, pero implementando el operador laplaciano como \[ \nabla^2_{\text{norm}} L(\vx, t) \simeq \frac{t}{Δt} \left(L(\vx, t + Δt) - L(\vx, t - Δt)\right)\] y buscando máximos y mínimos locales.

\subsubsection{DoH - Determinant of Hessian}

El Hessiano de la imagen ψ será una amtriz cuadrada con las segundas derivadas parciales de la función. Su cálculo en puntos críticos (gradiente nulo) y el posterior estudio de los autovalores nos dirá el tipo de punto: autovalores positivos implican mínimo, negativos implican máximo, y mixtos punto de silla.

Al ser el determinante el producto de autovalores, sólo tendremos que calcular eso: si es positivo, es máximo o mínimo, y si es negativo estamos en un punto de silla. Así, sólo tenemos que buscar los máximos locales en el determinante normalizado: \[ \det H_{\text{norm}}L(\vx, t) = t^2 (L_{xx} L_{yy} - L^2_{xy})\]

\begin{figure}[hbtp]
\centering
\includegraphics[width=0.8\textwidth]{img/Autovalores.png}
\caption{Interpretación geométrica de los autovalores viendo en qué transforman la bola unitaria.}
\label{fig:Autovalores}
\end{figure}

Los autovalores tienen una interpretación geométrica (ver \fref{fig:Autovalores}) que no se por qué podría resultar interesante pero que en cualquier caso es bastante obvia.

\subsection{Detección de puntos de interés}

La idea es buscar de forma precisa y robusta a ruido puntos invariantes a transformaciones típicas en la imagen. Hay varios métodos que pasamos a ver.

\subsubsection{Detector de Harris-Stephens}

Este detector se basa en un primer enfoque de fuerza bruta: cogemos una ventana en la imagen, la desplazamos un poquito y vemos en qué dirección cambia menos. Si no cambia en ninguna dirección, tenemos una zona plana. Si cambia muy poco en dos direcciones, tenemos un borde. Si cambia mucho en todas, probablemente estemos ante una esquina. Ese cambio se puede definir usando la suma de cuadrados de las diferencias.

Vamos a hacerlo algo formalmente. Tenemos nuestra imagen $\appl{ψ}{ℝ^2}{ℝ}$ y una ventana $W ⊂ ℝ^2$. Entonces, el error cuando desplazamos en la dirección del vector $\vu$ será \[ E(\vu) = \sum_{\vx ∈ W} \left(ψ(\vx + \vu) - ψ(\vx)\right)^2 \]

\begin{wrapfigure}{r}{0.4\textwidth}
\centering
\includegraphics[width=0.32\textwidth]{img/StructureTensorEigenvalues.png}
\caption{Ilustración de los posibles casos según los autovalores de $A$}
\label{fig:StructureTensor}
\vspace{-10pt}
\end{wrapfigure}

Para desplazamientos pequeños, podremos aproximar la diferencia por el gradiente \[ ψ(\vx + \vu) \simeq ψ(\vx) + \vu · \grad ψ(\vx) \] de tal forma que nos queda \[ E(\vu) = \sum_{\vx ∈ W} \left(\vu \grad ψ(\vx)\right)^2 = \vu A \trans{\vu} \] con $A$ el tensor de estructura dado por \[ A =  \sum_{\vx ∈ W} \begin{pmatrix} ψ_x^2 (\vx) & ψ_x (\vx) · ψ_y (\vx) \\ ψ_x (\vx) · ψ_y (\vx) & ψ_y^2 (\vx) \end{pmatrix} \] (ojo: sólo primeras derivadas).

El estudio de autovalores y autovectores de $A$ nos dirá para dónde cambia la imagen y si tenemos un borde, esquina o región plana (\fref{fig:StructureTensor}). Sin embargo, como calcular los autovalores es computacionalmente caro, lo que se hace es hacer una estimación con la función de respuesta a una esquina, dada por \[ R(\vx) = λ_1λ_2 - κ (λ_1 + λ_2)^2 = \det A - κ \tr^2 A \] con $κ ∈ [0.04, 0.15]$ un parámetro que se ajusta experimentalmente.

Este enfoque es invariante a rotaciones y cambios de iluminación globales, pero tenemos el problema de la selección de κ y del umbral de $R$. Además, no es invariante a cambios de escala, y detecta más bien regiones que puntos de interés.

En la práctica, para calcularlo se sacan las imágenes de gradientes horizontales y verticales, se sacan las imágenes para la información del gradiente del tensor de estructura y por último se hace un filtrado gaussiano para obtener en cada píxel el tensor de estructura correspondiente de su entorno.

En otras palabras, lo que hace es estimar en el entorno de cada píxel la magnitud relativa del gradiente, en su dirección de máxima variación, con respecto a la que presenta en la dirección perpendicular.

Este detector se podría ampliar para localizar máximos/mínimos locales en el espacio escala que también sean máximos-minimos locales del laplaciano (Laplacian-Harris detector).

\subsubsection{SIFT - Scale-Invariant Feature Transform}

\begin{figure}[hbtp]
\centering
\includegraphics[width=0.8\textwidth]{img/SIFT.png}
\caption{Espacio escala híbrido: hacia la derecha suavizamos, hacia abajo diezmamos.}
\label{fig:SIFTScaleSpace}
\end{figure}

Este es un algoritmo avanzado para encontrar puntos de interés y describir su entorno. Es robusto a cambios de escala, traslaciones, rotaciones y cambios de iluminación; aunque no tanto a transformaciones afines y además tiene muchos parámetros a ajustar. Se basa en cuatro pasos: detección inicial de PoIs con la DoG en el espacio escala, localización precisa y descarte de PoIS, identificación de las orientaciones principales del gradiente y descripción del entorno en cada orientación principal.

El primer paso consiste en la obtención de un espacio escala híbrido (\fref{fig:SIFTScaleSpace}), obteniendo después las DoG en cada octava (entre cada suavizado), y se buscan máximos o mínimos locales en cada octava (de nuevo, comparando sólo con los vecinos de cada suavizado).

Una vez hecho esto, se refinan esos puntos de interés. Se mira qué desplazamiento $\vu$ minimiza (o maximiza) el valor de la DoG. Este valor vendrá dado por \[ \vu = - \left(\dpd[2]{D}{\vx}\right)^{-1} \pd{D}{\vx} \]. Si sus componentes son menores que $0.5$ se asume que el extremo está en la posición $\vx + \vu$. Si no, se asume que el extremo está más cerca del vecino correspondiente y se repite el proceso en torno a él.

Posteriormente se eliminan extremos poco contrastados, eliminando los que tengan $\abs{D(\vx)} ≤ 0.03$; y se quitan también los que forman arte de los bordes. Se calculan los Hessianos de esos puntos y se analiza la curvatura de la función $D(\vx)$. Si hay una curvatura con dirección predominante ($r = \sfrac{λ_1}{λ_2} > 10$) se descarta. Para facilitar este cálculo, se usa la traza y determinante del Hessiano y la desigualdad que se aplica es \[ \frac{\tr^2 HD(\vx)}{\det HD(\vx)} = \frac{(r + 1)^2}{r} > \frac{11^2}{10} \]

Sobre los puntos que quedan se computan unos descriptores basados en los ángulos del gradiente. Se coge una ventana de $16 × 16$ píxeles en la escala en la que aparece el punto, y se calcula el ángulo del gradiente en cada píxel con respecto a la orientación detectada del punto de interés. Se descartan los ángulos pequeños y entonces para cada subcelda de $4 × 4$ píxeles se calcula un histograma de las orientaciones que quedan, representadas como flechas de distinta magnitud en las ocho posibles direcciones que se puedan tomar. En la \fref{fig:SIFTDescritors} aparecen unos ejemplos del resultado de esos descriptores.

\begin{figure}[hbtp]
\centering
\includegraphics[width=0.8\textwidth]{img/SIFT_Descriptors.png}
\caption{Descriptores de SIFT sobre dos imágenes.}
\label{fig:SIFTDescritors}
\end{figure}

\end{document}
