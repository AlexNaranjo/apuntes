\chapter{Códigos detectores y correctores de errores}
\section{Motivación}
\begin{example}
Supongamos que tenemos un robot en Marte (o en cualquier lugar muy lejano) y queremos comunicarnos con él, indicándole en cuál de las 4 direcciones naturales queremos que se mueva:
\[\left\{ \begin{array}{l}
N = 00 \\ E = 01 \\ O = 10 \\ S = 11
\end{array}\right.\]

En cualquier momento puede ocurrir un problema que dificulte la recepción del mensje por parte del robot, ya sea el ruido del propio canal de comunicación, o la aparición de un rayo cósmico que modifique algún bit.

El problema de esto es que, en cuanto el robot reciba un par de órdenes perturbadas, habremos perdido el control del mismo, pues no sabremos cuál es su posición.

La \textbf{clave del problema} es que, puesto que todas las palabras de longitud dos son válidas en nuestro código, el robot no puede darse cuenta de si el mensaje ha sido perturbado o no.

Para solventar este problema, una opción es emplear cadenas más largas para codificar cada movimiento, a fin de poder encontrar errores posibles. Por ejemplo, podemos duplicar el código que representa cada posible movimiento.

\[\left\{ \begin{array}{l}
N = 0000 \\ E = 0101 \\ O = 1010 \\ S = 1111
\end{array}\right.\]

Si enviamos al robot el mensaje $0101$ y el robot recibe $0111$, el robot detecta que ha habido un error, pero no puede corregirlo con exactitud.

Por tanto, lo que hará el robot será corregir el mensaje minimizando la probabilidad de fallo, es decir, trata de maximizar la probabilidad condicionada:
\[\mathbb{P} \left(\text{han enviado m} | \text{he recibido }\tilde{m} \right)\]

Esta probabilidad dependerá de las características del canal. De forma general tenemos que un bit muta con probabilidad pequeña $p$ y, consecuentemente, llega de forma adecuada con probabilidad $p-1$. Como hipótesis consideramos que $p$ es pequeño (desde luego $p<\frac{1}{2}$).

Para simplificar el problema supondremos, además, que los errores en los bits son independientes.

Puesto que $p$ es pequeño, es más probable que un bit llegue de forma correcta a que llegue mutado. Por tanto, a la hora de maximizar la probabilidad descrita anteriormente, necesitamos encontrar el mensaje original al que se puede llegar mediante el menor número de cambios posibles.

Así tenemos que, dado $\tilde{m} = 0111$
\[\left\{ \begin{array}{l}
m = N= 0000 \text{ con 3 cambios}\\ m= E = 0101 \text{ con 1 cambio }\\ m= O = 1010 \text{ con 3 cambios}\\ m =S = 1111 \text{ con 1 cambio}
\end{array}\right.\]

Por tanto, lo más probable es que la orden fuese desplazarse hacia el este o el sur, pero no sabemos cuál de las dos es.

\obs Si se produce un sólo error sabemos detectarlo, aunque no corregirlo. No obstante, si se producen dos errores es posible que no seamos capaces ni de detectarlo.

A fin de poder corregir un único error lo que haremos será repetir el código tres veces, de modo que tenemos

\[\left\{ \begin{array}{l}
N = 000000 \\ E = 010101 \\ O = 101010 \\ S = 111111
\end{array}\right.\]

Con este método, seremos capaces de detectar la presencia de hasta dos bits erróneos. 

Supongamos que recibimos el mensaje $\tilde{m}=011101$, mirando, como antes, el código original más probable tenemos:

\[\left\{ \begin{array}{l}
m = N= 000000 \text{ con 4 cambios}\\ m= E = 010101 \text{ con 1 cambio }\\ m= O = 101010 \text{ con 5 cambios}\\ m =S = 111111 \text{ con 2 cambios}
\end{array}\right.\]

En esta ocasión tenemos un \textbf{vecino más próximo} que será el mensaje $m=E$.

Además, si conseguimos garantizar que sólo habrá un error en el mensaje recibido, podemos estar seguros de que hemos encontrado el mensaje original, pues no hay forma de llegar hasta el mensaje que hemos recibido modificando un único bit de alguno de los otros mensajes posibles.

\obs Si se produce un error podemos corregirlo, dos errores podremos detectarlos, a partir de 3 errores es posible que no los detectemos.

Con este mismo esquema, supongamos que recibimos la palabra $\tilde{m} = 11101$. En este caso, como en el anterior, es sencillo detectar que se ha producido un error.

Si empleamos el mismo método que en el caso anterior, tendríamos que el vecino más cercano es $m=S$ pero estamos restingiéndonos a asumir que sólo hay un error. No obstante, el mensaje podría haber sido $m=E$ y encontrarnos con dos errores.

\obs O asumimos que sólo hay un error y lo corregimos o aceptamos que puede haber dos y no podemos corregirlo

\end{example}

Veamos ahora cómo generalizar este tipo de problemas y la solución al mismo.

\begin{defn}[Código q-nario]
Dado un alfabeto con $q$ elementos, $F_q$, un código (de bloques) q-nario de longitud $n$ es $\algb{C}\in F_q^n$.

Los números de teléfono de españa, por ejemplo, constituyen un código 10-ario de longitud 9: $F_{10}^9$
\end{defn}

De manera general, siendo $|\algb{C}| = M$ querremos encontrar un código con $M$ grande, para tener más ordenes posibles para el robot, con un $n$ pequeño, para que no tenga que enviar demasiada información y que permita corregir muchos errores.

\obs Todo a la vez es imposible. \textbf{La teoría de códigos trata de hacerlo lo mejor posible.} 

\section{Distancia de Hamming}
\begin{defn}[Distancia de Hamming]
Dados $x=(x_1,...,x_n)$, $y=(y_1,...,y_n) \in F^n$ se define la \textbf{Distancia de Hamming} como
\[d(x,y)=\#\{i: x_i \neq y_i\}\] 
\end{defn}

\begin{prop}[$d$ es una distancia]
Puesto que somos matemáticos debemos convencernos de la que la función $d$ definida es realmente una distancia. Para ello comprobamos que se cumplen las propiedades de una distancia:
\begin{enumerate}
\item 
\[d(x,y) \geq 0\]

Puesto que estamos mirando el cardinal de un conjunto, trivialmente esta propiedad es cierta

\item
\[d(x,y)=0 \iff x=y\]

Puesto que estamos contando el número de bits diferentes, si el número es 0, evidentemente las dos cadenas son la misma

\item 
\[d(x,y)=d(y,x)\]

Esta propiedad es trivial puesto que el operador $\neq$ es conmutativo

\item 
\[d(x,y) \leq d(x,z)+d(z,y)\]

Tenemos que comparar el número de cambios necesarios para convertir $x$ en $z$ y luego $z$ en $y$. Si tomamos $d=d_H$, donde $d_H$ es la distancia mínima, la desigualdad es obvia.

\begin{defn}[Distancia mínima]
Dado un código $\algb{C}\in F_q^n$, su \textbf{distancia mínima} se define como:
\[d(\algb{C}) = \min\{d(x,y) \ t.q. \ \ x,y \in \algb{C}, \ x \neq y\}\]
\end{defn}
\end{enumerate}
\end{prop}

Si utilizamos $\algb{C} \subset F_q^n$ con $|G|=M$ para transmitir $M$ mensajes, lo que queremos es que las palabras empleadas esten lo más espaciadas posible, de modo que un pequeño cambio en un elemento de una palabra no nos haga caer en otra palabra válida. Es decir, queremos que la distancia mínima del código, es decir, $d(\algb{C})$ sea grande.

\begin{theorem}
Sea $\algbC \subset F_q^n$ un código con $d(\algb{C})=d$, podemos \textbf{detectar} hasta $d-1$ errores 
\end{theorem}
\begin{proof}
El procedimiento estándar para detectar errores es:
\begin{enumerate}
\item Recibo $\tilde{x} \in F_1^n$

\item Si $\tilde{x}  \in \algb{C}$ seguimos trabajando. 

\item Si $\tilde{x} \notin  \algb{C}$ \textbf{PITO}.
\end{enumerate}

Si el número de errores es menor o igual que $d-1$, es imposible que desde un $x\in \algb{C}$ llegemos a otro $\tilde{x} \in \algb{C}$, puesto que, por definición, $d(x,\tilde{x})\geq d$.

Es decir, el algoritmo descrito nunca podrá confundirse si tenemos un máximo de $d-1$ errores.
\end{proof}

\begin{theorem}
Sea $\algb{C} \subset F_q^n$ un código con $d(\algb{C})=d$, podemos \textbf{corregir} hasta $\floor{\frac{d-1}{2}}$
\end{theorem}
\begin{proof}
Desde el punto de vista geométrico (o topológico) podemos ver que, dados dos mensajes $x,y \in \algb{C}$
\[B\left(x,\floor{\frac{d-1}{2}}\right) \cap B\left(y,\floor{\frac{d-1}{2}}\right) = \emptyset\]

Apoyándonos en esta idea, veamos la demostración del teorema.

El procedimiento para corregir errores es:
\begin{enumerate}
\item Dado un mensaje $\tilde{x} \notin \algb{C}$, a fin de corregir el error buscamos $x_0$ tal que
\[d(\tilde{x},x_0)=\min\{d(x,\tilde{x}): \ x\in \algb{C}\}\]

\item Si tenemos un empate, tomamos uno al azar.
\end{enumerate}

Este algoritmo podría fallar si y sólo si 
\[\exists x_0 \in \algb{C} \tq d(x_0,\tilde{x}) \leq d(x,\tilde{x})\]
Pero en este caso tenemos que
\[d(x,x_0) \leq d(x,\tilde{x})+d(\tilde{x},x_0) \leq 2d(x,\tilde{x}) \leq 2 \floor{\frac{d-1}{2}} \leq d-1 < d\]
con lo que llegamos a una contradicción, pues por definición $d(x,x_0)\geq d$.
\end{proof}

\begin{example}
Sea el lenguaje

\[\left\{ \begin{array}{l}
N = 000000 \\ E = 010101 \\ O = 101010 \\ S = 111111
\end{array}\right.\]

Supongamos que se envía ``S=111111'' y recibimos ``010111'' y empleamos un algoritmo de corrección de errores.

En este caso veremos que el mensaje más cercano es ``E=010101'' que implica un único error en el mensaje.

No obstante, en este caso el resultado es incorrecto pues en realidad se produjeron 2 errores.
\end{example}

Con lo visto hasta ahora, es evidente que la forma de conseguir detectar/corregir más errores pasa por emplear códigos con distancias mínimas grandes.

En general, podemos crear códigos con distancias mínimas tan grandes como queramos.

\begin{defn}[Codigo binario de repetición de longitud n]
Este código consiste en repetir cada bit n veces.
\[R_{2,n}=\{\underbrace{111....111}_{n\text{ veces}}, \underbrace{000....000}_{n\text{ veces}}\}\]

Si $F_q=\{a_1,...,a_q\}$ es un alfabeto, el \textbf{código de repetición de longitud n} es:
\[R_{2,n}=\{\underbrace{a_1a_1a_1....a_1a_1a_1}_{n\text{ veces}}, ... , \underbrace{a_qa_qa_q....a_qa_qa_q}_{n\text{ veces}}\}\]
\end{defn}

Podemos ver que las distancias mínimas para un código de repetición de longitud n será siempre $d(R_q,n)=n$.

El problema de este método es que acaba siendo muy costoso cuando queramos hacer $n$ grande. 

Para poder clasificar y estudiar la calidad de los diferentes códigos debemos definir algunos términos:

\begin{defn}[Distancia relativa]
Sea $\algb{C} \subset F_1^n$ con $d(\algb{C})=d$ y $|\algb{C}|=M=q^l$ definimos la \textbf{distancia relativa} como:
\[δ=\frac{d}{n}\]
\end{defn}

\begin{defn}[Tasa de transmisión]
Sea $\algb{C} \subset F_1^n$ con $d(\algb{C})=d$ y $|\algb{C}|=M=q^l$ definimos la \textbf{tasa de transmisión} como:
\[R=\frac{l}{n}\]
\end{defn}

\begin{example}
En un código de repetición siempre tendremos una distancia relativa igual a 1 y una tasa de transmisión inversamente proporcional a $n$. Es decir:
\[δ=1, \ \ R = \frac{1}{n}\]
\end{example}

\obs Dado un código con más de una palabra\footnote{Si sólo tenemos una palabra es un caso trivial}, es decir $|\algb{C}|>1$ tenemos:
\[0 < δ,R \leq 1\]

En general intentaremos que tanto δ cono $R$ sean cercanos a 1. No obstante, es imposible que ambas tiendan a 1 a la vez por lo que se tratará de optimizar, dependiendo de la situación concreta en que vayamos a trabajar.

\begin{defn}[Código lineal]
Diremos que un código $\algb{C}$ es un \textbf{código lineal} si:
\begin{enumerate}
\item $F_q$ es un cuerpo [o un anillo conmutativo si uno sabe lo que es un módulo sobre un anillo]

\item $\algb{C}$ es un espacio vectorial de $F_q^n$ [o es un submódulo si $F_q^n$ es un anillo]
\end{enumerate}
\end{defn}

\begin{example}
Todos los códigos que hemos visto hasta ahora son \textbf{códigos lineales}.

\[\algb{C}_2 = \{\vx\in F_2^4: x_1-x_3 = 0 , \  x_2-x_3 = 0\}\]
\[\algb{C}_3 = \{\vx \in F_3^6: x_1=x_3=x_5, \ x_2=x_4=x_6\}\]
\end{example}

\obs Sea $|\algb{C}|=M=q^l$ tenemos que $l=k=dim_{F_q}(\algb{C})$

\begin{defn}[Peso]
Si $F_q$ es un cuerpo tal que $x\in F_q^n$ llamamos \textbf{peso de x} a:
\[w(x) = \# \{i: \ x_i \neq 0\}\]
Es decir, dada una palabra nos cuenta la cantidad de 1s.
\end{defn}

\begin{defn}[Peso mínimo de $\algb{C}$]
\[w(\algb{C}) = \min \{ w(x) : \ \ x \in \algb{C}, x \neq 0\}\]
\end{defn}

\begin{lemma}
Si $\algb{C}$ es un código lineal tenemos que:
\begin{enumerate}
\item \[\forall x \neq y \in \algb{C} \ \ d(x,y)=w(x-y)\]
\item \[d(\algb{C})=w(\algb{C})\]
\end{enumerate}
\end{lemma}
\begin{proof}
\begin{enumerate}
\item 
\[d(x,y) = \# \{i \tq x_i\neq y_i \} = \# \{i \tq x_i - y_i \neq 0\} = w(x-y)\]
\item Vamos a demostrar el lemma viendo que los siguientes dos conjuntos son iguales. Es decir, vamos a probar que
\[\{d(x,y): \ x,y \in \algb{C}, x\neq y\} = \{w(z): \ z \in \algb{C}, \ z \neq 0\}\]
Esta igualdad es sencilla de ver comprobando los dos contenidos.

Para cada elemento del conjunto de la izquierda, tenemos uno en la derecha.
\[\forall x,y \in \algbC \ \ x \neq y \  d(x,y) = w(x-y)=w(z)\]

y viceversa
\[w(z) = d(z,0), \ 0 \in \algb{C}\]

Sólo nos queda la duda de si $0 \in \algb{C}$ o no, pero es trivial ver que si puesto que estamos trabajando con un código lineal.
\end{enumerate}
\end{proof}

\obs La ventaja de trabajar con el peso mínimo es que si $|\algb{C}|=M$ encontrar $d(\algb{C})$ requiere a priori ${M \choose 2}=\frac{M^2-M}{2}$ comparaciones mientras que calcular $w(\algb{C})$ sólo requiere $M-1$.

A partir de este momento vamos a empezar a mejorar los códigos que hemos empleado hasta ahora, pues sólo hemos visto códigos triviales basados en repetición.

\begin{example}
Consideramos que tenemos al mismo robot de siempre con las mismas 4 órdenes posibles. En esta ocasión vamos a definir:
\[\algb{C}_4 = \{x \in F_2^3 \ x_1+x_2+x_3 = 0 \}=\{000,011,101,110\}\]

La ecuación en las $x_i$ se denomina \concept{comprobador global de paridad}.

Con el código que acabamos de defiir tenemos:
\[d(\algb{C})=w(\algb{C}_4)=2=d(\algb{C})\]

Por tanto, este código es capaz de detectar un error, igual que hacía $\algb{C}_2$ pero a un precio menor puesto que la longitud de $\algb{C}_2$ es 4 mientras que la de $\algb{C}_4$ es 2.

Podemos ver también que ambos códigos tienen dimensión 2 lo que nos da unas tasas de transmisión y distancias relativas de:
\[R(\algb{C}_2)=\frac{2}{4}=\frac{1}{2}, δ(\algb{C}_2)=\frac{1}{2} \ \ \ R(\algb{C}_4)=\frac{2}{3}, δ(\algb{C}_4)=\frac{2}{3}\]
\end{example}

No obstante, el código $\algb{C}_4$ que acabamos de ver no es capaz de corregir errores. A fin de mejorar esta característica veamos un nuevo código.

\begin{example}
Consideramos esta vez el código
\[\algb{C}_3 = \left\{x\in F_2^6 \tq  \begin{array}{l} 
x_1+x_3 = 0 \\
x_1+x_5 = 0 \\
x_2+x_4 = 0 \\
x_2+x_6 = 0 
\end{array}\right\}\]

Lo que nos da una dimensión $k= 2$ y una distancia mínima $d=3$.
\end{example}

¿Podemos hacerlo mejor y obtener un código aún más corto?

\begin{example}
Sea el código:
\[\algb{C}_5 = \left\{x\in F_2^6 \tq  \begin{array}{l} 
x_1+x_2+x_3= 0 \\
x_1+x_4 = 0 \\
x_2+x_5 = 0 
\end{array}\right\}=\left\{\begin{array}{l} 
00000\\
01101\\
10110\\
11011 
\end{array}\right\}\]

Es sencillo ver que este código tiene dimensión $k=2$ y una dimensión $d=3$.

Por tanto tenemos característica iguales a las del ejemplo anterior salvo que con mensajes de longitud menor, por lo que claramente hemos mejorado el código del anterior ejemplo.
\end{example}

Hasta ahora hemos visto un algoritmo de detección/corrección de errores capaz de detectar hasta $d-1$ errores y de corregir hasta $\frac{d-1}{2}$, siendo $d$ la distancia mínima del código empleado.

No obstante este algoritmo puede mejorarse como nos garantiza el siguiente teorema.

\begin{theorem}
Sea $\algb{C}$ un código con $d(\algb{C}) = d=2r+s+1$, entonces \textbf{existe un algoritmo} que corrige hasta $r$ errores \textbf{y simultáneamente} detecta hasta $r+s$.
\end{theorem}
\begin{proof}
La mejor forma de demostrar este teorema es definir el algoritmo que satisface los requisitos esperados.
\begin{enumerate}
\item Recibo $\tilde{x} \in F_q^n$
\item Calculo $d_0 = \min\{d(x,\tilde{x}) \forall x \in \algb{C}\}$
\item Si $d_0 \leq r$ leo $x_0 \in \algb{C}$ tal que $d(x_0,\tilde{x})=d_0$
\item En otro caso \textbf{PITO}
\end{enumerate}

Este algoritmo esta asumiendo que $x_0$ calculado en el paso 3 es único. 

Esta asunción es totalmente legítima puesto que, en caso contrario:
\[x_1,x_0 \in \algb{C} \text{ con } d(x_o,\tilde{x})=d_0=d(x_1,\tilde{x}) \implies d(x_0,x_1)\leq 2d_0 \leq 2r < d\]
con lo que llegamos a una contradicción, pues $d$ es la \textbf{distancia mínima}

Por tanto, queda claro que hasta $r$ errores serán corregidos puesto que si se han producido menos de $r$ errores hemos encontrado un candidato que corrige esos errores: $x_0$.

Ahora nos queda comprobar que este algoritmo es capaz de detectar hasta $r+s$ errores.

Supongamos que se ha enviado el mensaje $x$ pero hemos recibido $\tilde{x}$ con $r+1\leq d(x,\tilde{x})\leq r+s$. 

Lo que debemos garantizar en este caso es que el algoritmo no va a leer un $x_0 \neq x$, cosa que ocurriría si caemos en el paso 3.

Pero si existe $x_0 \in \algb{C}$ con $d(x_0,\tilde{x})\leq r$ entonces 
\[d(x_0,x)\leq d(x_0,\tilde{x})+d(\tilde{x},x) \leq r+r+s < r\]

\end{proof}
%dice que detectamos hasta r errores pero nosotros no sabemos cuantos errores se han producido. Que ocurre si realmente hay r+k errores?
\begin{example}
Conisderamos el código:
\[\algb{C}_6=\algb{C}_4 \times \algb{C}_4 \times \algb{C}_4 \in F_2^4\]

Que nos permite escribir las posibles órdenes como:
\[\left\{ \begin{array}{l}
N = 00000000 \\ E = 011011011 \\ O = 101101101 \\ S = 110110110
\end{array}\right.\]
donde tenemos $n=9,\ \ dim=k=2, \ \ d=6$.

En este caso tendremos:
\begin{center}
\begin{tabular}{|l|c|c|}
\hline
Algoritmo & Corrige hasta & Detecta hasta \\
\hline
\hline
Algoritmo de corrección & 0 & -\\
\hline
$6 = 2 \times 2 + 1 + 1$ & 2 & 3 \\
\hline
$6=2\times 1 + 3 + 1 $ & 1 & 4 \\
\hline
Algoritmo de detección-corrección & 0 & 5 \\
\hline
\end{tabular}
\end{center}

Supongamos que recibimos $\tilde{x}=011111111$, según el algoritmo empleado tomaremos una decisión diferente.

En este caso tenemos:

\begin{minipage}{0.3\textwidth}
\[\left\{ \begin{array}{l}
d(\tilde{x},N) = 8 \\
d(\tilde{x},E) = 2 \\
d(\tilde{x},O) = 4 \\
d(\tilde{x},S) = 4
\end{array}\right.\]
\end{minipage}
\begin{minipage}{0.65\textwidth}
\begin{center}
\begin{tabular}{|l|c|}
\hline
Algoritmo & Acción \\
\hline
\hline
$6 = 2 \times 2 + 1 + 1$ & Leo E \\
\hline
$6=2\times 1 + 3 + 1 $ & \textbf{PITO} \\
\hline
Algoritmo de detección-corrección & \textbf{PITO}\\
\hline
\end{tabular}
\end{center}
\end{minipage}
\end{example}