\documentclass{apuntes}

\title{Aprendizaje Automático}
\author{Víctor de Juan}
\date{15/16 C1}

% Paquetes adicionales

% --------------------

\begin{document}
\pagestyle{plain}
\maketitle

\tableofcontents
\newpage
% Contenido.

\chapter{Introducción}


Aristóteles en el siglo IV a.c. dice que el razonamiento deductivo se analiza aplicando 2 silogismos fuertes.

\begin{defn}[Silogismos fuertes] Premisa:  $A \implies B$
\begin{itemize}
\item[1] Si observamos A, deducimos B.
\item[2] Si observamos $\neg B$, deducimos que $\neg A$.
\end{itemize}
\end{defn}

\begin{defn}[Silogismos débiles] 
Si vemos la calle mojada, $\implies$ lo más probable es que haya llovido.
\end{defn}


\section{Introducción a la probabilidad}

Vamos a repasar conceptos de probabilidad para poder utilizar los silogismos débiles.

\begin{defn}[dominio]
Todos los posibles resultados de un experimento aleatorio.
\end{defn}

\begin{defn}[Variable aleatoria]
Variable que identifica el experimento aleatorio
\end{defn}

\begin{defn}[Suceso]
Valor que toma la variable aleatoria al realizar el experimento aleatorio.
\end{defn}

\paragraph{Ejemplo:} Tirar un dado.

\begin{itemize}
	\item Dominio $= \{1,2,3,4,5,6\}$
	\item Variable $X$ representa tirar un dado.
	\item Posibles sucesos: $X=1,X=2,...,X=6$
\end{itemize}

A partir de esto ya podemos calcular probabilidades: 

$$P\left(primo\right)$$

$$P\left(par\right)$$



Más repaso de probabilidad: probabilidad condicionada (regla de la suma) 

\begin{defn}[Regla del producto]
\[P(A,B|I) = P(A|B,I) · P(B|I) = P(B|A,I) · P(A|I)\]
\end{defn}

Vamos a ver cómo utilizar esta regla combinándola con los silogismos:

Podemos incluir el primer silogismo fuerte \ref{silogismofuerte} haciendo $I= A\implies B$, entonces tenemos:
\[P(A,B|A\implies B) = ... = 1\]
Y aplicando el segundo silogismo fuerte, tenemos:
\[P(A|\bar{B}, A\implies B) = ... = 0\]

Sin embargo, utilizando los silogismos débiles:

\[ P(A|B,A\implies B) = \frac{P(A|A\implies B)}{P(B|A\implies B)} \geq P(A|A\implies B)\]


\section{Toma de decisiones}

A la hora de tomar decisiones, podemos utilizar 2 criterios. El criterio de máxima verosimilitud y el de ...


\begin{defn}[Criterio MAP]
\label{CriterioMAP}
Maximiza la probabilidad a posteriori.

\[H_i = \max_i\{p(H_i | D)\} = \max_i \{\frac{P(D|H_i)·P(H_i)}{P(D)}\} = \max_i \{P(D|H_i)·P(H_i)\}\]

Donde $\max_i$ devuelve la hipótesis en la que se encuentra el máximo.
\end{defn}

\begin{defn}[Criterio MV]
\label{CriterioMV}
Maximiza la máxima verosimilitud.
\[
H_i = \max_i\{P(D|H_i)\}
\]

\end{defn}

La diferencia entre los 2 criterios es la corrección y por tanto la complejidad. \textbf{MAP es más correcto}, pero \textbf{MV} es \textbf{más fácil.} 

Vamos a verlo con un ejemplo.

\begin{example}
Tenemos monedas justas y monedas trucadas. Las monedas trucadas tienen un 75\% de salir cara.

Hemos obtenido cara en un lanzamiento. ¿Cuál es la probabilidad de que sea una moneda justa?

$D=c, H=t|j$

\[
P(j|c) = \frac{P(c|j)·P(j)}{P(c)} = \frac{0.5·0.5}{P(c)}
\]
\[
P(t|c) = \frac{P(c|t)·P(j)}{P(c)} = \frac{0.75·0.5}{P(c)}
\]

Como $P(j|c) + P(t|c) = 1$, tenemos que $P(c) = 0.5·0.5 + 0.75·0.5 = 0.625$, entonces:

\[P(j|c) = 0.4\]
\[P(t|c) = 0.6\]

Según el criterio MAP diríamos que es más probable que la moneda esté trucada. 

Por algún motivo, calculamos:

\[P(j|x) = ... = \frac{2}{3}\]
\[P(j|x) = ... = \frac{1}{3}\]



\paragraph{Existen 4 modelos posibles:}
Tenemos:
\begin{itemize}
\item Espacio de atributos = $\{c,x\}$ 
\item Clase = $H\in\{j,t\}$
\end{itemize}

Vamos a construir los 4 modelos:

\begin{itemize}
	\item $h_1(c) = j$ y $h_1(x) = t$ 
	\item $h_2(c) = t$ y $h_2(x) = j$ 
	\item $h_3(c) = j$ y $h_3(x) = j$ 
	\item $h_4(c) = t$ y $h_4(x) = t$ 
\end{itemize}

Con lo calculado anteriormente, observamos que el modelo $h_1$ es elegido según el criterio MAP.

Pero podemos plantearnos, ¿qué modelo es mejor? Pues el que menos se equivoque. Vamos entonces a calcular las probabilidades de error de los modelos.

\subparagraph{$h_1$:} $P(error) = p(j,c) + p(t,x) = P(j|c)·P(c) + P(x|t)·P(t) = ... = 37.5\%$
\subparagraph{$h_2$:} $P(error) = p(j,x) + p(t,c) = P(j|x)·P(x) + P(c|t)·P(t) = ... = 62.5\%$
\subparagraph{$h_3$:} $P(error) = p(j,x) + p(t,x) = P(j|x)·P(x) + P(x|t)·P(t) = ... = 50\%$
\subparagraph{$h_4$:} $P(error) = p(j,c) + p(t,c) = P(j|c)·P(c) + P(c|t)·P(t) = ... = 50\%$
\end{example}


 \begin{defn}[Matriz de confusión]
 	Tabla para visualizar el error de un clasificador. Con el ejemplo de la moneda,

 	% % % Real
 	% % % justa -- trucada
 	% % j  25%      12.5
   	% % t  25%      37.5

 \end{defn}

Pensemos en el caso de una enfermedad y queremos, a partir de unos datos predecir si va a tener la enfermedad o no. Tenemos una enfermedad muy grave que padecen $10$ personas en una muestra de $15000$. Un modelo que diga que nadie la tiene, sólo falla en un $\frac{10}{150000} \sim 0$, un error muy bajo. ¿Es este modelo bueno? En casos como este, interesa saber detectar la enfermedad cuando se da y es un error mucho más grave no diagnosticar la enfermedad cuando sí la tiene, que diagnosticarla cuando no la tiene, con lo que, la matriz de confusión ya no es una buena manera de valorar modelos. Por ello construimos la matriz de coste o riesgo.

\begin{defn}[Matriz de coste (o riesgo)]
	Una matriz de coste o riesgo consiste en definir ponderaciones para la gravedad de cada uno de los tipos de error. 
\end{defn}


\paragraph{Regla simétrica vs Regla asimétrica}
 La \concept{Regla de Bayes simétrica} utiliza una matriz de coste que es igual que la matriz de confusión, es decir, la importancia de los errores es simétrica.  Por consiguiente, el lector avispado podrá imaginarse que la \concept{Relga de Bayes asimétrica} utiliza una matriz de costes distinta de la de confusión, es decir, la importancia de los errores no es simétrica.

 En el caso de la enfermedad descrito anteriormente, convendría utilizar una regla asimétrica. 

 \begin{example}
Vamos a ver un ejemplo de costes con el caso de las monedas. 

Sea $R$ el error cometido por $H_1$ y $R'$ el error cometido por $H_2$. 

En el caso de las monedas, suponemos $R = 3R'$, con lo que el cálculo de los costes quedaría así:


\subparagraph{$h_1$:} $P(error) = R' · P(j,c) + R · P(t,x) = ... = R' · 0.625$
\subparagraph{$h_2$:} $P(error) = R' · P(j,x) + R · P(t,c) = ... = R' · 1.37$
\subparagraph{$h_3$:} $P(error) = R' · P(j,x) + R · P(t,x) = ... = R' · 1.5$
\subparagraph{$h_4$:} $P(error) = R' · P(j,c) + R · P(t,c) = ... = R' · 0.5$

Entonces, es $h_4$ la hipótesis que minimiza el coste, aunque no es la que minimiza el error.


 \end{example}



\begin{problem}[lentillas]

Este es un ejemplo de problema multidimensional.

Aquí tenemos los datos de donde sacar las probabilidades:

\begin{tabular}{c|c|c|c|c}
\textbf{EDAD} & \textbf{LESIÓN} & \textbf{ASTIGM.} & \textbf{PROD\_LAGRIM} & \textbf{DIAGNOSTICO}\\\hline
joven & miopía & no & reducida & no \\
joven & miopía & no & normal & blandas \\

joven & miopía & sí & reducida & no \\
joven & miopía & no & normal & duras \\

joven & hipermetropía & no & reducida & no \\
joven & hipermetropía & no & normal & blandas \\

joven & hipermetropía & sí & reducida & no \\
joven & hipermetropía & sí & normal & duras \\

mediana & miopía & no & reducida & no \\
mediana & miopía & no & normal & blandas \\

mediana & miopía & sí & reducida & no \\
mediana & miopía & no & normal & duras \\

mediana & hipermetropía & no & reducida & no \\
mediana & hipermetropía & no & normal & blandas \\

mediana & hipermetropía & sí & reducida & no \\
mediana & hipermetropía & sí & normal & no \\

mediana & miopía & no & reducida & no \\
mediana & miopía & no & normal & no \\

mediana & miopía & sí & reducida & no \\
mediana & miopía & no & normal & duras \\

mediana & hipermetropía & no & reducida & no \\
mediana & hipermetropía & no & normal & blandas \\

mediana & hipermetropía & sí & reducida & no \\
mediana & hipermetropía & sí & normal & no 

\end{tabular}

\begin{itemize}
	\item Hipótesis: no llevar lentillas (n), lentillas duras (d) o lentilla blandas (b)
	\item Datos o atributos
	\subitem edad: joven (j); mediana (m); avanzada (a)
	\subitem lesión (l): miopía (m); hipermetropía (h)
	\subitem Astigmatismo(a): sí (s), no (n)
	\subitem Producción de lágrimas (p): normal (n), reducido (r)
\end{itemize}

\ppart Llega un paciente con $(l=m,p=n)$. ¿Diagnóstico?
\ppart Llega un paciente con $(l=h,p=n)$. ¿Diagnóstico?
\ppart Llega un paciente con $(l=m,p=r)$. ¿Diagnóstico?
\ppart Llega un paciente con $(l=h,p=r)$. ¿Diagnóstico?


\solution 

\spart \[P(d=n | l=m, p=n ) = \frac{P(l=m, p=n | d=n) P(d=n)}{P(l=m, p=n)}\]

\paragraph{Probabilidades a priori}
\begin{itemize}
	\item $P(d=n) = \frac{15}{24}$
	\item $P(d=d) = \frac{4}{24}$
	\item $P(d=b) = \frac{5}{24}$
\end{itemize}
\paragraph{Verosimilitues:}
\begin{itemize}
	\item $P(l=m,p=n | d = n) = \frac{1}{15}$
	\item $P(l=m,p=n | d=d) = \frac{3}{4}$
	\item $P(l=m,p=n | d=b) = \frac{2}{5}$
\end{itemize}

Por el criterio de máxima verosimilitud, diagnosticaríamos lentillas duras, ya que ese diagnóstico es el que tiene mayor verosimilitud.

Por el criterio de máxima probabilidad a posteriori (MAP) tendríamos:

\[P(d=n | l=m, p=n ) = \frac{P(l=m, p=n | d=n) P(d=n)}{P(l=m, p=n)} = \frac{\frac{1}{15}\frac{15}{24}}{...}\]
\[P(d=d | l=m, p=n ) = \frac{P(l=m, p=n | d=n) P(d=n)}{P(l=m, p=n)} = \frac{\frac{3}{4}\frac{4}{24}}{...} \impliedby MAP\]
\[P(d=b | l=m, p=n ) = \frac{P(l=m, p=n | d=n) P(d=n)}{P(l=m, p=n)} = \frac{\frac{2}{5}\frac{5}{24}}{...}\]

Por el criterio MAP, el diagnóstico será lentillas duras.

\spart $(l=h,p=n)$ Calculamos las probabilidades a posteriori (para el criterio MAP):

\[P(d=n | l=h, p=n) = ... = \frac{1}{3} = 33\%\]
\[P(d=d | l=h, p=n) = ... = \frac{1}{6} = 16\%\]
\[P(d=b | l=h, p=n) = ... = \frac{3}{6} = 50\%\]

La manera ``chapucera'' de hacer este cálculo es restringir la tabla a aquellos que tengan hipermetropía y una producción de lágrimas normal, y obtenemos una subtabla con 6 filas. De estas 6 filas contamos cuántos tienen diagnóstico blandas y encontramos 3. De las 6 filas, encontramos a 2 que tengan diagnóstico duras y a 1 que tenga diagnóstico blanda.

Calculamos las verosimilitudes:
\[P(l=h, p=n | d=n) = ... = \frac{2}{15} = 6\%\]
\[P(l=h, p=n | d=b) = ... = \frac{3}{5} = 60\%\]
\[P(l=h, p=n | d=d) = ... = \frac{1}{4} = 25\%\]

La manera ``chapucera'' de hacer este cálculo es restringir por diagnóstico. Nos quedamos con los que tengan diagnóstico no. De esas 15 filas, contamos los que tienen hipermetropía y producción normal de lágrimas. 


\spart $(l=m, p=r)$ Calculamos las probabilidades a posteriori (para el criterio MAP):

\[P(d=n | l=h, p=n) = ... = 100\%\]
\[P(d=d | l=h, p=n) = ... = 0\%\]
\[P(d=b | l=h, p=n) = ... = 0\%\]


Calculamos las verosimilitudes:
\[P(l=m, p=r | d=n) = ... = \]
\[P(l=m, p=r | d=b) = ... = \]
\[P(l=m, p=r | d=d) = ... = \]


\spart $(l=h, p=r)$ Calculamos las probabilidades a posteriori (para el criterio MAP):

\[P(d=n | l=h, p=r) = ... = 100\%\]
\[P(d=d | l=h, p=r) = ... = 0\%\]
\[P(d=b | l=h, p=r) = ... = 0\%\]

Calculamos las verosimilitudes:
\[P(l=h, p=r | d=n) = ... = \]
\[P(l=h, p=r | d=b) = ... = \]
\[P(l=h, p=r | d=d) = ... = \]

\end{problem}

\subsection{Naive Bayes}

Dimensión del espacio de atributos de lentillas: $3·2·2·2 = 24$ ejemplos distintos, los mismos que en la tabla. En ejemplos con un espacio de atributos así reducido, podemos dedicarnos a contar para hallar las probabilidades, pero en caso de tener 10 atributos binarios, tendríamos un espacio de 1024 donde ya es más difícil ponerse a contar.

A este fenómeno se le denomina \textbf{MALDICIÓN DE LA DIMENSIONALIDAD}. Oh my god.

\begin{defn}[Maldción de la dimensionalidad]
Básicamente, lo que dice esta maldición que nos ha caido es que:

El número de ejemplos necesario para cubrir el espacio de atributos de forma uniforme crece exponencialmente con el número de atributos.
\end{defn}

\paragraph{Solución inocente: } Suponemos que todos los atributos son independientes. Con esto conseguimos que $P(l=m,p=n | d=d) = P(l=m | d=d)·P(p=n | d=d)$ 

\paragraph{Obtención y explicación del clasificador}


Dado un vector de atributos (datos) $\gor{x} = (x_1,x_2,...,x_n)$, queremos obtener la clase más probable entre todas las posibilidades $H_i$, es decir, hay que calcular:

\[
P(H_i | \gor{x}) \forall i=1,...,k
\]

Aplicando el teorema de Bayes, hay que calcular:

\[
P(H_i|\gor{x} = \frac{P(\gor{x}|H_i)·P(H_i)}{P(\gor{x})} = \frac{P(x_1,...,x_n | H_i) · P(H_i)}{P(x_1,...,x_n)}
\]


Lo que dice el método Naive Bayes es utilizar la ingenuidad\footnote{Naive se \href{http://www.wordreference.com/es/translation.asp?tranword=Naive}{traduce} al español por ingenuo o inocente}  de que los atributos son independientes entre sí \textbf{dada la clase}, es decir: \[\frac{P(x_1,...,x_n | H_i) · P(H_i)}{P(x_1,...,x_n)} = \frac{P(x_1|H_i)·...·P(x_n|H_i) · P(H_1)}{P(x_1,x_n)}\]


Independiente dada la clase quiere decir que sólo podemos tener 
$P(x_1,...,x_n | H_i) = \prod{P(x_i | H_i)}$ y no $P(x_1,...,x_n) = \prod{P(x_i)}$, ya que en el segundo caso no están condicionados a la clase.

Entonces en el denominador seguimos teniendo $P(x_1,...,x_n)$, que no nos gusta. Para calcularlo, podemos utilizar: 
\[
P(x_1,...,x_n) = \sum_i \prod_j P(x_j | H_i) · P(H_i)
\]

Ahora ya podemos construir la regla de clasificación con Naive Bayes:

\begin{defn}[Naive Bayes]
\begin{mdframed}
	\[
		argmax_{H_i} \prod_{j=1}^n P(x_j|H_i) · P(H_i)
	\]
\end{mdframed}
Donde $argmax$ significa: ``el argumento que maximiza ... ''

\paragraph{Complejidad} $O(k·n)$, donde $n$ es el número de atributos y $k$ el número de hipótesis.


Sobre este método, al parecer las $P(h_j|H_i$ y $P(H_i)$ son \_\_\_\_\_ fáciles de estimar de los datos y son estimaciones relativamente buenas.
\end{defn}

\begin{example}
¿Cuánto de fiable puede ser este método? Vamos a ver un ejemplo con el problema de las lentillas.

\[
P(l=m,p=n | d=n) \simeq P(l=m | d=n) · P(p=n | d=n) = \frac{7}{15}·\frac{3}{15} = \frac{7}{75} 
\]
Por otro lado, sabemos:
\[
P(l=m,p=n | d=n) = \frac{1}{15}
\]

Y \textbf{obviamente}: \[\frac{1}{15} = \frac{5}{15} ≠ \frac{7}{75}\]

\end{example}



Para realizar el cálculo hay que obtener las tablas en entrenamiento que cruzan los atributos con las hipótesis. Estas tablas son las que se construyen durante el entrenamiento para agilizar el cáculo. En el fondo, estas tablas son las $P(x_j|H_i)$ de la fórmula

\begin{center}

\begin{tabular}{|c|c|c|c|}
\hline
lesión & n&b&d\\\hline
miopía & 7&2&3\\\hline
hipermetropía & 8&3&1\\\hline
\end{tabular}
\;\;\;
\begin{tabular}{|c|c|c|c|}
\hline
prod & n&b&d\\\hline
reducida & 12 & 0 & 0\\\hline
normal &3&5&4\\\hline
\end{tabular}
\end{center}

Vamos a calcular el resto de cosas para comparar cuánto nos desviamos al suponer independencia:

\[ P(d=n | l=m,p=n) = ... = 22\% ≠ 17\% \]
\[ P(d=b | l=m,p=n) = ... = 31\% ≠ 33\% \]
\[ P(d=d | l=m,p=n) = ... = 47\% ≠ 50\% \]

No está muy desviado y obtenemos la misma decisión. Podría ocurrir, que saliera una decisión distinta al simplificar el algoritmo ignenuamente (naively).


%% Apéndices (ejercicios, exámenes)
\appendix

\chapter{Ejercicios}
\input{tex/AA_ejs.tex}
\printindex

\end{document}
