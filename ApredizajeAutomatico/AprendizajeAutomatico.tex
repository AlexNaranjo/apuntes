\documentclass{apuntes}

\title{Aprendizaje Automático}
\author{Víctor de Juan}
\date{15/16 C1}

% Paquetes adicionales

% --------------------

\begin{document}
\pagestyle{plain}
\maketitle

\tableofcontents
\newpage
% Contenido.

\chapter{Introducción}


Aristóteles en el siglo IV a.c. dice que el razonamiento deductivo se analiza aplicando 2 silogismos fuertes.

\begin{defn}[Silogismos fuertes] Premisa:  $A \implies B$
\begin{itemize}
\item[1] Si observamos A, deducimos B.
\item[2] Si observamos $\neg B$, deducimos que $\neg A$.
\end{itemize}
\end{defn}

\begin{defn}[Silogismos débiles] 
Si vemos la calle mojada, $\implies$ lo más probable es que haya llovido.
\end{defn}


\section{Introducción a la probabilidad}

Vamos a repasar conceptos de probabilidad para poder utilizar los silogismos débiles.

\begin{defn}[dominio]
Todos los posibles resultados de un experimento aleatorio.
\end{defn}

\begin{defn}[Variable aleatoria]
Variable que identifica el experimento aleatorio
\end{defn}

\begin{defn}[Suceso]
Valor que toma la variable aleatoria al realizar el experimento aleatorio.
\end{defn}

\paragraph{Ejemplo:} Tirar un dado.

\begin{itemize}
	\item Dominio $= \{1,2,3,4,5,6\}$
	\item Variable $X$ representa tirar un dado.
	\item Posibles sucesos: $X=1,X=2,...,X=6$
\end{itemize}

A partir de esto ya podemos calcular probabilidades: 

$$P\left(primo\right)$$

$$P\left(par\right)$$



Más repaso de probabilidad: probabilidad condicionada (regla de la suma) 

\begin{defn}[Regla del producto]
\[P(A,B|I) = P(A|B,I) · P(B|I) = P(B|A,I) · P(A|I)\]
\end{defn}

Vamos a ver cómo utilizar esta regla combinándola con los silogismos:

Podemos incluir el primer silogismo fuerte \ref{silogismofuerte} haciendo $I= A\implies B$, entonces tenemos:
\[P(A,B|A\implies B) = ... = 1\]
Y aplicando el segundo silogismo fuerte, tenemos:
\[P(A|\bar{B}, A\implies B) = ... = 0\]

Sin embargo, utilizando los silogismos débiles:

\[ P(A|B,A\implies B) = \frac{P(A|A\implies B)}{P(B|A\implies B)} \geq P(A|A\implies B)\]


\section{Toma de decisiones}

A la hora de tomar decisiones, podemos utilizar 2 criterios. El criterio de máxima verosimilitud y el de ...


\begin{defn}[Criterio MAP]
\label{CriterioMAP}
Maximiza la probabilidad a posteriori.

\[H_i = \max_i\{p(H_i | D)\} = \max_i \{\frac{P(D|H_i)·P(H_i)}{P(D)}\} = \max_i \{P(D|H_i)·P(H_i)\}\]

Donde $\max_i$ devuelve la hipótesis en la que se encuentra el máximo.
\end{defn}

\begin{defn}[Criterio MV]
\label{CriterioMV}
Maximiza la máxima verosimilitud.
\[
H_i = \max_i\{P(D|H_i)\}
\]

\end{defn}

La diferencia entre los 2 criterios es la corrección y por tanto la complejidad. \textbf{MAP es más correcto}, pero \textbf{MV} es \textbf{más fácil.} 

Vamos a verlo con un ejemplo.

\begin{example}
Tenemos monedas justas y monedas trucadas. Las monedas trucadas tienen un 75\% de salir cara.

Hemos obtenido cara en un lanzamiento. ¿Cuál es la probabilidad de que sea una moneda justa?

$D=c, H=t|j$

\[
P(j|c) = \frac{P(c|j)·P(j)}{P(c)} = \frac{0.5·0.5}{P(c)}
\]
\[
P(t|c) = \frac{P(c|t)·P(j)}{P(c)} = \frac{0.75·0.5}{P(c)}
\]

Como $P(j|c) + P(t|c) = 1$, tenemos que $P(c) = 0.5·0.5 + 0.75·0.5 = 0.625$, entonces:

\[P(j|c) = 0.4\]
\[P(t|c) = 0.6\]

Según el criterio MAP diríamos que es más probable que la moneda esté trucada. 

Por algún motivo, calculamos:

\[P(j|x) = ... = \frac{2}{3}\]
\[P(j|x) = ... = \frac{1}{3}\]


\paragraph{Existen 4 modelos posibles:}
Tenemos:
\begin{itemize}
\item Espacio de atributos = $\{c,x\}$ 
\item Clase = $H\in\{j,t\}$
\end{itemize}

Vamos a construir los 4 modelos:

\begin{itemize}
	\item $h_1(c) = j$ y $h_1(x) = t$ 
	\item $h_2(c) = t$ y $h_2(x) = j$ 
	\item $h_3(c) = j$ y $h_3(x) = j$ 
	\item $h_4(c) = t$ y $h_4(x) = t$ 
\end{itemize}

Con lo calculado anteriormente, observamos que el modelo $h_1$ es elegido según el criterio MAP.

Pero podemos plantearnos, ¿qué modelo es mejor? Pues el que menos se equivoque. Vamos entonces a calcular las probabilidades de error de los modelos.

\subparagraph{$h_1$:} $P(error) = p(j,c) + p(t,x) = P(j|c)·P(c) + P(x|t)·P(t) = ... = 37.5\%$
\subparagraph{$h_2$:} $P(error) = p(j,x) + p(t,c) = P(j|x)·P(x) + P(c|t)·P(t) = ... = 62.5\%$
\subparagraph{$h_3$:} $P(error) = p(j,x) + p(t,x) = P(j|x)·P(x) + P(x|t)·P(t) = ... = 50\%$
\subparagraph{$h_4$:} $P(error) = p(j,c) + p(t,c) = P(j|c)·P(c) + P(c|t)·P(t) = ... = 50\%$
\end{example}


\subsection{Parte de Cazakas}
% % % Pedirselo a Parra.


\begin{problem}[lentillas]

Este es un ejemplo de problema multidimensional.

Aquí tenemos los datos de donde sacar las probabilidades:

\begin{tabular}{c|c|c|c|c}
\textbf{EDAD} & \textbf{LESIÓN} & \textbf{ASTIGM.} & \textbf{PROD\_LAGRIM} & \textbf{DIAGNOSTICO}\\\hline
joven & miopía & no & reducida & no \\
joven & miopía & no & normal & blandas \\

joven & miopía & sí & reducida & no \\
joven & miopía & no & normal & duras \\

joven & hipermetropía & no & reducida & no \\
joven & hipermetropía & no & normal & blandas \\

joven & hipermetropía & sí & reducida & no \\
joven & hipermetropía & sí & normal & duras \\

mediana & miopía & no & reducida & no \\
mediana & miopía & no & normal & blandas \\

mediana & miopía & sí & reducida & no \\
mediana & miopía & no & normal & duras \\

mediana & hipermetropía & no & reducida & no \\
mediana & hipermetropía & no & normal & blandas \\

mediana & hipermetropía & sí & reducida & no \\
mediana & hipermetropía & sí & normal & no \\

mediana & miopía & no & reducida & no \\
mediana & miopía & no & normal & no \\

mediana & miopía & sí & reducida & no \\
mediana & miopía & no & normal & duras \\

mediana & hipermetropía & no & reducida & no \\
mediana & hipermetropía & no & normal & blandas \\

mediana & hipermetropía & sí & reducida & no \\
mediana & hipermetropía & sí & normal & no 

\end{tabular}

\begin{itemize}
	\item Hipótesis: no llevar lentillas (n), lentillas duras (d) o lentilla blandas (b)
	\item Datos o atributos
	\subitem edad: joven (j); mediana (m); avanzada (a)
	\subitem lesión (l): miopía (m); hipermetropía (h)
	\subitem Astigmatismo(a): sí (s), no (n)
	\subitem Producción de lágrimas (p): normal (n), reducido (r)
\end{itemize}

\ppart Llega un paciente con $(l=m,p=n)$. ¿Diagnóstico?
\ppart Llega un paciente con $(l=h,p=n)$. ¿Diagnóstico?
\ppart Llega un paciente con $(l=m,p=r)$. ¿Diagnóstico?
\ppart Llega un paciente con $(l=h,p=r)$. ¿Diagnóstico?


\solution 

\spart \[P(d=n | l=m, p=n ) = \frac{P(l=m, p=n | d=n) P(d=n)}{P(l=m, p=n)}\]

\paragraph{Probabilidades a priori}
\begin{itemize}
	\item $P(d=n) = \frac{15}{24}$
	\item $P(d=d) = \frac{4}{24}$
	\item $P(d=b) = \frac{5}{24}$
\end{itemize}
\paragraph{Verosimilitues:}
\begin{itemize}
	\item $P(l=m,p=n | d = n) = \frac{1}{15}$
	\item $P(l=m,p=n | d=d) = \frac{3}{4}$
	\item $P(l=m,p=n | d=b) = \frac{2}{5}$
\end{itemize}

Por el criterio de máxima verosimilitud, diagnosticaríamos lentillas duras, ya que ese diagnóstico es el que tiene mayor verosimilitud.

Por el criterio de máxima probabilidad a posteriori (MAP) tendríamos:

\[P(d=n | l=m, p=n ) = \frac{P(l=m, p=n | d=n) P(d=n)}{P(l=m, p=n)} = \frac{\frac{1}{15}\frac{15}{24}}{...}\]
\[P(d=d | l=m, p=n ) = \frac{P(l=m, p=n | d=n) P(d=n)}{P(l=m, p=n)} = \frac{\frac{3}{4}\frac{4}{24}}{...} \impliedby MAP\]
\[P(d=b | l=m, p=n ) = \frac{P(l=m, p=n | d=n) P(d=n)}{P(l=m, p=n)} = \frac{\frac{2}{5}\frac{5}{24}}{...}\]

Por el criterio MAP, el diagnóstico será lentillas duras.

\spart $(l=h,p=n)$ Calculamos las probabilidades a posteriori (para el criterio MAP):

\[P(d=n | l=h, p=n) = ... = \frac{1}{3} = 33\%\]
\[P(d=d | l=h, p=n) = ... = \frac{1}{6} = 16\%\]
\[P(d=b | l=h, p=n) = ... = \frac{3}{6} = 50\%\]

La manera ``chapucera'' de hacer este cálculo es restringir la tabla a aquellos que tengan hipermetropía y una producción de lágrimas normal, y obtenemos una subtabla con 6 filas. De estas 6 filas contamos cuántos tienen diagnóstico blandas y encontramos 3. De las 6 filas, encontramos a 2 que tengan diagnóstico duras y a 1 que tenga diagnóstico blanda.

Calculamos las verosimilitudes:
\[P(l=h, p=n | d=n) = ... = \frac{2}{15} = 6\%\]
\[P(l=h, p=n | d=b) = ... = \frac{3}{5} = 60\%\]
\[P(l=h, p=n | d=d) = ... = \frac{1}{4} = 25\%\]

La manera ``chapucera'' de hacer este cálculo es restringir por diagnóstico. Nos quedamos con los que tengan diagnóstico no. De esas 15 filas, contamos los que tienen hipermetropía y producción normal de lágrimas. 


\spart $(l=m, p=r)$ Calculamos las probabilidades a posteriori (para el criterio MAP):

\[P(d=n | l=h, p=n) = ... = 100\%\]
\[P(d=d | l=h, p=n) = ... = 0\%\]
\[P(d=b | l=h, p=n) = ... = 0\%\]


Calculamos las verosimilitudes:
\[P(l=m, p=r | d=n) = ... = \]
\[P(l=m, p=r | d=b) = ... = \]
\[P(l=m, p=r | d=d) = ... = \]


\spart $(l=h, p=r)$ Calculamos las probabilidades a posteriori (para el criterio MAP):

\[P(d=n | l=h, p=r) = ... = 100\%\]
\[P(d=d | l=h, p=r) = ... = 0\%\]
\[P(d=b | l=h, p=r) = ... = 0\%\]

Calculamos las verosimilitudes:
\[P(l=h, p=r | d=n) = ... = \]
\[P(l=h, p=r | d=b) = ... = \]
\[P(l=h, p=r | d=d) = ... = \]

\end{problem}

\subsection{Naive Bayes}

Dimensión del espacio de atributos de lentillas: $3·2·2·2 = 24$ ejemplos distintos, los mismos que en la tabla. En ejemplos con un espacio de atributos así reducido, podemos dedicarnos a contar para hallar las probabilidades, pero en caso de tener 10 atributos binarios, tendríamos un espacio de 1024 donde ya es más difícil ponerse a contar.

A este fenómeno se le denomina \textbf{MALDICIÓN DE LA DIMENSIONALIDAD}. Oh my god.

\begin{defn}[Maldción de la dimensionalidad]
Básicamente, lo que dice esta maldición que nos ha caido es que:

El número de ejemplos necesario para cubrir el espacio de atributos de forma uniforme crece exponencialmente con el número de atributos.
\end{defn}

\paragraph{Solución inocente: } Suponemos que todos los atributos son independientes. Con esto conseguimos que $P(l=m,p=n | d=d) = P(l=m | d=d)·P(p=n | d=d)$ 


%% Apéndices (ejercicios, exámenes)
\appendix

\chapter{Ejercicios}
\input{tex/AA_ejs.tex}
\printindex

\end{document}
