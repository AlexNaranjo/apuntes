% -*- root: ../EstadisticaII.tex -*-

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% 																						%%%
%%% 								HOJA 	1												%%%
%%% 																						%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Hoja 1}

\begin{problem}[1]
Sea $\vec{Y} = (Y_1,Y_2,Y_3)' ≡ N_3(\vec{µ},Σ)$, donde \[\vec{µ} = (0,0,0)'\;
Σ =\begin{pmatrix}
1&0&0\\
0&2&−1\\
0&−1&2
\end{pmatrix}
\]


\ppart  Calcula la distribución del vector $\vec{X} = (X_1,X_2)$, donde $X_1 = Y_1 + Y_3$ y $X_2 = Y_2 + Y_3$.
\ppart ¿Existe alguna combinación lineal de las variables aleatorias $Y_i$ que sea independiente de $X_1$?

\solution
\doneby{Dejuan}


\spart 
\[
\begin{pmatrix}X_1 \\ X_2 \end{pmatrix} = \begin{pmatrix} Y_1 + Y_3 \\ Y_2 + Y_3 \end{pmatrix} = \begin{pmatrix} 1&0&1\\0&1&1 \end{pmatrix} \begin{pmatrix} Y_1\\Y_2\\Y_3 \end{pmatrix} 
\]

Ya tenemos la matriz $A$ que cumple $\vec{X} = A \vec{Y}$. Utilizando las propiedades de esperanza y varianza (\ref{propiedades:esperanzaYvarianza}):

\[\esp{\vec{X}} = \esp{A\vec{Y}} = A\esp{\vec{Y}} = \begin{pmatrix} 1&0&1\\0&1&1 \end{pmatrix} \begin{pmatrix} 0 \\0\\0\end{pmatrix} = \begin{pmatrix} 0\\0\end{pmatrix}\]

\[\var{\vec{X}} = \esp{A\vec{Y}} = AΣA' = \begin{pmatrix} 1&0&1\\0&1&1 \end{pmatrix} \begin{pmatrix}
1&0&0\\
0&2&−1\\
0&−1&2
\end{pmatrix} \begin{pmatrix} 1&0\\0&1\\1&1 \end{pmatrix} = \begin{pmatrix}3&1\\1&2\end{pmatrix}\]

\paragraph{Conclusión:}

\[\begin{pmatrix}X_1 \\ X_2 \end{pmatrix} \equiv N_1\left( \begin{pmatrix}0\\0 \end{pmatrix},\begin{pmatrix}3&1\\1&2\end{pmatrix} \right)\]


\spart Llamos $Z = a_1 Y_1 + a_2Y_2+a_3Y_3$.


Estas variables serán independientes si se distribuyen conjuntamente como una normal multidimensional y si $\cov{Z,X_1} = 0$.


Vamos a ver la covarianza. Utilizando la propiedad definida en \ref{propiedad:CovCombinacionLineal}, tenemos que

\[
\cov{a_1Y_1 + a_2Y_2 + a_3Y_3 , X_1} = \cov{A\vec{Y},B\vec{Y}}
\]

Siendo $A = (a_1,a_2,a_3)$ y $B=(1,0,1)$

Entonces \[\cov{A\vec{Y},B{\vec{Y}}} = (a_1,a_2,a_3) \begin{pmatrix} 1&0&0\\0&2&-1\\0&-1&2\end{pmatrix} \begin{pmatrix} 1\\0\\1 \end{pmatrix}\]

Operando obtenemos $\cov{A\vec{Y},X_1} = a_1 - a_2 + 2a_3$.



Ahora sólo hace falta ver que se distribuyen conjuntamente como una normal bivariante. Esto lo tenemos asegurado, pues ``\textit{El vector se distribuye normalmente porque lo podemos escribir en la forma AY, para una matriz A.}''\footnote{Cito textualmente de un correo envíado por José Ramón, profesor de la asignatura}
\end{problem}


\begin{problem}[2]

Sea $\vec{X} = (X_1 , X_2 , X_3 )$ un vector aleatorio con distribución normal tridimensional con vector de medias $\vec{µ} = (0, 0, 0)$ y matriz de covarianzas
\[ Σ = \begin{pmatrix}
4&0&−1 \\
0&5&0\\
−1&0&2
\end{pmatrix}
\]

\ppart Determina razonadamente cuáles de los siguientes pares de variables o vectores aleatorios son independientes y cuáles no:  

\textbf{(i)}: $X_1$ y $X_2$

\textbf{(ii)}: $(X_1 , X_3 )$ y $X_2$ 

\textbf{(iii)}: $X_1$ y $X_1 + 3X_2 − 2X_3$

\ppart Determina una matriz B tal que la variable aleatoria $(X_2 , X_3 )B(X_2 , X_3)'$ tenga distribución $χ^2_2$.

\solution

\spart 

\textbf{(i)} $X_1$ y $X_2$ son independientes porque son marginales de una distribución multivariante conjunta y tienen covarianza 0 (elemento $a_{12}$ de la matriz)

\textbf{(ii)} $X_1$ y $X_2$ son independientes porque son marginales de una distribución multivariante conjunta y tienen de matriz de covarianzas el vector idénticamente nulo. Vamos a verlo, aunque para ello construimos $\vec{Z} = (X_1,X_3,X_2)$, cuya matriz de covarianzas es:\[ Σ_z = \begin{pmatrix}
4&-1&0\\
-1&5&0\\
0&0&2
\end{pmatrix}\]
Entonces $\cov{(X_1,X_3)',X_2} = \begin{pmatrix}\cov{X_1,X_2} \\ \cov{X_3,X_2}\end{pmatrix} = \begin{pmatrix}0\\0\end{pmatrix}$

\textbf{(iii)} $X_1$ y $X_1 + 3X_2 − 2X_3$. Utilizamos: $\cov{X_1 + 3X_2 − 2X_3,X_1} = \cov{A\vec{X},B\vec{X}} = AΣB' = BΣA'$
\[
\cov{X_1 + 3X_2 − 2X_3,X_1} =
(1,3,-2) \begin{pmatrix}
4&0&−1 \\
0&5&0\\
−1&0&2
\end{pmatrix} \begin{pmatrix}1\\0\\0\end{pmatrix} = ... = 6
\]

Como la covarianza no es cero, entonces existe una relación lineal entre las variables y por ello no son independientes.

\spart 

Una $\chi^2_k$ es la distribución que tiene la suma de variables normales estandarizadas al cuadrado. Los $k$ grados de libertad corresponden a la cantidad de variables normales que sumamos.

Vemos que si tomamos $B=I$, obtenemos:

\[(X_2,X_3) \begin{pmatrix} 1&0\\0&1 \end{pmatrix} \begin{pmatrix}X_2\\X_3\end{pmatrix} = X_2^2 + X_3^2\]

Ya tenemos la suma los cuadrados de normales. Ahora sólo falta que estén estandarizadas, es decir que $X_i \sim N(0,1)$.

Ya están centradas en 0, con lo que sólo falta dividir por la varianza, es decir:

\[(X_2,X_3) \begin{pmatrix} \frac{1}{5}&0\\0&\frac{1}{2} \end{pmatrix} \begin{pmatrix}X_2\\X_3\end{pmatrix} = \frac{1}{5}X_2^2 + \frac{1}{2}X_3^2 = Z_2^2 + Z_3^2\]

donde 
\[Z_2 = \frac{1}{5}X_2^2 = \left(\frac{X_2}{\sqrt{5}}\right)^2 \to Z_2 \sim N(0,1)\]
\[Z_3 = \frac{1}{2}X_2^2 = \left(\frac{X_2}{\sqrt{2}}\right)^2 \to Z_3 \sim N(0,1)\]


\end{problem}
\begin{problem}[3]
Sea $(X, Y )$ un vector aleatorio con distribución normal bidimensional. Tanto $X$ como $Y$ tienen
distribución normal estándar. La covarianza entre $X$ e $Y$ es $ρ$, donde $|ρ| < 1$.

\ppart Determina cuál es la distribución del vector $(2X − 3Y , X + Y ) $.
\ppart Determina cuál es la distribución de la variable $(X^2 − 2ρXY + Y^2 )/(1 − ρ^2 )$.

\solution

\doneby{Dejuan}

\spart 
Llamamos 
\[
C = \begin{pmatrix} 2&-3\\1&1 \end{pmatrix}\begin{pmatrix}X\\Y\end{pmatrix}
\]

Tenemos que calcular $\esp{C},\var{C}$. Para ello, utilizamos las fórmulas de siempre

\[
\esp{C} = \esp{A\begin{pmatrix}X\\Y\end{pmatrix}} = A\esp{(X,Y)'} = A(0,0)' = \begin{pmatrix}0\\0\end{pmatrix}
\]
\[
\var{C} = \var{C(X,Y)'} = CΣC' = \begin{pmatrix} 2&-3\\1&1 \end{pmatrix} \begin{pmatrix} 1&\rho\\\rho &1\end{pmatrix}\begin{pmatrix} 2&1\\-3&1 \end{pmatrix} 
\]

La distribución del vector $(X,Y) \sim N_2\left(\esp{C},\var{C}\right)$


\spart 

Sea \[Z = \frac{Z_n}{Z_d} = \frac{(X^2 − 2ρXY + Y^2 )}{(1 − ρ^2 )}\]

Vemos que \[ Z_n = (X,Y)\begin{pmatrix} a&b\\c&d \end{pmatrix}\begin{pmatrix} X\\Y \end{pmatrix} = aX^2 + cXY+bXY+dY^2\implies \left\{ \begin{array}{c} a=d=1\\c+b=-2\rho \to c=b=-\rho \end{array}\right.\]

Ahora, dividimos todo por $Z_d$. ¿Qué hemos obtenido?

\[
\frac{1}{1-\rho^2}\begin{pmatrix}1&-\rho\\-\rho&1\end{pmatrix}
\]


Casualmente, esta matriz es la inversa de $Σ$

\[
\begin{pmatrix}1&\rho\\\rho&1\end{pmatrix}\frac{1}{1-\rho^2}\begin{pmatrix}1&-\rho\\-\rho&1\end{pmatrix} = \begin{pmatrix}1&0\\0&1\end{pmatrix}
\]

Con lo que \[Z = (X,Y)Σ^{-1}(X,Y)' = (X-0,Y-0)Σ^{-1}(X-0,Y-0)' \sim \chi^2_2\]

\end{problem}

\begin{problem}[4]
\label{ejrc:4-hoja1}
Sean $Y_1$ e $Y_2$ dos variable aleatorias independientes con distribución normal estándar.

\ppart  Demuestra que el vector $\vec{Y} = (Y_1 , Y_2 )$ tiene distribución normal bidimensional y calcula la distribución del vector $\vec{X} = (2Y_1 + Y_2 , Y_2 − 2Y_1 )$.

\ppart  ¿Son las dos distribuciones marginales de X independientes? Determina una matriz B tal que $X'BX$ tenga distribución $χ^2$ con 2 grados de libertad.
\solution

\doneby{Dejuan}

\approvedby{Jorge}

\spart
\doneby{Jorge}

Tomemos la función característica del vector aleatorio que tiene ambas v.a. $Y=(Y_1,Y_2)$:

\[φ_Y(t) = \mathbb{E}(e^{it'Y}) = \mathbb{E}(e^{it_1Y_1+it_2Y_2})=\]

Puesto que $Y_1,Y_2$ son independientes:

\[= \mathbb{E}(e^{it_1Y_1})·\mathbb{E}(e^{it_2Y_2}) = φ_{Y_1}(t_1)·φ_{Y_2}(t_2) = e^{-\frac{t_1^2}{2}}·e^{-\frac{t_2^2}{2}}= e^{-\frac{t_1^2+t_2^2}{2}}\]

Que coincide con la función característica de una normal bidimensional $Y\sim N_2(0,I)$.

El vector de $n$ normales independientes se distribuye normalmente. En este caso, como $Y_1,Y_2$ son normales independientes, $(Y_1,Y_2) \sim N(µ,Σ)$, donde:

\[µ = \begin{pmatrix}0\\0\end{pmatrix}\;\;\; Σ = \begin{pmatrix}1&0\\0&1\end{pmatrix}\]

\[\vec{X} = (2Y_1 + Y_2 , Y_2 − 2Y_1 ) \to \begin{pmatrix}X_1\\X_2\end{pmatrix} = \begin{pmatrix}2&1\\1&-2\end{pmatrix}\begin{pmatrix}Y_1\\Y_2\end{pmatrix}\]

Entonces, vamos a calcular la distribución de $\vec{X}$

\[
\esp{\vec{X}} = \esp{A\vec{Y}} = A\esp{Y} = \begin{pmatrix}0\\0\end{pmatrix}
\]
\[
\var{\vec{X}} = \var{A\vec{Y}} = A\var{\vec{Y}}A' = AA' = AA = \begin{pmatrix} 5&-3\\-3&5\end{pmatrix}
\]

\spart $X_i \sim N(0,5)$. Además, $\corr{X_1,X_2} ≠ 0$.

Por tanto no son independientes debido a que la correlación entre ambas no es cero.

Sabemos que una $χ_2^2$ es la suma de dos normales estandarizadas al cuadrado $Σ^{-1/2}(X-μ)=Y\sim N_2(0,I)$:

\[χ_2^2=Y_1^2+Y_2^2 = Y'Y = (X-μ)'Σ^{-1/2}Σ^{-1/2}(X-μ) \overset{μ=0}{=} X'Σ^{-1}X\]

Por tanto la $B$ que pide el enunciado no es más que:
\[\begin{pmatrix} 5&-3\\-3&5\end{pmatrix}^{-1}\]


\end{problem}

\begin{problem}[5]

Sea $(X, Y )$ un vector aleatorio con función de densidad

\[
f (x, y) = \frac{1}{2π}\exp\left[\frac{1}{2}\left(x^2 − 2xy + 2y^2\right)\right]
\]

\ppart Calcula la distribución condicionada de $X$ dado $Y = y$, y la de $Y$ dado $X = x$.

\solution

Mirando la función de densidad y comparándola con la de la normal, podemos escribir:
\[
\begin{pmatrix}X\\Y \end{pmatrix} \equiv N_2\left(\begin{pmatrix}0\\0\end{pmatrix},\begin{pmatrix}1&-1\\-1&2\end{pmatrix}^{-1}\right) \equiv N_2\left(\begin{pmatrix}0\\0\end{pmatrix},\begin{pmatrix}2&1\\1&1\end{pmatrix}\right)
\]

Aplicando las fórmulas vistas en teoría \ref{form::EspVarCondicionada}, nos damos cuenta de que tenemos que calcular $X_2|X_1$ y $X_1|X_2$, con lo que cada caso tendrá una pequeña variación en la fórmula: 

\[
E(X|Y=y) = μ_y + Σ_{12}Σ_{22}^{-1}(X-μ_x) = 0 + \frac{1}{1}(y-0) = y
\]
\[
E(Y|X=x) = μ_x + Σ_{21}Σ_{11}^{-1}(Y-μ_y) = 0 + \frac{1}{2}(x-0) = \frac{x}{2}
\]

\end{problem}

\begin{problem}[6]
Sea $\vec{X} = (X_1 , X_2 )$ un vector aleatorio con distribución normal bidimensional con vector de medias $(1, 1)$ y matriz de covarianzas
\[Σ = \begin{pmatrix} 3&1\\1&2\end{pmatrix}\]

Calcula la distribución de $X_1 + X_2$ condicionada por el valor de $X_1 − X_2$ .
\solution

\[\begin{pmatrix}Z_1\\Z_2\end{pmatrix} = \begin{pmatrix}1&1\\1&-1\end{pmatrix}\begin{pmatrix}X_1\\X_2\end{pmatrix}\]

Entonces, calculando como siempre obtenemos:

\[
\begin{pmatrix}Z_1\\Z_2\end{pmatrix} \equiv N_2 \left(\begin{pmatrix}2\\0\end{pmatrix},\begin{pmatrix}7&1\\1&3\end{pmatrix}\right)
\]

Sabemos que la distribución va a ser normal, por lo que necesitamos $\esp{Z_1 | Z_2}$ y $\var{Z_1 | Z_2}$


Utilizando las fórmulas tenemos:

\[
\esp{Z_1|Z_2} = µ_1 + Σ_{12}Σ_{22}^{-1}(Z_2 - µ_2) = 2 + 1 \frac{1}{3}(Z_2 - 0) = \frac{7}{3}Z_2
\]

\[
\var{Z_1|Z_2} = Σ_{11} - Σ_{12}Σ_{22}^{-1}Σ_{21} = 7 - 1\frac{1}{3}1 = \frac{20}{3}
\]

Entonces, \[
(Z_2 | Z_1) = (X_1+X_2 | X_1 - X_2) \sim N_2\left( \frac{7}{3}(X_1 - X_2), \frac{20}{3} \right)
\]


\end{problem}

\begin{problem}[7]
Sea $X = (X1,X2,X3)'$ un vector aleatorio con distribución normal tridimensional con vector de medias $(0,0,0)'$ y matriz de covarianzas
\[
Σ =
\begin{pmatrix}
1&2&−1\\
2&6&0\\
−1&0&4
\end{pmatrix}
\]


Definamos las v.a. $Y_1 = X_1 + X_3, Y_2 = 2X_1 − X_2 $ e $ Y_3 = 2X_3 − X_2$. Calcula la distribución de $Y_3$ dado que $Y_1=0$ e $Y_2=1$.

\solution


Lo primero es descubrir la matriz de la combinación lineal y calcular la distribución, esto es:
\[
\begin{pmatrix} Y_1\\Y_2\\Y_3\end{pmatrix} = \begin{pmatrix}1&0&1\\2&-1&0\\0&-1&2\end{pmatrix}\begin{pmatrix}X_1\\X_2\\X_3\end{pmatrix} \equiv N_3 \left( \begin{pmatrix}0\\0\\0\end{pmatrix}, \begin{pmatrix}3&-2&4\\-2&2&-2\\4&-2&22 \end{pmatrix} \right)
 \]



Ahora vamos a calcular las condicionadas.  Sabemos que $Y_3 | Y_1=0, Y_2=1 \sim N_1 (µ_{2.1},Σ_{2.1})$. 

Hacemos la división:

\[
Σ = \left(\begin{array}{c|c}Σ_{11} & Σ_{12}\\\hlineΣ_{21} & Σ_{22}\end{array}\right) = \left(\begin{array}{cc|c} 3 & -2 & 4\\-2&2&-2\\\hline 4&-2&22\end{array}\right)
\]

\[
E(Y_3|Y_1=0,Y_2 = 1) = µ_2 + Σ_{21}Σ_{11}^{-1}\begin{pmatrix}Y_1 - µ_1\\Y_2-µ_2\end{pmatrix} =  0 + (4,-2) \begin{pmatrix} 3&-2\\-2&2\end{pmatrix}^{-1} \begin{pmatrix}0-0\\1-0\end{pmatrix}
\]

\[
V(Y_3|Y_1=0,Y_2=1) = Σ_22 - Σ_{21}Σ_{11}^{-1}Σ_{12} =  22 - \left(4,-2\right) \begin{pmatrix} 3&-2\\-2&2 \end{pmatrix}^{-1} \begin{pmatrix}4\\-2\end{pmatrix} 
\]

Terminando las cuentas: $E(Y_3|Y_1=0,Y_2 = 1) = 1$ y $V(Y_3|Y_1=0,Y_2=1) = 16$.

Entonces, la distribución de $(Y_3|Y_1=0,Y_2=1) = N_1(1,16)$

\end{problem}

\begin{problem}[8]
Sea $Y = (Y_1,...,Y_n)$ un vector normal multivariante tal que las coordenadas $Y_i$ tienen distribución
$N(0, 1)$ y, además, $\cov{Y_i , Y_j} = \rho$, si $i≠j$.

\ppart Escribe el vector de medias y la matriz de covarianzas del vector $X = (Y_1+Y_2,Y_1−Y_2)'$ . ¿Son $Y_1+Y_2 $ e $Y_1−Y_2$ dos variables aleatorias independientes?

\ppart Si $Σ$ es la matriz de covarianzas de $X$, ¿cuál es la distribución de la variable aleatoria $Z = X'Σ^{−1}X$?

\ppart Si $ρ = 1/2$, calcula la varianza de la media muestral $\gor{Y}= (Y_1 + · · · + Y_n )/n$ (en función del tamaño muestral $n$).

\solution
\doneby{Dejuan}

\approvedby{Jorge}

\spart 
Tenemos:
\[
	X = \left(\begin{array}{c} Y_1+Y_2\\ Y_1 - Y_2\end{array}\right) =
	\left(\begin{array}{c c c c c}
		1 & 1 & 0 & … & 0\\
		1 & -1 & 0 & … & 0
	\end{array}\right)
	\left(\begin{array}{c}
		Y_1\\
		\vdots\\
		Y_n
	\end{array}\right)
\]

El vector de medias es $µ = \esp{A\vec{Y}} = A\esp{\vec{Y}} = (0,0)'$

La matriz de covarianzas:

\[
\var{A\vec{Y}} = A\var{\vec{Y}}A' = … = \begin{pmatrix}2+2\rho&0\\0&2-2\rho \end{pmatrix}
\]

Como $\corr{X_1,X_2} = 0$ y ambas variables vienen de un vector normal, concluimos que son independientes.


Otra manera mucho más corta es utilizar la \ref{prop:tema1_pepino}.

En este caso, $A = (1,1,0,...,0)$ y $B = (1,-1,0,...,0)$. Como $AB' = 0\implies $ $AY=(Y_1+Y_2)$ y $BY = (Y_1-Y_2)$ son independientes. 

\textcolor{red}{¿Boom?}

\spart 

Una $\chi^2_2$ ya que estamos sumando 2 variables normales estandarizadas (se estandarizan al tener la forma cuadrática $Σ^{-1}$ y tener vector de medias nulo).

\spart 

Tenemos la matriz de combinación lineal $A = \left( \frac{1}{n}, ...,\frac{1}{n}\right)$. Como sólo nos piden la varianza:

\[
\var{A\vec{Y}} = A\var{\vec{Y}}A' = \frac{1}{n^2}1_nΣ1_n' = 
\]
\[
\frac{1}{n^2}(1,1,...,1) 
\begin{pmatrix}
	1		& 	\frac{1}{2}	& \dots			& 	\dots		& 	\frac{1}{2} \\
\frac{1}{2}	& 			1	& \frac{1}{2}	& 	\dots		& 	\frac{1}{2} \\
	\vdots 	& 	  			& \ddots 		& 				& 	\vdots 		\\
\frac{1}{2}	& 				& \dots			& 	\frac{1}{2}	& 	1			
\end{pmatrix}\begin{pmatrix}1\\1\\\vdots\\1\end{pmatrix} = ... = \frac{1}{n^2}\frac{n(n+1)}{2} = \frac{n+1}{2n}
\]

\[\var{\gor{Y}} = \var{A\vec{Y}} = \frac{n+1}{2n}\]

\end{problem}

\begin{problem}[9]
Demuestra que si $X$ es un vector aleatorio con distribución $N_k (μ, Σ)$, entonces existen $λ_1,... ,λ_k ∈ℝ^+$ y v.a.i.i.d. $Y_1 , . . . , Y_k$ con distribución $χ_1^2$ tales que $||X −μ||^2$ se distribuye igual que $λ_1 Y_1 +· · ·+λ_k Y_k$.

En particular, deduce que si $Σ$ es simétrica e idempotente y $μ = 0$, entonces $||X||^2$ tiene distribución $\chi_r^2$ donde $r$ es la traza de $Σ$

\solution

Sabemos que $Σ = CDC'$ con $C$ una matriz formada por autovectores ortonormales. Puesto que $X-μ\sim N(0,Σ)$ {\color[rgb]{0.6039,0.3137,0.74901} PODEMOS} continuar de la siguiente forma:
\[Z = C'(X-μ) \sim N_k(0,D)\]
\[\norm{X-μ}^2 = (X-μ)'(X-μ) = Z'\underbrace{C'C}_{I}Z = Z'Z = \sum_{i=1}^k Z_i^2\]

Ya que $Z_i\sim N(0,λ_i)$ con $λ_i$ el elemento i-ésimo de la matriz diagonal $D$, se tiene que:
\[Y_i = \frac{Z_i^2}{λ_i} \sim χ_1^2\]

Y por tanto $\sum_{i=1}^k Z_i^2 = \sum_{i=1}^k λ_iY_i$

En el caso particular de que $Σ$ sea simétrica e idempotente, sus autovalores son $λ_i=0,1$, de modo que se pasa a tener (con $μ=0$):
\[\norm{X}^2 = \sum_{i=1}^k Z_i^2 = \sum_{i=1}^r Y_i \sim χ_r^2\]

Donde $r$ es el número de autovalores $λ_i = 1$ de $D$, dicho número coincide precisamente con el rango de $Σ$.

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% 																						%%%
%%% 								HOJA 	2												%%%
%%% 																						%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Hoja 2}


\begin{problem}[1] Calcula la distribución exacta bajo la hipótesis nula del estadístico de Kolmogorov-Smirnov para muestras de tamaño 1.

\solution



La hipótesis sería $H_0 : F = F_0$ continua, con $X \sim F$

En este caso,

\[D=||F_1 - F_0||_{\inf} = (1) = \max\{F_0(x), 1 - F_0(x)\}\]

$(1)$ hay 2 posibles caminos. Al dibujar lo que nos dicen (una muestra de tamaño 1) podemos sacarlo por intuición. Sino, aplicamos la fórmula de los estadísticos.

Ahora calculamos:

\[ P_{F_0}(D\leq x) = P_{F_0} = \left\{\max \{ ... \}\leq x\right\} = P_{F_0} = P_{F_0}\{ 1-x \leq F_0(x) \leq x \}\]

\textcolor{red}{No entiendo porqué } $P_{F_0} \left\{\max \{ ... \}\leq x\right\} = \{ 1-x \leq F_0(x) \leq x \}$ y no es $\{ x \leq F_0(x) \leq 1-x \}$

Resolvemos la desigualdad, aplicando que $F_0$ es una uniforme.

\[
P\{1-x \leq U \leq x\} = \left\{ \begin{array}{cc} 0 & x\leq \frac{1}{2} \\ 2x-1 & x\geq \frac{1}{2}\end{array} \right. \implies D \sim \mathcal{U}\left(\frac{1}{2},1\right)
\]

Ya que $1-x > x \dimplies x\le \frac{1}{2}$

\end{problem}
\begin{problem}[2] Se desea contrastar la hipótesis nula de que una única observación X procede de una distribución N(0,1). Si se utiliza para ello el contraste de Kolmogorov-Smirnov, determina para qué valores de X se rechaza la hipótesis nula a nivel α = 0,05.
\solution

Este ejercicio está muy relacionado con el primero. Es una aplicación al caso de la normal.


Mirando en la tabla, encontramos que para $α = 0.05$, entonces $d_α = 0.975$. Con esta inormación podemos construir la región crítica:
\[ R = \left\{\max\{\Phi(x), 1 - \Phi(x))\} > 0.975\right\} = \{\Phi(x) > 0.975\} \cup \{1 - \Phi(x) > 0.975\} =\]
\[ \{ X>\Phi^{-1}(0.975)\} \cup \{X < \Phi^{-1}(0.025)\}\]

Consultando las tablas, vemos que $\Phi^{-1}(0.025) = 1.96$ y por simetría, $\Phi^{-1}(0.975) = -1.96$

\[R = \{|X| > 1.96\}\]


\obs Es interesante saber que, al ser simétrica la normal, la interpretación gráfica es muy fácil. Si dividimos la normal en 3 intervalos, \[(-∞ , -1.96) , (-1.96,1.96) , (1.96, ∞)\], el área encerrada en las colas es el nivel de significación, en este caso: \[\text{Area }\left((-∞ , -1.96)\cup (1.96, ∞)\right) = 0.05\]

\end{problem}



\begin{problem}[3] Da una demostración directa para el caso k = 2 de que la distribución del estadístico del contrast $\chi^2$ de bondad de ajuste converge a una distribución $\chi_1^2$ , es decir,
\[
T = \frac{(O1 − E1)^2}{E1} +
\frac{(O2 − E2)^2}{E2} \convs[d] \chi_1^2\]

\label{ej::2.3}

[Indicación: Hay que demostrar que $T = X^2_n$ , donde $X_n\convs[d] N(0,1)$. Para reducir los dos sumandos a uno, utilizar la relación existente entre O1, E1 y O2, E2.]
\solution

Si tenemos $n$ datos, vamos a construir la tabla de contingencia. Creo que consideramos una binomial porque, al sólo tener 2 clases, o eres de una o eres de la otra con una probabilidad $p$.

\begin{center}
\begin{tabular}{c|cc}
 & $A_1$ & $A_2$ \\\hline
 Obs & $n\gor{p}$ & $n(1-\gor{p})$\\
 Esp  & $np_0$ & $n(1-p_0)$\\
\end{tabular}
\end{center}

\[ T = \sum_{i=1}^2 \frac{(O_i - E_i)^2}{E_i} = \frac{n^2(\gor{p}-p_0)^2}{n} + \frac{n^2(\gor{p}-p_0)}{n(1-p_0)}  = ... \]
Simplificando, llegamos a:

\[
T = \left(\frac{|\gor{p}-p_0|}{\sqrt{\frac{p_0(1-p_0)}{n}}} \right)
\]

Está contando un montón de cosas interesantes que me estoy perdiendo.



Entre ellas, tenemos que $\sqrt{T} \convs[d]N(0.1)$ por el teorema central del límite ( es el caso particular para una binomial), con lo que $T\convs[d] \chi^2$. ¿Porqué 1 grado de libertad? Porque sólo estamos estimando 1 parámetro, el $\gor{p}$.

Esto responde también al problema 11. 

\end{problem}



\begin{problem}[4] El número de asesinatos cometidos en Nueva Jersey cada día de la semana durante el año 2003 se muestra en la tabla siguiente:

\begin{center}
\begin{tabular}{c|ccccccc}
Día & Lunes & Martes & Miércoles & Jueves & Viernes & Sábado & Domingo \\\hline
Frecuencia & 42 & 51 & 45 & 36 & 37 & 65 & 53
\end{tabular}
\end{center}

\ppart Contrasta a nivel α = 0,05, mediante un test $χ2$, la hipótesis nula de que la probabilidad de que se cometa un asesinato es la misma todos los días de la semana.

\ppart ¿Podría utilizarse el test de Kolmogorov-Smirnov para contrastar la misma hipótesis? Si tu
respuesta es afirmativa, explica cómo. Si es negativa, explica la razón.


\ppart Contrasta la hipótesis nula de que la probabilidad de que se cometa un asesinato es la misma desde el lunes hasta el viernes, y también es la misma los dos días del fin de semana (pero no es necesariamente igual en fin de semana que de lunes a viernes).

\solution

\spart Tenemos $n = 329$, $E_i = \frac{329}{7} = 47$ y $H_0 : p_i = \frac{1}{7}$

Calculamos el estadístico \[T = \sum_{i=1}^7 \frac{O_i^2}{E_i} - 329 = \left(\frac{42^2}{47} + \frac{51^2}{47} + \frac{45^2}{47} + ... + \frac{53^2}{47}\right) - 329 = 13.32\]

Por otro lado, $\chi^2_{6;0.05} = 12.59$, con lo que rechazamos la hipótesis.

\spart No podría utilizarse al tratarse de algo discreto y KS sólo sirve para continuas.

\spart

Tenemos la siguiente tabla:

\begin{center}
\begin{tabular}{c|ccccccc}
Día & Lunes & Martes & Miércoles & Jueves & Viernes & Sábado & Domingo \\\hline
Frecuencia & p & p & p & p & p & q & q
\end{tabular}
\end{center}


\obs
Podríamos plantearnos contrastar que es uniforme de lunes a viernes ($H_1$) y otra uniforme distinta en fines de semana ($H_2$). Entonces tendríamos $H_0 : H_1 \cap H_2$, y construir la región $R = R_1 \cup R_2$. ¿Cuál es el problema de este camino?

El nivel de significación, ya que $P_{H_0}(R_1 \cup R_2) = P_{H_0}(R_1) + P_{H_0}(R_2) - P_{H_0}(R_1\cap R_2) = 2α - α^2 \sim 2α$. 

Podríamos tomar, chapucerillamente $α = \frac{α}{2}$ para que al final, $P_{H_0} ( R_1 \cup R_2) = α$. Aquí surge otro problema, que es que estamos despreciando la probabilidad de la intersección y tomándolo como independiente cuando no tiene porqué serlo. Es una aproximación ``buena'' que a veces se utiliza, pero pudiendo hacerlo bien... \paragraph{Vamos a hacerlo bien:}



Tenemos que $5p + 2q = 1 \implies q = \frac{1-5p}{2}$. Pero para utilizar el contraste de homogeneidad $\chi^2$ necesitamos tener $p$ (y $q$). Como no disponemos de ellos, vamos a estimarlos. ¿Cómo? Con el estimador de máxima verosimilitud que es el molón. En el apéndice hay un pequeño recordatorio: \fref{sec:estimadorMaximaVerosimilitud}

En este caso, nuestra función de densidad es: \[f(x) = \left\{ \begin{array}{cl} p & x∈[\text{lunes,martes,miércoles,jueves,viernes}]\\\frac{1-5p}{2}&x∈[\text{sábado,domingo}] \end{array}\right.\]

¿Cuál es la probabilidad de 7 asesinatos entre semana? Pues la intersección de los 7 sucesos, es decir $p·p·...·p = p^7$. Razonando así, tenemos \[ e.m.v.(p) =L(p;\text{datos})= p^{42+51+...+37} \left( \frac{1-5p}{2} \right)^{65+53} \]

Ahora, despejamos tomando $l(p) = \ln(L(p)) = 211 \ln(p) + 118\ln\left(\frac{1-5p}{2}\right)$ y maximizamos:

\[
l'(p) = 0 \implies ... \left\{\begin{array}{c} \gor{p} = 0.128\\ \gor{q} = 0.179 \end{array}\right.
\]


Ahora que ya tenemos $p$ y $q$, las frecuencias esperadas son:

\[E_i = n·\left( p,p,p,p,p,q,q\right) = (42.2,...,42.2,58.91,58.91)\]
 
Ya estamos en condiciones de construir el estadístico:

\[
T = \sum_{i=1}^7 \frac{O_i^2}{\gor{E}_i^2} - n = ... = 5.4628
\]

Y comparamos con la $\chi^2$. ¿Cuántos grados de libertad? Si tenemos $7$ clases, siempre perdemos uno, con lo que serían 6. Sin embargo hemos estimado un parámetro, con lo que son $5$ grados de libertad. Entonces: $ c = \chi^2_{5;0.05} = 11.07$

Como $T < c$, no podemos rechazar la hipótesis.



\end{problem}



\begin{problem}[5] Para estudiar el número de ejemplares de cierta especie en peligro de extinción que viven en un
bosque, se divide el mapa del bosque en nueve zonas y se cuenta el número de ejemplares de cada
zona. Se observa que 60 ejemplares viven en el bosque repartidos en las 9 zonas de la siguiente
forma:


\begin{center}
\begin{tabular}{|c|c|c|}
\hline
8&7&3 \\\hline
5&9&11 \\\hline
6&4&7 \\\hline
\end{tabular}
\end{center}

Mediante un contraste de hipótesis, analiza si estos datos aportan evidencia empírica de que los
animales tienen tendencia a ocupar unas zonas del bosque más que otras.

Tomamos $α = 0.01$
\solution

$T = 7.47$, $\chi^2_{8;0.001} = 20.09$

Aceptamos la hipótesis $H_0 : $ la especie se reparte uniformemente.

\end{problem}
\begin{problem}[6] Se ha desarrollado un modelo teórico para las diferentes clases de una variedad de moscas. Este
modelo nos dice que la mosca puede ser de tipo L con probabilidad p
2
, de tipo M con probabilidad
q
2 y de tipo N con probabilidad 2pq (p + q = 1). Para confirmar el modelo experimentalmente
tomamos una muestra de 100 moscas, obteniendo 10, 50 y 40, respectivamente.
\ppart
Hallar la estimación de máxima verosimilitud de p con los datos obtenidos.
\ppart
¿Se ajustan los datos al modelo teórico, al nivel de significación 0’05?
\solution

\doneby{Jorge}

\approvedby{Dejuan}

\spart
Primero calculamos la función de verosimilitud para $p$:
\[L_n(p) = L_n(p) = \prod_{i=0}^n f(x_i;p) = (p^2)^{10} · (q^2)^{50} · (2pq)^{40}\]

El EMV lo obtendremos maximizando $\log L_n(p)$:
\[\log L_n(p) = 20 \log p + 100 \log q + 40 \log 2pq\]
\[\frac{\partial}{\partial p} \log L_n(p) = \frac{20}{p} - \frac{100}{1-p} + 40 \frac{2-4p}{2p(1-p)} = 0 \]

Maximizamos con $\hat{p}=\frac{3}{10} \implies \hat{q}=\frac{7}{10}$.

\spart
En este caso tomamos $H_0 \equiv P(X∈L)=p^2, P(X∈M)=q^2, P(X∈N)=2pq$

Usando el estado el contraste de bondad de ajuste de la $χ^2$, el estadístico de Pearson queda:
\[T = \sum_{i=1}^3 \frac{\left(O_i - \hat{E}_i\right)^2}{\hat{E}_i} = \sum_{i=1}^3 \frac{O_i^2}{\hat{E}_i} - n =\]
\[ = \frac{10^2}{p^2·100} + \frac{50^2}{(1-p)^2 · 100} + \frac{40^2}{2p(1-p)·100} - 100 ≈ 0.22\]

Puesto que en este caso $k=3$ y hemos estimado 1 parámetro ($p$), tenemos que $T$ se distribuye como una $χ^2_{3-1-1}$. En las tablas nos encontramos con que $χ^2_{1;0.05}=3.84 > T$ y no rechazamos $H_0$, es decir los datos se ajustan al modelo teórico.

\end{problem}
\begin{problem}[7]
\ppart
Aplica el test de Kolmogorov-Smirnov, al nivel 0.05, para contrastar si la muestra (3.5, 4, 5, 5.2, 6) procede de la $U(3,8)$.
\ppart
Aplica el test de Kolmogorov-Smirnov, al nivel 0.05, para contrastar la hipótesis de que la
muestra (0, 1.2, 3.6) procede de la distribución $N(µ~=~1;σ~=~5)$.
\solution
\doneby{Jorge}

\approvedby{Dejuan}

\spart
La función de distribución de una $U(3,8)$ es:
\[
	F(x)=
	\begin{cases}
		0 & ,x<3 \\
		\frac{x-3}{5} & ,3≤x≤8 \\
		1 & ,x>8
	\end{cases}
\]

\begin{center}
	\begin{tabular}{ c | c | c | c | c }	
		$x_{(i)}$ & $\frac{i}{n}$ & $F_0(x_{(i)})$ & $D_n^{+}$ & $D_n^{-}$ \\ \hline
		3.5 & 0.2  & 0.1  & 0.1  & 0.1   \\ \hline
		4   & 0.4  & 0.2  & 0.2  & 0     \\ \hline
		5   & 0.6  & 0.4  & 0.2  & 0     \\ \hline
		5.2 & 0.8  & 0.44 & 0.36 & -0.16 \\ \hline
		6   & 1    & 0.6  & 0.4  & -0.2
	\end{tabular}
\end{center}

Tendremos por tanto que $D_n=\norm{F_n-F_0}_∞=0.4$. Si nos vamos a la tabla del contraste K-S vemos que $c=0.565$ para $α=0.05$.

Como $D_n<c$ \textbf{no rechazamos} la hipótesis nula de que las muestras vienen de la uniforme.

\spart
\begin{center}
	\begin{tabular}{ c | c | c | c | c }	
		$x_{(i)}$ & $\frac{i}{n}$ & $F_0(x_{(i)})$ & $D_n^{+}$ & $D_n^{-}$ \\ \hline
		0 & 0.3 & 0.42 & -0.12 & 0.42   \\ \hline
		1.2 & 0.6 & 0.52 & 0.08 & 0.22     \\ \hline
		3.6 & 1 & 0.7 & 0.3 & 0.1
	\end{tabular}
\end{center}

Tendremos por tanto que $D_n=\norm{F_n-F_0}_∞=0.42$. Si nos vamos a la tabla del contraste K-S vemos que $c=0.708$ para $α=0.05$.

Como $D_n<c$ \textbf{no rechazamos} la hipótesis nula de que las muestras vienen de la $N(1,5)$.

\end{problem}



\newpage
\begin{problem}[8] Se ha clasificado una muestra aleatoria de 500 hogares de acuerdo con su situación en la ciudad
(Sur o Norte) y su nivel de renta (en miles de euros) con los siguientes resultados:
\begin{center}
	\begin{tabular}{c c c}
		\hline
		Renta & Sur & Norte\\ \hline
		0 a 10 & 42 & 53 \\
		10 a 20 & 55 & 90 \\
		20 a 30 & 47 & 88 \\
		más de 30 & 36 & 89\\ \hline
	\end{tabular}
\end{center}

\ppart
A partir de los datos anteriores, contrasta a nivel α = 0,05 la hipótesis nula de que en el sur los
hogares se distribuyen uniformemente en los cuatro intervalos de renta considerados.

\ppart
A partir de los datos anteriores, ¿podemos afirmar a nivel α = 0,05 que la renta de los hogares
es independiente de su situación en la ciudad?
\solution


\spart
Tenemos $H_0: p_i=\frac{1}{4}$ y usando el contraste de bondad de ajuste de la $χ^2$:
\[T = \sum_{i=1}^4 \frac{O_i^2}{E_i} - n_{\text{sur}} = \frac{42^2 + 55^2 + 47^2 + 36^2}{\frac{1}{4}·180} - 180 = 4.31\]

En las tablas encontramos que $χ^2_{k-1;α} = χ^2_{3;0.05} = 7.815$. Como $T<χ^2_{3;0.05}$, \textbf{no podemos rechazar} la hipótesis nula de que en el sur los hogares se distribuyen uniformemente en los cuatro intervalos de renta considerados.

\spart
Lo primero que haremos es estimar las probabilidades de que la v.a. caiga en cada una de las 6 clases que tenemos ($A_i$ serán los intervalos de renta y $B_i$ si el hogar es del norte o del sur):
\[p(x∈A_1) = \frac{42+53}{500} = 0.19\]
\[p(x∈A_2) = \frac{55+90}{500} = 0.29\]
\[p(x∈A_3) = \frac{47+88}{500} = 0.27\]
\[p(x∈A_4) = \frac{36+89}{500} = 0.25\]

\[p(x∈B_1) = \frac{42+55+47+36}{500} = 0.36\]
\[p(x∈B_2) = \frac{53+90+88+89}{500} = 0.64\]

Bajo la $H_0$ consideramos $A_i$ independiente de $B_i$, de modo que $p_{i,j} = p_i·p_j$ tal y como se muestra en la siguiente tabla:

\begin{center}
	\begin{tabular}{c | c}
		$p_{1,1} = 0.0684$ & $p_{1,2} = 0.1216$\\ \hline
		$p_{2,1} = 0.1044$ & $p_{2,2} = 0.1856$\\ \hline
		$p_{3,1} = 0.0972$ & $p_{3,2} = 0.1728$\\ \hline
		$p_{4,1} = 0.09$ & $p_{4,2} = 0.16$
	\end{tabular}
\end{center}

Sabiendo que $\hat{E}_{ij} = n·p_{i,j}$:

\begin{center}
	\begin{tabular}{c | c}
		$\hat{E}_{1,1} = 34.2$ & $\hat{E}_{1,2} = 60.8$\\ \hline
		$\hat{E}_{2,1} = 52.2$ & $\hat{E}_{2,2} = 92.8$\\ \hline
		$\hat{E}_{3,1} = 48.6$ & $\hat{E}_{3,2} = 86.4$\\ \hline
		$\hat{E}_{4,1} = 45$ & $\hat{E}_{4,2} = 80$
	\end{tabular}
\end{center}

\[T=\sum_{j=1}^2 \sum_{i=1}^4 \frac{O_{ij}^2}{\hat{E}_{ij}} - n = 8.39\]

Si nos vamos a las tablas vemos que $χ^2_{(k-1)(p-1); α} = χ^2_{3·1; 0.05} = 7.815 < T$ y por tanto \textbf{rechazamos la hipótesis nula} de que la renta de los hogares es independiente de su situación en la ciudad.

\doneby{Dejuan}
\[T=\sum_{j=1}^2 \sum_{i=1}^4 \frac{O_{ij}^2}{\hat{E}_{ij}} - n = 5.91 < 7.815\] y por tanto \textbf{aceptamos la hipótesis nula} de que la renta de los hogares es independiente de su situación en la ciudad.


\end{problem}
\begin{problem}[9] A finales del siglo XIX el físico norteamericano Newbold descubrió que la proporción de datos
que empiezan por una cifra d, p(d), en listas de datos correspondientes a muchos fenómenos
naturales y demográficos es aproximadamente:
p(d) = log10
d + 1
d
!
, d = 1,2,...,9.
Por ejemplo, p(1) = log10 2 ≈ 0,301030 es la frecuencia relativa de datos que empiezan por 1. A raíz
de un artículo publicado en 1938 por Benford, la fórmula anterior se conoce como ley de Benford.
El fichero poblacion.RData incluye un fichero llamado poblaciones con la población total de los
municipios españoles, así como su población de hombres y de mujeres.
(a) Contrasta a nivel α = 0,05 la hipótesis nula de que la población total se ajusta a la ley de Benford.
(b) Repite el ejercicio pero considerando sólo los municipios de más de 1000 habitantes.
(c) Considera las poblaciones totales (de los municipios con 10 o más habitantes) y contrasta a nivel
α = 0,05 la hipótesis nula de que el primer dígito es independiente del segundo.
(Indicación: Puedes utilizar, si te sirven de ayuda, las funciones del fichero benford.R).
\solution

\end{problem}
\begin{problem}[10] Se ha llevado a cabo una encuesta a 100 hombres y 100 mujeres sobre su intención de voto. De
las 100 mujeres, 34 quieren votar al partido A y 66 al partido B. De los 100 hombres, 50 quieren
votar al partido A y 50 al partido B.
\ppart
Utiliza un contraste basado en la distribución $χ^2$ para determinar si con estos datos se puede
afirmar a nivel $α = 0,05$ que el sexo es independiente de la intención de voto.
\ppart
Determina el intervalo de valores de α para los que la hipótesis de independencia se puede
rechazar con el contraste del apartado anterior.
\solution

Este ejercicio ha caido en un examen.

\doneby{Jorge}

\approvedby{Dejuan}

\spart
Procediendo como en el ejercicio anterior obtendremos que bajo la hipótesis nula de independencia:
\[p_{A,\text{mujer}} = p_{A, \text{hombre}} = 0.21\]
\[p_{B,\text{mujer}} = p_{B, \text{hombre}} = 0.29\]

Por tanto:
\[T=\sum_{j=1}^2 \sum_{i=1}^2 \frac{O_{ij}^2}{\hat{E}_{ij}} - 200 = 5.25\]

Si nos vamos a las tablas vemos que $χ^2_{(k-1)(p-1); α} = χ^2_{1; 0.05} = 3.841 < T$, y por tanto \textbf{rechazamos la hipótesis nula} de que el sexo es independiente de la intención de voto.

\paragraph*{En clase: } hemos contrastado homogeneidad (las intenciones de voto se distribuyen igual) en vez de independencia, pero viene a ser lo mismo.

\spart
El p-valor asociado a $T=5.25$ es $\left[1 - F_{χ^2_{1}}(5.25)\right] = 0.02$, por tanto para $α~∈~[0.02,1]$ rechazamos la hipótesis de independencia del apartado anterior.

Para calcular el p-valor, utilizamos que una $\chi^2_1$ es una normal al cuadrado, es decir:

\[p = P(X>5.25) = P ( Z^2 > 5.25) = P(|Z| > 2.29) = 0.022\]

siendo $Z\sim N(0,1)$



\end{problem}
\begin{problem}[11] Sea X1,...,Xn una muestra de una distribución Bin(1, p). Se desea contrastar H0 : p = p0. Para ello hay dos posibilidades: 

\ppart  Un contraste de proporciones basado en la región crítica
$R = \{|\gor{p} − p_0|\} > z\frac{α}{2} p p0(1 − p0)/n $
\ppart un contraste $χ2$ de bondad de ajuste con k = 2 clases. ¿Cuál es la relación entre ambos contrastes?
\solution

Consultar el ejercicio \ref{ej::2.3}.

\end{problem}

\begin{problem}[12] En un estudio de simulación se han generado 10000 muestras aleatorias de tamaño 10 de una
distribución $N(0,1)$. Para cada una de ellas se ha calculado con R el estadístico de Kolmogorov-Smirnov
para contrastar la hipótesis nula de que los datos proceden de una distribución normal
estándar, y el correspondiente p-valor.
\ppart
Determina un valor x tal que la proporción de estadísticos de Kolmogorov-Smirnov mayores
que x, entre los 10000 obtenidos, sea aproximadamente igual a 0.05. ¿Cuál es el valor teórico al que
se debe aproximar la proporción de p-valores menores que 0.1 entre los 10000 p-valores obtenidos?
\ppart
¿Cómo cambian los resultados del apartado anterior si en lugar de considerar la distribución
normal estándar se considera una distribución uniforme en el intervalo (0,1)?
\solution

\doneby{Jorge}

\spart
\begin{itemize}
	\item La $x$ que nos piden es $f_{D,α=0.05}$ ($f_D$ es la función de densidad del estadístico K-S). Si acudimos a la tabla vemos que para $n=10$ $x = f_{D,0.05} = 0.41$. Un poco más explicado el razonamiento:
	\[
		\underbrace{\frac{\#\{i : D_i > x\}}{10000}}_{P(D>x)} \simeq 0.05
	\]

	\item Precisamente el $10\%$ de los p-valores debería ser menor que $0.1$, ya que hacer un contraste nivel de significación $α=0.1$ significa que en el $10\%$ de los casos rechazamos la hipótesis nula, es decir, en le $10\%$ de los casos los p-valores son $<0.1$.

	Esto se debe al concepto de nivel de significación, ya que si el nivel de significación es $0.01$, entonces nos estamos equivocando en 1 de cada 100 contrastes que hagamos, es decir:

	\[
		\frac{\# \{ i : p^{(i)} < α\}}{10000} \simeq α
	\]
\end{itemize}

\spart
\begin{itemize}
	\item Al contrastar con una distribución $U(0,1)$ cabría esperar que las $1000$ $D_i$ tomaran valores más altos, pues la distancia entre $F_n$ (que se monta a partir de datos que vienen de una $N(0,1)$) y $F_0=F_{U(0,1)}$ sería más grande que al tomar como $F_0$ la de una $N(0,1)$. Por tanto \textbf{el valor $x$ debería ser mayor}.

	\item Por otra parte la proporción de p-valores menores que $0.1$ debería aumentar, ya que el test debería devolver p-valores más pequeños (pues debería de rechazar la hipótesis de que los datos vienen de una $U(0,1)$).
\end{itemize}

\paragraph{Solución de clase:}

Al tener muchas muchas muestras, las frecuencias deberían ser las probabilidades.

\end{problem}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% 																						%%%
%%% 								HOJA 	3												%%%
%%% 																						%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newpage
\section{Hoja 3}
\begin{problem}[1] La Comunidad de Madrid evalúa anualmente a los alumnos de sexto de primaria de todos los colegios sobre varias materias. Con las notas obtenidas por los colegios en los años 2009 y 2010 (fuente: diario El País) se ha ajustado el modelo de regresión simple:
\[Nota2010 = β_0 + β_1Nota2009 + ε,\]
en el que se supone que la variable de error ε verifica las hipótesis habituales. Los resultados
obtenidos con R fueron los siguientes:\\[1em]

Coefficients:

\begin{tabular}{c | c | c | c | c}
	~ & Estimate & Std. Error & t-value & Pr(>|t|) \\
	(Intercept) & 1.40698 & 0.18832 & 7.471 & 1.51e-13 \\
	nota09 & 0.61060  & 0.02817 & 21.676 & < 2e-16
\end{tabular}

 Residual standard error: 1.016 on 1220 degrees of freedom

 Multiple R-squared: 0.278,Adjusted R-squared: 0.2774

 F-statistic: 469.8 on 1 and 1220 DF,  p-value: < 2.2e-16 \\[1em]

También se sabe que en 2009 la nota media de todos los colegios fue 6,60 y la cuasidesviación típica fue 1,03 mientras que en 2010 la media y la cuasidesviación típica fueron 5,44 y 1,19, respectivamente.

\ppart ¿Se puede afirmar a nivel $α = 0,05$ que existe relación lineal entre la nota de 2009 y la de 2010? Calcula el coeficiente de correlación lineal entre las notas de ambos años.

\ppart Calcula un intervalo de confianza de nivel $95\%$ para el parámetro $β_1$ del modelo.

\ppart Calcula, a partir de los datos anteriores, un intervalo de confianza de nivel $95\%$ para la nota
media en 2010 de los colegios que obtuvieron un 7 en 2009.


\solution
\doneby{Jorge}

\approvedby{Dejuan}

\spart
Poniendo $H_0: β_1=0$ (no hay relación lineal entre las notas de uno y otro año) tendremos:
\[\frac{\hat{β_1}}{S_R / \sqrt{S_{xx}}} \equiv t_{n-2}\]

La salida nos dice que este estadístico sale $21.676$, y el p-valor asociado es $<2e-16<0.05=α$. Por tanto rechazamos la hipótesis nula $H_0$, y podemos afirmar que existe relación lineal entre la nota de 2009 y la de 2010.

{\color{gray} \underline{Jorge}: no lo tengo muy claro, pero creo que la segunda pregunta de este apartado pide $\hat{β_1}$. Y según la salida de R eso es 0.61}

\spart
La definición del intervalo de confianza de nivel $95\%$ para $β_1$ es:
\[IC_{1-α}(β_1) = \left[ \hat{β_1} \mp t_{n-2;\frac{α}{2}} \frac{S_R}{\sqrt{S_{xx}}} \right] \overbrace{=}^{\text{salida R}} \left[ 0.61 \mp t_{1220;0.025} · 0.02 \right]\]

Si buscamos en las tablas de la $t$, no encontramos para más grados de libertad que $100$. ¿Por qué? Porque una $t$ con tantos grados de libertad es indistinguible a una normal, con lo que: $t_{1220;0.025} = 1.96$.

\spart
En este caso nos piden estimar $m_0 = E(Y_0|X_0=7)$, y sabemos que el intervalo de confianza para este parámetro
está definido como:
\[IC_{0.95}(m_0) = \left[ \hat{m}_0 \mp t_{n-2;\frac{α}{2}} · S_R\sqrt{\frac{1}{n} + \frac{(x_0 - \overline{x})^2}{S_{xx}}} \right]\]

\[\hat{Y_0} = \hat{m}_0 = \hat{β}_0 + \hat{β}_1x_0 = 1.4 + 0.61·7 = 5.67\]
\[S_R = 1.016, \overline{x}=6.60, S_{xx} = (n-1)·S_x= 1221·1.03^2 \]
\[ \sqrt{\frac{1}{1220}+\frac{(7-6.6)^2}{1221·1.03}} = 0.31 \]

$S_{x}$ sabemos que es $1.03^2$ porque $S_{x}$ es la cuasivarianza y en el enunciado nos dan la cuasi-desviación típica.


El resultado final es:
\[
IC = [5.67 \mp \underbrace{(1.96)(1.016)(0.031)}_{0.06}]
\]

\end{problem}



\begin{problem}[2]
Dada una muestra de 10 observaciones, se ha ajustado un modelo de regresión simple por mínimos cuadrados, resultando:
  \[Y_i =1+3x_i,\ R^2 =0.9,\ S_R^2 =2\]
Calcula un intervalo de confianza para la pendiente de la recta con un nivel de confianza 0.95. ¿Podemos rechazar, con un nivel de significación de 0.05, la hipótesis nula de que la variable x no influye linealmente en la variable Y?

\solution 

{\color{blue} Solución de clase:}

Con los datos del ejercicio tendremos:
\[S_R^2 = 2 = \frac{SCE}{n-2} \implies SCE = 2·8 = 16\]
y también:
\[R^2=0.9=\frac{SCR}{SCT} = 1 - \frac{SCE}{SCT} = 1 - \frac{16}{SCT} \implies SCT = 160
\]

Para obtener el error típico de $\hat{β}_1$ necesitamos obtener $\sqrt{S_{xx}}$:
\begin{align*}
	SCR &= \sum_{i=1}^n (\hat{Y}_i - \overline{Y})^2 = \sum_{i=1}^n (\overline{Y} + \hat{β}_1(x_i - \overline{x}) - \overline{Y})^2 = \hat{β}_1^2·S_{xx} \\
	%
	&\implies S_{xx}=\frac{SCR}{\hat{β}_1^2} = \frac{SCT - SCE}{9} = \frac{144}{9} = 16
\end{align*}

De modo que ya podemos calcular $ET(\hat{β}_1) = \frac{S_R}{\sqrt{S_{xx}}} = \frac{\sqrt{2}}{4} ≈ 0.35$, y por tanto nuestro intervalo de confianza para $β_1$ será:
\[IC_{0.95}(β_1) = \left[ \hat{β}_1 \mp t_{8,0.025} · ET(\hat{β}_1) \right] = \left[ 3 \mp 0.8152 \right]\]

\textbf{¿Podemos rechazar, con un nivel de significación de 0.05, la hipótesis nula de que la variable x no influye linealmente en la variable Y?}

Para este contraste tendremos $H_0: β_1=0$, y si nos construimos una tabla nos resultará más fácil llegar al estadístico $F$ que necesitamos para hallar la región de rechazo:

\begin{center}
	\begin{tabular}{ c c c c c }
	  Fuente & $SC$ & $gl$ & $CM$ & $F$ \\ \hline
	  Explicada & 144 & 1 & 144 & 72 \\
	  No explicada & 16 & 8 & 2 \\ \hline
	  Total & 160 & 9
	\end{tabular}
\end{center}

Sabemos que $R=\left\{ F > F_{1,8;0.05} \right\}$, y puesto que $72=F > F_{1,8;0.05}$ rechazamos $H_0$.

\doneby{Jorge}

A la vista del modelo de regresión lineal presentado en el enunciado tendremos $\hat{β}_0=1$ y $\hat{β}_1=3$. Sabemos que un intervalo de confianza $0.95$ para $β_1$ es:
\[IC_{1-α}(β_1) = \left[ \hat{β_1} \mp t_{n-2;\frac{α}{2}} \frac{S_R}{\sqrt{S_{xx}}} \right]\]

{\color{gray} \underline{Jorge}: me imagino que con $R$ se refiere a $S_{xx} = \frac{\sum_i (x_i - \overline{x})^2}{n}$, porque si no, no se me ocurre cómo calcularla sin saber $\overline{x}$ ni cada $x_i$}.

Tenemos que $t_{8;0.025} = 3.83$, por lo que el intervalo de confianza queda:

\[IC_{0.95}(β_1) = \left[ 3 \mp 3.83 · \frac{\sqrt{2}}{0.94} \right]\]

Veamos ahora si podemos decir que la variable x no influye linealmente en la variable~Y ($H_0: β_1=0$):

Sabemos que $\frac{\hat{β}_1 - β_1}{S_R / \sqrt{S_{xx}}} \equiv t_{n-2}$ sigue una t-student con n-2 grados de libertad, y bajo $H_0$ tendremos que $\frac{\hat{β}_1 }{S_R / \sqrt{S_{xx}}} \equiv t_{n-2}$. Si queremos rechazar $H_0$ con nivel de significação $α=0.05$ la región de rechazo será:

\[R = \left\{ \frac{\hat{β}_1 }{S_R / \sqrt{S_{xx}}} > t_{n-2;\frac{α}{2}} \right\} = \left\{ \frac{3}{\sqrt{2} / 0.94} > t_{8;0.025} \right\} = \left\{ 1.5 > 3.83 \right\}\]

Por tanto \textbf{no caemos en la región de rechazo} que nos permitiría afirmar que x inluye linealmente en la variable Y.

\doneby{Dejuan}

Lo primero es saber qué es $R^2$. En el ejercicio anterior, vemos que hay un ``Adjusted R-squared''. Gracias a nuestro conocimiento del inglés, R-squared es $R^2$, lo que nos conduce a pensar que ese $R^2$ es el ``adjusted r-squared''. La \href{https://en.wikipedia.org/wiki/Coefficient_of_determination}{definicion}  dice 
\[
R^2 = 1 - \frac{\sum (y_i-\hat{y})^2}{\sum (y_i-\gor{y})^2} = 1-\frac{\sum e_i^2}{\sum (y_i-\gor{y})^2} = 1 - \frac{S_R(n-2)}{S_{xx}}
\]

Entonces, despejamos $S_{xx}$ de la ecuación:

\[
0.9 = 1 - \frac{S_R(n-2)}{S_{xx}} = 1 - \frac{\sqrt{2}·8}{S_{xx}} \to 0.1 = \frac{16}{S_{xx}} \to S_{xx} = 160
\]

Ahora ya podemos construir el intervalo de confianza:

\[ IC_{1-α}(β_1) = \left[ \hat{β_1} \mp t_{n-2;\frac{α}{2}} \frac{S_R}{S_{xx}} \right] = \left[ 3\mp 3.83·\frac{\sqrt{2}}{160} \right] = \left[ 3\mp 0.034\right]\]


Veamos ahora si podemos decir que la variable x no influye linealmente en la variable~Y ($H_0: β_1=0$): Deberíamos poder rechazar (y por bastante), ya que si nuestra estimación es $\hat{β_1} = 3$ y en realidad es 0... vaya mierda de estimación hemos hecho. Además, que $R^2 = 0.9$ valor cercano a 1 (valor máximo que puede tomar) también nos dice que el modelo construido es muy bueno.

Sabemos que $\frac{\hat{β}_1 - β_1}{S_R / \sqrt{S_{xx}}} \equiv t_{n-2}$ sigue una t-student con n-2 grados de libertad, y bajo $H_0$ tendremos que $\frac{\hat{β}_1 }{S_R / \sqrt{S_{xx}}} \equiv t_{n-2}$. Si queremos rechazar $H_0$ con nivel de significación $α=0.05$ la región de rechazo será:

\[R = \left\{ \frac{\hat{β}_1 }{S_R / \sqrt{S_{xx}}} > t_{n-2;\frac{α}{2}} \right\} = \left\{ \frac{3}{\sqrt{2} / 160} > t_{8;0.025} \right\} = \left\{ 339.41 > 3.83 \right\}\implies \]

\end{problem}



\begin{problem}[3]

3. Supongamos que la muestra $(x_1,Y_1),…,(x_n,Y_n)$ procede de un modelo de regresión lineal simple en el que se verifican las hipótesis habituales. Consideramos el siguiente estimador de la pendiente
del modelo (se supone $x_1≠\overline{x}$):
\[\tilde{β}_1 = \frac{Y_1 - \overline{Y}}{x_1 - \overline{x}}\]

\ppart ¿Es $\tilde{β}_1$ un estimador insesgado?

\ppart Calcula la varianza de $\tilde{β}_1$.

\ppart Supongamos que la varianza de los errores del modelo, $σ^2$, es un parámetro conocido. Escribe la fórmula de un intervalo de confianza de nivel $1 − α$ para $β_1$ cuyo centro sea el estimador $\tilde{β}_1$.

\solution
\doneby{Jorge}

\textbf{Corregido en clase, aunque el apartado b se ha hecho de otra manera}

\spart

Para este cálculo utilizamos: 
$$ \esp{Y_i} = β_0 + β_1x_i + \esp{ε_i} = β_0 + β_1x_i$$
Ya que $ε_i \equiv N(0,σ^2)$

Además, como las $x$ son constantes: $\esp{x_1 - \gor{x}} = x_1 - \gor{x}$.

Vamos a calcular el sesgo:


\[\esp{\tilde{β}_1} = \frac{1}{x_1 - \overline{x}} (\esp{Y_1} - \esp{\overline{Y}}) = \frac{1}{x_1 - \overline{x}} (β_0+β_1x_1 - \esp{\overline{Y}})\]

Vamos a ver el valor de $\esp{\overline{Y}}$:
\[\esp{\overline{Y}} = \frac{1}{n} \sum_{i=0}^n \esp{Y_i} = \frac{1}{n} \sum_{i=0}^n (β_0 + β_1 x_i) = β_0 + β_1\overline{x}\]

Por tanto al sustituir en la primera ecuación de este apartado obtenemos que $\esp{\tilde{β}_1}=β_1$, y por tanto el estimador es insesgado.

\spart
\[\var{\tilde{β}_1} = \var{\frac{Y_1 - \overline{Y}}{x_1 - \overline{x}}} = \frac{1}{(x_1-\overline{x})^2} \left[ \var{Y_1} + \var{\overline{Y}} - 2\cov{Y_1, \overline{Y}} \right]\]

Ya sabemos que en el modelo de regresión lineal $\var{Y_i}=σ^2,\ ∀i$, luego lo siguiente que haremos es calcular los otros dos términos del corchete por separado:
\[\var{\overline{Y}} = \var{\frac{\sum Y_i}{n}} \overset{Y_i \text{ independientes}}{=} \frac{1}{n^2}\sum \var{Y_i} = \frac{σ^2}{n}\]

Ahora miramos la covarianza:
\[\cov{Y_1,\overline{Y}} = \cov{ (1,0,0,…,0) \overrightarrow{Y}, \frac{1}{n}(1,1,1,…,1) \overrightarrow{Y}} = (1,…,0) · σ^2I · \frac{1}{n} \left(\begin{array}{c} 1\\ 1\\ \vdots\\ 1 \end{array}\right) = \frac{σ^2}{n}\]

Y sustituyendo en la primera ecuación del apartado obtenemos que:
\[\var{\tilde{β}_1} = \frac{σ^2}{(x_1 - \overline{x})^2} \left(1 - \frac{1}{n}\right)\]


\spart
Puesto que podemos expresar $\tilde{β}_1$ como:
\[\tilde{β}_1 = \frac{1}{x_1 - \overline{x}} \left( (1,…,0) - \frac{1}{n}(1,…,1)\right) · \overrightarrow{Y}\]

Donde:
\[\overrightarrow{Y} = \left(\begin{array}{c} Y_1\\ Y_2\\ \vdots\\ Y_n \end{array}\right)\]
es un vector de normales $Y_i$ independientes. Así que podemos decir que $\tilde{β}_1$ es una combinación lineal de normales, y por tanto seguirá una distribución normal:
\[\tilde{β}_1 \equiv N\left(β_1,\ \underbrace{\frac{σ^2}{(x_1 - \overline{x})^2} \left(1 - \frac{1}{n}\right)}_{v}\right)\]

Por tanto $\frac{\tilde{β}_1 - β_1}{\sqrt{v}} \equiv N(0,1)$, y podemos definir el intervalo de confianza:
\[IC_{1-α}(β_1) = \left[ \tilde{β_1} \mp Z_{\frac{α}{2}} · \sqrt{v} \right]\]

Si te preguntas porqué es $\mathcal{Z}$ y no $\mathcal{T}$, revisa la construcción del intervalo de confianza para $β_1$ (en \ref{subsubsec:ICparaB1})

\end{problem}


\begin{problem}[4]
Se considera el siguiente modelo de regresión simple a través del origen:
\[Y_i =β_1x_i+ε_i,\ ε_i ≡N(0,σ^2)\text{ independientes},\ i=1,...,n.\]

\ppart Calcula el estimador de mínimos cuadrados de $β_1$ y deduce su distribución.

\ppart  Sean $e_i,\ i = 1,...,n$ los residuos del modelo. Comprueba si se cumplen o no las siguientes propiedades:  $\sum_{i=1}^n e_i = 0$ y  $\sum_{i=1}^n e_ix_i = 0$.

\ppart Si la varianza de los errores $σ^2$ es conocida, deduce la fórmula de un intervalo de confianza de nivel $1 − α$ para el parámetro $β_1$.

\solution

\spart 

\[
\hat{β_1} = \frac{\sum x_iy_i}{\sum x_i^2}
\]

Entonces, $ø(β) = \sum(y_i-βx_i)^2$. Derivando e igualando a 0, obtendríamos el estimador. Luego habría que calcular is es mínimo o máximo. Pero máximo no puede ser, porque siempre se puede construir una recta peor. Por otro lado, la función $ø$ es convexa, con lo que el punto tiene que ser mínimo.

Otra manera de hacerlo es utilizando lo que hemos visto en regresión múltiple para modelos lineales, definiendo la matriz de diseño $X$ como...

Vamos a calcular su esperanza y su varianza para la distribución:

\[
\esp{\hat{β}} = β
\]

\[
\var{\hat{β}} = \frac{\sum x_i^2σ^2}{\left(\sum x_i^2\right)^2} = \frac{σ^2}{\sum x_i^2}
\]

\spart 

Como no hay término independiente, los residuos no suman 0. Esto tiene varios razonamientos intuitivos.



Si en la matriz de diseño no hay una columna que sea todo 1's, (porque no haya término independiente) entonces el vector de residuos no es ortogonal a $V$.


\spart 

\[IC = \left[ \hat{β}_1 \mp Z_{\frac{α}{2}}\frac{σ}{\sqrt{\sum x_i^2}} \right]\]

Al no conocer $σ$, tendríamos que cambiar $σ$ por $S_R$ que es un dato que sí tenemos. Entonces, construiríamos:

\[IC = \left[ \hat{β}_1 \mp t_{n-1;\frac{α}{2}}\left(\frac{S_R}{\sqrt{\sum x_i^2}}\right) \right]\]

\end{problem}


\begin{problem}[5]


\[Y_i = βx_i + E_i\] 
con $ε_i \sim N(0,σ^2x_i^2)$.

Este es un modelo heterocedástico.

\ppart ¿insesgado?

\solution

\spart Es razonable que sea insesgado, ya que en media sí puede tener sentido. El problema será la varianza... vamos a calcular la distribución del estimador de mínimos cuadrados:

Como hemos calculado en el ejercicio anterior:
\[
\esp{β_1} = β_1 
\]
\[
\var{β_1} = \var{\frac{\sum x_iy_i}{\sum x_i^2}} = ... = σ^2 \frac{\sum x_i^4}{\left( \sum x_i^2\right)^2}
\]


Vamos a pensar... ¿De qué puntos nos podemos fiar más? ¿De los pequeños o de los grandes? Al ser heterocedástico, donde menor varianza hay es en los $x_i$ cercanos al origen, con lo que deberíamos fiarnos más de ellos. Esta ``confianza'' la implementamos con una ponderación, obteniendo el \concept{Mínimos cuadrados ponderados}



Los cálculos se dejan para el lector, aunque el resultado será:

\begin{itemize}
	\item Ambos son insesgados.
	\item En términos de varianza, es mejor el ponderado.
	\item ¿Cuál es el problema de ponderar? Que no sabemos con exactitud que $ε_i \sim N(0,σ^2x_i^2)$. ¿Y si fuera $ε_i \sim N(0,σ^2x_i^4)$? Entonces no podríamos aplicar los pesos calculados y es muy problemático en ese sentido.
	
\end{itemize}

\end{problem}


\begin{problem}[9]
Se considera el siguiente modelo de regresión lineal múltiple:
\begin{equation}
	\label{eq:ej9_hoja3}
	Y_i=β_0 + β_1x_{i1} + β_2x_{i2} + β_3x_{i3} + \epsilon_i,\ \epsilon_i \equiv N(0, σ^2)
\end{equation}

Se dispone de $n = 20$ observaciones con las que se ajustan todos los posibles submodelos del modelo \ref{eq:ej9_hoja3}, obteniéndose para cada uno de ellos las siguientes sumas de cuadrados de los errores (todos los submodelos incluyen un término independiente).

\scalebox{0.8} {
	\begin{tabular}{ c c | c c }
		Variables incluidas en el modelo & $SCE$ & Variables incluidas en el modelo & $SCE$ \\ \hline
		Sólo término independiente & 42644.00 & $x_1$ y $x_2$ & 7713.13 \\
		$x_1$ & 8352.28 & $x_1$ y $x_3$ & 762.55 \\
		$x_2$ & 36253.69 & $x_2$ y $x_3$ & 32700.17 \\
		$x_3$ & 36606.19 & $x_1, x_2$ y $x_3$ & 761.41
	\end{tabular}
}

(\textbf{Ejemplo en negrita}: Para el modelo ajustado $\hat{Y}_i = \hat{β}_0 + \hat{β}_2x_{i2} + \hat{β}_3x_{i3}$, la suma de cuadrados de los errores es 32700.17).

\ppart Calcula la tabla de análisis de la varianza para el modelo \ref{eq:ej9_hoja3} y contrasta a nivel α = 0,05 la hipótesis nula $H_0: β_1 =β_2 =β_3 =0$.

\ppart En el modelo \ref{eq:ej9_hoja3}, contrasta a nivel $α=0.05$ las dos hipótesis nulas siguientes:
\begin{itemize}
	\item $H_0: β_2 = 0$
	\item $H_0: β_1 = β_3 = 0$
\end{itemize}

\ppart Calcula el coeficiente de correlación entre la variable respuesta y la primera variable regresora sabiendo que es positivo.

\solution
\textbf{OJO : en clase dijo que este era uno de los problemas difíciles de un control}

\spart
Bajo $H_0: β_1 =β_2 =β_3 =0$ tendremos que $Y_i=β_0 + ε_i$ y que $\hat{β}_0 = \overline{Y}$, y por tanto:
\[SCE_0=\sum (Y_i - \hat{Y}_i)^2 \underbrace{=}_{H_0} \sum (Y_i - \overline{Y})^2 = SCT\]

En este caso tenemos que llevar a cabo el cálculo del estadístico del contraste de la regresión (véase \ref{prop:estad_contr_regresion}) $F = \frac{SCR/k}{SCE/(n-k-1)}$. Como sabemos que $SCT = SCE + SCR \implies SCR = SCE_0-SCE = 42644.00 - 761.41 = 41882.59$ podemos obtener la tabla con la que conseguimos el estadístico:

\begin{center}
	\begin{tabular}{ c c c c c }
		Fuente & $SC$ & gl & $CM$ & $F$ \\ \hline
		Explicada & $SCR=41882.59$ & $k=3$ & $13960.86$ & $293.37$ \\
		No explicada & $SCE=761.41$ & $n-k-1=16$ & $47.59=S_R^2$ \\ \hline
		Total & $42644$ & $19$
	\end{tabular}
\end{center}

Sabemos que la región de rechazo será: $R = \{ F > F_{3,16;0.05} = 3.24 \}$, y por tanto \textbf{rechazamos $H_0$}.

\spart
\begin{itemize}
	\item $H_0: β_2=0$. En este caso contrastamos el incremento de variabilidad relativa entre el modelo en el que solo tenemos en cuenta $x_1,x_3$, frente al modelo completo en el que tenemos en cuenta $x_1,x_2,x_3$:

	\[F = \frac{\frac{SCE_0 - SCE}{p=1}}{\frac{SCE}{n-k-1}} = \frac{SCE_0 - SCE}{S_R^2} = \frac{762.55 - 761.41}{47.59} ≈ 0.024\]

	En este caso la región de rechazo es $R = \{ F > F_{1,16;0.05} = 4.49 \}$, y por tanto no rechazamos la hipótesis nula $H_0$.

	\item $H_0= β_1=β_3=0$, aplicando el mismo criterio que en caso anterior obtenemos:

	\[F = \frac{\frac{SCE_0 - SCE}{2}}{S_R^2} = \frac{\frac{36253.69 - 761.41}{2}}{47.59} = 372.9\]

	Puesto que $F_{2,16;0.05}=3.63$, rechazamos esta hipótesis nula.
\end{itemize}

\spart
Correlación entre $Y$ y $x_1$:
\[r^2 = R^2 = 1 - \frac{SCE}{SCT} = 1 - \frac{8352.28}{42644} = 0.8041\]

De modo que tendremos $r=\pm \sqrt{0.8041}$, y con la ayuda del enunciado podemos decir que $r = +\sqrt{0.8041} = 0.8967$

\end{problem}



\begin{problem}[12]
Considera el modelo de regresión múltiple $T=Xβ + ε$, donde $ε$ verifica las hipótesis habituales.

\spart Define el vector de valores ajustados $\gor{Y} = (\gor{Y_1},...,\gor{Y_n})$
\solution
\ppart
\[\hat{Y} = X\hat{β} = HY\]
\[\hat{Y} \equiv N(Xβ,σ^2H)\]
Sabemos que esa es la varianza porque $Y \equiv N_n(Xβ,σ^2I_n)$ y aplicamos $\hat{Y} = AY \to \var{\hat{Y}} = AΣA'$

\ppart No son independientes en general porque $H$ no es siempre diagonal.
No porque no tienen la misma varianza ni la misma media.

\ppart Lo que nos piden es la traza de $H$. Como $H$ es idempotente, tenemos $σ^2\tr(H) = σ^2\rg(H) = σ^2(k+1) = 4σ^2$

Sabemos que $\rg(H) = k+1$ por hipótesis (tenemos $k$ variables más el término independiente). En este caso $k=3$.

\end{problem}


\begin{problem}[14]
Tres vehículos se encuentran situados en los puntos $0 < β_1 < β_2 < β_3$ de una carretera recta. Para estimar la posición de los vehículos se toman las siguientes medidas (todas ellas sujetas a errores aleatorios de medición independientes con distribución normal de media $0$ y varianza $σ^2$ ):

\begin{itemize}
	\item Desde el punto $0$ medimos las distancias a los tres vehículos dando $Y_1 , Y_2 e Y_3$
	\item Nos trasladamos al primer vehículo y medimos las distancias a los otros dos, dando dos
nuevas medidas $Y_4,Y_5$ .
	\item Nos trasladamos al segundo vehículo y medimos la distancia al tercero, dando una medida
adicional $Y_6$.
\end{itemize}

\spart Expresa el problema de estimación como un modelo de regresión múltiple indicando clara-
mente cuál es la matriz de diseño.

\spart Calcula la distribución del estimador de mínimos cuadrados del vector de posiciones $(β_1 , β_2 , β_3 )$.


\spart Se desea calcular un intervalo de confianza de nivel 95 \% para la posición del primer vehículo $β_1$ a partir de $6$ medidas (obtenidas de acuerdo con el método descrito anteriormente) para las que la varianza residual resultó ser $S_R^2 = 2$. ¿Cuál es el margen de error del intervalo?

\solution


\ppart 

\begin{gather*}
Y_1 = β_1 + ε_1\\
Y_2 = β_2 + ε_2\\
Y_3 = β_3 + ε_3\\
Y_4 = β_2 - β_1 + ε_4\\
Y_5 = β_2 - β_1 + ε_5\\
Y_6 = β_3 - β_2 + ε_6
\end{gather*}

Vamos a construir la matriz de diseño. Será de la forma:

\[
Y = \begin{pmatrix} X \end{pmatrix}\begin{pmatrix}β_1\\β_2\\β_3\end{pmatrix} + ε
\]

De esta manera: 
\[
X = \begin{pmatrix}
1&0&0\\
0&1&0\\
0&0&1\\
-1&1&0\\
-1&0&1\\
0&-1&1
\end{pmatrix}
\]


\paragraph{Se ha dejado caer en clase un posible ejercicio de examen: } ¿Cuál es la matriz de diseño óptima para estimar los $β_i$?

\ppart Con esta matriz de diseño, podemos calcular:

\[
\hat{β} = N_3\left(β, σ^2\underbrace{\begin{pmatrix} 
\frac{1}{2} & \frac{1}{4} & \frac{1}{4}\\
\frac{1}{4} & \frac{1}{2} & \frac{1}{2}\\
\frac{1}{4} & \frac{1}{4} & \frac{1}{2}
\end{pmatrix}}_{(X'X)^{-1}}  \right)
\]

\ppart 

\[ IC_{0.95}(β_1) \equiv \left[\hat{β_1} \mp t_{6-3;0.025}S_R\sqrt{q_{11}} \right]\]
\[ IC_{0.95}(β_1) \equiv \left[\hat{β_1} \mp t_{6-3;0.025}\sqrt{2}\sqrt{\frac{1}{2}} \right]\]

Con lo que el margen de error es $t_{6-3;0.025}$

\end{problem}

\begin{problem}[15]

\solution

\ppart 

\[Y = \begin{pmatrix}0&1\\1&0\\1&1\end{pmatrix}\begin{pmatrix}µ\\λ\end{pmatrix}+\begin{pmatrix}ε_1\\ε_2\\ε_3\end{pmatrix}\]

\ppart 

Tenemos una fórmula para calcularlo.

\[
\begin{pmatrix}\hat{λ}\\\hat{µ}\end{pmatrix} = (X'X)^{-1}X'Y
\]

En caso de no sabernos la fórmula, podemos recurrir al método largo y tradicional:

\[φ(λ,µ) = (Y_1-µ)^2  + (Y_2-λ)^2 + (Y_3-(λ+µ))^2\]
Y resolvemos el sistema:

\[
\left.\begin{array}{cc}
\dpa{φ}{λ} &= 0\\
\dpa{φ}{µ} &= 0
\end{array}\right\}
\]

De esta manera deberíamos llegar a la misma solución.

\[\begin{pmatrix}\hat{λ}\\\hat{µ}\end{pmatrix} = \begin{pmatrix}
\frac{2Y_2 + Y_3 - Y_1}{3}\\\frac{2Y_1 + Y_3 - Y_2}{3}
\end{pmatrix}\]

Podríamos comprobar si son insesgado o no.

\end{problem}


\begin{problem}[16]

Típico ejercicio de modelo unifactorial

\solution

\[Y_{1·} = 4.99; S_1 = 4.19\]

Vamos a construir la tabla ANOVA. Para ello:

\[Y_{··} ≠ \frac{\sum Y_{i·}}{4}\]
Ya que el número de alumnos es distinto en cada grupo. La media total sería:

\[Y_{··} = \frac{\sum n_i Y_{i·}}{\sum n_i}\]

Ahora podemos calcular $SCR = \sum_{i=1}^4 n_i (Y_{i·} - Y_{··})^2 = ... = 10.93$

\[SCE = \sum_i \sum_j(Y_{ij}-\gor{Y_{i·}})^2\]
\begin{center}
\begin{tabular}{ccccc}
Fuente & SC & gl & CM&F\\\hline
$SCR$ & $10.93$ & $4-1$ & $\frac{10.93}{3} = 3.64$&$\frac{3.64}{5.09} = 0.72$\\
$SCE$ & $1785.17$ & $n-k = 351$ & $\frac{1785.17}{351} = 5.09$ & 
\end{tabular}
\end{center}

Ahora buscamos $F_{3,351;0.05} = 2.60 > 0.72$, por lo que no hemos encontrado diferencias significativas de que el grupo influya en la nota. Aceptamos $H_0$.

\end{problem}


\begin{problem}[17]
Una fabricante de botas para el agua está estudiando tres posibles colores para su nuevo modelo de bota super resistente. Las opciones que se proponen son verde, amarillo y azul. Para analizar si el color tiene algún efecto sobre el número de ventas, se eligen 16 tiendas de tamaño similar. Se envían las botas de color verde a 6 tiendas elegidas al azar, las amarillos a 5 tiendas y las azules a las 5 restantes. Después de varios días se comprueba el número de botas vendidas en cada tienda, obteniéndose los siguientes resultados:
\begin{center}
\begin{tabular}{ccc}
Verdes&Amarillas&Azules\\\hline
43&52&61\\
52&37&29\\
59&38&38\\
76&64&53\\
61&74&79\\
81&&
\end{tabular}
\end{center}

\solution
Es igual que el anterior. Se deja para otro.

\end{problem}


\section{Hoja 4}

\begin{problem}[1]

\ppart Estima a partir de estos datos, la función lineal discriminante de Fisher.

\ppart Clasifica la observación $xx = (2,7)'$ utilizando la regla obtenida en el apartado anterior.
\solution

\spart Vamos a estimar las medias de cada población: 
$$\hat{µ_0} = \gor{x_0} = (3,6)'$$
$$\hat{µ_1} = \gor{x_1} = (5,8)'$$

Y la estimación de la matriz de varianzas, para lo que necesitamos:

\[
S_0 = \begin{pmatrix}1&1.5\\1.5&3\end{pmatrix} \quad
S_1 = \begin{pmatrix}1&0.5\\0.5&1\end{pmatrix}
\]

\[
\hat{Σ} = \frac{(n_0-1)S_0  + (n_1 - 1)S_1}{n_0+n_1-2} = \frac{S_1+S_2}{2} = \begin{pmatrix}1&1\\1&2\end{pmatrix}
\]

Por último, la dirección proyección de la regla de fisher es 
\[
ω = \hat{Σ^{-1}}(\gor{x}_1  - \gor{x}_0) = \begin{pmatrix}2&-1\\-1&1\end{pmatrix}\begin{pmatrix}2\\2\end{pmatrix} = \begin{pmatrix}2\\0\end{pmatrix}
\]

Entonces, utilizando la fórmula de clasificación de la regla de fisher, obtenemos:

\[
2x_1 > (2,0)\begin{pmatrix}4\\7\end{pmatrix} \to x_1 > 4
\]

\spart Como $x_1 = 2 \neg > 4$, el punto $x=(2,7)'$ lo clasificamos como $P_0$.

\obs La frontera es una linea vertical. Las segundas coordenadas no importan nada, es curioso.
\end{problem}

\begin{problem}[2]
Considera los datos sobre enfermedades coronarias en Sudáfrica (infartos.RData). Calcula la
función lineal discriminante de Fisher para clasificar entre sano (clase = 0) o enfermo (clase = 1) a
un individuo en función de las 8 variables regresoras contenidas el fichero. Compara los coeficientes
de las variables con los correspondientes a la regla de clasificación basada en regresión logística.
¿Son muy diferentes?
\solution

\begin{lstlisting}[style=mystyle]
# X <- matriz de los datos con $p = 8$ columnas y $n$ filas.
# clases <- vector de $0$ o $1$ si el individuo (columna) ha sufrido infarto (1) o no (0).
infartos <- lda(X,clases,prior=c(0.5,0.5))
\end{lstlisting}


\end{problem}

\begin{problem}[3]
Para 100 lirios, 50 de ellos correspondientes a la especie Versicolor (Y = 1) y otros 50 corres-
pondientes a la especie Virginica (Y = 0) se ha medido la longitud (Long) y la anchura (Anch) del
pétalo en milímetros. Con los datos resultantes se ha ajustado un modelo de regresión logística con
el objetivo de clasificar en alguna de las dos especies un lirio cuya especie se desconoce a partir de las medidas de su pétalo. A continuación se muestra un resumen de los resultados 
(algunos valores han sido suprimidos o sustituidos por letras):


\begin{lstlisting}[style=mystyle]

glm(formula = y ~ Long + Anch, family = binomial)

Deviance Residuals:

Min		1Q		Median		3Q		Max
-1.8965923		-0.0227388		0.0001139		0.0474898		1.7375172

Coefficients:

		Estimate		Std.Error		z-value		Pr(>|z|)
(Intercept)		45.272		13.610		3.327		0.00088
Long		-5.755		2.306		*****		BBBB
Anch		-10.447		3.755		-2.782		0.00540


Null deviance: 138.629 on 99 degrees of freedom
Residual deviance: AAAA on 97 degrees of freedom

AIC: 26.564
\end{lstlisting}

\ppart Escribe la fórmula de lo que en la salida de $R$ se llama ``Deviance residuals'' y calcula la suma de estos residuos al cuadrado.

\ppart Calcula la desviación residual AAAA y contrasta, usando el método de razón de verosimilitudes, la hipótesis de que ningunad elas 2 medidas influte en la variable respuesta: $H_0: β_1 = β_2 = 0$

\ppart Calcula el p-valor BBBB y contrasta a nivel α = 0,05 la hipótesis nula de que la longitud del
pétalo no es significativa para explicar la respuesta.

\ppart Para un lirio se sabe que la longitud del pétalo es de 4.9 mm y la anchura es 1.5 mm. ¿En cuál
de las dos especies se debe clasificar?

\solution

\spart Clacular la suma de los resudios y la desviación residual es lo mismo. Es el valor objetivo que sale al maximizar.

\[
	l(\hat{β}) = \sum D_i^2
\]

Si recordamos la información de Akaike (\ref{def:AIC}), tenemos:

	\[AIC = -2l(\hat{β}) + 2(k+1) = -2A  + 6 \to A = -2l(\hat{β}) = 26.564 - 6 =  20.564 =  \sum_{i=1}^{n} D_i^2\]

\spart \[138.629 - 20.564 = 118.065\]

Y comparamos este valor con $\chi^2_{2;0.05} = 5.99$, con lo que rechazamos la hipótesis y concluimos que las medidas de la planta influyen en la clase.

\spart Vamos a utilizar el test de Wald (\ref{test:Wald}) para contrastar $H_0 : β_1 = 0$

Tenemos $z = \frac{-5.755}{2.306} = -2.4957$.

\[B = P(|z| > 2.4957) \simeq 0.0128\]

\spart Clasificar en $Y=1$, entonces:

\[
	\frac{1}{1+e^{-x'β}} > \frac{1}{2} \to \hat{β}x > 0
\]

Es decir, la regla de clasificación logística en este caso es:

\[
45.272 - 5.755·\underbrace{Long}_{4.9} - 10.447 · \underbrace{Anch}_{1.5} > 0
\]

\obs ¿Y cuál es la probabilidad estimada de clasificar como $Y=1$?  No es lo mismo obtener en la regla anterior 0.001 o 0.9, que ambos son positivos. Para ello:

\[
	\frac{1}{1+e^{-x'β}} = \frac{1}{1+e^{1.4020}} = 0.19
\]
\textcolor{red}{Esto es un poco raro.}

\end{problem}

\begin{problem}[4]


\solution
La dificultad de este problema radica en cómo introducir en $R$ los datos para aplicar el comando $glm$.

Para ello, tenemos que meter $n$ datos, por cada $n$ insectos expuestos a un nivel de dosis.

Nuestro vector $X$ entonces es:

\[X = \left( \underbrace{1.69, ... , 1.69}_{59}, \underbrace{1.7242,...,1.7242}_{60}, ... \right)\]

Y nuestro vector de clases sería:

\[
Y = \left( \underbrace{1,...,1}_{6}, \underbrace{0,...,0}_{53}, ... \right)
\]


En $R$ sería:

\begin{lstlisting}[style=mystyle]
y  = c(rep(1,6),rep(0,53),rep(1,13),rep(0,60-13),...)
\end{lstlisting}

Y ahora con los datos ya podemos calcular
\begin{lstlisting}[style=mystyle]
reg <- glm(y ~ dosis, family='binomial')
\end{lstlisting}

Y ahora ya podemos utilizar:

	\[\hat{P}(Y=1 | X=1.8) = \frac{1}{1+\exp{-\hat{β}_0 - \hat{β}_1(1.8)}} = 0.72\]

\end{problem}


\begin{problem}[5]
Para tratar la meningitis bacteriana es vital aplicar con urgencia un tratamiento con antibióticos.
Por ello, es importante distinguir lo más rápidamente posible este tipo de meningitis de la menin-
gitis vírica. Con el fin de resolver este problema se ajustó con R un modelo de regresión logística a
las siguientes variables medidas en 164 pacientes del Duke University Medical Center:

\begin{center}
\begin{tabular}{cc}
Nombre variable & Descripción\\\hline
age & Edad en años \\
bloodgl & Concentración de glucosa en la sangre \\
gl & Concentración de glucosa en el líquido cefalorraquídeo \\
pr & Concentración de proteína en el líquido cefalorraquídeo \\
whites & Leucocitos por mm 3 de líquido cefalorraquídeo \\
polys & Porcentaje de leucocitos que son leucocitos polimorfonucleares \\
abm & Tipo de meningitis: bacteriana (abm=1) o vírica (abm=0) 
\end{tabular}
\end{center}


El resultado del ajuste se muestra a continuación (algunos valores se han sustituido por letras):



\ppart Calcula el valor de A en la salida anterior sabiendo que hay 68 pacientes con meningitis
bacteriana en la muestra.

\ppart Calcula el valor de B en la salida anterior. A nivel $α = 0.1$, ¿puede afirmarse que al aumentar la
cantidad de leucocitos en el líquido cefalorraquídeo disminuye la probabilidad de que la meningitis
sea de tipo vírico?

\ppart En un análisis realizado a un paciente de 15 años se han determinado los siguientes valores para
el resto de variables:
\begin{tabular}{cc}
bloodgl&119\\
gl&72\\
pr&53\\
whites&262\\
polys&41
\end{tabular}

¿En cuál de los dos tipos de meningitis debe clasificarse este paciente?

\solution
Tenemos $k=6$ y una proporción de $\frac{68}{164}$ individuos con meningitis bacteriana.

\spart \textit{Null deviance} $\equiv A \equiv D_0^2 \equiv -2\log(\hat{B}^{(0)})$ bajo $H_0 : β_1 = ... = β_k = 0$

Bajo $H_0$, $Y_1,..,Y_n \overset{iid}{\sim} B(1,p)$.

El E.M.V. de $p$ es $\hat{p} = \frac{68}{164}$, entonces:

\[L(p) = \prod_{i=1}^n p^{Y_i}(1-p)^{1-Y_i} \to L(\hat{p}) = \sum_{i=1}^n\left[Y_i\log(\hat{p}) + (1-Y_i)\log(1-\hat{p})\right] = 68\log\left(\frac{68}{164}\right) + (164-68)\log\left(1-\frac{68}{164}\right)\]

\spart $B$ es el estadístico de Wald para la variable ``white''

¿Cuál es nuestra $H_0$? Tenemos que si aumenta ``white'', entonces $P(Y=0|x)$ disminuya. Esto no es la hipótesis nula, sino la hipótesis alternativa. Para construir la hipótesis nula, si ``white'' aumenta, etonces $H_0: P(Y=1|x)$ disminuya $\dimplies β_5 \leq 0$\footnote{Es importante darnos cuenta de que 0 es vírico y 1 bacteriano, al revés que la pregunta}

Entonces $B = z = \frac{\hat{β}_5}{e.t.(\hat{β}_5)} = \frac{0.00079971}{0.0005108} \simeq 1.56$

Para el contraste con $α=0.1$ y $H_0: β_5 \leq 0$.

\[R = \left\{  \right\}\]

	\spart \[\hat{β_0} + \sum_{i=1}^6 \hat{β}_ix_i = ... = -4.3136 < 0\]
	Al ser negativo, lo clasificamos como vírico.
\end{problem}


\begin{problem}[6]

Supongamos que la distribución de X condicionada a Y = 1 es normal con vector de medias μ 1 y
matriz de covarianzas Σ, mientras que la distribución de X condicionada a Y = 0 es normal con
vector de medias μ 0 y la misma matriz de covarianzas Σ (caso homocedástico). Demuestra que el
error de la regla Bayes (error Bayes) del correspondiente problema de clasificación es:

donde $∆^2 = (μ_0 − μ_1 ) Σ^{−1}(μ_0 − μ_1 )$ es el cuadrado de la distancia de Mahalanobis entre los dos
vectores de medias y Φ es la función de distribución de una v.a. normal estándar. (Se supone que
las probabilidades a priori de ambas poblaciones son iguales, π 0 = π 1 = 1/2)
\solution

En este caso, la regla de bayes es la regla de Fisher.

Definimos $\appl{g^{\ast}}{ℝ^k}{\{0,1\}}$ definida como:

\[
g^{\ast} = \left\{ 
\begin{array}{cc} 
1 & ω'\left(x-\frac{µ_0 + µ_1}{2}\right) > 0\\
0 & ω'\left(x-\frac{µ_0 + µ_1}{2}\right) \leq 0
\end{array} \right. \text{ donde } ω = Σ^{-1}(µ_1 - µ_0)
\]

Para calcular el error, $L^{\ast} = P(g^{\ast}(x) \neq Y) = P(g^{\ast}(x) = 1, Y=0) + P(g^{\ast}(x) = 0 Y=1)$.

Vamos a calcular sólo uno de ellos:

\[P(g^{\ast}(x) = 1, Y=0) = P(g^{\ast}(x) = 1 | Y=0) \underbrace{P(Y=0)}_{\frac{1}{2}} \]
Por otro lado,

\[P(g^{\ast}(x) = 1 | Y=0) = P\left(ω'\left(x-\frac{µ_0 + µ_1}{2}\right) > 0 | Y=0\right)\]

¿Y cuál es la distribución de $\displaystyle ω'\left(x-\frac{µ_0 + µ_1}{2}\right) |_{Y=0}$? Es una \textbf{normal} (no se muy bien porqué)

Ahora, calculamos la media
\[
ω'\left(x-\frac{µ_0 + µ_1}{2}\right) = \frac{1}{2}(µ_1-µ_0)'Σ^{-1}(µ_0-µ_1) = ... = -\frac{1}{2}\Delta
\]

y la varianza:

\[
\var{ω'x | Y=0} = ω'Σω = ... = \Delta^2
\]


Entonces,

\[
ω'\left(x-\frac{µ_0 + µ_1}{2}\right) |_{Y=0} \equiv N\left(-\frac{1}{2}\Delta,\Delta^2\right)
\]

\paragraph{Por último}, siendo $z\sim N(0,1)$

\[
P\left(ω'\left(x-\frac{µ_0 + µ_1}{2}\right) > 0 | Y=0\right) = P\left(z>\frac{0 - \left(-\frac{\Delta^2}{2}\right)}{1}\right) = P\left(z>\frac{1}{2}\right) = 1-\Phi\left(\frac{\Delta}{2}\right)
\]

¿Tiene esto sentido? $L^{\ast}$ es una función decreciente de $Δ = \left((µ_1 - µ_0)'Σ^{-1}(µ_1-µ_0)\right)^{\frac{1}{2}}$. Esto quiere decir que si $µ_0 = µ_1$ (y como teníamos $Σ_1 = Σ_2 = Σ$), necesariamente $L^{\ast} = \frac{1}{2}$. \footnote{Si las distribuciones son exactamente iguales, no tenemos manera de distinguirlas}. Por otro lado, cuando $Δ\to ∞$, tenemos un error que tiende a 0, consecuencia con sentido también. 

\end{problem}