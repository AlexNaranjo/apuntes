% -*- root: ../EstadisticaII.tex -*-
\section{Contrastes no paramétricos}

Hipótesis no paramétrica: hipótesis que no se formula en términos de un número finito de parámetros.

\begin{enumerate}
\item Bondad de ajuste: A partir de una muestra $X_1,...,X_n \equiv F$ de variables aleatorias independientes idénticamente distribuidas, contrastar:
\begin{itemize}
\item $H_0: F=F_0$ donde $F_0$ es una distribución prefijada.
\item $H_0: F \in \{F_{\theta} : {\theta}\in H\}$, donde H es el espacio paramétrico.
\end{itemize}
\item Homogeneidad: Dados $X_1,...,X_n \equiv F$ y $Y_1,...,Y_n \equiv G$ de variables aleatorias independientes idénticamente distribuidas. Contrastar $H_0: F=G$.
\item Hipótesis de independencia: Dada $(X_1,Y_1),...,(X_n,Y_n) \equiv F$ de variables aleatorias independientes idénticamente distribuidas. Contrastar $H_0: X$ e $Y$ son independientes.
\end{enumerate}

\subsection{Contraste $\chi^2$ de bondad de ajuste}
Consideramos una distribución totalmente especificada bajo $H_0: X_1,...,X_n \equiv F$ de variables i.i.d.

$H_0: F=F_0$ es la hipótesis nula y queremos ver que F, que es la distribución obtenida con los datos verdaderos (obtenidos empíricamente) es igual a $F_0$ que es la distribución teórica.

Vamos a definir los pasos que tenemos que seguir para comprobar si $H_0$ es cierta:
\begin{enumerate}
\item Se definen k clases $A_1,...,A_k$. En el caso del dado, los valores de cada cara.
\item Se calculan las frecuencias observadas de datos en cada clase.
\subitem \[O_i = \#\{j\tq X_j ∈ A_i\}\]
\[O_i \sim Bin\left(n,p_i = p_{H_0}(A_i)\right) \text{ es una variable aleatoria}\]
\item Se calculan las frecuencias esperadas si $H_0$ fuera cierta:
	\subitem\[\esp{}_i = \esp{}_{H_{0}} = np_i \text{ al ser} O_i \text{ una binomial}\]
\item Se comparan $O_i$ con $E_i$ mediante el \concept{estadístico de Pearson}, para comprobar si lo observado se parece a lo esperado.

\[T = \sum_{i=1}^k \frac{(O_i - E_i)^2}{E_i}\]

\subitem Más adelante justificaremos porqué este estadístico es el utilizado. Además, el estadístico puede calcularse de otra manera:

\[
T = \sum \frac{O_i^2}{E_i} - n
\]
\item Se rechaza $H_0$ en la región crítica $R = \{ T>c\}$, donde $c$ depende del nivel de significación $α$.
\subitem $c$ se obtiene consultando en las tablas para $α = P_{H_0}(T>c)$
\end{enumerate}

Pero se nos presenta el siguiente problema, ¿Cuál es la distribución de $T$ bajo $H_0$?

\begin{theorem}[Distribución del estadístico de Pearson]
Bajo $H_0$

\[
T = \sum_{i=1}^k \frac{(O_i - E_i)^2}{E_i} \convs[d] \chi^2_{k-1}
\]
\end{theorem}

\begin{proof}
\textcolor{red}{Para otro momento}
\end{proof}

\begin{example}
Tiramos un dado 100 veces y obtenemos:

\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Resultados & 1 & 2 & 3 & 4 & 5 & 6 \\
\hline
Frecuencia & 10 & 20 & 20 & 10 & 15 & 25\\
\hline
\end{tabular}

Y consideramos $H_0: p_i=\frac{1}{6} \; \forall i=1,...,6$. Es decir que el dado no está trucado y cada cara tiene la misma probabilidad ($p_i$) de salir.

¿Es cierta la hipótesis, con un nivel de confianza/significación del 95\%?


Las clases son cada uno de los posibles resultados y las frecuencias observadas se encuentran en la tabla.

Vamos a calcular el estadístico $T$ \[T = \sum_{i=1}^6 \frac{O_i^2}{E_i} - n = \frac{6}{100} (\sum O_1^2) - 100 = ... = 11\]

Ahora, consultando las tablas buscamos el valor $\chi^2_{5;0.05} = 11.07 > T = 11$, entonces no estamos en la región crítica, por lo que no podemos rechazar la hipótesis.

Al ser valores muy próximos, observamos que el p-valor\footnote{el menor nivel de confianza para poder rechazar} del contraste tendrá que ser algo mayor que $0.05$.

\end{example}







\subsubsection{Hipótesis nula compuesta}

Vamos a estudiar el siguiente problema. Sea $X_1,...,X_n \overset{}{\sim}{iid}  F$ y una hipótesis compuesta: $H_0: F\in \{ F_{\theta}, \theta\in Ω \subset ℝ^n\}$. Esta hipótesis compuesta puede ser ``los datos se distribuyen normalmente, con media y varianza desconocidas''

Los pasos a seguir son:
\begin{enumerate}
	\item Definir las clases $A_1,...,A_k$
	\item Calcular las frecuencias observadas $O_1,...,O_n$
	\item Estimamos $\theta$ por el método de máxima verosimilitud. Sea $\gor{\theta}$ el e.m.v.
	\subitem Pero para calcular las frecuencias esperadas, no tenemos una única normal. La idea intuitiva sería: hay unos parámetros que son los que mejor ajustan la distribución. ¿Cuál es la que mejor ajusta? La que tenga los parámetros estimados.
	\item Ahora ya podemos calcular las frecuencias esperadas:

	$E_i = n\gor{p_i}$ donde $\gor{p_i} = P_{\theta}(A_i)$, con $i=1,...,k$
	\item Ya podemos calcular el estadístico de Pearson:

	\[
	T= \sum_{i=1}^k \frac{(O_i - \gor{E_i})^2}{\gor{E_i}}
	\]

	\subitem ¿Qué distribución tiene este estadístico? Antes hemos visto que es una $\chi^2_{k-1}$ cuando se dan unas ciertas condiciones.

	En este caso, es de esperar que $T$ tienda a tomar valores menores que en el caso simple.

	Además, al estimar $r$ parámetros (las $r$ componenetes del vector $\theta$)) se introducen $r$ nuevas restricciones sobre el vector $(O_1,...,O_r)$.

Se puede probar, \footnote{bajo ciertas condiciones de regularidad que las distribuciones que conocemos cumplen y que son demasiado complicadas de enunciar} que:

\[
	\sum_{i=1}^k \frac{(O_i - \gor{E_i})^2}{\gor{E_i}} \convs[d] \chi^2_{k-1-r}
\]

	\item Se rechaza $H_0$ en  la región crítica \[R = \{ T > \chi^2_{k-1-r}\}\]
\end{enumerate}

\begin{example}
\paragraph{Los bombardeos de Londres}

Los alemanes bombardeaban mucho a Londres durante la guerra mundial, y los ingleses quérían saber si los alemanes podían dirigir los misiles, o los impactos eran aleatorios.

Para ello, alguien hizo el estudio estadístico, para contrastar la hipótesis ``los impactos son aleatorios''.

Los impactos deberían seguir una poisson \footnote{ya que es el límite de una binomial, en la que consideramos los impactos como éxitos}. Para ello, dividió Londres en $n=576$ cuadrados, cada uno de ellos será la variable $X_i$ que debería seguir una Poisson.

La idea del contraste es: la Poisson que más se puede parecer es la que tenga de media el e.m.v. Si esa Poisson no se parece, entonces ninguna Poisson se puede parecer.



\begin{enumerate}
	\item[Clases] Los valores que toma la Poisson (recordamos que son número naturales). En este caso sólo se han definido 5, ya que la última es $>4$ \footnote{Se recomienda no definir más de 5 clases, para que la estimación no pierda demasiada información. }
	\item[Obs] $O_i = \{\#j : X_j=i\}$, por ejemplo, las frecuencias observadas de la clase $0$, es decir, $O_0 = 229$, donde ese número es el número de las $n$ regiones de Londres en donde no cayó ningún misil.
	\item[e.m.v.] El e.m.v. de una Poisson es la media muestral, con lo que $\hat{λ} = \gor{x} = 0.9323$
	\item[Esp] Calculamos las frecuencias esperadas utilizando el parámetro estimado $$\hat{E_i} = n\hat{p_i} = 576 · e^{-0.9323}\frac{(0.9323)^i}{i!}$$
	\item[T] El estadístico $\chi^2$ de Pearson es: \[T = \sum_{i=1}^k \frac{(O_i - \gor{E_i})^2}{\gor{E_i}} = 1.01 \]

	Desde esta información, ya podemos hacernos una idea de si vamos a poder rechazar. ¿Por que? Al estimar un único parámetro $T \overset{iid}{\sim} \chi^{2}_{5-1-1}$, y además $\mathbb{E}(\chi^2_k) = k$, con lo que  debería habernos salido $T=3$. Al ser un vector bastante normal, podemos ver que tiene muy poca pinta de que vayamos a poder rechazar la hipótesis nula. Al $T$ estar por debajo de $3$ no estaremos en la región crítica.

\end{enumerate}


\obs Este ejemplo se encuentra también en las transparencias, donde podemos ver los valores y algunas gráficas explicativas.
\end{example}


\subsection{Contrastes con $R$}
\paragraph{Hipótesis simple}
Con el siguiente código de $R$, podemos hacer el contraste de bondad de ajuste de una $\chi^2$ fácilmente.

\begin{lstlisting}[style=mystyle]
obs = c(10,20,20,10,15,25)
ls.str(chisq.test(obs))
\end{lstlisting}

Si a \textit{chisq.test} no le damos más argumentos, supondrá hipótesis simple con equiprobabilidad de $p$. Podríamos darle otro argumento, y hacer lo siguiente para el ejemplo de los misiles:

\begin{lstlisting}[style=mystyle]
res = c(seq(0,4),7)
obs = c(229,211,93,35,7,1)
n = sum(obs)
lambda = sum(res*obs)/n
prob = dpois(res,lambda)
esp = n*prob
# Se agrupan las dos ultimas clases:
obs = c(obs[1:4],sum(obs[5:6]))
prob = c(p[1:4],1-sum(p[1:4]))
esp = c(esp[1:4],n-sum(esp[1:4]))
# Codigo para el grafico de barras:
matriz = rbind(p,obs/n)
rownames(matriz) = c('Frecuencias','Poisson')
barplot(matriz,beside=TRUE,names.arg=c(0:4),legend.text=TRUE,
col=c('lightgreen','orange'))
# Test chi 2
t = chisq.test(obs,p=prob)$statistic
pvalor = 1 - pchisq(t,3)
\end{lstlisting}



\begin{defn}[Contraste de Bondad de Ajuste de Kolmogorov-Smirnov]

$X_1,...,X_n \overset{iid}{\sim} F$ y $H_0 : F=F_0$, con $F_0$ totalmente especificada.

Se define la función de distribución empírica correspondiente a $X_i$ como \[F_n(x) = \frac{1}{n}\#\{ i : X_i\leq x\}\]

\end{defn}

\begin{example}
$X_1 = 1$, $X_2 = 6$, $X_3 = 4$

Recordamos los \concept{estadísticos de orden: } $X_{(i)}$ significa el $X_j$ que ocupa la posición $i-esima$ cuando ordenamos de menor a mayor. Es decir, $X_{(1)} = \min\{X_j\}$ y $X_{(n)} = \max\{X_j\}$ 

Es una función de distribución constante a trozos con saltos de magnitud $\frac{1}{n}$ en cada valor muestral $X_i$ (si no hay empates). En este caso, la función de distribución es $f(a) = \frac{1}{3}, a=\{1,4,6\}$

$F_n$ es un estimador de la verdadera distrubición de $F$ \[F(x)) = P(X\leq x)\]
\end{example}

\obs \[\# \{i: X_i \leq x\}\] ¿Qué distribución tiene esta variable aleatoria? Es una binomial con parámetros: $B(n,F(x))$ 

\[ \esp{F_n(x)} = \frac{1}{n}nF(x) = F(x)\] INSESGADO ¿Por qué?

\[\var{F_n(x)} = \frac{1}{n^2}nF(x)(1-F(x)) \convs[] 0\]

Como consecuencia, $F_n \convs[P] F(x)$ y decimos que es \concept{consistente}\footnote{El estimador converge en probabilidad a la función}

¿Esto a qué viene? A entender la función de distribución empírica.

Además de la convergencia en probabilidad tenemos un resultado más potente:

\begin{theorem}[Glivenco-Cantelli]
\[ ||F_n-F||_α = \sup_{x\inℝ} |F_n(x) - F(x)| \convs[c.s.] 0\]

Es decir, la función de distribución empírica converge en distribución a la real y además esa convergencia es uniforme.
\end{theorem}

\begin{proof}
La demostración se sale del temario de este curso.
\end{proof}

\paragraph{¿Cómo se hace realmente el contraste?} Si $H_0$ fuese cierta, se espera que $D_n = ||F_n - F_0||$ sea ``pequeño''. La idea es rechazar en la región crítica $R = \{D_n > c\}$ para un valor $c$ tal que $P_{H_0}(D_n > c) = α$, siendo $α$ un nivel de significación predeterminado.

Para poder calcular $P_{H_0}(D_n > c)$ necesitamos cálculos algo chungos, pero hay un resultado que nos puede ayudar mucho a resolver este problema.

\begin{lemma}
La distribución bajo $H_0$ de $D_n$ es la misma para cualquier distribución continua $F_0$

Por tanto el valor de $c$ en la región crítica es el mismo para cualquier distribución continua $F_0$ y está tabulado.
\end{lemma}


\begin{proof}
Para poder probar este lemma, necesitamos una proposición:

\begin{prop}
Si una $v.a.$ tiene distribución continua $F_0$, entonces la variable aleatoria $F_0(X)\sim \mathbb{U}(0,1)$\label{prop::funcionsobredistribucioncontinua}
\end{prop}
\begin{proof}
\[
P\{F_0(x) \leq u\} \overset{?}{=} u 
\]
Hemos tomado distribuciones continuas para evitar saltos. Entonces:

$F_0 \text{ continua} \implies \exists x \tq F_0(x) = u, P\{F_0(X)\leq F_0(x)\}$

Donde:
\[
\{F_0(X)\leq F_0(x)\} = \underbrace{\{F_0(X)\leq F_0(x), X\leq x\}}_{\{X\leq x\}} \cup \{F_0(X)\leq F_0(x), X\ge x\}
\]

Vamos a estudiar cada uno de los términos:

\[\{F_0(X)\leq F_0(x), X\leq x\} = \{X\leq x\}\]
Se debe a que $X\leq x \implies F_0(X)\leq F_0(x)$

Por otro lado, el otro término:
\[\{F_0(X)\leq F_0(x), X\ge x\} = P\{F_0(X)\leq F_0(x), X>x\} = 0\]
Ya que $F_0$ es necesariamente constante entre $x$ y $X$.


\textbf{Entonces:}

\[
\{F_0(X)\leq F_0(x)\} = P\{X\leq x\} = F(x) = u
\]
\end{proof}
\obs Existe un recíproco que permite calcular otras distribuciones a partir de uniformes invirtiendo la función de distribución.


Ahora que ya tenemos esa propiedad demostrada podemos seguir con la demostración del lema.

Vamos a demostrar que $D_n = \max\{\sup_{x\in ℝ} (F_n(x) - F_0(x)),\sup_{x\in ℝ} (F_0(x) - F_n(x)) \}$

Lo hemos separado en 2 para quitarnos el valor absoluto. Aquí solo estudiaremos uno de los términos, y el otro se deja como ejercicio para el lector.

Por definición de la distribución empírica, tenemos: $x\in [X_{(i)},x_{(i+1)}), F_n(x) = \frac{i}{n}$

\textbf{Entonces:}

\[
\sup_{x\in ℝ} (F_n(x) - F_0(x)) = \max_{i=0,...,n} \sup_{x\in [X_{(i)},X_{(i+1)})} (F_n(x) - F_0(x))
\]


Ahora podemos sustituir $F_n(x)$, obteniendo
\[
 = \max_{i=0,...,n} \left( \frac{i}{n} - F_0(x_{(i)})\right) = \max \left\{0,\max_{i=0,...,n} \left( \frac{i}{n} - F_0(x_{(i)})\right)\right\}
\]

Por el otro lado obtendríamos:

\[
\sup_{x\in ℝ} (F_0(x) - F_n(x)) = ... = \max \left\{0,\max_{i=0,...,n} \left\{0, \left(F_0(X_{(i)}) - \frac{i-1}{n}\right)\right\}\right\}
\]

\textbf{Conclusión:}
\[D_n = ||F_n - F_0||_{\infty} = \max \left\{ 0, \max_{i=0,...,n} \left( \frac{i}{n} - F_0(x_{(i)})\right), \max_{i=0,...,n} \left\{0, \left(F_0(X_{(i)}) - \frac{i-1}{n}\right)\right\}\right\}\]

Es decir, $D_n$ depende de $F_0$ a través $F_0(X_{(i)})$ y, aplicando la proposición \ref{prop::funcionsobredistribucioncontinua} en $X_1,...,X_n \overset{iid}{\sim} F_0 \to X_{(1)} \leq ... \leq X_{(n)}$

Entonces: 
\[
F_0(X_1),...,F_0(X_n) \overset{iid}{\sim} \mathbb{U}(0,1) \to F_0(X_{(1)})\leq ... \leq F_0(X_{(n)})
\]


Son los estadísticos de de orden de una muestra de tamaño $n$ de v.a.i.i.d $\mathbb{U}(0,1)$ para cualquier $F_0$ continua.

\end{proof}

