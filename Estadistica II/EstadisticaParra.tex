\chapter{Distribución normal multivariante}

\section{Esperanza, varianza y covarianza de variables aleatorias}

Dada una variable aleatoria definimos:
\begin{itemize}
\item Esperanza: $\mu = \mathbb{E}(X) = \int_{-\infty}^{\infty}x\cdot f_P(x) dx$

Propiedades:
\begin{enumerate}
\item $\mathbb{E}(aX) = a\mathbb{E}(X)$
\item $\mathbb{E}(X+Y) = \mathbb{E}(X)+\mathbb{E}(Y)$
\item $\mathbb{E}(X+c) = \mathbb{E}(X)+c$ (La esperanza de una constante es la propia constante)
\end{enumerate}
\item Varianza: $Var(X) = \mathbb{E}((X-\mathbb{E}(X))^2) =\mathbb{E}((X-\mu)^2) = \mathbb{E}(X^2)-\mu^2$

Propiedades:
\begin{enumerate}
\item $Var(X+b)=Var(X)$
\item $Var(aX)=a^2Var(X)$
\item $Var(X)\geq 0$
\end{enumerate}
\item Covarianza (entre dos variables aleatorias $X_i$, $X_j$): $\sigma_{i,j} = Cov(X_i,X_j) = \mathbb{E}\left((X_i-\mathbb{E}(X_i))(X_j-\mathbb{E}(X_j))\right) = \mathbb{E}(X_i X_j)-\mathbb{E}(X_i)\mathbb{E}(X_j)$

Dos propiedades importantes de la covarianza son:

\begin{enumerate}
\item Cov(X,X)= Var(X)
\item $Cov(X,Y)=Cov(Y,X)$
\end{enumerate}

\end{itemize}

\section{Esperanza, varianza y covarianza de vectores aleatorios}

Un vector aleatorio es un vector de variables aleatorias.

Notación: como durante el curso vamos a trabajar con vectores aleatorios, vamos a generalizar los símbolos que iremos usando:
\begin{itemize}
\item $X = (X_1, X_2,...,X_p)'$ será un vector de p variables aleatorias. Las variables aleatorias serán $X_1, X_2,...,X_p$. La comilla simple $'$ indica que $X$ es un vector columna.
\item $\mu$ será la esperanza del vector aleatorio X: $\mathbb{E}(X)$. Las esperanzas de cada variable aleatoria serán $\mu_1, \mu_2,...,\mu_p$.
\item Si A es una matriz, A' es su traspuesta
\end{itemize}

Por tanto, dado un vector de p variables aleatorias (vector aleatorio p-dimensional), definimos:

\begin{itemize}
\item Esperanza. Será un vector columna con las esperanzas de cada variable aleatoria.
\[
\mathbb{E}(X) = \mu = (\mu_1, \mu_2,..., \mu_p)'
\]

Donde cada $\mu_i = \mathbb{E}(X_i)$.

Ejemplo p=3:
\[
\mathbb{E}(X)=
\mathbb{E}\left[
\left(
\begin{array}{c}
X_1\\
X_2\\
X_3
\end{array}
\right)
\right]=
\left(
\begin{array}{c}
\mathbb{E}(X_1)\\
\mathbb{E}(X_2)\\
\mathbb{E}(X_3)
\end{array}
\right)=
\left(
\begin{array}{c}
\mu_1\\
\mu_2\\
\mu_3
\end{array}
\right)=
\mu
\]

Propiedades:
\begin{enumerate}
\item $\mathbb{E}(X+c) = \mathbb{E}(X)+c$. Como en el caso de variables aleatorias.
\item $\mathbb{E}(AX) = A\mathbb{E}(X)$. Donde A es una matriz de dimensión $p$x$p$ siendo p la dimensión de X.

Lo vemos para p=3:

\[
\mathbb{E}(AX)=
\mathbb{E}\left[
\left(
\begin{array}{ccc}
a_{1,1}& a_{1,2}& a_{1,3}\\
a_{2,1}& a_{2,2}& a_{2,3}\\
a_{3,1}& a_{3,2}& a_{3,3}
\end{array}
\right)
\left(
\begin{array}{c}
X_1\\
X_2\\
X_3
\end{array}
\right) \right]=
\mathbb{E}\left[
\left(
\begin{array}{c}
a_{1,1}X_1 - a_{1,2}X_2 - a_{1,3}X_3\\
a_{2,1}X_1 - a_{2,2}X_2 - a_{2,3}X_3\\
a_{3,1}X_1 - a_{3,2}X_2 - a_{3,3}X_3
\end{array}
\right)
\right]=
\]

\[
=\left(
\begin{array}{c}
a_{1,1}\mathbb{E}(X_1) - a_{1,2}\mathbb{E}(X_2) - a_{1,3}\mathbb{E}(X_3)\\
a_{2,1}\mathbb{E}(X_1) - a_{2,2}\mathbb{E}(X_2) - a_{2,3}\mathbb{E}(X_3)\\
a_{3,1}\mathbb{E}(X_1) - a_{3,2}\mathbb{E}(X_2) - a_{3,3}\mathbb{E}(X_3)
\end{array}
\right)=
\left(
\begin{array}{ccc}
a_{1,1}& a_{1,2}& a_{1,3}\\
a_{2,1}& a_{2,2}& a_{2,3}\\
a_{3,1}& a_{3,2}& a_{3,3}
\end{array}
\right)
\left(
\begin{array}{c}
\mathbb{E}(X_1)\\
\mathbb{E}(X_2)\\
\mathbb{E}(X_3)
\end{array}
\right)=
\]
\[
=A\mathbb{E}(X)
\]

\end{enumerate}

\item Varianza. La varianza va a ser una matriz, donde cada elemento va a ser la covarianza entre dos de las p variables aleatorias que conforman el vector. Será por tanto una matriz simétrica (ya que $\sigma_{i,j}=Cov(X_i,X_j)=Cov(X_j,X_i)=\sigma_{j,i}$). La matriz resultante será la llamada matriz de covarianzas $\Sigma$.
\[
Var(X)=\mathbb{E}\left((X-\mu)(X-\mu)'\right) = \mathbb{E}(XX')-\mu \mu'=\Sigma
\]

\begin{proof}
\[
Var(X)=\mathbb{E}\left((X-\mu)(X-\mu)'\right) = \mathbb{E}(XX'- \mu X' - X \mu'+\mu \mu')=
\]
\[
\mathbb{E}(XX')-\mathbb{E}(\mu X')-\mathbb{E}(X\mu')+\mathbb{E}(\mu \mu')= \mathbb{E}(XX')-\mu \mathbb{E}(X')-\mu' \mathbb{E}(X)+\mu\mu'=
\]
\[
 \mathbb{E}(XX')-\mu \mu'-\mu' \mu+\mu \mu' = \mathbb{E}(XX')-\mu \mu'=\Sigma
\]
\end{proof}

Ejemplo p=3:

\[
Var(X)=
\mathbb{E}\left[
\left(
\begin{array}{c}
X_1-\mu_1\\
X_2-\mu_2\\
X_3-\mu_3
\end{array}
\right)
(X_1-\mu_1, X_2-\mu_2, X_3-\mu_3)\right]=
\left(
\begin{array}{ccc}
\sigma_{1,1}& \sigma_{1,2}& \sigma_{1,3} \\
\sigma_{2,1}& \sigma_{2,2}& \sigma_{2,3} \\
\sigma_{3,1}& \sigma_{3,2}& \sigma_{3,3}
\end{array}
\right)=
\]

\[
=\left(
\begin{array}{ccc}
Var(X_1)& \sigma_{1,2}& \sigma_{1,3} \\
\sigma_{2,1}& Var(X_2)& \sigma_{2,3} \\
\sigma_{3,1}& \sigma_{3,2}& Var(X_3)
\end{array}
\right) = \Sigma
\]

Donde se cumple que $\sigma_{1,2}=\sigma_{2,1}$, $\sigma_{1,3}=\sigma_{3,1}$ y $\sigma_{3,2}=\sigma_{2,3}$. Y por tanto $\Sigma$ es simétrica.
\end{itemize}




\textcolor{red}{Mirar si tiene importancia lo de $\Sigma$ semidefinida positiva y tal}

\section{Función característica}
La función característica de un vector aleatorio X es:
\[
\phi_X(t)=\mathbb{E}(e^{it'X})
\]

Siendo X y t p-dimensionales.

Se llama función característica porque es única para cada distribución de X. Es decir:
\begin{prop} Sean X e Y dos vectores aleatorios:
\[
\phi_X(t)=\phi_Y(t) \Leftrightarrow X \stackrel{d}{=} Y
\]
\end{prop}

\begin{prop} Mecanismo de Cramer-Wold: Dados dos vectores aleatorios X e Y:
\textcolor{red}{preguntar que es a'X (dos vectores columna multiplicados?)}
\[
a'X \stackrel{d}{=} a'Y \text{ } \forall a \in \mathbb{R}^p \Leftrightarrow X \stackrel{D}{=} Y
\] 
\begin{proof}
\begin{itemize}
\item $\Leftarrow)$ Trivial
\item $\Rightarrow)$ Aplicamos la función característica y tenemos que:
$$\phi_{a'X}(t) \phi_{a'Y}(t) \text{ } \forall t \in \mathbb{R}$$

Por tanto, también es cierto para t=1:
$$\phi_{a'X}(1) = \phi_{a'Y}(1) \Rightarrow \mathbb{E}(e^{ia'X})=\mathbb{E}(e^{ia'Y}) \Rightarrow \phi_{X}(a)=\phi_{Y}(a)$$
\end{itemize}
\end{proof}
\end{prop}

Esta función caracteriza la distribución de X:




\section{Matriz de covarianzas}
Como ya dijimos anteriormente la matriz de covarianzas $\Sigma$ define la varianza de un vector aleatorio y es simétrica. Por tanto podemos expresar $\Sigma$ de la siguiente forma:
\[
\Sigma = CDC^{-1}
\]

Siendo D una matriz diagonal.

$C^{-1}=C'$ ya que las columnas de C son vectores ortonormales.
Por tanto:
\[
\Sigma = CDC'  \text{ y } \Sigma^{-1} = CD^{-1}C'
\]

Ejemplo p=2:
\[
\mu=\left(
\begin{array}{c}
0\\
0
\end{array}
\right)
\text{ , }
\Sigma=\left(
\begin{array}{cc}
\lambda_1& 0 \\
0 & \lambda_2
\end{array}
\right)
\]
Tenemos:
\[
(X_1, X_2)
\left(
\begin{array}{cc}
\lambda_1& 0 \\
0 & \lambda_2
\end{array}
\right)
\left(
\begin{array}{c}
X_1\\
X_2
\end{array}
\right) = cte
\Rightarrow
\frac{X_1^2}{\lambda_1}+\frac{X_2^2}{\lambda_2}=cte
\]
Luego:
\[
(X-\mu)'\Sigma(X-\mu) = cte \Rightarrow (X-\mu)'CD^{-1}C'(X-\mu) = cte \Rightarrow (\tilde{X}-\tilde{\mu})'CD^{-1}C'(\tilde{X}-\tilde{\mu}) = cte
\]

\begin{defn}[Correlación]
La correlación entre dos vectores aleatorios $X_1$ y $X_2$ se define como:
\[
cor(X_1,X_2)=\frac{cov(X_1,X_2)}{\sqrt{Var(X_1)Var(X_2)}}
\]

Es por tanto una matriz, en su diagonal principal esta formada por 1's. \textcolor{red}{Explicar algo más del significado geométrico de la correlación}


\end{defn}

\section{Estandarización multivariante}
\begin{defn}
Sea X una variable aleatoria. X es normal si tiene densidad dada por:
\[
f(x) = \frac{1}{\sigma\sqrt{2 \pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}
\]

Además, si cogemos $Y=\frac{X-\mu}{\sigma}$ entonces $Y\equiv N(0,1)$
\end{defn}

\begin{defn}
Sea un vector aleatorio X, es normal p-dimensional con vector de medias $\mu$ y matriz de covarianzas $\Sigma$ (notación: $X\equiv N_p(\mu, \Sigma)$) si tiene densidad dada por:

\[
f(x)=\abs{\Sigma}^{-1/2}(2\pi)^{-p/2} e^{\left( -\frac{1}{2}(x-\mu)'\Sigma^{-1}(x-\mu) \right)}
\]
\end{defn}

\begin{prop} Si $X \equiv N_p(\mu, \Sigma)$ y definimos $Y = \Sigma^{-1/2}(X-\mu)$, entonces $Y_1,...,Y_p$ son i.i.d. N(0,1).\end{prop}

\begin{proof}
Sabemos por definición que:
\[
f_X(x)=\abs{\Sigma}^{-1/2}(2\pi)^{-p/2} \exp \left\{ -\frac{1}{2}(x-\mu)'\Sigma^{-1}(x-\mu) \right\}
\]

Vamos a aplicar un cambio de variable en la fórmula de la densidad:

Despejando de $Y = h(X)= \Sigma^{-1/2}(X-\mu)$, obtenemos que $\Sigma^{1/2}Y+\mu=h^{-1}(Y)=X$.

Y ahora cogemos el Jacobiano de $h^{-1}(Y)=X$ que será $\Sigma^{1/2}$ ($\mu$ es una constante e Y es la variable).

\textcolor{red}{Esto de coger el jacobiano a qué se debe? A que luego la función de densidad se integra?}


Por tanto nos quedaría:
\[
f(x) = \abs{\Sigma}^{-1/2}(2\pi)^{-p/2} \exp \left\{ -\frac{1}{2}(x-\mu)'\Sigma^{-1}(x-\mu) \right\} = f(h^{-1}(y))\cdot\abs{Jh(x)} =
\]
\[ 
\abs{\Sigma}^{-1/2}(2 \pi)^{-p/2} \exp\left\{-\frac{1}{2}(\Sigma^{1/2}y+\mu-\mu)'\Sigma^{-1}(\Sigma^{1/2}y+\mu-\mu)  \right\} \abs{\Sigma^{1/2}}  =
\]
\[
= \abs{\Sigma}^{-1/2}(2 \pi)^{-p/2} \exp\left\{-\frac{1}{2}y'\Sigma'^{1/2}\Sigma^{-1}\Sigma^{1/2}y)  \right\} \abs{\Sigma}^{1/2} =
\]

Por ser $\Sigma$ simétrica tenemos que: $\Sigma = \Sigma'$
\[
= \abs{\Sigma}^{-1/2}(2 \pi)^{-p/2} \exp\left\{-\frac{1}{2}y'\Sigma^{1/2}\Sigma^{-1}\Sigma^{1/2}y)  \right\} \abs{\Sigma}^{1/2} =
\]
\[
= (2 \pi)^{-p/2} \exp\left\{-\frac{1}{2}(y'y) \right\} = \prod_{i=1}^{p} \frac{1}{\sqrt{2\pi}} e^{-\frac{(y_i)^2}{2}} =  \prod_{i=1}^{p} \frac{1}{\sqrt{2\pi}} e^{-\frac{(x_i-\mu)^2}{2\sigma^2}} =\prod_{i=1}^{p} f_{X_{i}}(x)
\]

\textcolor{red}{Multiplicamos un vector columna por un vector fila, sería al reves no?}

Hemos usado un teorema que dice que n variables aleatorias $X_1,...,X_n$ son independientes si y solo si el $f(x_1,...,x_n)=\prod_{i=1}^{n}f(x_i)$ siendo f la función de densidad.

\end{proof}

\textcolor{blue}{Comprobar esto:}
\begin{obs}
Si $X \equiv N_p(\mu, \Sigma)$ y definimos $Y = \Sigma^{-1/2}(X-\mu)$, entonces $Y\equiv N_p(0_p, I_p)$. Siendo $0_p$ un vector de 0's de dimensión p, e I la matriz identidad de rango p:
\end{obs}
\[
\mathbb{E}(Y)=\mathbb{E}\left( \Sigma^{-1/2}(X-\mu) \right) = \Sigma^{-1/2} \mathbb{E}\left( (X-\mu) \right) = \Sigma^{-1/2}(\mu -\mu)=0
\]

\[
Var(Y)=Var\left(\Sigma^{-1/2}(X-\mu) \right)=
\]
\[
=\mathbb{E}\left( \left( \Sigma^{-1/2}X-\Sigma^{-1/2}\mu-\Sigma^{-1/2}\mu+\Sigma^{-1/2}\mu \right) \left( \Sigma^{-1/2}X-\Sigma^{-1/2}\mu-\Sigma^{-1/2}\mu+\Sigma^{-1/2}\mu \right)' \right)=
\]
Usamos que $\Sigma$ es simétrica:
\[
=\mathbb{E}\left( \left( \Sigma^{-1/2}(X-\mu) \right) \left( \Sigma^{-1/2}(X-\mu) \right)' \right) = \mathbb{E}\left( \Sigma^{-1/2}(X-\mu)(X-\mu)'\Sigma^{-1/2}  \right) = \Sigma^{-1/2}\Sigma\Sigma^{-1/2} = I
\]


\textbf{Estandarización paso por paso: }Vamos a ver qué es lo que hacemos con la estandarización paso por paso. Sea X el vector aleatorio:
\begin{enumerate}
\item $Y=(X-\mu)$. Aquí lo que hacemos es simplemente una traslación del vector X. 
\item $Y=C'(X-\mu)$. Aquí giramos los datos. C' es una matriz de giro ya que su determinante es 1 (de hecho es ortonormal). Esta rotación elimina la correlación \textcolor{red}{¿Por qué?}. Calculamos la varianza:
\[
Var\left( C'(X-\mu) \right) = \mathbb{E}\left( \left( C'X-C'\mu-C'\mu+C'\mu \right) \left( C'X-C'\mu-C'\mu+C'\mu \right)' \right)
\]
\[
\mathbb{E}\left( \left( C'(X-\mu) \right) \left( C'(X-\mu) \right)' \right) = \mathbb{E}\left( C'(X-\mu)(X-\mu)'C  \right) = C'\Sigma C=C'CDCC'=D
\]
\item $Y=D^{-1/2}C'(X-\mu)$. Con esto hacemos un cambio de escala para que las varianzas sean 1. Calculamos la varianza. Usamos que $Var(AX)=AVar(X)A'$ y que $D=D'$:
\[
Var\left( D^{-1/2}C'(X-\mu) \right) = D^{-1/2}DD^{-1/2} = I
\]
\item $Y=CD^{-1/2}C'(X-\mu)$. Deshacemos el giro de antes. Calculamos la varianza:
\[
Var\left( CD^{-1/2}C'(X-\mu) \right)=CIC' = I
\]
\end{enumerate}
\includegraphics[scale=0.75]{img/estandarizacionMultivariante.png}

\textbf{Consecuencias de la estandarización:}
\begin{enumerate}
\item Si $X\equiv N_p(\mu, \Sigma)$, entonces $\mathbb{E}(X)=\mu$ y $Var(X)=\Sigma$.

Esto es cierto ya que tal y como hemos visto antes $X=\Sigma{1/2}Y+\mu$ y entonces  (Usando que $C'=C$)$\mathbb{E}(X)=0+\mu$ y $Var(X)=Var(\Sigma{1/2}Y+\mu)=Var(\Sigma{1/2}Y)=\Sigma^{1/2}Var(Y)\Sigma'^{1/2}=CD^{1/2}C'IC'D^{1/2}C=\Sigma$

\item Si $X\equiv N_p(\mu, \Sigma)$, entonces $\phi_X(t)=exp\left\{ it'\mu-\frac{1}{2}t'\Sigma t \right\}$:
%\[
%\phi_X(t)=\mathbb{E}\left( exp\left\{ it'(\Sigma^{1/2}Y+\mu) \right\} \right) = exp \left\{ it'\mu  \right\} \mathbb{E}\left( exp\left\{ it'\Sigma^{1/2}Y \right\} \right)
%\]
\item La distribución de $(X-\mu)'\Sigma(X-\mu)$ es $\chi^2_p$:

Siendo $X\equiv N_p(\mu,\Sigma)$ con $X=\Sigma^{-1/2}Y+\mu$. Entonces (sabiendo que $\Sigma$ es simétrica):
\[
(X-\mu)'\Sigma^{-1}(X-\mu)=Y'\Sigma^{1/2}\Sigma^{-1}\Sigma^{1/2}Y=Y'Y=\sum_{i=1}^{p} Y_i^2
\]
\textcolor{red}{Otra vez líos con vector columna o fila}
Usando que $Y_i\equiv N(0,1)$, entonces:
\[
\sum_{i=1}^{p} Y_i^2 = \chi^2_p \text{ chi-cuadrado con p grados de libertad}
\]

\end{enumerate}

\section{Transformaciones afines de vectores normales}
\begin{prop}
Si $X \equiv N_p(\mu, \Sigma)$, A es matriz qxp y $b\in \mathbb{R}^q$, entonces $AX+b \equiv N_q(A\mu +b, A\Sigma A')$
\end{prop}
\begin{proof}
\[
\phi_[AX+b](t)=\mathbb{E}\left( exp\left\{ it'(AX+b) \right\} \right) = e^{it'b}\mathbb{E}\left( e^{it'AX} \right) = e^{it'b} exp\left\{ it'A\mu-\frac{1}{2}t'A\Sigma A't \right\}
\]
\textcolor{blue}{sin terminar...}
\end{proof}

Una consecuencia de esta proposición es lo siguiente: Si X sigue una distribución normal p-dimensional, y se expresa como $X=(X_1|X_2)$, con $X_1\in \mathbb{R}^q$ y $X_2\in \mathbb{R}^{p-q}$, y consideramos las particiones correspondientes de $\mu$ y $\Sigma$:
\[
\mu=(\mu_1 | \mu_2) \text{ , } \Sigma=
\left(
\begin{array}{c|c}
\Sigma_{11}& \Sigma_{12} \\
\hline
\Sigma_{21}& \Sigma_{22} 
\end{array}
\right)
\]
entonces $X_1\equiv N_q(\mu_1, \Sigma_{11})$

\begin{example}
Sea un vector de variables aleatorias $Y=(Y_1, Y_2,Y_3, Y_4, Y_5)$ tal que $Y\equiv N_5(\mu, \Sigma)$ (Y es normal 5-dimensional) con vector de medias $\mu=(\mu_1, \mu_2, \mu_3, \mu_4, \mu_5)$ y sea $X_1=(Y_1,Y_2,Y_3)$ y $X_2=(Y_4,Y_5)$. 

\[
\mu=
\left(
\begin{array}{c}
\mu_{X_1}\\
\hline
\mu_{X_2}
\end{array}
\right)=
\left(
\begin{array}{c}
\mu_{Y_1}\\
\mu_{Y_2}\\
\mu_{Y_3}\\
\hline
\mu_{Y_4}\\
\mu_{Y_5}
\end{array}
\right)
\]
\[
\Sigma_Y=
\left(
\begin{array}{ccc|cc}
\Sigma_{11}& \Sigma_{12} & \Sigma_{13} & \Sigma_{14} & \Sigma_{15}\\
\Sigma_{21}& \Sigma_{22} & \Sigma_{23} & \Sigma_{24} & \Sigma_{25}\\
\Sigma_{31}& \Sigma_{32} & \Sigma_{33} & \Sigma_{34} & \Sigma_{35}\\
\hline
\Sigma_{41}& \Sigma_{42} & \Sigma_{43} & \Sigma_{44} & \Sigma_{45}\\
\Sigma_{51}& \Sigma_{52} & \Sigma_{53} & \Sigma_{54} & \Sigma_{55}
\end{array}
\right) \text{ , }
\Sigma_X=
\left(
\begin{array}{c|c}
\Sigma_{11}& \Sigma_{12} \\
\hline
\Sigma_{21}& \Sigma_{22} 
\end{array}
\right)
\]

Entonces $X_1\equiv N_3(\mu_{X_1}, \Sigma_{11})$ para la matriz $\Sigma_X$.
\end{example}

\begin{prop}
\textcolor{red}{Si $X=(X_1,X_2)$ es normal n-dimensional siendo n la suma de la dimension de $X_1+X_2$, entonces :}Dado $X_1$ y $X_2$ vectores aleatorios, son independientes si y solo si $\Sigma_{12}=\Sigma_{21}=0$
\end{prop}

\begin{obs}
\begin{itemize}
\item Si dos variables aleatorias tienen distribución normal y además $Cov(X,Y)=0$, esto no implica que X e Y sean independientes. Esto sería cierto si el vector (X,Y) fuera normal bidimensional.
\item Si dos variables aleatorias X e Y tienen distribución normal y $a,b \in \mathbb{R}$, la combinación linear de $aX+bY$ no tiene necesariamente distribución normal. Esto sería cierto si el vector (X,Y) fuera normal bidimensional.
\item Aunque todas las marginales de un vector aleatorio p-dimensional X tengan distribución normal, esto no implica que X tenga distribución normal p-dimensional. Esto sería cierto si todas ellas fueran independientes entre si.
\end{itemize}
\end{obs}

\section{Ejercicio 1}
Definimos el siguiente vector aleatorio: $X = (X_1,X_2,X_3)' \equiv N_3(\mu, \Sigma)$ con:

\[
\mu=
\left(
\begin{array}{c}
0\\
0\\
0
\end{array}
\right) \text{,       }
\Sigma=
\left(
\begin{array}{ccc}
7/2& 1/2& -1 \\
1/2& 1/2& 0 \\
-1& 0& 1/2
\end{array}
\right)
\]

\ppart Calcula las distribuciones marginales $X_i \equiv N(\mathbb{E}(X_i), Var(X_i))$:

$X_1\equiv N(0, 7/2)$

$X_2\equiv N(0, 1/2)$

$X_3\equiv N(0, 1/2)$

Para calcular estos valores solo hace falta mirar los datos que nos da el problema, el vector de medias $\mu$ y la matriz de covarianzas $\Sigma$:

\[
\Sigma=\left(
\begin{array}{ccc}
Var(X_1)& \sigma_{1,2}& \sigma_{1,3} \\
\sigma_{2,1}& Var(X_2)& \sigma_{2,3} \\
\sigma_{3,1}& \sigma_{3,2}& Var(X_3)
\end{array}
\right)
\]

\[
\mu=
\left(
\begin{array}{c}
\mathbb{E}(X_1)\\
\mathbb{E}(X_2)\\
\mathbb{E}(X_3)
\end{array}
\right)=
\left(
\begin{array}{c}
\mu_1\\
\mu_2\\
\mu_3
\end{array}
\right)
\]

\ppart Calcula la distribución del vector $(X_1,X_2)'$:

Este vector sigue una distribución normal que puede obtener de las matriz $\Sigma$ y el vector de medias $\mu$:
\[
\left(
\begin{array}{c}
X_1\\
X_2
\end{array}
\right)
\equiv N_2\left[
\left(
\begin{array}{c}
0\\
0
\end{array}
\right)
\text{, }
\left(
\begin{array}{cc}
7/2& 1/2 \\
1/2 & 1/2
\end{array}
\right)
\right]
\]

\ppart ¿Son $X_2$ y $X_3$ independientes?

Sí son independientes ya que la covarianza entre ambas variables es 0. La covarianza entre $X_2$ y $X_3$ es el elemento de la fila 3 y la columna 2 de la matriz de covarianzas $\Sigma$, (que al ser $\Sigma$ simétrica coincide con el elemento de la fila 2 y la columna 3).

\ppart ¿Es $X_3$ independiente del vector $(X_1, X_2)'$?
No, no lo es, tenemos que ver que ciertos elementos de la matriz de covarianzas son 0:
\[
\Sigma=
\left(
\begin{array}{cc|c}
7/2& 1/2& -1 \\
1/2& 1/2& 0 \\
\hline
-1& 0& 1/2
\end{array}
\right)
\]

Y vemos que hay un '-1' y un '0', si fueran los dos elementos 0, si serían independientes, pero al haber un elemento distinto de 0, no lo son.


\ppart Calcula la  distribución de la variable aleatoria $(2X_1-X_2+3X_3)$. Utilizando la proposición anterior:

Si $X \equiv N_p(\mu, \Sigma)$, A es matriz qxp y $b\in \mathbb{R}^q$, entonces $AX+b \equiv N_q(A\mu +b, A\Sigma A')$ 

Procedemos de la siguiente manera: $X\equiv N_3(\mu, \Sigma)$, $A=(2,-1,3)$ tiene dimensión 1x3 y b=0. Por tanto:
\[
\mu=AX+b = (2,-1,3)\cdot
\left(
\begin{array}{c}
0 \\
0 \\
0
\end{array}
\right) = 0 
\]

\[
\Sigma = A\Sigma A'=(2,-1,3)
\left(
\begin{array}{ccc}
7/2& 1/2& -1 \\
1/2& 1/2& 0 \\
-1& 0& 1/2
\end{array}
\right) \left(
\begin{array}{c}
2 \\
-1 \\
3
\end{array}
\right)=5
\]

Por tanto, $(2X_1-X_2+3X_3)\equiv N(0,5)$



\section{Distribuciones condicionadas}

\begin{prop}

Sea $X=(X_1|X_2)$ con $X_1∈ℝ^p$ y $X_2∈ℝ^{p-q}$. Consideramos las particiones correspondientes de $µ$ y de $\Sigma$ y suponemos que $\Sigma_{11}^{-1}$ existe. Entonces:
\[
X_2|X_1 \equiv N_{p-q}(\mu_{2.1},\Sigma_{2.1})
\]
donde:
\[
\mu_{2.1}=\mu_2 +\Sigma_{21}\Sigma_{11}^{-1}(X_1-\mu_1)
\]
\[
\Sigma_{2.1}=\Sigma{22}-\Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12}
\]

\begin{itemize}
\item $\mu_{2.1}=\mathbb{E}(X_2|X_1)$ es una función lineal (afín) de $X_1$
\item $\Sigma_{2.1}$ no depende de $X_1$ (homocedasticidad)
\end{itemize}

\end{prop}

\begin{example}

Sea \[\left(
\begin{array}{c}
X \\
Y
\end{array}
\right) \equiv N_2 \left( \left(
\begin{array}{c}
0 \\
0
\end{array}
\right), \begin{pmatrix}10&3\\3&1\end{pmatrix} \right)\]

A)Distribución $Y|X$:
Y hace de $X_2$ en la fórmula vista anteriormente (es el segundo elemento del vector), y X de $X_1$.

\[
\mu_{2.1}=E(Y|X)=\mu_2+\Sigma_{21}\Sigma_{11}^{-1}(X-\mu_1)=0+3\cdot \frac{1}{10}\cdot X = \frac{3}{10}X
\]
\[
\Sigma_{2.1}=V(Y|X) = \Sigma_{22}-\Sigma_{21}\Sigma_{11}^{-1}\Sigma_{12} = 1-3\cdot\frac{1}{10}\cdot3 = \frac{1}{10}
\]

B)Distribución $X|Y$:
Al hacer la distribución de $X_1|X_2$ cambiamos el orden de los índices en las fórmulas:

\[
\mu_{1.2}=E(X|Y)=\mu_1+\Sigma_{12}\Sigma_{22}^{-1}(Y-\mu_2)=0+3\cdot \frac{1}{1}\cdot Y = 3Y
\]
\[
\Sigma_{1.2}=V(X|Y) = \Sigma_{11}-\Sigma_{12}\Sigma_{22}^{-1}\Sigma_{21} = 10-3\cdot\frac{1}{1}\cdot3 = 1
\]

\end{example}


\begin{example}

Sea \[\left(
\begin{array}{c}
X \\
Y
\end{array}
\right) \equiv N_2 \left( \left(
\begin{array}{c}
1 \\
1
\end{array}
\right), \begin{pmatrix}3&1\\1&2\end{pmatrix} \right)\]

Sea $Z_1 = X+Y$ y $Z_2 = X-Y$. Calcula la  distribución condicionada de $Z_1$ a $Z_2=1$

Primero vamos a calcular el vector aleatorio $(Z_1,Z_2)$, por la proposición vista anteriormente tenemos que: $Z_1 \equiv N(A\mu+b,A\Sigma A')$ con:
\[
A=
\left(
\begin{array}{cc}
1 & 1 \\
1 & -1
\end{array}
\right)
\]

Nos queda:
\[
A\mu =
\left(
\begin{array}{cc}
1 & 1 \\
1 & -1
\end{array}
\right)
\left(
\begin{array}{c}
1 \\
1
\end{array}
\right)
=
\left(
\begin{array}{c}
2 \\
0
\end{array}
\right)
\]

Y por otro lado:
\[
A\Sigma A' =
\left(
\begin{array}{cc}
1 & 1 \\
1 & -1
\end{array}
\right)
\left(
\begin{array}{cc}
3& 1 \\
1& 2 
\end{array}
\right)
\left(
\begin{array}{cc}
1 & 1 \\
1 & -1
\end{array}
\right) =
\left(
\begin{array}{cc}
7 & 1 \\
1 & 3
\end{array}
\right)
\]

Por tanto nos queda:
\[\left(
\begin{array}{c}
Z_1 \\
Z_2
\end{array}
\right) \equiv N_2 \left( \left(
\begin{array}{c}
2 \\
0
\end{array}
\right), \begin{pmatrix}7&1\\1&3\end{pmatrix} \right)\]

Ahora vamos a calcular la distribución de $Z_1|Z_2$, otra vez tenemos los subíndices cambiados con respecto a la fórmula general, por tanto:
\[
\mu_{1.2}=E(Z_1|Z_2)=2+\frac{1}{3}Z_2 \stackrel{Z_2=1}{\Rightarrow} \frac{7}{3}
\]
\[
\Sigma_{1.2}=V(Z_1|Z_2) = 7-\frac{1}{3} = \frac{20}{3}
\]
Por tanto:
\[
\left(
Z_1|Z_2 \\
\right) \equiv N_2 \left( 
\frac{7}{3}
,  \frac{20}{3} \right)\]

\end{example}

\chapter{Contrastes no paramétricos}
Hipótesis no paramétrica: hipótesis que no se formula en términos de un número finito de parámetros.

\begin{enumerate}
\item \textbf{Bondad de ajuste}: A partir de una muestra $X_1,...,X_n \stackrel{iid}{\sim} F$ de observaciones ({\textcolor{red}{Parra: son muestras o variables aleatorias o es simple notación?}}{\textcolor{blue}{Jorge: son muestras que provienen de v.a. $X_i$ con distribución F}}) ($\stackrel{iid}{\sim}$ significa que son muestras aleatorias independientes idénticamente distribuidas que siguen una distribución F en este caso), contrastar:
\begin{itemize}
\item $H_0: F=F_0$ donde $F_0$ es una distribución prefijada.
\item $H_0: F \in \{F_{\theta} : \theta\in H\}$ H es el espacio paramétrico.
\end{itemize}
\item \textbf{ Homogeneidad}: Dados $X_1,...,X_n \stackrel{iid}{\sim} F$ y $Y_1,...,Y_n \stackrel{iid}{\sim} G$ de observaciones. Contrastar $H_0: F=G$.

(Por ejemplo para ver si el salario de los hombres $F$ tiene la misma distribución que el de las mujeres $G$).

\item \textbf{Hipótesis de independencia}: Dada $(X_1,Y_1),...,(X_n,Y_n) \stackrel{iid}{\sim} F$ de observaciones. Contrastar $H_0: X$ e $Y$ son independientes.

(Por ejemplo para $X$ salario e $Y$ sexo, querríamos ver si el salario es independiente del sexo).
\end{enumerate}

Antes de explicar los contrastes en detalle, vamos a definir y tratar de entender bien algunos conceptos. (quien ya lo entienda que pase de este apartado, que el profesor no lo ha explicado):

\begin{defn}{\textbf{$H_0$ = Hipótesis nula. }}
Más que una definición, es una interpretación: La hipótesis nula es lo que queremos rechazar cuando hacemos el contraste de hipótesis.

\begin{expla}
Es decir, nosotros lo que hacemos es obtener una muestra empírica de unos datos, y lo que vamos a hacer es mirar si podemos decir que NO siguen una distribución en concreto, o por el contrario, no podemos decir nada. Por tanto, el objetivo del contraste es ver si podemos rechazar que los datos siguen esa distribución definida por la hipótesis nula. Pero cuidado, el que no la rechacemos no significa que los datos sigan la distribución, sino que no tenemos suficiente evidencia estadística para afirmar que NO la siguen....
\end{expla}
\end{defn}


\begin{defn}{\textbf{$\alpha$ = nivel de significación. }}
Es la probabilidad máxima que queremos tener de equivocarnos si rechazamos la hipótesis nula. No depende de nada, lo asignamos nosotros en cada problema que queramos resolver.

\begin{expla}
Es decir, si hacemos un contraste de hipótesis con un nivel de significación $\alpha =0.05$, quiere decir, que si finalmente rechazamos la hipótesis nula, asumimos que lo estamos haciendo con un máximo de un $5\%$ de probabilidades de equivocarnos.
\end{expla}
\end{defn}

\begin{defn}{\textbf{p-valor. }}
valor de $\alpha$ mínimo con el que se empieza a rechazar la hipótesis nula. Depende de los datos de partida y de la hiṕotesis nula.
\begin{expla}
Interpretación del p-valor: El p-valor es un número entre 0 y 1, y representa la probabilidad que tenemos de equivocarnos si rechazamos la hipótesis nula. Dicho de otra forma,  el p-valor nos muestra la probabilidad de haber obtenido el resultado que hemos obtenido si suponemos que la hipótesis nula es cierta.

Razonémoslo con un ejemplo:
Supongamos que el p-valor sale 0.40. Esto quiere decir que si rechazamos la hipótesis nula, tenemos un $40\%$ de posibilidades de equivocarnos, por tanto, lo mejor es no rechazarla. Esto cuadra con la teoría, ya que si el p-valor es 0.40 (bastante alto), su valor estadístico asociado (T), es muy pequeño, y por tanto la región de rechazo ($R=\{T>c\}$) es bastante pequeña.

Según la otra interpretación tenemos que el resultado que hemos obtenido tendría un $40\%$ de posibilidades de obtenerse si consideramos que las variables aleatorias siguen la distribución que indica la hipótesis nula (en lugar de la que nos sale empíricamente). Esto también es razonable, ya que un $40\%$ es una probabilidad bastante alta como para rechazarla, por tanto, lo que hacemos es no rechazar la hipótesis nula.

Sin embargo, si nos sale un p-valor igual a 0.01, quiere decir que si rechazamos la hipótesis nula tenemos un $1\%$ de posibilidades de equivocarnos, que es bastante poco, por tanto, tenderemos a rechazarla.
\end{expla}
\end{defn}

Ahora vamos a ver la relación entre el p-valor y el nivel de significación:
\begin{expla}
Supongamos que queremos hacer el contraste de hipótesis con $\alpha=0.05$ y con una hipótesis nula $H_0$ cualquiera. Estudiamos los datos, y obtenemos un p-valor de 0.40. Por tanto, esto significa que si rechazamos la hipótesis nula tendríamos un $40\%$ de posibilidades de equivocarnos. Como nuestro $\alpha=0.05$ significa que solo estamos dispuestos a rechazar la hipótesis nula si tuviéramos un $5\%$ de probabilidades de equivocarnos, pero hemos visto que tenemos un $40\%$, por tanto, no rechazamos.

Supongamos ahora que queremos hacer el contraste de hipótesis con $\alpha=0.05$ pero obtenemos un p-valor de $0.02$. Por tanto, esto significa que si rechazamos la hipótesis nula tendríamos un $2\%$ de equivocarnos. Como hemos decidido que estamos dispuestos a rechazar la hipótesis nula con hasta un $5\%$ de probabilidades de equivocarnos, rechazamos. Si por el contrario imponemos $\alpha=0.01$, no rechazaríamos ya que sólo estaríamos dispuestos a equivocarnos como máximo un $1\%$ de las veces, y el p-valor solo nos asegura un $2\%$.
\end{expla}

\begin{defn}[T = valor Estadístico]
El estadístico es un valor que depende, al igual que el p-valor, de los datos de partida y de $H_0$. P-valor y estadístico están totalmente relacionados, si cambia uno, cambia el otro. Lo utilizamos para construir la región de rechazo.
\end{defn}

\section{Contraste $\chi^2$ de bondad de ajuste}
Consideramos una distribución totalmente especificada bajo $F_0$. Y consideramos una muestra empírica $X_1,...,X_n \stackrel{iid}{\sim} F$.

$H_0: F=F_0$ es la hipótesis nula y queremos ver que F, que es la distribución obtenida con los datos verdaderos (las muestras $X_i$ obtenidas empíricamente) es igual a $F_0$ que es la distribución teórica.

\textbf{Notación: }$P_A(B)$ es la probabilidad de B condicionada a A.

Vamos a definir los pasos que tenemos que seguir para comprobar si $H_0$ es cierta:
\begin{enumerate}
\item Se definen k clases $A_1,...,A_k$. 

\item Se cuentan cuántos datos caen en cada clase (frecuencias observadas). Cada clase la llamaremos $O_i=\#\{j:X_j\in A_i\}$.

\item Se calculan las frecuencias esperadas para cada clase si $H_0$ fuese cierta. A este dato lo llamaremos $\mathbb{E}_i$ o $\mathbb{E}_{H_0}(O_i)$: 
\[
\mathbb{E}_{H_0}(O_i) = np_i
\]

\obs Las $O_i$ son variables aleatorias que se distribuyen como una binomial $B(n, p_i=P_{H_0}(A_i))$. Siendo $n$ el número de intentos y $p_i$ la probabilidad de que una muestra pertenezca a la clase $A_i$ bajo la hipótesis nula. 

La notación puede resultar liosa, a grades rasgos:
\begin{itemize}
\item $O_i$ tendrá un valor que será la frecuencia observada de una clase i. Es decir, el número de observaciones que caen en una la clase i.
\item $\mathbb{E}_{H_0}(O_i)$ será el valor esperado de $O_i$ considerando la hipótesis nula como cierta. La esperanza de una $B(n,p)$ es igual a $np$.
\end{itemize}

\item Se comparan las frecuencias observadas y esperadas mediante el:

\begin{defn}[estadístico de Pearson]
\[T = \sum_{i=1}^n \frac{(O_i-E_i)^2}{E_i}\]
\end{defn}

Se divide entre $E_i$ para darle más importancia a la diferencia si el valor es pequeño, Por ejemplo, si E=100 y O=101, no es lo mismo que si E=1 y O=2. Sin embargo, si no dividiéramos por $E_i$ nos daría el mismo resultado.

\item Se rechaza $H_0$ en la región crítica $R=\{T > c\}$ donde c es tal que $\alpha=P_{H_0}(T>c)$. Es decir, $\alpha$ (también llamado 'nivel de significación') es la probabilidad de rechazar la hipótesis nula siendo esta cierta. O dicho de otra forma, la probabilidad de entrar en la región de rechazo $'T>c'$ considerando que $H_0$ es cierta.
\end{enumerate}


Ahora vamos a ver qué podemos decir del estadístico de Pearson 'T':
\[
O_i=B(n,p_i) \simeq N(np_i, np_i(1-p_i))
\]

Imaginémonos por un momento que podríamos despreciar el término $np_i^2$ de la varianza de la normal. Nos quedaría:
\[
\simeq N(np_i, np_i(1-p_i)) \simeq N(np_i, np_i) \simeq N(E_i, E_i)
\]

Que por el Teorema Central del Límite (\url{https://es.wikipedia.org/wiki/Teorema_del_l%C3%ADmite_central}) nos queda:
\[
\frac{O_i-E_i}{\sqrt{E_i}} \simeq N(0,1)
\]

Y como deberíamos saber, una distribución $\chi^2_k$  no es más que una distribución de probabilidad continua con un parámetro k que representa los grados de libertad de la variable aleatoria $X = Z_1^2 + \cdots + Z_k^2$, donde $Z_i$ son variables aleatorias normales independientes de media cero y varianza uno. Por tanto:

\[
\frac{(O_i-E_i)^2}{E_i} \simeq \chi^2_1
\]

Por tanto, como tenemos k clases, podríamos tener $T\simeq \chi^2_k$. pero por otro lado sabemos que $O_1+O_2+…+O_k=n$, esta restricción hace que no haya una independencia entre todos los sumandos $O_i$, por tanto nos queda: $T\simeq \chi^2_{k-1}$.

Finalmente nos queda que la región de rechazo, dado un nivel de significación $\alpha$, se alcanza cuando el estadístico de Pearson T, obtenido a partir de los datos muestrales, vale más que $\chi^2_{k-1,α}$. $\chi^2_{k-1,α}$ es, dada la función de densidad de una $\chi^2_{k-1}$, el valor del eje de abscisas que hace que se quede un $\alpha*100\%$ del área encerrada bajo la curva a la derecha de ese valor:

%DIBUJOOOO

\obs
\begin{enumerate}
\item  Tal y como lo hemos definido tenemos que $\sum_{i=1}^k O_i = n$ y que $\sum_{i=1}^k E_i = \sum_{i=1}^k np_i= n(p_1+p_2+...+p_k) = n$, por tanto tenemos:
\[
T=\sum_{i=1}^k \frac{(O_i-E_i)^2}{E_i} = \sum_{i=1}^k \frac{O_i^2}{E_i}-n
\]

\item Por deefinición de $\chi^2_{k-1}$, su esperanza es:
\[
\mathbb{E}_{H_0}(T) \simeq k-1
\]

\item Por definición de $\chi^2_{k-1}$, su varianza es:
\[
\mathrm{Var}_{H_0}(T) \simeq 2(k-1)
\]




\end{enumerate}

\begin{example}
Tiramos un dado 100 veces y obtenemos:

\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Resultados & 1 & 2 & 3 & 4 & 5 & 6 \\
\hline
Frecuencia & 10 & 20 & 20 & 10 & 15 & 25\\
\hline
\end{tabular}

Y consideramos $H_0: p_i=1/6 \text{ } \forall i=1,...,6$. Es decir que el dado no está trucado y cada cara tiene la misma probabilidad ($p_i$) de salir.

Por otro lado consideramos $H_1: \exists i$ tal que $p_i\neq 1/6$. Es decir, que el dado está trucado y hay caras que salen mas que otras.

Seguimos los pasos:
\begin{enumerate}
\item En este caso cada clase será la cara del dado que sale, habrá por tanto 6 clases: k=6.
\item Se cuentan cuantos datos caen en cada clase: $O_1=10$, $O_2=20$, $O_3=20$, $O_4=10$, $O_5=15$, $O_6=25$
\item Se calculan las frecuencias esperadas si $H_0$ fuese cierta. En este caso $n=10+20+20+10+15+25=100$ y $p_i=1/6 \text{ } \forall i=1,...,6$. Nos queda: $\mathbb{E}_i=np_i = 100 \frac{1}{6}=100/6$.
\item Se obtiene el estadístico de Pearson:
\[
T=\sum_{i=1}^n \frac{O_i^2}{E_i}-n = \frac{6}{100}(10^2+20^2+20^2+10^2+15^2+25^2)-100=11
\]
\item Rechazamos $H_0$ si $T>c$. En este caso, consideramos un nivel de significación $\alpha = 0.05$. Sabemos que $\alpha = P_{H_0}(T>c)$ Como tenemos 6 clases, el estadístico de Pearson tendrá una distribución $\chi^2_5$. Buscamos en la tabla (mirar apéndice) y obtenemos que $\chi^2_5, 0.05 = 11.07$. Este será nuestro valor de c.

Puesto que nuestra región de rechazo es $R=(T>c)$, y tenemos que $11>11.07$, no podemos rechazar la hipótesis nula, y por tanto, no podemos concluir que el dado esta trucado. El p-valor en este caso será mayor que $0.05$.

Si consideramos un valor de significación $\alpha = 0.06$ si hubiéramos rechazado la hipótesis nula (aunque con un 6\% de opciones de equivocarnos) y hubiéramos concluido que el dado esta trucado. 

Lo vemos en el siguiente dibujo, que representa la gráfica de una $\chi^2_5$:

\includegraphics[scale=0.75]{img/ejemploEstadistica.png}

La raya vertical se sitúa en $x=11.07$, que es el valor que sale si se mira la tabla de la distribución $\chi^2$ con 5 grados de libertad (ver apéndice) y nivel de significación, por tanto, la zona sombreada es la región de rechazo de la hipótesis nula. Como ha salido T=11, no entramos dentro de esa región de rechazo (por poco) y no podemos rechazar la hipótesis nula.

\end{enumerate}
\end{example}

\begin{theorem}
Bajo $H_0$:
\[
\sum_{i=1}^k \frac{(O_i-E_i)^2}{E_i} \stackrel{d}{\rightarrow} \chi^2_{k-1} \text{ , si } n \rightarrow \infty
\]
\end{theorem}
\begin{proof}
\textcolor{blue}{Esta demostración es un poco liosa, si no la entendéis, a otra cosa (great pareado).}

Definimos los vectores aleatorios $\xi_1,...,\xi_n$ de la siguiente forma: $\xi_i=(0,...,\overbrace{1}^{(j)},...,0)' \in \mathbb{R}^k \Leftrightarrow x_i \in A_j$. Es decir cada $\xi_i$ va a ser un vector de 0's, salvo porque van a tener un 1 en una posición j. Esta posición j les identificará con la clase $A_j$. Tenemos que:
\[
\xi_1+...+\xi_n=(O_1,...,O_k)'
\]
Es decir, que su suma nos da un vector con las frecuencias de aparición de cada clase (Recordemos que 'k' es el número de clases). Por ejemplo, en el ejemplo del dado tendríamos que $\xi_1+...+\xi_6=(10,20,20,10,15,25)=(O_1,...,O_6)$

\textbf{Notación: } $p\equiv (p_1,...,p_k)'$. $np=(E_1,...,E_k)'$, entonces:
\[
(O_1-E_1,...,O_k-E_k)'=\left\{\sum_{i=1}^n(\xi_i)\right\}-np=n(\overline{\xi}-p)
\]

Definimos la matriz $\mathbb{P}$, que tiene rango k, se define con las probabilidades $p_i$ en la diagonal y 0 el resto de elementos:
\[
\mathbb{P}=\left(
\begin{array}{cccc}
p_1    & 0      & \cdots & 0 \\
0      & p_2    & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0      & 0      & \cdots & p_k \\
\end{array}
\right)
\]

Y cogiendo la raíz del estadístico de Pearson y sabiendo que $\sqrt{E_i}=\sqrt{np_i}$ nos queda:
\[
\left(\frac{O_1-E_1}{\sqrt{(E_1)}},...,\frac{O_k-E_k}{\sqrt{(E_k)}}  \right) = \mathbb{P}^{-1/2}\sqrt{n} (\overline{\xi}-\mathbb{P})
\]

Por otro lado: 
\[
v=(v_1,...,v_k) \rightarrow v'(\frac{v_1}{\lambda_1},...,\frac{v_k}{\lambda_k})
\]

Así, tomamos $\xi_1,...,\xi_n$ independientes y distribuidas como un vector $\xi$ tal que:
\[
\mathbb{E}(\xi)=p
\]
\[
V(\xi)=\mathbb{E}(\xi \xi') -pp' = \mathbb{P}-pp' \equiv \Sigma 
\]

$\mathbb{E}(\xi\xi')=\mathbb{P}$ ya que tenemos:
\[	ξ_r ξ_l =
	\begin{cases}
		0, & r≠l \\
		ξ_r^2 = ξ_r, & r=l \text{ ,pues } ξ_r \text{ es una Bernoulli}
	\end{cases}
\]

Por otra parte: 
\[
T=\sum_{i=1}^k \frac{(O_i-E_i)^2}{E_i} = \norm{p^{-1/2}\sqrt{n}(\xi-p)}^2
\]

Por el TCL:
\[
\sqrt{n}(\overline{\xi}-p) \stackrel{d}{\rightarrow} N_k(0, \Sigma) \implies \mathbb{P}^{-1/2}\sqrt{n}(\overline{\xi}-p) \stackrel{d}{\rightarrow} N_k(0, \mathbb{P}^{-1/2} \Sigma  \mathbb{P}^{-1/2}) 
\]
\[
\Rightarrow \norm{P^{-1/2} \sqrt{n} (\xi-p)}^2 \stackrel{d}{\rightarrow} \norm{Y}^2 \text{ con } Y \equiv N_k(0,\mathbb{P}^{-1/2} \Sigma  \mathbb{P}^{-1/2})
\]

Queda claro que $\mathbb{P}^{-1/2} Σ  \mathbb{P}^{-1/2}$ es simétrica, veamos que es idempotente:
\[\mathbb{P}^{-\frac{1}{2}} (\mathbb{P} - pp') \mathbb{P}^{-\frac{1}{2}} = I-\sqrt{p}\sqrt{p}'\]
donde $\sqrt{p}=(\sqrt{p_1}, …, \sqrt{p_2})'$.

\[
(I-\sqrt{p}\sqrt{p}')(I-\sqrt{p}\sqrt{p}') = I-2\sqrt{p}\sqrt{p}'+\sqrt{p}\underbrace{\sqrt{p}'\sqrt{p}}_{\sum p_i=1}\sqrt{p}'=I-\sqrt{p}\sqrt{p}'
\]

De el ejercicio 9 de la hoja 1 sabemos que una normal multivariante de media 0 y cuya matriz de covarianzas es simétrica e idempotente, cumple que su norma al cuadrado se distribuye como:
\[
\norm{Y}^2 \equiv \chi^2_{k-1}
\]
Los grados de libertad vienen de la traza de $\Sigma$, y de que $traza(I)=k$ y $traza(\sqrt{p}\sqrt{p'})=1$: 
\[
traza(\Sigma)=traza(I-\sqrt{p}\sqrt{p}') = traza(I) - traza(\sqrt{p}\sqrt{p}')=k-1
\]
\end{proof}

\section{Contraste de bondad de ajuste $\chi^2$ para hipótesis nula compuesta}

Problema: $X_1,...,X_n \stackrel{iid}{\sim} F$. Suponemos como hipótesis nula:
\[
H_0 : F \in \{F_{\theta}: \theta \in H \subset \mathbb{R}^r\}
\]

\textcolor{red}{La diferencia es que ahora la hipótesis nula que consideramos es que los datos van a seguir una distribución teórica $F_0$ que no está totalmente especificada, ya que va a depender de un parámetro. Por ello, decimos con palabras que:}

\textcolor{red}{La hipótesis nula es que los datos muestrales van a tener una función de distribución $F$, que va a ser igual a $F_{\theta}$, siendo $\theta$ el parámetro del que dependerá, el cual pertenece a un espacio paramétrico $H$}

Pasos:
\begin{enumerate}
\item Se definen k clases $A_1,...,A_k$. 

\item Se cuentan cuántos datos caen en cada clase (frecuencias observadas). Cada clase la llamaremos $O_i=\#\{j:X_j\in A_i\}$. Hasta aquí todo igual que antes.

\item Para estimar/calcular las frecuencias esperadas se sigue un método ligeramente diferente:

Se estima $\theta$ por el método de máximo verosimilutd. Sea $\hat{\theta}$ el EMV.

\textcolor{red}{explicar bien esto}

\item Se calculan las frecuencias esperadas estimadas bajo $H_0$: $\hat{E}_i=n\hat{p}_i$ con $i=1,...,k$ donde $\hat{p}_i = p_{\hat{\theta}}(A_i)$.

\item Calculamos el estadístico $\chi^2$ de Pearson:
\[
T=\sum_{i=1}^k \frac{(O_i-\hat{E}_i)^2}{\hat{E}_i}
\]

Ahora puedo elegir de todas las posibles distribuciones, aquella que más se parece. De modo que cabe esperar que T tienda a tomar valores menores que en el caso simple.

Además, al estimar r (\textcolor{red}{¿De dónde sale r? Es la dimensión del parámetro estimado??}) parámetros se introducen r nuevas restricciones sobre el vector $O_1,O_2,...,O_r$.

Se puede probar bajo condiciones de regularidad:
\[
\sum_{i=1}^k \frac{(O_i-\hat{E}_i)^2}{\hat{E}_i} \stackrel{d}{\rightarrow} \chi^2_{k-1-r} \text{ bajo } H_0 \text{ si } n \rightarrow \infty
\]

\item Se rechaza $H_0$ en la región crítica: $R=\{T>\chi^2_{k-1-r;\alpha}  \}$

Tal y como se ha hecho en el caso anterior.

\end{enumerate}

\begin{example}
Los bombardeos de Londres. El problema trata de estudiar los bombardeos que sufrío Londres entre 1944 y 1945. Se quería saber si los impactos sobre la ciudad de Londres eran en lugares aleatorios o estaban dirigidos a lugares concretos.

La fórmula de Poisson se ajusta bastante a un modelo de distribución aleatoria de impactos. Por tanto, tendríamos que estimar el parámetro $\lambda$ de la distribución de Poisson, que tiene por función de densidad:

$$ f(k,\lambda)=\frac{e^{-\lambda}\lambda^k}{k!} $$

Donde:
\begin{itemize}
\item k es el número de ocurrencias del evento o fenómeno (la función nos da la probabilidad de que el evento suceda precisamente k veces).

\item λ es un parámetro positivo que representa el número de veces que se espera que ocurra el fenómeno durante un intervalo dado. Por ejemplo, si el suceso estudiado tiene lugar en promedio 4 veces por minuto y estamos interesados en la probabilidad de que ocurra k veces dentro de un intervalo de 10 minutos, usaremos un modelo de distribución de Poisson con λ = 10×4 = 40.
\end{itemize} 
Dicho esto, vamos a seguir los pasos anteriormente detallados:



\begin{enumerate}
\item Se definen k clases $A_1,...,A_k$. En nuestro caso, las clases van a ser el número de impactos que ha habido en un cuadrado. Por tanto los cuadrados que pertenezcan a $A_1$ serán aquellos que han sufrido un único impacto.

\item Se cuentan cuántos datos caen en cada clase (frecuencias observadas). Cada clase la llamaremos $O_i=\#\{j:X_j\in A_i\}$. En nuestro caso tenemos: $O_0=229$, $O_1=211$, $O_2=93$, $O_3=35$, $O_4=8$ ($O_4$ es 4 o más impactos).


\item Para estimar/calcular las frecuencias esperadas se estima $\theta$ por el método de máximo verosimilutd. Sea $\hat{\theta}$ el EMV. En este caso, nuestro $\theta$ sera $\lambda$ y nuestro $\hat{\theta}$ será $\hat{\lambda}$, que será el parámetro de la distribución de Poisson:

$$ \hat{\lambda} = \frac{0\cdot229 + 1\cdot211+2\cdot93+3\cdot35+4\cdot7+5\cdot1}{576}=0.9323$$

\textcolor{red}{explicar por qué esto es el EMV, ya que en estadistica 1 hacíamos u lio increible pa sacarlo}

\item Se calculan las frecuencias esperadas $\hat{E}_i=n\hat{p}_i$ con $i=1,...,k$ donde $\hat{p}_i = p_{\hat{\theta}}(A_i)$. En nuestro caso:

$$\hat{E}_k = n\hat{p}_k = 576\cdot e^{-\hat{\lambda}\frac{\hat{\lambda}^k}{k!}}$$

Sustituimos $\lambda = 0.9323$ y $k=0,...,5$ y nos queda: $\hat{E}_0 = 226.74 $, $\hat{E}_1 = 211.34 $, $\hat{E}_2 = 98.54$, $\hat{E}_3 = 30.62$, $\hat{E}_4 = 8.71$.

\item Calculamos el estadístico $\chi^2$ de Pearson:
\[
T=\sum_{i=1}^k \frac{(O_i-\hat{E}_i)^2}{\hat{E}_i} = 1.0176
\]

Bajo $H_0$ tenemos que $T \equiv \chi^2_3$. El 3 sale de k=5 clases menos 1 parámetro estimado menos 1 como hacíamos antes.

\item Se rechaza $H_0$ en la región crítica: $R=\{T>\chi^2_{k-1-r;\alpha}  \}$

En nuestro caso, tomando $\alpha = 0.05$, tenemos: $$R=\{T>\chi^2_{3;\alpha} \} \rightarrow \{1.0176>7.815 \} \rightarrow \text{No se puede rechazar } H_0$$

Podemos calcular el p-valor mirando:
$$P\{\chi^2_3 > 1.0176\} = 0.797$$

Efectivamente, si miramos la tabla de la $\chi^2_3$, con $\alpha =0.797$, T valdría aproximadamente 1.
\end{enumerate}

\end{example}

\begin{example}
Ejemplo con R de los bombardeos:

Tenemos el siguiente comando para contrastes de bondad de ajuste de $\chi^2$:
\begin{verbatim}
chisq.test(datos,p=...)
\end{verbatim}

\begin{itemize}
\item datos: La muestra de la que disponemos.
\item p: Es el vector de probabilidades esperadas.
\item Por defecto, se contraste la hipótesis de que los datos siguen una distribución uniforme.
\item Se supone que bajo $H_0$ la distribución está completamente especificada (k-1 grados de libertad)
\end{itemize}
\textcolor{red}{Tengo anotado que R sólo funciona con hipótesis simples, y no compuestas, donde tenemos en cuenta eso?}

Exponemos el código a ejecutar y explicamos a continuación lo que hace:

\begin{verbatim}
res = c(seq(0,4),7)
obs = c(229,211,93,35,7,1)
n = sum(obs)
lambda = sum(res*obs)/n
prob = dpois(res,lambda)
esp = n*prob
\end{verbatim}

\begin{enumerate}
\item Guarda en \verb|res| un vector con las clases. Es decir, el número de impactos que ha habido en un cuadrado. Se obtiene:

\verb|res = 0 1 2 3 4 7|

\item Guarda en \verb|obs| un vector con el número de cuadrados de cada clase. Se obtiene:

\verb|obs = 229 211  93  35   7   1|

\item Guarda en \verb|n| el tamaño de la muestra, que es la suma de los elementos del vector \verb|obs|. Se obtiene \verb|n = 576|
\item Guarda en lambda el parámetro de la distribución de Poisson. Se obtiene \verb|lambda = 0.9322917|. Y sale de esta fórmula:
$$ \hat{\lambda} = \frac{0\cdot229 + 1\cdot211+2\cdot93+3\cdot35+4\cdot7+5\cdot1}{576}=0.9323$$
\item Guarda en \verb|prob| un vector con las probabilidades de aparición de cada clase, como la Poisson es una función de distribución discreta, que depende de dos parámetros, lo único que hacemos es sustituir en esta fórmula con $\lambda$ = \verb|lambda| y los valores de k = \verb|res|:
$$e^{-\hat{\lambda}\frac{\hat{\lambda}^k}{k!}}$$

Se obtiene:

\verb|prob = 3.9365e-01 3.6699e-01 1.7107e-01 5.3163e-02 1.2391e-02 4.7812e-05|

\item Guarda en \verb|esp| un vector con las esperanzas de cada clase. Se obtiene:

\verb|esp = 226.74272 211.39035  98.53873  30.62227   7.13722   0.02754|
\end{enumerate}

Continuamos agrupando las clases 4 y 5 es una sola clase, es decir, obteniendo una sola clase que serán los cuadrados con 4 o más impactos:

\begin{verbatim}
obs = c(obs[1:4], sum(obs[5:6]))
prob = c(prob[1:4], 1-sum(prob[1:4]))
esp = c(esp[1:4], n-sum(esp[1:4]))
\end{verbatim}

\begin{enumerate}
\item Obtenemos: \verb|obs = 229 211  93  35   8|
\item Obtenemos: \verb|prob = 0.393650 0.366997 0.171074 0.053163 0.015114|
\item Obtenemos: \verb|esp = 226.7427 211.3903  98.5387  30.6222   8.7059|
\end{enumerate}

Ahora vamos a dibujar el gráfico de barras:

\includegraphics[scale=0.5]{img/contrasteba.png}

\begin{verbatim}
matriz = rbind(prob, obs/n)
rownames(matriz) = c('Frecuencias', 'Poisson')
barplot(matriz, beside=TRUE, names.arg=c(0:4),
 legend.text=TRUE, col=c('lightgreen','orange'))
\end{verbatim}

\begin{enumerate}
\item Guardamos en \verb|matriz| una matriz de dos filas, la primera son las probabilidades teóricas esperadas, la segunda las muestrales:
\begin{verbatim}
          [,1]      [,2]      [,3]       [,4]       [,5]
prob 0.3936506 0.3669971 0.1710742 0.05316368 0.01511444
     0.3975694 0.3663194 0.1614583 0.06076389 0.01388889
\end{verbatim}
\item Asignamos a la primera fila el nombre de 'Frecuencias' y a la segunda 'Poisson'.
\item Pintamos las barras con \verb|barplot|, con leyenda, y como nombre de cada par de barras ponemos 0,1,2,3 y 4, identificando las clases.
\end{enumerate}

Por último calculamos los valores importantes en un contraste, que son el p-valor, que es el mínimo valor de $\alpha$ a partir del cual podemos rechazar la hipótesis nula.

\begin{verbatim}
t = chisq.test(obs,p=prob)$statistic
pvalor = 1-pchisq(t,3)
\end{verbatim}

Obtenemos \verb|t =  1.017589  | y \verb|pvalor = 0.7969959|. El pvalor es muy alto, por tanto no podemos rechazar la hipótesis nula, es decir, no podemos rechazar que los datos proceden de una distribución de Poisson.  El nivel habitual de rechazo sería con $\alpha = 0.05$ que implica que si lo rechazamos tenemos un $5\%$ de posibilidades de equivocarnos. Si quisiéramos rechazar con un $\alpha = 0.79$, tendríamos una probabilidades del $79\%$ de equivocarnos.



\end{example}



\section{Contraste de bondad de ajuste de Kolmogorov-Smirnov}
Sea $X_1,...,X_n \stackrel{iid}{\sim} F$. Definimos la función de distribución empírica, correspondiente a $X_1,...,X_n$ como:

$$ F_n(x)=\frac{1}{n}\#\{i: X_i \leq x\} $$

Es una función de distribución constante a trozos, y con saltos de magnitud $\frac{1}{n}$ en cada valor muestral de $X_i$. Aunque ponga $F_n$, solo hay una para la muestra entera (ya que las variables aleatorias están idénticamente distribuidas), sólo se pone $F_n$ porque depende directamente del número de elementos de la muestra.

Consideramos como hipótesis nula $H_0: F=F_0$. Siendo $F_0$ una distribución previamente especificada

Así, $F_n$ es un estimador de la verdadera distribución F. Que como toda distribución se define como $F(X)=P(X\leq x)$.

\begin{example}
Consideramos una muestra con 3 elementos: $X_1=1, X_2=4, X_3=6$. Ahora, para que sea más fácil construir la función de distribución ordenamos la muestra y nos queda:
$$ X_{(1)}=1 \text{, }X_{(2)}=4 \text{, }X_{(3)}=6 \rightarrow \text{Estos son los estadísticos de orden}$$

Por tanto, la función de distribución queda:

\includegraphics[scale=0.5]{img/contrasteks.png}

Y es bastante razonable. Por ejemplo $P(X=1) = F(1^+)-F(1^-)=\frac{1}{3}-0=\frac{1}{3}$. Algo similar ocurre con $P(X=4) = P(X=6) = \frac{1}{3}$. Lo cual es razonable si nos limitamos únicamente a observar la muestra. Además, para el resto de valores de X, la probabilidad es 0: $P(X=2)=F(2^+)-F(2^-) = \frac{1}{3}-\frac{1}{3}=0$
\end{example}

\begin{obs}
\begin{enumerate}
\item Esta observación sale de sustituir en las fórmulas con las definiciones que hemos dado. Sabiendo que la esperanza de una binomial es $\mathbb{E}\Big(B(n,p)\Big)=np$ $$nF_n(x) = \#\{i:X_i\leq x \} \equiv B(n, F(x))  \Rightarrow \mathbb{E}\Big[F_n(X)\Big] = \frac{1}{n} n F(x) = F(x)$$
\item Con el mismo razonamiento, pero sabiendo que si $X \sim B(n,p)$, entonces $Var(X)=np(1-p)$:
$$ Var(F_n(X))=\frac{1}{n^2}nF(x)(1-F(x)) \stackrel{n \rightarrow \infty}{\rightarrow} 0$$
\item Como consecuencia:
$$ F_n(X) \stackrel{P}{\rightarrow} F(X)$$

Convergencia en probabilidad o en medida: Si $\forall \epsilon >0$, $\lim_{n \rightarrow \infty}P(\abs{X-X_n}\geq \epsilon)=0$.
\end{enumerate}
\end{obs}
De hecho, se cumple que (lema de Glivenko-Cantelli):
$$\norm{F_n-F}_{\infty} = \sup\abs{(F_n(X)-F(x)} \stackrel{c.s.}{\rightarrow} 0$$

Si $H_0: F=F_0$ fuese cierta, se espera que $D_n = \norm{F_n-F_0}_{\textcolor{red}{\infty}}$ sea pequeño ($D_n$ es el estadístico de Kolmogorov-Smirnov). La idea es rechazar en la región $R=\{D_n > C\}$, para un valor c tal que $P_{H_0}(D_n > c)=\alpha$, donde $\alpha$ es el nivel de significación.

\textcolor{red}{\textbf{Importante:}la distribución bajo $H_0$ de $D_n$ es la misma para cualquier distribución continua $F_0$. El valor de c en la región crítica es el mismo para cualquier distribución continua $F_0$ y esta tabulado. $F_0$ es la distribución teórica a la que queremos ver si pertenecen los datos. Mientras que $F=F_n$ que es la empírica.}

\begin{prop}
Si una v.a. X tiene distribución continua (\textcolor{red}{Continua por la derecha en todo caso}) $F_0$, entonces la v.a. $F_0(X) \sim U(0,1)$ (Uniforme en (0,1)).
\end{prop}
\begin{proof}
Queremos ver que $P(F_0(X)\leq u)=u$ $\forall u \in [0,1]$ (que es lo que ocurriría si $F_0$ siguiera una distribución uniforme entre 0 y 1).

Así, sea $F_0$ continua, entonces existe un x tal que $F_0(x)=u$. Y tendríamos que:
$$ P(F_0(X)\leq u) = P(F_0(X)\leq F_0(x)) $$

Ahora sabiendo que la función de distribución es monótona creciente (m.c.), del primer miembro nos quitamos $F_0(X)\leq F_0(X)$ y del segundo, el menor o igual, ya que si $X>x$ solo puede ser que $F_0(X)= F_0(X)$:
$$\{ F_0(X)\leq F_0(x) \} = \{ F_0(X)\leq F_0(x), X \leq x\} \cup \{ F_0(X)\leq F_0(x), X > x\} = $$
$$ =  \{ X \leq x\} \cup \{ F_0(X) \underbrace{=}_{m.c.} F_0(x), X > x\}$$

Y, basándonos en que $F_0(X)=P(X \leq x)=u$ y en que la probabilidad es 0 en un trozo donde la función de distribución es constante, nos qued: 
$$P(F_0(X)\leq F_0(x)) = P(X \leq x) + P(F_0(X) = F_0(x), X \geq x) = F_0(X) + 0 = u$$
\end{proof}

\begin{obs}
Existe un recíproco de la proposición: Si $U \sim U(0,1) \Rightarrow F^{-1}(U) \sim F$. \textcolor{red}{Explicar mejor este recíproco}
\end{obs}

La $D_n$ de la que estábamos hablando antes de meternos en la proposición se conoce como:

\begin{defn}[Estadístico Kolmogorov-Smirnov]
	\[D_n = \max \left\{ 0, \max_{i=1,…,n} \left( \frac{i}{n} - F_0(x_{(i)}) \right), \max_{i=1,…,n} \left(F_0(x_{(i)}) - \frac{i-1}{n})\right) \right\}\]
\end{defn}

Y a continuación vamos a demostrar por qué tiene la expresión que aparece en la definición.
\begin{proof}

$$ D_n = \max \left\{ \sup_{x\in\mathbb{R}} \Big( F_n(x)-F_0(x)\Big), \sup_{x\in\mathbb{R}} \left( F_0(x)-F_n(x)\right) \right\} $$

Si representamos los estadísticos de orden de la muestra en una recta, y llamamos a $X_{(0)}=-\infty$ y $X_{(n+1)}=\infty$:

\includegraphics[scale=0.5]{img/contrasteks2.png}

Nos queda que si x está entre $X_{(i)}$ y $X_{(i+1)}$, entonces $F_n(x)=\frac{i}{n}$.

Desarrollando el primer término de $D_n$ nos queda:
$$ \sup_{x\in\mathbb{R}} \Big( F_n(x)-F_0(x)\Big) = \max_{i=0,...,n} \Bigg(\sup_{x\in(X_{(i)},X_{(i+1)})} \Big( F_n(x)-F_0(x)\Big) \Bigg) = $$

$$ = \max_{i=0,...,n} \Big( \frac{i}{n} - F_0(X_{(i)}) \Big) = \max\Big\{ 0, \max_{i=1,...,n} \Big( \frac{i}{n} - F_0(X_{(i)}) \Big) \Big\}$$

\textcolor{red}{Explicar por qué es $\frac{i}{n}$} % Jorge: Es por definición de la función de distrib. empírica.

Desarrollando el segundo término nos queda:
\[\sup_{x∈ℝ}\left( F_0(x) - F_n(x) \right) = \max_{j=0,…,n} \left( \sup_{x∈(x_{(j)},x_{(j+1)})} \left( F_0(x) - F_n(x) \right) \right) =\]

\[= \max_{j=0,…,n} \left( F_0(x_{(j+1)}) - \frac{j}{n} \right) \underbrace{=}_{i=j+1} \max_{i=1,…,n+1} \left( F_0(x_{(i)}) - \frac{i-1}{n} \right) =  \]
$$ = \left\{ 0, \max_{i=1,...,n} \left( F_0(X_{(i)}) - \frac{i-1}{n} \right) \right\} $$

\textcolor{red}{Explicar por qué es $\frac{i-1}{n}$}

Por tanto, finalmente nos queda:
$$ D_n =  \norm{F_n-F_0}_{\infty} = \max\Big\{ 0, \max_{i=1,...,n} \Big( \frac{i}{n} - F_0(X_{(i)}) \Big) , \max_{i=1,...,n} \Big( F_0(X_{(i)}) - \frac{i-1}{n} \Big)$$

Por tanto concluimos que $D_n$ depende de $F_0$ a través de los valores de $F_0(X_{(1)}), F_0(X_{(2)}),...,F_0(X_{(n)})$. Si tengo una muestra de $X_1,...,X_n \stackrel{iid}{\sim}F_0$, entonces $F_0(X_1),...,F_0(X_1) \stackrel{iid}{\sim} U(0,1)$. Ordenándolos los elementos: $X_{(1)}\leq...\leq X_{(n)}$, entonces $F_0(X_{(1)})\leq...\leq F_0(X_\textbf{{(n)}}) \stackrel{iid}{\sim} U(0,1)$. Que son los estadísticos de orden de una muestra de tamaño n, de variables aleatorias iid, que siempre van a seguir una distribución de una $U(0,1)$ para toda $F_0$ continua.

\textbf{Notación}
$$D_n^+=\max_{i=0,...,n} \Big( \frac{i}{n} - F_0(X_{(i)}) \Big)$$
$$D_n^-= \max_{i=0,...,n} \Big( F_0(X_{(i)}) - \frac{i-1}{n} \Big)$$

\end{proof}

\begin{example}
Ejemplo con R:

Tenemos el siguiente comando para contrastes de bondad de ajuste de Kolmogorov-Smirnov:
\begin{verbatim}
ks.test(datos,distribucion,parametros)
\end{verbatim}

\begin{itemize}
\item \verb|datos|: La muestra de la que disponemos.
\item \verb|distribucion|: Distribución bajo $H_0$. Es la distribución que creemos teórica de los datos, la que hemos llamado $F$. (Por ejemplo, $pnorm$).
\item \verb|parametros|: Parámetros de la distribución $F$. 
\end{itemize}
Vamos a probar a usar los datos 'Kevlar'. Corresponden al tiempo hasta el fallo (en horas) de 101 barras de un material utilizado en los transbordadores espaciales.

Obtenemos los datos de \url{http://www.uam.es/personal_pdi/ciencias/acuevas/docencia/estI/Datos-kevlar.txt}. Los metemos en un archivo de texto $kevlar.txt$.

Ejecutamos:

\begin{verbatim}
kev = scan('kevlar.txt')

boxplot(kev)

hist(kev)

plot(ecdf(kev), verticals=TRUE, do.points=FALSE)
curve(pexp(x), add=TRUE, col='red')
\end{verbatim}
 
Y obtenemos estas tres figuras:

\includegraphics[scale=0.8]{img/contrasteks3.png}
\includegraphics[scale=0.5]{img/contrasteks4.png}

En esta última observamos perfectamente la función $F_n$ constante a trozos con valores $\frac{1}{n}$. Hemos contrastado la muestra con la hipótesis nula de que los datos siguen una distribución exponencial de parámetro $\lambda = 1$. (esta es la recta roja que sale con $pexp(x)$).

Por último ejecutamos:
\begin{verbatim}
ks.test(kev,pexp)
\end{verbatim}

Y obtenemos:
\begin{verbatim}
data:  kev
D = 0.087038, p-value = 0.4286
alternative hypothesis: two-sided
\end{verbatim}

Si ejecutamos \verb|ks.test(kev, pnorm)$statistic|, obtenemos solo el valor del estadístico: \verb|0.08703787| 

\end{example}

\begin{example}
Contrastar a nivel $\alpha=0.01$ si la muestra $X_1=16, X_2=8, X_3=10, X_4=12, X_5=6$ procede de una distribución exponencial de media $11.5$.

Sea X una v.a con distribución exponencial, tiene función de distribución:
$$F_0(X) = 1-e^{-\lambda x} \text{ si x } \geq 0$$

Y sabemos que $\mathbb{E}(X)=\frac{1}{\lambda}$. De esto, sacamos que en nuestro caso $\lambda = \frac{1}{11.5}$ 

\begin{tabular}{|c|c|c|c|c|}
\hline
 $X_{(i)}$ & i/n & $F_0(X_{(i)})$ & $D_n^+$ & $D_n^-$ \\
\hline
6 & 0.2 & 0.41 & -0.21 & 0.41 \\
\hline
8 & 0.4 & 0.5 & -0.1 & 0.3 \\
\hline
10 & 0.6 & 0.58 & 0.02 & 0.18 \\
\hline
12 & 0.8 & 0.65 & 0.15 & 0.05 \\
\hline
16 & 1 & 0.75 & 0.25 & -0.05 \\
\hline
\end{tabular}

Así, nos queda que $D_n = 0.41$.

Y mirando en la tabla de la exponencial con nivel de significación $\alpha = 0.01$, tenemos que c= \textcolor{red}{terminar ejercicio}

\end{example}

\section{Gráficos de probabilidad}
Sean $X_1,...,X_n \stackrel{iid}{\sim} F \Rightarrow F(X_1),...,F(X_n) \stackrel{iid}{\sim} U(0,1)$. Si ordenamos las F nos quedan los estadísticos de orden de una $U(0,1)$: $F(X_{(1)}),...,F(X_{(n)})$.

Por tanto, si tengo una muestra de tamaño 2, entonces la media sería que F del dato más pequeño $F(X_{(1)})$ sea $\frac{1}{3}$ y que el dato más grande $F(X_{(2)})$ sea $\frac{2}{3}$. Ya que hemos estimado que F sigue una distribución uniforme en $[0,1]$.

De la misma forma, si hay n datos, la media sería que el dato mínimo se encuentre en $\frac{1}{n+1}$ y el dato máximo en $\frac{n}{n+1}$.

En definitiva, tenemos que la media del valor de F del dato i-ésimo es:
$$\mathbb{E}(F(X_{(i)}) \approx \frac{i}{n+1}$$

Es decir, tendríamos que $F(X_{(i)}) \approx \frac{i}{n+1}$ y por tanto, debería ocurrir que $X_{(i)} \approx F^{-1}(\frac{i}{n+1})$. Si esto ocurre, tendríamos una gráfica que representaría la recta y=x, en el eje de ordenadas tendríamos $F^{-1}(\frac{i}{n+1})$ y en el eje de abscisas tendríamos $X_{(i)}$. La idea es que si esto ocurre los datos vienen de una normal, es decir: $X_1,...,X_n \stackrel{iid}{\sim} F=N(\mu, \sigma)$.

Además, sea $\Phi \sim N(0,1)$. Sea $F(X) = \Phi(\frac{x-\mu}{\sigma})$ entonces:
$$ X_{(i)} = F^{-1}\Big(\frac{i}{n+1}\Big) = \sigma \Phi^{-1}\Big(\frac{i}{n+1}\Big)+ \mu $$

Se representa la gráfica:
$$ \bigg( X_{(i)}, \Phi^{-1}\Big(\frac{i}{n+1}\Big) \bigg)$$

Si la gráfica es una recta, no necesariamente de pendiente 1, quiere decir que los datos son normales.

Aquí tenemos 12 ejemplos:

\includegraphics[scale=0.6]{img/graficos1.png}

\includegraphics[scale=0.6]{img/graficos2.png}

\section{Contraste $\chi^2$ de homogeneidad}
Sean un conjunto de muestras tomados de diferentes lugares o contextos $M_1,...,M_p$. Cada conjunto de muestras seguirá una distribución $F_1,..,F_p$.

\begin{align*}
M_1 \equiv & X_{11} .... X_{1\cap 1} \stackrel{iid}{\sim} F_1 \\
& \vdots \\
M_p \equiv & X_{p1} .... X_{p\cap p} \stackrel{iid}{\sim} F_p
\end{align*}

\textcolor{red}{lo de la intersección en $X_{p\cap p}$ que es??}
\textcolor{blue}{No es un símbolo de intersección sino la letra $n$, refiriendose a que el tamaño muestral de cada muestra puede ser diferente: $X_{1n_1},X_{1n_2},...,X_{1n_p}$}

Consideraremos como hipótesis nula: $H_0: F_1=...=F_p$. Es decir, queremos ver si las muestras tomadas de diferentes lugares siguen la misma distribución.

Dividimos los datos de cada conjunto de muestras en clases $A_1,...,A_k$, todos los conjuntos $M_1,...,M_p$ tendrán los mismos tipos de clases. Y consideramos las frecuencias observadas: $O_{ij}=$ nº de datos de $M_j$ en $A_i$.

Llamamos tabla de contingencia a la siguiente tabla:

\begin{tabular}{cccc}
& $M_1$ & ... & $M_p$ \\
$A_1$ & $O_{11}$ &  & $O_{1p}$ \\
\vdots &  & $O_{ij}$ &  \\
$A_k$ & $O_{k1}$ &  & $O_{kp}$ \\
\end{tabular}

Cada elemento $O_{ij}$ de la tabla es el número de muestras de una clase para cada conjunto de muestras. Queremos estimar este valor mediante una binomial (\textcolor{red}{por que?} \textcolor{blue}{ Porque una binomial Bin(n,p) modela el número de éxitos en n experimentos independientes donde la probabilidad de éxito es p. En este caso, $O_{ij}$ es el número de observaciones de la muestra j que caen en la clase i, que es lo mismo que el numero de éxitos entre nj observaciones (las que hay en la muestra j) que caen en la clase i }). Así:
$$ O_{ij,H_0} \equiv B(n_j, p_i)  \text{ con } p_i=P_{H_0}(A_i)$$

Pero desconocemos este valor $P_{H_0}(A_i)$, por lo que lo tenemos que estimar.

Llamamos $E_ij =n_j p_i$ frecuencia esperada bajo $H_0$.

Realizamos la siguiente operación \textcolor{red}{El por qué es aún un misterio}:
$$ \sum_j \underbrace{\sum_i \frac{(O_{ij}-E_ij)^2}{E_{ij}}}_{\chi^2_{k-1}} \stackrel{d}{\rightarrow} \underbrace{\chi^2_{p(k-1)}}_{p\chi^2_{k-1}} $$

Queremos estimar $p_1,...,p_k$:
$$\hat{p}_i=\frac{\sum_{j=1}^p O_{ij}}{n}$$

Con $n=n_1+...+n_p$, como tenemos homogeneidad, es como si tuviéramos $n_1+n_2+...+n_p$ datos en total. 

Ahora podemos calcular la esperanza estimada:
$$\hat{E}_{ij} = n_j \hat{p}_i=n_j \cdot \frac{\sum_j O_{ij}}{n}$$

\textbf{Notación:} 
\begin{itemize}
\item $\sum_j O_{ij} = O_{i\cdot}$
\item $\sum_j O_{ij} = n_j =  O_{\cdot j}$
\end{itemize}
Por tanto:
$$\hat{E}_{ij} = \frac{O_{i\cdot} O_{\cdot j}}{n}$$

Y ahora hacemos una tabla parecida a la anterior pero con las esperanzas estimadas:

\begin{tabular}{cccc}
& $M_1$ & ... & $M_p$ \\
$A_1$ & $\hat{E}_{11}$ &  & $\hat{E}_{1p}$ \\
\vdots &  & $\hat{E}_{ij}$ &  \\
$A_k$ & $\hat{E}_{k1}$ &  & $\hat{E}_{kp}$ \\
\end{tabular}

Ahora con los estimadores obtenidos:
$$ \sum_i \sum_j \frac{(O_{ij}-\hat{E}_{ij})^{\textcolor{red}{2}}}{\hat{E}_{ij}} \stackrel{d}{\rightarrow} \chi^2_{(p-1)(k-1)} $$

Como podemos observar, la $\chi^2$ no es de $p(k-1)$ como antes sino que es de $p(k-1)-(k-1)$, los últimos $k-1$ son el nº de parámetros estimados. Ya que estimamos $\hat{p}_1,...,\hat{p}_k$, pero con la condición $\hat{p}_1+...+\hat{p}_k  1$.

La región de rechazo quedaría:
$$ R=\{T > \chi^2_{(k-1)(p-1),\alpha}\}$$

\begin{obs}
Se puede comprobar que:
$$ T = \sum_{i=1}^k \sum_{j=1}^p \frac{O_{ij}}{E_{ij}}-n$$

\textcolor{red}{Y antes dónde habíamos definido T??}
\end{obs}

\begin{example}
Tenemos 3 muestras, una de España, otra de Italia y otra de Francia, todas de tamaño $n=100$.Las clases son 'no fumadores' (NF), 'fumadores ocasionales' (FO) y 'fumadores habituales' (FH).

Tenemos la siguiente tabla de contingencia:

\begin{tabular}{ccccc}
& $M_1=$España & $M_2=$Italia & $M_3=$Francia & \\
$A_1=$NF & $O_{11}=30$ & $O_{12}=15$ & $O_{13}=20$ & $O_{1\cdot}=65$ \\
$A_2=$FO & $O_{21}=50$ & $O_{22}=40$ & $O_{23}=50$ & $O_{2\cdot}=140$ \\
$A_3=$FH & $O_{31}=20$ & $O_{32}=45$ & $O_{33}=30$ & $O_{3\cdot}=95$ \\
& $O_{\cdot1}=n_1=100$ & $O_{\cdot2}=n_2=100$ & $O_{\cdot3}=n_3=100$ & 300 \\
\end{tabular}

Recordamos la fórmula de la esperanza estimada:
$$\hat{E}_{ij} = n_j \cdot \frac{\sum_j O_{ij}}{n} = \frac{O_{i\cdot} O_{\cdot j}}{n}$$

Vamos a calcular la esperanza estimada $\hat{E}_{12}$ es decir, la de Italia y no fumadores:

$$\hat{E}_{12} = n_2 \hat{p}_1 = n_2 \cdot \frac{\sum_{j=1}^3 O_{1j}}{n} = 100 \cdot \frac{30+15+20}{300} = 21,\stackrel{\frown}{6}$$

Así, la tabla de esperanzas quedaría:

\begin{tabular}{ccccc}
& España & Italia & Francia & \\
NF & $\hat{E}_{11}=21,\stackrel{\frown}{6}$ & $\hat{E}_{12}=21,\stackrel{\frown}{6}$ & $\hat{E}_{13}=21,\stackrel{\frown}{6}$ & 65 \\
FO & $\hat{E}_{21}=46,\stackrel{\frown}{6}$ & $\hat{E}_{22}=46,\stackrel{\frown}{6}$ & $\hat{E}_{23}=46,\stackrel{\frown}{6}$ & 140 \\
FH & $\hat{E}_{31}=31,\stackrel{\frown}{6}$ & $\hat{E}_{32}=31,\stackrel{\frown}{6}$ & $3\hat{E}_{33}=31,\stackrel{\frown}{6}$ & 95 \\
& 100 & 100 & 100 & 300 \\
\end{tabular}

Ahora calculamos el estadístico T:
$$ T = \sum_{i=1}^k \sum_{j=1}^p \frac{O_{ij}}{E_{ij}}-n = $$
$$\frac{30}{21,\stackrel{\frown}{6}} + \frac{15}{21,\stackrel{\frown}{6}} + \frac{20}{21,\stackrel{\frown}{6}} + \frac{50}{46,\stackrel{\frown}{6}} + \frac{40}{46,\stackrel{\frown}{6}} +
\frac{50}{46,\stackrel{\frown}{6}} +
\frac{20}{31,\stackrel{\frown}{6}} +
\frac{45}{31,\stackrel{\frown}{6}} +
\frac{30}{31,\stackrel{\frown}{6}} = 9$$

\textcolor{red}{No sale lo esperado, revisar y terminar}

La región de rechazo es:
$$ R=\{T > \chi^2_{(k-1)(p-1),\alpha}\}$$

En nuestro caso, suponiendo un nivel de significación $\alpha=0.05$:
$$ R=\{T > \chi^2_{(3-1)(3-1),0.05}\} \Rightarrow R=\{T > \chi^2_{4,0.05}\} \Rightarrow  R=\{T > 9.488\}$$
\end{example}

\section{Contraste Kolmogorov-Smirnov de homogeneidad}
Este contraste sólo es válido para dos muestras, y para distribuciones continuas. Al igual que antes queremos ver que las dos muestras tienen la misma distribución.

Así, tenemos $X_1,...,X_n \stackrel{iid}{\sim} F$ y $Y_1,...,Y_n \stackrel{iid}{\sim} G$, con F y G continuas. La hipótesis nula será $H_0: F=G$, es decir, los datos de la primera muestra están distribuidos con la misma función de distribución que los datos de la segunda muestra.

Para ello calculamos el estadístico K-S para dos muestras:

$$D_{n,m} = \norm{F_n -G_m}_{\infty} = \sup_{x \in \mathbb{R}} \abs{F_n(x)-G_m(x)}  $$

Bajo $H_0$ la distribución $D_{n,m}$ no depende de F=G y está tabulada.
$$ R=\left\{D_{n,m} > C_{\alpha}\right\} $$

\section{Contraste $\chi^2$ de independencia}
Sea $(X_1, Y_1),...,(X_n, Y_n) \stackrel{iid}{\sim} F$. Y sea la hipótesis nula $H_0 :$ X e Y son independientes.

\textcolor{red}{terminar esto en otro momento que no lo veo muy claro}


\chapter{Regresión}
El objetivo de la regresión es predecir una/s variable/s en función de la/s otra/s.


\section{Regresión lineal}

Observamos dos variables, X e Y , el objetivo es analizar la relación existente entre ambas, de forma que podamos predecir o aproximar el valor de la variable Y a partir del valor de la variable X.

\begin{itemize}
\item La variable Y se llama variable respuesta.
\item La variable X se llama variable regresora o explicativa.
\end{itemize}

Por ejemplo:
\begin{center}
\includegraphics[scale=0.5]{img/RentaVsFracaso.png}
\end{center}

Queremos predecir el fracaso escolar en función de la renta. La variable respuesta es el fracaso escolar, mientras que la variable regresora es la renta.

\subsection{Regresión lineal simple}

Frecuentemente existe una relación lineal entre las variables. En el caso del fracaso escolar,queremos construir una recta $Y_i = β_0 X_i + β_1\; i=1,...,n$ que minimice el error.

El problema es estimar los parámetros $β_0,β_1$. Una manera de hacer esto es:

\subsubsection{Recta de mínimos cuadrados}

\begin{defn}[Recta de mínimos cuadrados]
Estimando $β_i$ por $\hat{β_i}$ obtenemos: \[\hat{Y_i} = \hat{β}_0 + \hat{β}_1 x_i\]

La reca viene dada por los valores $\hat{β_0}, \hat{β_1}$ para los que se minimiza el error cuadrático, es decir:
\[\sum_{i=1}^n \left(Y_i - \hat{Y_i}\right)^2 =  \sum_{i=1}^n \left[ Y_i - (\hat{β_0} + \hat{β_1}x_i) \right]^2\]
\end{defn}

\begin{example}
\begin{center}
\includegraphics[scale=0.6]{img/ejemploRectaRegresionLineal.png}
\end{center}
\end{example}

\paragraph{Cómo calcular la pendiente} de la recta de mínimos cuadrados.


Vamos a ver unas pocas maneras de calcular la recta de mínimos cuadrados.

\begin{itemize}

	\item El sistema habitual:

	\[ \hat{β_1} = \frac{\sum_{i=1}^n(x_i - \bar{x})(Y_i - \bar{Y})}{\sum_{i=1}^n (x_i - \bar{x})^2} = \frac{S_{xy}}{S_{xx}} \]
	Donde
		\[S_{xy} = \sum_{i=1}^n(x_i - \bar{x})(Y_i - \bar{Y}) \]
		\label{Ssubxx}
		\[S_{xx} = \sum_{i=1}^n (x_i - \bar{x})^2\]
	\subitem \[β_0 = \bar{Y} - β_1\bar{x}\]

	\textbf{Entonces:}
	\[\text{recta} \equiv y - \bar{y} = \frac{S_{xy}}{S_{xx}}(x - \bar{x} ) \]

	\item Mínimos cuadrados como promedio de pendientes:
	\label{rmc::promediopendientes}
	\[
	\hat{β_1} = \frac{S_{xy}}{S_{xx}} = \sum_{i=1}^n \frac{(x_i - \bar{x})^2}{S_{xx}} \left( \frac{(Y_i - \bar{Y})}{x_i - \bar{x}} \right) = \sum_{i=1}^n ω_i \left( \frac{(Y_i - \bar{Y})}{x_i - \bar{x}} \right)
	\]

	Vemos que hemos ponderado la pendiente de cada recta que une cada punto con la media. Este peso es mayor cuanto mayor es la distancia \textbf{horizontal}.

	\item Mínimos cuadrados como promedio de respuestas:

	\[
	\hat{β_1} = \frac{\sum_{i=1}^n  (x_i - \bar{x}) (Y_i - \bar{Y})}{S_{xx}} = \sum α_i Y_i
	\]
	Es interesante ver unas propiedades de estos $α_i$
	\begin{prop}


		\begin{itemize}
			\item[] $\sum α_i = 0$
			\item[] $\sum α_ix_i = 1$
			\item[] $\sum α_i^2 = \frac{1}{S_{xx}}$
		\end{itemize}

	\end{prop}
	\begin{proof}
	\textcolor{red}{Por hacer}
	\end{proof}

\end{itemize}


\begin{defn}[Residuo]
En una recta de mínimos cuadrados: Sea $y_i = β_1x_i - β_0$ y sea $\hat{y}_i = \hat{β}_1x_i - \hat{β}_0$, llamamos residuo a $$e_i = y_i - \hat{y}_i$$

Los residuos cumplen:

\[
\sum_{i=1}^n e_i = 0
\]

Esto es intuitivo, ya que los errores se compensan y además es una buena propiedad.
\end{defn}



\begin{prop}
Sean $\{e_i\}$ una variable aleatoria que cumple \footnote{Se ha utilizado la $e$ porque es útil en cuanto a los residuos de la recta de mínimos cuadrados}:
\[\sum e_i = 0\]

Entonces:
\[\sum e_i x_i = 0 \implies \cov(e,x) = 0\]
\end{prop}

\begin{proof}
\[\sum (e_i - \vec{µ}) x_i = \sum (e_i - \vec{µ}) (x_i - \vx) \]
Por otro lado:

\[
\sum e_ix_i = \sum e_ix_i - \vx \sum e_i = \sum e_i(x_i - \vx)
\]
\end{proof}


\begin{example}
\[
\sum (x_i - \vx)(y_i -\vy) = \sum (x_i - \vx)y_i - \sum \vy\sum(x_i - \vx) \overset{(1)}{=} \sum(x_i - \vx y_i)
\]
\[
(1) \to \sum(x_i - \vx) = 0
\]
\end{example}

Esto tiene la siguiente explicación ``intuitiva'': La recta de mínimos cuadrados contiene toda la información lineal que $X$ puede dar sobre $Y$ (debido a que la covarianza entre los residuos y $X$ es 0).


\subsubsection{Fallos de la recta de mínimos cuadrados}

Vamos a ver un par de ejemplos ilustrativos:

\begin{example}[Sobre los datos atípicos]

Esta es una recta de mínimos cuadrados calculada para una nube de puntos a la que se ha añadido un punto atípico. Se ve una cierta tendencia de que la pendiente debería ser positiva, pero el dato atípico provoca un cambio brusco.
\begin{center}
%\includegraphics[scale=0.9]{img/rmc_atipico1.png}
\includegraphics[scale=0.9]{img/rmc_atipico2.png}
\end{center}

\end{example}

\begin{example}[Sobre la distancia horizontal]

¿Y da igual lo atípico que sea un dato? La respuesta es que no. Si el dato es muy atípico en la variable respuesta ($Y$), pero es muy \textit{típico} en la variable regresora, la recta no se devía tanto. Vamos a verlo y después explicamos la razón.

Esta es la recta, en la que hemos ignorado los 3 datos que parecen ``atípicos''.
\begin{center}
\includegraphics[scale=0.9]{img/sobredistanciahorizontal1.png}
\end{center}

Ahora calculamos las rectas teniendo en cuenta sólo uno de los puntos.

\begin{center}
\includegraphics[scale=0.4]{img/sobredistanciahorizontal1.png}
\includegraphics[scale=0.4]{img/sobredistanciahorizontal2.png}
\end{center}

Vemos que la recta azul no se desvía apenas de la original, mientras que la recta verde si se desvía un montón. ¿Esto a qué se debe? A que importa más la distancia horizontal de la media que la distancia vertical. Si vamos a la expresión de la recta de mínimos cuadrados como promedio de las pendientes \label{rmc::promediopendientes} vemos que hay un término $\frac{(x_i - \gor{x})}{S_{xx}}$ que hemos tomado como pesos para ponderar y en este caso, la distancia horizontal $(x_i - \gor{x})$ está multiplicando en el numerador.



\end{example}





\subsubsection{Introduciendo ``aleatoreidad'' para poder hacer IC}

Sea $\{ε_i\}$ siendo $ε_i \sim N(0,σ^2)$. Lo habitual es no saber cómo han sido generados los datos y es probable que no vayamos a conocer con exactitud absoluta la recta de mínimos cuadrados. Es por ello que suponemos el siguiente modelo para la variable respuesta:

\[
Y_i = β_1 x_i + β_0 + ε_i
\]


Tenemos que $\bar{y}_i \sim N$, ya que es una combinación lineal de variables normales \textbf{independientes} (como vimos en el Tema 1).


\begin{example}
Sea $σ=1, β_0 = 0 y β_1 = 1$.

Entonces el modelo es:

\[
Y_i = x_i + ε_i
\]

Fijamos $n=10$ y generamos las respuestas para $x_i = i$. Además, repetimos el experimento 6 veces y calculamos las rectas de mínimos cuadrados, obteniendo:

\begin{center}
\includegraphics[scale=0.6]{img/6ejemplosRegresion.png}
\end{center}

Vemos que obviamente las rectas no son las mismas. Esto se debe al $ε_i$ introducido. ¿Cuáles son los valores que toman $β_1$ y $β_0$? Habiendo repetido el experimento 1000 veces, obtenemos los siguientes histogramas:

\begin{center}
\includegraphics[scale=0.3]{img/1000vecesb0.png}
\includegraphics[scale=0.3]{img/1000vecesb1.png}
\end{center}

Vemos que no siempre es el mismo valor. Sabemos (por cómo hemos construido los datos) que $β_0 = 0$ y $β_1 = 1$, pero nuestra manera de calcularos (debido a $ε_i$) no siempre nos da el valor concreto.


\end{example}

El ejemplo anterior nos muestra que en realidad, estamos estimando $β_i$, aunque no nos guste y ahora tenemos que planternos ¿cómo de buenos son nuestros estimadores? Tal vez son una mierda, o tal vez son insesgados.

Para ello, vemos que al haber añadido un error $\epsilon_i \sim N(0,σ^2)$, tenemos:

\[
Y_i = β_0 + β_1x + ε_i \implies Y_i \equiv N(β_0 + β_1X_i, σ^2)
\]


\subsubsection{Estimando $β_1$}

\begin{prop}
Nuestro estimador ``pendiente de la recta de mínimos cuadrados:'' $\hat{β_1}$  cumple

\[
\hat{β_1} \equiv N\left(β_1,\frac{σ^2}{S_{xx}} \right)
\]

\end{prop}

\begin{proof}
Él en clase lo ha hecho al revés. Muchos cálculos para llegar a la conclusión, pero aquí molamos más. En algún momento \textcolor{red}{revisará} alguien los apuntes y completará.

\begin{itemize}
	\item $\esp{\hat{β_1}} = β_1$
	\item $\var{\hat{β_1}} = ... = \displaystyle\frac{σ^2}{S_{xx}}$
\end{itemize}
\end{proof}

\subsubsection{Estimando $β_0$}

\begin{prop}
Nuestro estimador ``término independiente de la recta de mínimos cuadrados:'' $\hat{β_0}$  cumple

\[
\hat{β_0} = N\left(β_0 , σ^2 \left( \frac{1}{n} + \frac{\gor{x}^2}{S_{xx}}\right)  \right)
\]
\end{prop}

\begin{proof}
\begin{itemize}
	\item $\esp{\hat{β_0}} = β_0$
	\item
	$\var{\hat{β_0}} = \var{\gor{Y}} + \var{\hat{β_1}\gor{X}} - 2 \cov{(\gor{Y},\hat{β_1}\gor{X}}$

 	\subitem Calculamos: $\cov{(\gor{Y},\hat{β_1}\gor{X}}$ utilizando cosas del tema 1

 	\[
		\cov{(\gor{Y},\hat{β_1}\gor{X}} = \cov{(\frac{1'_n \gor{Y}}{n},α\gor{Y}} = \frac{1}{n}1'_nσ^2
 	\]
 	debido a que $α = 0$.

 	Ademas de ser incorrelados, son \textbf{independientes}. ¿Porqué? Porque conjutamente son normales, es decir \[
 		\begin{pmatrix} \gor{Y} \\ \hat{β_1} \end{pmatrix} \equiv A\gor{Y} \equiv N_2
 	\]
\end{itemize}

\end{proof}


\textbf{Conclusiones:}
\begin{align*}
\gor{Y} &\text{ es indepediente de } \hat{β_1}\\
\hat{β_1} &\equiv \left(β_1,\frac{σ^2}{S_{xx}}\right)\\
\hat{β_0} &\equiv \left(β_0,σ^2 \left( \frac{1}{n} + \frac{\vec{x}^2}{S_{xx}}\right)\right)
\end{align*}

¿Son estas las variables $\hat{β_1} $ y $\hat{β_2}$ normales una normal conjunta? No, \textbf{no son una normal conjunta} ya que no son independientes. Intuitivamente es fácil de ver. En una recta, si aumentamos la pendiente (y estamos en el primer cuadrante) entonces el término independiente disminuye. Esta dependencia tiene que aparecer. Vamos a estudiar la covarianza entre los estimadores:

\[
\cov{(β_1,β_2} = \cov{(\gor{Y} - \hat{β_1}\gor{x}, \hat{β_0}} = ... = -\gor{x}\frac{σ^2}{S_{xx}}
\]



\subsubsection{IC y Contrastes para $β_1$}

Recordamos que \[ \hat{β}_1 \equiv N\left(β_1,\frac{σ^2}{S_{xx}}\right)\]

Podemos normalizar y buscar una catidad pivotal (como hacíamos en estadística I)

\[
\frac{\hat{β_1} - β_1}{\frac{σ}{S_{xx}}} \equiv N\left(0,1\right)
\]

Pero aquí nos encontramos con que necesitamos $σ$, la varianza de los errores. Esta varianza a menudo no es conocida (porque no sabemos con exactitud cuál es la recta verdadera) y tenemos que estimarla.

Para estimarla, parece razonable usar \[ \hat{σ} = S_R =\frac{\sum_{i=1}^n e_i^2}{n-2}\]

\begin{expla}
Recordamos que para que estimar la varinza, utilizamos (por el lema de fisher) $n-1$ de denominador para que el estimador sea insesgado. Esto sale de que en la demostración, hay una matriz de rango $n-1$ ya que existe una restricción.

Siguiendo este razonamiento, en este caso tenemos 2 restricciones\footnote{$\sum e_i = 0$ y $\sum e_ix_i = 0$}, por lo que si lo demostráramos rigurosamente, aparecería una matriz de rango $n-2$ y por eso es el denomiador. De esta manera, conseguimos un estimador insesgado.

\end{expla}

Además, $S_R$ se denomina \concept{varianza residual}

\begin{prop}
Una pequeña generalización del lema de Fisher:
\[
\frac{(n-2)S_{R}^2}{σ^2} \equiv \chi_{n-2}^2
\]

Además, es independiente de $\hat{β_1}$

\end{prop}



\begin{proof}
Esta proposición es un caso particular de un teorema que veremos más adelante.
\end{proof}


Ahora que ya tenemos estimada la varianza, podemos calcular:


\[
\frac{\hat{β_1}-β_1}{\frac{S_R}{\sqrt{S_{xx}}}} = \frac{\hat{β_1}-β_1}{\displaystyle\frac{\frac{σ}{\sqrt{S_{xx}}}}{\frac{S_R}{σ}}}
\]

En el numerador tenemos una $N(0,1)$ y en denominador una $\chi^2$ dividida por sus grados de libertad. Esto es por definición de $\mathcal{T}$ \footnote{T de Student} es una $\mathcal{T}$ (T-Student) con $n-2$ grados de libertad.

\begin{prop}
Ahora que conocemos la distribución, podemos calcular el intervalo de confianza  para la pendiente de la recta.

\textcolor{red}{No entiendo nada de esto.}
\[
IC_{1-α}(β_1) \equiv \left[ \hat{β_1} \pm \mathcal{T}_{n-2,\frac{α}{2}}\frac{S_R}{\sqrt{S_{xx}}}\right] \equiv \left[ \gor{Y} \pm \mathcal{T}_{n-1,\frac{α}{2}}\frac{S_R}{\sqrt{n}} \right]
\]
\end{prop}

\subsubsection{Contraste en R}


\begin{lstlisting}
> # Ajusta el modelo
> regresion = lm(Fracaso~Renta)
> summary(regresion)

> lm(formula = Fracaso ~ Renta)
Residuals:	Min	1Q	Median	3Q	Max
		-7.8717 -3.7421	0.5878	3.0368	11.5423
---
Coefficients:	Estimate Std.	Error 	t-value	Pr(>|t|)
(Intercept)	38.4944		3.6445	10.562	7.37e-10 ***
Renta 		-1.3467		0.2659	-5.065	5.14e-05 ***
---
Signif. codes: [...]
Residual standard error: 4.757 on 21 degrees of freedom
Multiple R-Squared: 0.5499,
Adjusted R-squared: 0.528
\end{lstlisting}


Aquí, la fila de intercept es el término independiente y renta es la pendiente. Además, los p-valores son para el contraste $\hat{β_i} = 0$, dentro de la hipótesis $β_i \geq 0$. \footnote{Si queremos contrastar si es positivo, nos vamos al caso límite que lo separa y contrastamos eso}.

En este caso, el p-valor para $\hat{β_1} = 7.37e-10$, con lo que no podemos rechazar la hipótesis.


\subsubsection{Predicciones}

Sea $(x_1,y_1),...,(x_n,y_n) \to y_i = β_0 + β_1x_i + ε_i$.

Dado una nueva observación $x_0$, tenemos 2 problemas para predecir:

\begin{itemize}
	\item \textbf{Inferencia sobre $m_0 \equiv \esp{y_0 | x_0} = β_0 + β_1x_0$}

	En este caso, $$\hat{m_0} = \hat{β_0} + \hat{β_1}x_0$$

	¿Cómo es este estimador?

	\[\esp{\hat{m_0}} = β_0 + β_1x_0 = m_0\]
	\[\var{\hat{m_0}} = ... = σ^2\left[\frac{1}{n} + \frac{(x_0-\bar{x})^2}{S_{xx}} \right] \]

	\subitem Intuitivamente, lo que significa el segundo sumando de la varianza es que ``cuanto más cerca esté $x_0$ de la media, mejor será la estimación''.

	\textbf{Conclusión:}

	\[
		\hat{m_0} \sim N\left( m_0, σ^2\left[\frac{1}{n} + \frac{(x_0-\bar{x})^2}{S_{xx}} \right]\right)
	\]



	\subitem \textbf{Intervalo de confianza} para $m_0$ utilizando la fórmula de intervalos de confianza:

	\[
IC_{1-α}(m_0) \equiv \left[ \hat{m_0} \pm \mathcal{T}_{n-2,\frac{α}{2}}S_R\sqrt{\frac{1}{n} + \frac{(x - \gor{x})^2}{S_{xx}}}\right]
\]

	\item \textbf{Predecir $Y_0$} usamos de nuevo:

	\[
\hat{Y_0} = \hat{β_0} + \hat{β_1}x \to Y_0 - Y \equiv N\left( 0, σ^2\left( 1 + \frac{1}{n}+  \frac{(x-\gor{x})^2}{S_{xx}}\right) \right)
	\]

	Donde la varianza ha sido calculada:

	\[
	\var{Y_0 - \hat{Y_0}} = \underbrace{\var{Y_0}}_{σ^2} - \var{\hat{Y_0}} + \underbrace{2 \cov{Y_0,\hat{Y_0}}}_{ = 0 \text{ (indep.) }} = σ^2 + σ^2\left( \frac{1}{n}+  \frac{(x-\gor{x})^2}{S_{xx}} \right)
	\]


	Este es un problema más complicado, ya que tenemos que tener en cuenta el término de error $ε_i$ y es por esto que aparece el 1 en la varianza. Tenemos que tener en cuenta la incertidumbre.

	Estandarizando y cambiando $σ$ por $S$, tenemos:

	\[
	\frac{Y_0 - \hat{Y_0}}{S_r \sqrt{1 + \frac{1}{n} + \frac{(x-\gor{x})^2}{S_{xx}}}} \equiv \mathcal{T}_{n-2}
	\]

	Ya que tenemos una normal estandarizada dividida por su .... que por definición, es una $\mathcal{T}$ de student.

	Ahora, vamos a construir el \concept{intervalo de predicción} (cambia ligeramente la interpretación)

	\[
1 - α = P\left\{ -\mathcal{T}_{n-2;\frac{α}{2}} < \frac{Y_0 - \hat{Y_0}}{...} < \mathcal{T}_{n-2;\frac{α}{2}}    \right\} = P \left\{ Y_0 \in \left[ \hat{Y_0} \pm \mathcal{T}_{n-2;\frac{α}{2}} S_R \sqrt{1+\frac{1}{n}+...} \right]  \right\}
	\]
\end{itemize}

Ahora vamos a hacer unos ejemplos numéricos.

\begin{example}Seguimos con el ejemplo de la renta.

\begin{center}
\begin{tabular}{c|c|c}
&media&desviación típica\\\hline
\% fracaso & 20.73 & 6.927\\
renta &13.19 $·10^{3}$ & 3.814
\end{tabular}
\end{center}

\setcounter{part}{0}
\ppart IC para $β_1$ de nivel $95\%$.
\ppart IC para \% de fracaso medio si la renta es de $14.000$ euros.

\spart

\[
-1.3467 \pm \mathcal{T}_{21;0.025} · (0.2659)
\]

Donde el $-1.3467$ es el estimador $\gor{m_0}$ que obtenemos de la salida de $R$. Lo mismo el $0.2659$, que es el error típico.

\spart
\[ \gor{Y_0} = 38.49 - (0.3467) · \underbrace{14}_{x_0} = 19.64\%\]

Siendo este el estimador, vamos a construir el intervalo de confianza. \footnote{Podría ser que nos pidieran el intervalo de predicción, pero en ese caso estarían pidiendo el intervalo de ...... para predecir.}

\[
IC = 19.64 \pm (2.06)(4.757)\sqrt{\frac{1}{23}+\frac{(14-13.19)^2}{S_{xx}}}
\]
Donde $S_{xx} = 320.06$ y podemos calcularlo despejando de cualquiera de las fórmulas:

\[
E.T.(\gor{β_1}) = \frac{S_R^2}{S_{xx}}
\]
\[
6. = \frac{S_{xx}}{n-1}
\]

\end{example}


\obs Todos estos cálculos y todas estas fórmulas se basan en muchas hipótesis (como que la distribución del error sigue una distribución normal). Pero podría ser que esto no ocurriera y estuviéramos suponiendo un modelo falso. Para ello, en estadística existe el \concept{Diagnóstico del modelo}. Este diagnóstico, consiste en comprobar si las hipótesis del modelo son \textbf{aceptables} para los datos disponibles. ¡Ojo! Aceptable... Puede haber muchos modelos aceptables para un mismo conjunto de datos.

Este diagnóstico se suele basar en el análisis de los residuos del modelo.

\begin{example}
	Vamos a ver a ojo unos cuantos ejemplos. Vamos a utilizar que $cor{e,\gor{y}} = 0$ bajo el modelo (como calculamos anteriormente)

\begin{center}
\includegraphics[scale=0.6]{img/diagmodelo.png}
\end{center}

De estos 4 gráficos, el bueno es el primero, ya que los demás no complen alguno.
\end{example}

\begin{example}
Vamos a ver otro ejemplo, donde arriba están los datos y abajo los residuos. Mirando sólo la fila de arriba podríamos saber si nuestro modelo para la regresión se cumple o sino.


\begin{center}
\includegraphics[scale=0.6]{img/diagmodelo_2.png}
\end{center}

Vemos que el primero y el último si tienen este modelo como aceptable, ya que en los residuos no hay ningún patrón (y se cumple que la correlación es 0).

En el segundo, podríammos suponer que es bueno, pero al diagnosticar el modelo mirando los residuos, vemos que no. El diagnóstico del model \textbf{magnifica los errores}.

En el cuarto, vemos más claro que es heterocedástico y que no se cumple el modelo supuesto.
\end{example}

En regresión múltiple veremos que no podemos ver los datos, ya que son demasiadas variables, pero sí podemos estudiar los residuos como acabamos de hacer en los ejemplos anteriores.

\subsection{Regresión lineal múltiple}

El ejemplo que vamos a estudiar en regresión múltiple es el consumo de gasolina en EEUU intentando predecirlo a partir de unas cuantas variables. Las variables regresoras son:

\begin{center}
\begin{tabular}{cccccccc}
State&Drivers&FuelC&Income&Miles&MPC&Pop&Tax\\\hline
AL&3559897&2382507&23471&94440&12737.00&3451586&18.0\\
AK&472211&235400&30064&13628&7639.16&457728&8.0\\
AZ&3550367&2428430&25578&55245&9411.55&3907526&18.0
\end{tabular}
\end{center}

\subsubsection{Notación}


\begin{itemize}
	\item $n$ es el número de observaciones, en este caso, el número de estados.
	\item $k$ es el número de atributos.
	\item $ε_i \sim N(0,σ^2)$
	\item $n\geq k+2$: esta hípótesis  es muy necesaria.\footnote{En la estadística, habría que rehacer el modelo para cuando $k>n$. ¿Y cuándo $k>n$? ¿Cuándo puede ocurrir esto? Cada vez más hay más información para cada individuo. En estudios genéticos por ejemplo, que hay millones de genes pero no se pueden hacer el estudio con millones de personas... \textbf{LA MALDICIÓN DE LA DIMENSIONALIDAD} que decimos en Introducción previa a los Fundamentos Básicos del Aprendizaje Automático.\\ Una posible solución al problema es un algoritmo que filtre los atributos que son importantes.}
\end{itemize}

Regresión simple es un caso particular de múltiple, tomando $k=1$.

\subsubsection{Modelo}

El modelo es:

\textcolor{red}{Completar de las traspas}

Podemos agruparlo en forma matricial:

\textcolor{red}{Completar de las traspas}

\paragraph{Recordamos} que en el tema 1 vimos unas cuantas formas cuadráticas útiles para normales multivariantes con matriz de variazas $σ^2I_n$ y media arbitraria.

\textcolor{red}{Completar de las traspas}

¿Cómo estimarías $β$ a partir de $Y$ y $X$?

Podemos hacer la proyección de $Y$ sobre $\mathcal{V}$

Con esto, parece razonable estimar $µ$ mediante la proyección ortogonal de $T$ sobre $\mathcal{V}$ para obtener $\gor{Y} = X\gor{β}$. Equivalentemente: $||Y-X\gor{β}||^2 \leq || Y-Xβ||^2, ∀β\in ℝ^{k+1}$


\textcolor{red}{completar cosas que faltan}

\paragraph{Resumen}
Si
\[ Y \equiv N_n (Xβ,σ^2I_n)\]

entonces, la proyección sobre \_\_\_\_\_\_\_ es:
\[
\hat{Y} = X\hat{β} = HY
\]
donde $H = X(X'X)^{-1}X^{-1}$. Además, \[\hat{β} = (X'X)^{-1}X'Y\]


Esto tiene como consecuencia que el vector de residuos es: $e = Y-\hat{Y} = (I-H) Y$

En cuanto a la interpretación geométrica, los residuos es la recta vertical que une la proyección ($\hat{Y}$) con el vector real ($Y$).

\subsubsection{Distribución de $\hat{β}$}
\[
β \equiv N_{k+1}\left(β,σ^2(X'X)^{-1}\right)
\]

Y la regresión simple, es un caso particular de esta fórmula.

\paragraph{Consecuencias:}

\begin{itemize}
	\item ¿Cuál es la distribución marginal de $\hat{β_j}$ a partir de la que hemos visto de la conjunta? Como vimos en el tema 1, es también una normal, con el correspondiente valor del vector $β$ como media y el elemento $j,j$ de la diagonal.
	\[ β_j \equiv N\left(β_j, σ^2q_{jj}\right)\]

	Ahora, podemos estandarizar:

	\[
	\frac{\hat{β_j}-β_j}{σ\sqrt{q_{jj}}} \equiv N(0,1)
	\]

	Y utilizando que $S_R$ es independiente de $σ$ y la definición de $t-$student tenemos:

	\[
	\frac{\hat{β_j}-β_j}{S_R\sqrt{q_{jj}}} \equiv \mathcal{T}_{n-k-1}
	\]

	¿Cuál es el intervalo de confianza?

	\[
		IC_{n-α}(β_j) \equiv \left[\hat{β_j}\pm \mathcal{T}_{n-k-1}\underbrace{S_R\sqrt{q_{jj}}}_{\text{Error típico de }β_j} \right]
	\]

	Y, como en regresión simple, estudiamos $H_0 : β_j = 0$:
	\[
		R = \left\{ \frac{|β_j|}{S_R\sqrt{q_{jj}}} > \mathcal{T}_{n-k-1;\frac{α}{2}} \right\}
	\]
\end{itemize}

En las traspas encontramos una salida de regresión múltiple de $R$. La columna estimate es el vector $\hat{β}$, el p-valor
