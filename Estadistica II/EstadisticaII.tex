\documentclass[nochap]{apuntes}

\usepackage{hyperref}

\usepackage{tikztools}
\usepackage{fastbuild}
\usepackage{tikz-3dplot}

\usepackage{tikz}
\usepackage{graphicx}
\usepackage{latexsym, amsfonts, amsmath, amssymb, amscd, epsfig,amsthm}
\input xy
\xyoption{all} %%!!
\usetikzlibrary{calc, intersections}
\author{Alberto Parramón}
\date{2014/2015 2º cuatrimestre}

\renewcommand*{\arraystretch}{1.5}
\title{Estadística II}
\precompileTikz

\begin{document}

\pagestyle{plain}
\maketitle

\tableofcontents
\newpage

\chapter{Distribución normal multivariante}

\section{Esperanza, varianza y covarianza de variables aleatorias}
Dada una variable aleatoria definimos:
\begin{itemize}
\item Esperanza: $\mu = \mathbb{E}(X) = \int_{-\infty}^{\infty}x\cdot f_P(x) dx$

Propiedades:
\begin{enumerate}
\item $\mathbb{E}(aX) = a\mathbb{E}(X)$
\item $\mathbb{E}(X+Y) = \mathbb{E}(X)+\mathbb{E}(Y)$
\item $\mathbb{E}(X+c) = \mathbb{E}(X)+c$ (La esperanza de una constante es la propia constante)
\end{enumerate}
\item Varianza: $Var(X) = \mathbb{E}((X-\mathbb{E}(X))^2) =\mathbb{E}((X-\mu)^2) = \mathbb{E}(X^2)-\mu^2$

Propiedades:
\begin{enumerate}
\item $Var(X+b)=Var(X)$
\item $Var(aX)=a^2Var(X)$
\item $Var(X)\geq 0$
\end{enumerate}
\item Covarianza (entre dos variables aleatorias $X_i$, $X_j$): $\sigma_{i,j} = Cov(X_i,X_j) = \mathbb{E}\left((X_i-\mathbb{E}(X_i))(X_j-\mathbb{E}(X_j))\right) = \mathbb{E}(X_i X_j)-\mathbb{E}(X_i)\mathbb{E}(X_j)$

Dos propiedades importantes de la covarianza son:

\begin{enumerate}
\item Cov(X,X)= Var(X)
\item $Cov(X,Y)=Cov(Y,X)$
\end{enumerate}

\end{itemize}

\section{Esperanza, varianza y covarianza de vectores aleatorios}

Un vector aleatorio es un vector de variables aleatorias.

Notación: como durante el curso vamos a trabajar con vectores aleatorios, vamos a generalizar los símbolos que iremos usando:
\begin{itemize}
\item $X = (X_1, X_2,...,X_p)'$ será un vector de p variables aleatorias. Las variables aleatorias serán $X_1, X_2,...,X_p$. La comilla simple $'$ indica que $X$ es un vector columna.
\item $\mu$ será la esperanza del vector aleatorio X: $\mathbb{E}(X)$. Las esperanzas de cada variable aleatoria serán $\mu_1, \mu_2,...,\mu_p$.
\item Si A es una matriz, A' es su traspuesta
\end{itemize}

Por tanto, dado un vector de p variables aleatorias (vector aleatorio p-dimensional), tenemos un resultado parecido.

\begin{itemize}
\item Esperanza. Será un vector columna con las esperanzas de cada variable aleatoria.
\[
\mathbb{E}(X) = \mu = (\mu_1, \mu_2,..., \mu_p)'
\]

Donde cada $\mu_i = \mathbb{E}(X_i)$.

Ejemplo p=3:
\[
\mathbb{E}(X)=
\mathbb{E}\left[
\left(
\begin{array}{c}
X_1\\
X_2\\
X_3
\end{array}
\right)
\right]=
\left(
\begin{array}{c}
\mathbb{E}(X_1)\\
\mathbb{E}(X_2)\\
\mathbb{E}(X_3)
\end{array}
\right)=
\left(
\begin{array}{c}
\mu_1\\
\mu_2\\
\mu_3
\end{array}
\right)=
\mu
\]

Propiedades:
\begin{enumerate}
\item $\mathbb{E}(X+c) = \mathbb{E}(X)+c$. Como en el caso de variables aleatorias.
\item $\mathbb{E}(AX) = A\mathbb{E}(X)$. Donde A es una matriz de dimensión $pxp$ siendo p la dimensión de X.

Lo vemos para p=3:

\[
\mathbb{E}(AX)=
\mathbb{E}\left[
\left(
\begin{array}{ccc}
a_{1,1}& a_{1,2}& a_{1,3}\\
a_{2,1}& a_{2,2}& a_{2,3}\\
a_{3,1}& a_{3,2}& a_{3,3}
\end{array}
\right)
\left(
\begin{array}{c}
X_1\\
X_2\\
X_3
\end{array}
\right) \right]=
\mathbb{E}\left[
\left(
\begin{array}{c}
a_{1,1}X_1 - a_{1,2}X_2 - a_{1,3}X_3\\
a_{2,1}X_1 - a_{2,2}X_2 - a_{2,3}X_3\\
a_{3,1}X_1 - a_{3,2}X_2 - a_{3,3}X_3
\end{array}
\right)
\right]=
\]

\[
=\left(
\begin{array}{c}
a_{1,1}\mathbb{E}(X_1) - a_{1,2}\mathbb{E}(X_2) - a_{1,3}\mathbb{E}(X_3)\\
a_{2,1}\mathbb{E}(X_1) - a_{2,2}\mathbb{E}(X_2) - a_{2,3}\mathbb{E}(X_3)\\
a_{3,1}\mathbb{E}(X_1) - a_{3,2}\mathbb{E}(X_2) - a_{3,3}\mathbb{E}(X_3)
\end{array}
\right)=
\left(
\begin{array}{ccc}
a_{1,1}& a_{1,2}& a_{1,3}\\
a_{2,1}& a_{2,2}& a_{2,3}\\
a_{3,1}& a_{3,2}& a_{3,3}
\end{array}
\right)
\left(
\begin{array}{c}
\mathbb{E}(X_1)\\
\mathbb{E}(X_2)\\
\mathbb{E}(X_3)
\end{array}
\right)=
\]
\[
=A\mathbb{E}(X)
\]

\end{enumerate}

\item Varianza. La varianza va a ser una matriz, donde cada elemento va a ser la covarianza entre dos de las p variables aleatorias que conforman el vector. Será por tanto una matriz simétrica (ya que $\sigma_{i,j}=Cov(X_i,X_j)=Cov(X_j,X_i)=\sigma_{j,i}$). La matriz resultante será la llamada matriz de covarianzas $\Sigma$.
\[
Var(X)=\mathbb{E}\left((X-\mu)(X-\mu)'\right) = \mathbb{E}(XX')-\mu \mu'=\Sigma
\]

\begin{proof}
\[
Var(X)=\mathbb{E}\left((X-\mu)(X-\mu)'\right) = \mathbb{E}(XX'- \mu X' - X \mu'+\mu \mu')=
\]
\[
\mathbb{E}(XX')-\mathbb{E}(\mu X')-\mathbb{E}(X\mu')+\mathbb{E}(\mu \mu')= \mathbb{E}(XX')-\mu \mathbb{E}(X')-\mu' \mathbb{E}(X)+\mu\mu'=
\]
\[
 \mathbb{E}(XX')-\mu \mu'-\mu' \mu+\mu \mu' = \mathbb{E}(XX')-\mu \mu'=\Sigma
\]
\end{proof}

Ejemplo p=3:

\[
Var(X)=
\mathbb{E}\left[
\left(
\begin{array}{c}
X_1-\mu_1\\
X_2-\mu_2\\
X_3-\mu_3
\end{array}
\right)
(X_1-\mu_1, X_2-\mu_2, X_3-\mu_3)\right]=
\left(
\begin{array}{ccc}
\sigma_{1,1}& \sigma_{1,2}& \sigma_{1,3} \\
\sigma_{2,1}& \sigma_{2,2}& \sigma_{2,3} \\
\sigma_{3,1}& \sigma_{3,2}& \sigma_{3,3}
\end{array}
\right)=
\]

\[
=\left(
\begin{array}{ccc}
Var(X_1)& \sigma_{1,2}& \sigma_{1,3} \\
\sigma_{2,1}& Var(X_2)& \sigma_{2,3} \\
\sigma_{3,1}& \sigma_{3,2}& Var(X_3)
\end{array}
\right) = \Sigma
\]

Donde se cumple que $\sigma_{1,2}=\sigma_{2,1}$, $\sigma_{1,3}=\sigma_{3,1}$ y $\sigma_{3,2}=\sigma_{2,3}$. Y por tanto $\Sigma$ es simétrica.
\end{itemize}

Propiedades:
\begin{enumerate}
\item $Var(AX+b) = \mathbb{E}\left[ A(X-\mu)(X-\mu)'A' \right]=A \Sigma A'$.  Donde A es una matriz de dimensión $pxp$ siendo p la dimensión de X.
\begin{proof}
\[
Var(AX+b) = \mathbb{E}\left[ (AX+b-A\mu-b)(AX+b-A\mu-b)' \right] =
\]
\[
 =\mathbb{E}\left[ (AX-A\mu)(AX-A\mu)' \right] = \mathbb{E}\left[A(X-\mu)(X-\mu)'A'\right] = A\mathbb{E}\left[(X-\mu)(X-\mu)'\right]A' =
 \]
 \[
 =A \Sigma A'
\]

\end{proof}
\end{enumerate}

\textcolor{red}{Mirar si tiene importancia lo de $\Sigma$ semidefinida positiva y tal}

\section{Función característica}
La función característica de un vector aleatorio X es:
\[
\phi_X(t)=\mathbb{E}(\exp^{it'X})
\]

Siendo X y t p-dimensionales.

\begin{prop} Mecanismo de Cramer-Wold... \end{prop}

Esta función caracteriza la distribución de X:
\begin{prop} Sean X e Y dos vectores aleatorios:
\[
\phi_X(t)=\phi_Y(t) \Leftrightarrow X \stackrel{d}{=} Y
\]

\textcolor{red}{Completar este apartado consultando a Elena}
\end{prop}
\section{Matriz de covarianzas}
Como ya dijimos anteriormente la matriz de covarianzas $\Sigma$ define la varianza de un vector aleatorio y es simétrica. Por tanto podemos expresar $\Sigma$ de la siguiente forma:
\[
\Sigma = CDC^{-1}
\]

Siendo D una matriz diagonal.

\textcolor{red}{$C^{-1}=C'$ ya que las columnas de C son vectores ortogonales. OJO CUIDAO, que tienen que ser ortonormales...
Una matriz real A es ortogonal si y sólo si sus vectores filas o vectores columna son cada uno un conjunto ortonormal de vectores.}
Por tanto:
\[
\Sigma = CDC'  \text{ y } \Sigma^{-1} = CD^{-1}C'
\]

\textcolor{blue}{Caso particular:
\[
p=2 \text{ , }
\mu=\left(
\begin{array}{c}
0\\
0
\end{array}
\right)
\text{ , }
\left(
\begin{array}{cc}
\lambda_1& 0 \\
0 & \lambda_2
\end{array}
\right)
\]
Tenemos:
\[
(X_1, X_2)
\left(
\begin{array}{cc}
\lambda_1& 0 \\
0 & \lambda_2
\end{array}
\right)
\left(
\begin{array}{c}
X_1\\
X_2
\end{array}
\right) = cte
\Rightarrow
\frac{X_1^2}{\lambda_1}+\frac{X_2^2}{\lambda_2}=cte
\]
}


\section{Estandarización multivariante}
\begin{defn}
Sea un vector aleatorio X, es normal p-dimensional con vector de medias $\mu$ y matriz de covarianzas $\Sigma$ (notación: $X\equiv N_p(\mu, \Sigma)$) si tiene densidad dada por:

\[
f(x)=\abs{\Sigma}^{-1/2}(2\pi)^{-p/2} exp \left( -\frac{1}{2}(x-\mu)' \right)
\]
\end{defn}

\begin{prop} Si $X \equiv N_p(\mu, \Sigma)$ y definimos $Y = \Sigma^{-1/2}(X-\mu)$, entonces $Y_1,...,Y_p$ son i.i.d. N(0,1).\end{prop}

\begin{proof}
Sabemos por definición que:
\[
f_X(x)=\abs{\Sigma}^{-1/2}(2\pi)^{-p/2} exp \left( -\frac{1}{2}(x-\mu)' \right)
\]

Vamos a aplicar un cambio de variable en la fórmula de la densidad:

Despejando de $Y = h(X)= \Sigma^{-1/2}(X-\mu)$, obtenemos que $\Sigma^{1/2}Y+\mu=h^{-1}(Y)=X$.

Y ahora cogemos el Jacobiano de $h^{-1}(Y)=X$ que será $\Sigma^{1/2}$ ($\mu$ es una constante e Y es la variable).

También hay que considerar la exponencial de la fórmula de la densidad, ahi hacemos el cambió de variable de:

$$e^X \text{por} e^{h^{-1}(Y)}=e^{\Sigma^{1/2}Y+\mu}$$

Y el Jacobiano sería $e^{\Sigma^{1/2}Y}$:


Por tanto nos quedaría:
\[
f(X) = f(h^{-1}(Y))*\abs{Jh(x)} = \abs{\Sigma}^{-1/2}(2 \pi)^{-p/2} \exp\left(-\frac{1}{2}(\Sigma^{-1/2}Y+\mu-\mu)'  \right) \exp\left( \Sigma^{1/2}Y \right) \Sigma^{1/2}  =
\]
\[
= \abs{\Sigma}^{-1/2}(2 \pi)^{-p/2} \exp\left(-\frac{1}{2}(\Sigma^{-1/2}Y)' \right) \exp\left( \Sigma^{1/2}Y \right) \abs{\Sigma}^{1/2} =
\]
\[
\abs{\Sigma}^{-1/2}(2 \pi)^{-p/2} \exp\left(-\frac{1}{2}(Y'\Sigma^{-1/2}\Sigma^{1/2}Y \right) \abs{\Sigma^{1/2}} = (2 \pi)^{-p/2} \exp\left(-\frac{1}{2}(Y'Y) \right)
\]
\end{proof}


\section{Ejercicio 1}
Definimos el siguiente vector aleatorio: $X = (X_1,X_2,X_3)' \equiv N_3(\mu, \Sigma)$ con:

\[
\mu=
\left(
\begin{array}{c}
0\\
0\\
0
\end{array}
\right) \text{,       }
\Sigma=
\left(
\begin{array}{ccc}
7/2& 1/2& -1 \\
1/2& 1/2& 0 \\
-1& 0& 1/2
\end{array}
\right)
\]

\ppart Calcula las distribuciones marginales $X_i \equiv N(\mathbb{E}(X_i), Var(X_i))$:

$X_1\equiv N(0, 7/2)$

$X_2\equiv N(0, 1/2)$

$X_3\equiv N(0, 1/2)$

Para calcular estos valores solo hace falta mirar los datos que nos da el problema, el vector de medias $\mu$ y la matriz de covarianzas $\Sigma$:

\[
\Sigma=\left(
\begin{array}{ccc}
Var(X_1)& \sigma_{1,2}& \sigma_{1,3} \\
\sigma_{2,1}& Var(X_2)& \sigma_{2,3} \\
\sigma_{3,1}& \sigma_{3,2}& Var(X_3)
\end{array}
\right)
\]

\[
\mu=
\left(
\begin{array}{c}
\mathbb{E}(X_1)\\
\mathbb{E}(X_2)\\
\mathbb{E}(X_3)
\end{array}
\right)=
\left(
\begin{array}{c}
\mu_1\\
\mu_2\\
\mu_3
\end{array}
\right)
\]

\ppart Calcula la distribución del vector $(X_1,X_2)'$:

Este vector sigue una distribución normal que puede obtener de las matriz $\Sigma$ y el vector de medias $\mu$:
\[
\left(
\begin{array}{c}
X_1\\
X_2
\end{array}
\right)
\equiv N_2\left[
\left(
\begin{array}{c}
0\\
0
\end{array}
\right)
\text{, }
\left(
\begin{array}{cc}
7/2& 1/2 \\
1/2 & 1/2
\end{array}
\right)
\right]
\]

\ppart ¿Son $X_2$ y $X_3$ independientes?

Sí son independientes ya que la covarianza entre ambas variables es 0. La covarianza entre $X_2$ y $X_3$ es el elemento de la fila 3 y la columna 2 de la matriz de covarianzas $\Sigma$, (que al ser $\Sigma$ simétrica coincide con el elemento de la fila 2 y la columna 3).

\ppart ¿Es $X_3$ independiente del vector $(X_1, X_2)'$?
???

\ppart Calcula la  distribución de la variable aleatoria $(2X_1-X_2+3X_3)$.

Procedemos de la siguiente manera:

\[
(2X_1-X_2+3X_3)=(2,-1,3)\left(
\begin{array}{c}
X_1\\
X_2\\
X_3
\end{array}
\right)\equiv
N\left( 0,  \right)
\]



\section{Distribuciones condicionadas}

\begin{prop}

Sea $X=(X_1|X_2)$ con $X_1∈ℝ^p$ y $X_2∈ℝ^{p-q}$. Consideramos las particiones correspondientes de $µ$ y de $\Sigma$.

\end{prop}

\begin{proof}
Definimos $X_{2.1} = X_2 - Σ_{21}Σ_{11}^{-1}X_1$.

\[
\begin{pmatrix}
X_1\\
X_{2.1}
\end{pmatrix} =
\begin{pmatrix}
I &| &0\\
\hline
- Σ_{21}Σ_{11}^{-1}  &| &I
 \end{pmatrix}
\]

Como es una combinación lineal de $(X_1,X_{2.1})'$, entonces $X_{2.1}$ es normal multivariante.

Vamos a calcular la media y la matriz de covarianzas de $X_{2.1}$

$X_{2-1} = N\left( µ_2-Σ_{21}Σ_{11}^{-1}µ_1 , \begin{pmatrix} Σ_{11} &|&0\\\hline 0&|&Σ_{2.1} \end{pmatrix} \right)$

Donde las covarianzas se calculan: $AΣA'$, siendo $A$ la matriz de la combinación lineal, es decir:

\[
A=\begin{pmatrix}
I &| &0\\
\hline
- Σ_{21}Σ_{11}^{-1}  &| &I
 \end{pmatrix}
\]



\paragraph{Conclusiones:}

\begin{itemize}
	\item $X_1$ es independiende de $X_{2.1}$
	\item $X_{2.1}$ es normal, con media y varianza calculadas anteriormente.
	\subitem $X_{2.1}|X_1$, al ser independientes, también se distribuye normalmente, con los mismos parámetros.
	\item Dado $X_1$, los vectores $X_{2.1}$ y $X_2$  difieren en el vector constante $Σ_{21}Σ_{11}^{-1}X_1 \implies X_2|X_1 = N\left( µ_{2.1}, Σ_{2.1} \right)$
\end{itemize}

\end{proof}

\begin{example}
Vamos a considerar $X_1, X_2$ como escalares, para entender la proposición. Este ejemplo le surgió a un investigador que quería predecir la estatura de los hijos en función de la de los padres (que no padres y madres, sólo padres).


\[
\begin{pmatrix}
X\\Y
\end{pmatrix} \equiv N_2\left( \begin{pmatrix} µ_x \\ µ_y \end{pmatrix}, \begin{pmatrix}
σ_x^2&σ_{xy}\\σ_{xy}&σ_y^2
\end{pmatrix} \right)
\]
\label{form::EspVarCondicionada}
Definimos $\gor{Y} = E(Y|X) = µ_y + \frac{σ_{xy}}{σ_x^2}(x-µ_x)$. La esperanza de la altura del hijo condicionada a la altura del padre será la media de las alturas de los hijos corregida por un factor en el que influye la diferencia de altura del padre con respecto a su media. Es de esperar que si Yao Ming tiene un hijo, sea más alto que la media.

El factor de corrección $\frac{σ_{xy}}{σ_x^2}$ es importante y no me he enerado bien de dónde sale.

Ahora vamos a calcular $V(Y|X) = σ_{y}^2 - \frac{σ_{xy}^2}{σ_x^2} = σ_y^2 \left( 1- \rho^2\right)$ donde $\rho = \frac{σ_{xy}^2}{σ_x^2σ_y^2}$, el coeficiente de correlación.

Ha dicho algo así como \textbf{La única relación que puede existir entre 2 variables normales es una relación lineal.}


Este coeficiente de correlación aparece también en la expresión de la esperanza. Vamos a verlo:

 \[\gor{Y} = µ_y + \frac{σ_{xy}}{σ_x^2}(x-µ_x) \dimplies \frac{\gor{Y}-µ_y}{σ_y} = \frac{σ_{xy}}{σ_xσ_y}\frac{x-µ_x}{σ_x}\]

 Es decir:

 \[
\frac{\gor{Y}-µ_y}{σ_y} = \rho \frac{x-µ_x}{σ_x}
 \]

Aplicado a la estatura de los hijos respecto de los padres, se interpreta como: ``Si un padre es muy alto, su hijo será alto pero no destacará tanto como el padre''. Este fenómeno lo definió como \concept{Regresión a la mediocridad}.

\end{example}

\begin{defn}[Homocedástico]
$Σ_{2.1}$ no depende de $X_1$.

Esto se da cuando $\begin{pmatrix}X_1,X_2\end{pmatrix}$ es normal multivariante. Si no fueran normal multivariante, serían heterocedásticas. \footnote{Un ejemplo sería $X_1$ la renta de una familia y $X_2$ los ahorros de la misma. Los datos no se distribuyen conjuntamente normal, con lo que la $Σ_{2.1}$ si depende de $X_1$. Ya veremos más adelante este concepto con mayor detalle.}
\end{defn}


\begin{example}

Ahora vamos a ver un par de ejemplos numéricos:

Sea \[\begin{pmatrix}X,Y\end{pmatrix} \equiv N_2 \left( \begin{pmatrix}0,0\end{pmatrix}, \begin{pmatrix}10&3\\3&1\end{pmatrix} \right)\]

\paragraph{Distribución $Y|X$:}

\[E(Y|X) = \frac{3}{10}x\]
\[V(Y|X) = \frac{1}{10}\]

\paragraph{Distribución $X|Y$:}

\[E(X|Y) = 3y\]
\[V(X|Y) = 1\]

Ambas son normales unidimensionales ya que $(X Y)$ es normal multivariante.

Sea \[\begin{pmatrix}X,Y\end{pmatrix} \equiv N_2 ...\]

Sea $Z_1 = X+Y$ y $Z_2 = X-Y$.

\[
\begin{pmatrix}Z_1//Z_2\end{pmatrix} = \begin{pmatrix}1&1\\1&-1\end{pmatrix}\begin{pmatrix}X\\Y\end{pmatrix} \implies \begin{pmatrix}Z_1\\Z_2\end{pmatrix} = N_2\left(\begin{pmatrix}2\\0\end{pmatrix},\begin{pmatrix}7&1\\1&3\end{pmatrix}\right)
\]

Ahora vamos a calcular lo que nos piden: $E(Z_1|Z_2=1)$.

\[E(Z_1|Z_2=1) = 2 + \frac{1}{3}(1-0) = \frac{7}{3}\]

Es importante destacar que la distribución no depende del valor concreto por ser homocedásticas .

\end{example}

\chapter{Contrastes no paramétricos}
Hipótesis no paramétrica: hipótesis que no se formula en términos de un número finito de parámetros.

\begin{enumerate}
\item Bondad de ajuste: A partir de una muestra $X_1,...,X_n \stackrel{iid}{\sim} F$ de observaciones \textcolor{red}{(son muestras o variables aleatorias o es simple notación?)} ($\stackrel{iid}{\sim}$ significa que son muestras aleatorias independientes idénticamente distribuidas que siguen una distribución F en este caso), contrastar:
\begin{itemize}
\item $H_0: F=F_0$ donde $F_0$ es una distribución prefijada.
\item $H_0: F \in \{F_{\theta} : \theta\in H\}$ H es el espacio paramétrico.
\end{itemize}
\item Homogeneidad: Dados $X_1,...,X_n \stackrel{iid}{\sim} F$ y $Y_1,...,Y_n \equiv G$ de observaciones. Contrastar $H_0: F=G$.
\item Hipótesis de independencia: Dada $(X_1,Y_1),...,(X_n,Y_n) \stackrel{iid}{\sim} F$ de observaciones. Contrastar $H_0: X$ e $Y$ son independientes.
\end{enumerate}

\section{Contraste $\chi^2$ de bondad de ajuste}
Consideramos una distribución totalmente especificada bajo $F_0$. Y consideramos una muestra empírica $X_1,...,X_n \stackrel{iid}{\sim} F$.

$H_0: F=F_0$ es la hipótesis nula y queremos ver que F, que es la distribución obtenida con los datos verdaderos (las muestras $X_i$ obtenidas empíricamente) es igual a $F_0$ que es la distribución teórica.

\textbf{Notación: }$P_A(B)$ es la probabilidad de B condicionada a A.

Vamos a definir los pasos que tenemos que seguir para comprobar si $H_0$ es cierta:
\begin{enumerate}
\item Se definen k clases $A_1,...,A_k$. 

\item Se cuentan cuántos datos caen en cada clase (frecuencias observadas). Cada clase la llamaremos $O_i=\#\{j:X_j\in A_i\}$.

\item Se calculan las frecuencias esperadas para cada clase si $H_0$ fuese cierta. A este dato lo llamaremos $\mathbb{E}_i$ o $\mathbb{E}_{H_0}(O_i)$: 
\[
\mathbb{E}_{H_0}(O_i) = np_i
\]

\obs Las $O_i$ son variables aleatorias que se distribuyen como una binomial $B(n, p_i=P_{H_0}(A_i))$. Siendo $n$ el número de intentos y $p_i$ la probabilidad de que una muestra pertenezca a la clase $A_i$ bajo la hipótesis nula. 

La notación puede resultar liosa, a grades rasgos:
\begin{itemize}
\item $O_i$ tendrá un valor que será la frecuencia observada de una clase i. Es decir, el número de observaciones que caen en una la clase i.
\item $\mathbb{E}_{H_0}(O_i)$ será el valor esperado de $O_i$ considerando la hipótesis nula como cierta. La esperanza de una $B(n,p)$ es igual a $np$.
\end{itemize}

\item Se comparan las frecuencias observadas y esperadas mediante el estadístico de Pearson:
\[
T = \sum_{i=1}^n \frac{(O_i-E_i)^2}{E_i}
\]

Se divide entre $E_i$ para darle más importancia a la diferencia si el valor es pequeño, Por ejemplo, si E=100 y O=101, no es lo mismo que si E=1 y O=2. Sin embargo, si no dividiéramos por $E_i$ nos daría el mismo resultado.

\item Se rechaza $H_0$ en la región crítica $R=\{T > c\}$ donde c es tal que $\alpha=P_{H_0}(T>c)$. Es decir, $\alpha$ (también llamado 'nivel de significación') es la probabilidad de rechazar la hipótesis nula siendo esta cierta. O dicho de otra forma, la probabilidad de entrar en la región de rechazo $'T>c'$ considerando que $H_0$ es cierta.
\end{enumerate}


Ahora vamos a ver qué podemos decir del estadístico de Pearson 'T':
\[
O_i=B(n,p_i) \simeq N(np_i, np_i(1-p_i))
\]

Imaginémonos por un momento que podríamos despreciar el término $np_i^2$ de la varianza de la normal. Nos quedaría:
\[
\simeq N(np_i, np_i(1-p_i)) \simeq N(np_i, np_i) \simeq N(E_i, E_i)
\]

Que por el Teorema Central del Límite (\url{https://es.wikipedia.org/wiki/Teorema_del_l%C3%ADmite_central}) nos queda:
\[
\frac{O_i-E_i}{\sqrt{E_i}} \simeq N(0,1)
\]

Y como deberíamos saber, una distribución $\chi^2_k$  no es más que una distribución de probabilidad continua con un parámetro k que representa los grados de libertad de la variable aleatoria $X = Z_1^2 + \cdots + Z_k^2$, donde $Z_i$ son variables aleatorias normales independientes de media cero y varianza uno. Por tanto:

\[
\frac{(O_i-E_i)^2}{E_i} \simeq \chi^2_1
\]

Por tanto, como tenemos k clases, podríamos tener $T\simeq \chi^2_k$. pero por otro lado sabemos que $O_1,O_2,...,O_k=n$, esta restricción hace que no haya una independencia entre todos los sumandos $O_i$, por tanto nos queda: $T\simeq \chi^2_{k-1}$.

Finalmente nos queda que la región de rechazo, dado un nivel de significación $\alpha$, se alcanza cuando el estadístico de Pearson T, obtenido a partir de los datos muestrales, vale más que $\chi^2_{k-1}, \alpha$. $\chi^2_{k-1}, \alpha$ es, dada la función de densidad de una $\chi^2_{k-1}$, el valor del eje de abscisas que hace que se quede un $\alpha*100\%$ del área encerrada bajo la curva a la derecha de ese valor:

%DIBUJOOOO

\obs
\begin{enumerate}
\item  Tal y como lo hemos definido tenemos que $\sum_{i=1}^k O_i = n$ y que $\sum_{i=1}^k E_i = \sum_{i=1}^k np_i= n(p_1+p_2+...+p_k) = n$, por tanto tenemos:
\[
T=\sum_{i=1}^k \frac{(O_i-E_i)^2}{E_i} = \sum_{i=1}^k \frac{O_i^2}{E_i}-n
\]

\item Por definición de $\chi^2_{k-1}$, su esperanza es:
\[
\mathbb{E}_{H_0}(T) \simeq k-1
\]

\item Por definición de $\chi^2_{k-1}$, su varianza es:
\[
Var_{H_0}(T) \simeq 2(k-1)
\]

\begin{defn}[p-valor]
valor de $\alpha$ mínimo con el que se empieza a rechazar la hipótesis nula.
\end{defn}

\end{enumerate}

\begin{example}
Tiramos un dado 100 veces y obtenemos:

\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Resultados & 1 & 2 & 3 & 4 & 5 & 6 \\
\hline
Frecuencia & 10 & 20 & 20 & 10 & 15 & 25\\
\hline
\end{tabular}

Y consideramos $H_0: p_i=1/6 \text{ } \forall i=1,...,6$. Es decir que el dado no está trucado y cada cara tiene la misma probabilidad ($p_i$) de salir.

Por otro lado consideramos $H_1: \exists i$ tal que $p_i\neq 1/6$. Es decir, que el dado está trucado y hay caras que salen mas que otras.

Seguimos los pasos:
\begin{enumerate}
\item En este caso cada clase será la cara del dado que sale, habrá por tanto 6 clases: k=6.
\item Se cuentan cuantos datos caen en cada clase: $O_1=10$, $O_2=20$, $O_3=20$, $O_4=10$, $O_5=15$, $O_6=25$
\item Se calculan las frecuencias esperadas si $H_0$ fuese cierta. En este caso $n=10+20+20+10+15+25=100$ y $p_i=1/6 \text{ } \forall i=1,...,6$. Nos queda: $\mathbb{E}_i=np_i = 100 \frac{1}{6}=100/6$.
\item Se obtiene el estadístico de Pearson:
\[
T=\sum_{i=1}^n \frac{O_i^2}{E_i}-n = \frac{6}{100}(10^2+20^2+20^2+10^2+15^2+25^2)-100=11
\]
\item Rechazamos $H_0$ si $T>c$. En este caso, consideramos un nivel de significación $\alpha = 0.05$. Sabemos que $\alpha = P_{H_0}(T>c)$ Como tenemos 6 clases, el estadístico de Pearson tendrá una distribución $\chi^2_5$. Buscamos en la tabla (mirar apéndice) y obtenemos que $\chi^2_5, 0.05 = 11.07$. Este será nuestro valor de c.

Puesto que nuestra región de rechazo es $R=(T>c)$, y tenemos que $11>11.07$, no podemos rechazar la hipótesis nula, y por tanto, no podemos concluir que el dado esta trucado. El p-valor en este caso será mayor que $0.05$.

Si consideramos un valor de significación $\alpha = 0.06$ si hubiéramos rechazado la hipótesis nula (aunque con un 6\% de opciones de equivocarnos) y hubiéramos concluido que el dado esta trucado. 

Lo vemos en el siguiente dibujo:

\includegraphics[scale=0.75]{img/ejemploEstadistica.png}

La raya vertical se sitúa en $x=11.07$, que es el valor que sale si se mira la tabla de la distribución $\chi^2$ con 5 grados de libertad (ver apéndice) y nivel de significación, por tanto, la zona sombreada es la región de rechazo de la hipótesis nula. Como ha salido T=11, no entramos dentro de esa región de rechazo (por poco) y no podemos rechazar la hipótesis nula.

\end{enumerate}
\end{example}

\begin{theorem}
Bajo $H_0$:
\[
\sum_{i=1}^k \frac{(O_i-E_i)^2}{E_i} \stackrel{d}{\rightarrow} \chi^2_{k-1} \text{ , si } n \rightarrow \infty
\]
\end{theorem}
\begin{proof}
\textcolor{blue}{Esta demostración es un poco liosa, si no la entendéis, a otra cosa (great pareado). De hecho no tengo ni idea de lo que escribo a partir de la mitad.}

Definimos los vectores aleatorios $\xi_1,...,\xi_n$ de la siguiente forma: $\xi_i=(0,...,1,...,0)' \in \mathbb{R}^k \Leftrightarrow \xi \in A_j$. Es decir cada $\xi_i$ va a ser un vector de 0's, salvo porque van a tener un 1 en una posición j. Esta posición j les identificará con la clase $A_j$. Tenemos que:
\[
\xi_1+...+\xi_n=(O_1,...,O_k)'
\]
Es decir, que su suma nos da un vector con las frecuencias de aparición de cada clase (Recordemos que 'k' es el número de clases). Por ejemplo, en el ejemplo del dado tendríamos que $\xi_1+...+\xi_6=(10,20,20,10,15,25)=(O_1,...,O_6)$

\textbf{Notación: } $p\equiv (p_1,...,p_k)'$. $np=(E_1,...,E_k)'$, entonces:
\[
(O_1-E_1,...,O_k-E_k)'=\textcolor{red}{\sum_{i=1}^n(\xi_i)}-np=n(\overline{\xi}-p)
\]

Definimos la matriz $\mathbb{P}$, que tiene rango k, se define con las probabilidades $p_i$ en la diagonal y 0 el resto de elementos. Para p=3 quedaría:
\[
\mathbb{P}=\left(
\begin{array}{ccc}
p_1& 0 & 0 \\
0 & p_2& 0 \\
0 & 0 & p_3
\end{array}
\right)
\]

Y cogiendo la raíz del estadístico de Pearson y sabiendo que $\sqrt{E_i}=\sqrt{np_i}$ nos queda:
\[
\left(\frac{O_1-E_1}{\sqrt{(E_1)}},...,\frac{O_k-E_k}{\sqrt{(E_k)}}  \right) = \mathbb{P}^{-1/2}\sqrt{n} (\overline{\xi}-\mathbb{P})
\]

Por otro lado: 
\[
v=(v_1,...,v_k) \rightarrow v'(\frac{v_1}{\lambda_1},...,\frac{v_k}{\lambda_k})
\]

Así, tomamos $\xi_1,...,\xi_n$ independientes y distribuidas como un vector $\xi$ tal que:
\[
\mathbb{E}(\xi)=p
\]
\[
V(\xi)=\stackrel{\mathbb{P}}{\mathbb{E}(\xi \xi')} -pp' = p-pp' \equiv \Sigma 
\]

$\mathbb{E}(\xi\xi')=\mathbb{P}$ ya que tenemos que $\xi_r \xi_l = 0 si r \neq l$ y $\xi_r \xi_l = \xi_r^2=\xi_r si r = l$ 

Por otra parte: 
\[
T=\sum_{i=1}^k \frac{(O_i-E_i)^2}{E_i} = \norm{p^{-1/2}\sqrt{n}(\xi-p)}^2
\]

Por el TCL:
\[
\sqrt{n}(\overline{\xi}-p) \stackrel{d}{\rightarrow} N_k(0, \Sigma) \Rightarrow \mathbb{P}^{-1/2}N_k(0, \Sigma) \stackrel{d}{\rightarrow} N_k(0, \mathbb{P}^{-1/2} \Sigma  \mathbb{P}^{-1/2}) 
\]
\[
\Rightarrow \norm{P^{-1/2} \sqrt{n} (\xi-p)}^2 \stackrel{d}{\rightarrow} \norm{Y}^2 \text{ con } Y \equiv N(0,\mathbb{P}^{-1/2} \Sigma  \mathbb{P}^{-1/2})
\]

Vamos a ver que $\mathbb{P}^{-1/2} \Sigma  \mathbb{P}^{-1/2})$ es simétrica idempotente:
\[
\mathbb{P}^{-1/2} (p-pp')\mathbb{P}^{-1/2}) = I-2\sqrt{p}\sqrt{p'}-\sqrt{p}\sqrt{p'}\sqrt{p}\sqrt{p'}=I-\sqrt{p}\sqrt{p'}
\]

hemos usado que $p_1+p_2+...+p_k=1$

Además sabíamos que:
\[
\norm{Y}^2 \equiv \chi^2_{k-1}
\]
Los grados de libertad vienen de la traza de $\Sigma$, y de que $traza(I)=k$ y $traza(\sqrt{p}\sqrt{p'})=1$: 
\[
traza(\Sigma)=traza(I-\sqrt{p}\sqrt{p'})=k-1
\]
\end{proof}

\section{Contraste de bondad de ajuste $\chi^2$ para hipótesis nula compuesta}

Problema: $X_1,...,X_n \stackrel{iid}{\sim} F$. Suponemos como hipótesis nula:
\[
H_0 : F \in \{F_{\theta}: \theta \in H \subset \mathbb{R}^r\}
\]

\textcolor{red}{La diferencia es que ahora la hipótesis nula que consideramos es que los datos van a seguir una distribución teórica $F_0$ que no está totalmente especificada, ya que va a depender de un parámetro. Por ello, decimos con palabras que:}

\textcolor{red}{La hipótesis nula es que los datos muestrales van a tener una función de distribución $F$, que va a ser igual a $F_{\theta}$, siendo $\theta$ el parámetro del que dependerá, el cual pertenece a un espacio paramétrico $H$}

Pasos:
\begin{enumerate}
\item Se definen k clases $A_1,...,A_k$. 

\item Se cuentan cuántos datos caen en cada clase (frecuencias observadas). Cada clase la llamaremos $O_i=\#\{j:X_j\in A_i\}$. Hasta aquí todo igual que antes.

\item Para estimar/calcular las frecuencias esperadas se sigue un método ligeramente diferente:

Se estima $\theta$ por el método de máximo verosimilutd. Sea $\hat{\theta}$ el EMV.

\textcolor{red}{explicar bien esto}

\item Se calculan las frecuencias esperadas estimadas bajo $H_0$: $\hat{E}_i=n\hat{p}_i$ con $i=1,...,k$ donde $\hat{p}_i = p_{\hat{\theta}}(A_i)$.

\item Calculamos el estadístico $\chi^2$ de Pearson:
\[
T=\sum_{i=1}^k \frac{(O_i-\hat{E}_i)^2}{\hat{E}_i}
\]

Ahora puedo elegir de todas las posibles distribuciones, aquella que más se parece. De modo que cabe esperar que T tienda a tomar valores menores que en el caso simple.

Además, al estimar r (\textcolor{red}{¿De dónde sale r? Es la dimensión del parámetro estimado??}) parámetros se introducen r nuevas restricciones sobre el vector $O_1,O_2,...,O_r$.

Se puede probar bajo condiciones de regularidad:
\[
\sum_{i=1}^k \frac{(O_i-\hat{E}_i)^2}{\hat{E}_i} \stackrel{d}{\rightarrow} \chi^2_{k-1-r} \text{ bajo } H_0 \text{ si } n \rightarrow \infty
\]

\item Se rechaza $H_0$ en la región crítica: $R=\{T>\chi^2_{k-1-r;\alpha}  \}$

Tal y como se ha hecho en el caso anterior.

\end{enumerate}

\begin{example}
Los bombardeos de Londres. El problema trata de estudiar los bombardeos que sufrío Londres entre 1944 y 1945. Se quería saber si los impactos sobre la ciudad de Londres eran en lugares aleatorios o estaban dirigidos a lugares concretos.

La fórmula de Poisson se ajusta bastante a un modelo de distribución aleatoria de impactos. Por tanto, tendríamos que estimar el parámetro $\lambda$ de la distribución de Poisson, que tiene por función de densidad:

$$ f(k,\lambda)=\frac{e^{-\lambda}\lambda^k}{k!} $$

Donde:
\begin{itemize}
\item k es el número de ocurrencias del evento o fenómeno (la función nos da la probabilidad de que el evento suceda precisamente k veces).

\item λ es un parámetro positivo que representa el número de veces que se espera que ocurra el fenómeno durante un intervalo dado. Por ejemplo, si el suceso estudiado tiene lugar en promedio 4 veces por minuto y estamos interesados en la probabilidad de que ocurra k veces dentro de un intervalo de 10 minutos, usaremos un modelo de distribución de Poisson con λ = 10×4 = 40.
\end{itemize} 
Dicho esto, vamos a seguir los pasos anteriormente detallados:



\begin{enumerate}
\item Se definen k clases $A_1,...,A_k$. En nuestro caso, las clases van a ser el número de impactos que ha habido en un cuadrado. Por tanto los cuadrados que pertenezcan a $A_1$ serán aquellos que han sufrido un único impacto.

\item Se cuentan cuántos datos caen en cada clase (frecuencias observadas). Cada clase la llamaremos $O_i=\#\{j:X_j\in A_i\}$. En nuestro caso tenemos: $O_0=229$, $O_1=211$, $O_2=93$, $O_3=35$, $O_4=8$ ($O_4$ es 4 o más impactos).


\item Para estimar/calcular las frecuencias esperadas se estima $\theta$ por el método de máximo verosimilutd. Sea $\hat{\theta}$ el EMV. En este caso, nuestro $\theta$ sera $\lambda$ y nuestro $\hat{\theta}$ será $\hat{\lambda}$, que será el parámetro de la distribución de Poisson:

$$ \hat{\lambda} = \frac{0\cdot229 + 1\cdot211+2\cdot93+3\cdot35+4\cdot7+5\cdot1}{576}=0.9323$$

\textcolor{red}{explicar por qué esto es el EMV, ya que en estadistica 1 hacíamos u lio increible pa sacarlo}

\item Se calculan las frecuencias esperadas $\hat{E}_i=n\hat{p}_i$ con $i=1,...,k$ donde $\hat{p}_i = p_{\hat{\theta}}(A_i)$. En nuestro caso:

$$\hat{E}_k = n\hat{p}_k = 576\cdot e^{-\hat{\lambda}\frac{\hat{\lambda}^k}{k!}}$$

Sustituimos $\lambda = 0.9323$ y $k=0,...,5$ y nos queda: $\hat{E}_0 = 226.74 $, $\hat{E}_1 = 211.34 $, $\hat{E}_2 = 98.54$, $\hat{E}_3 = 30.62$, $\hat{E}_4 = 8.71$.

\item Calculamos el estadístico $\chi^2$ de Pearson:
\[
T=\sum_{i=1}^k \frac{(O_i-\hat{E}_i)^2}{\hat{E}_i} = 1.0176
\]

Bajo $H_0$ tenemos que $T \equiv \chi^2_3$. El 3 sale de k=5 clases menos 1 parámetro estimado menos 1 como hacíamos antes.

\item Se rechaza $H_0$ en la región crítica: $R=\{T>\chi^2_{k-1-r;\alpha}  \}$

En nuestro caso, tomando $\alpha = 0.05$, tenemos: $$R=\{T>\chi^2_{3;\alpha} \} \rightarrow \{1.0176>7.815 \} \rightarrow \text{No se puede rechazar } H_0$$

Podemos calcular el p-valor mirando:
$$P\{\chi^2_3 > 1.0176\} = 0.797$$

Efectivamente, si miramos la tabla de la $\chi^2_3$, con $\alpha =0.797$, T valdría aproximadamente 1.
\end{enumerate}

\end{example}






\appendix
\chapter{Ejercicios}
\input{tex/EstII_ejercicios.tex}

\chapter{Distribuciones, tablas}
\includepdf[pages={1}]{pdf/_chicuadrado.pdf}


\chapter{Prácticas}
Se incluyen las soluciones de las prácticas:
\includepdf[pages={1-6}]{pdf/_p1E2.pdf}



\end{document}
