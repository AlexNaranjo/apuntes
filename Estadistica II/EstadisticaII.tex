\documentclass[nochap]{apuntes}

\usepackage{hyperref}

\usepackage{tikztools}
\usepackage{fastbuild}
\usepackage{tikz-3dplot}

\usepackage{tikz}
\usepackage{graphicx}
\usepackage{latexsym, amsfonts, amsmath, amssymb, amscd, epsfig,amsthm}
\input xy
\xyoption{all} %%!!
\usetikzlibrary{calc, intersections}
\author{Alberto Parramón}
\date{2014/2015 2º cuatrimestre}

\renewcommand*{\arraystretch}{1.5}
\title{Estadística II}
\precompileTikz

\begin{document}

\pagestyle{plain}
\maketitle

\tableofcontents
\newpage

\section{Introducción}
Se presentan apuntes de Estadística II, tomados de la clase dada por José Berrendero.

El profesor nos facilita unas diapositivas, por tanto, se mostrarán las mismas y se explicaran con detalle.

\section{Distribución normal multivariante}

\includepdf[frame=true, noautoscale=true, delta=10 10, nup=1x2,pages={2-3}, scale=1]{pdf/_tema1.pdf}

\subsection{Esperanza, varianza y covarianza de variables aleatorias}
Dada una variable aleatoria definimos:
\begin{itemize}
\item Esperanza: $\mu = \mathbb{E}(X) = \int_{-\infty}^{\infty}x\cdot f_P(x) dx$

Propiedades:
\begin{enumerate}
\item $\mathbb{E}(aX) = a\mathbb{E}(X)$
\item $\mathbb{E}(X+Y) = \mathbb{E}(X)+\mathbb{E}(Y)$
\item $\mathbb{E}(X+c) = \mathbb{E}(X)+c$ (La esperanza de una constante es la propia constante)
\end{enumerate}
\item Varianza: $Var(X) = \mathbb{E}((X-\mathbb{E}(X))^2) =\mathbb{E}((X-\mu)^2) = \mathbb{E}(X^2)-\mu^2$

Propiedades:
\begin{enumerate}
\item $Var(X+b)=Var(X)$
\item $Var(aX)=a^2Var(X)$
\item $Var(X)\geq 0$
\end{enumerate}
\item Covarianza (entre dos variables aleatorias $X_i$, $X_j$): $\sigma_{i,j} = Cov(X_i,X_j) = \mathbb{E}\left((X_i-\mathbb{E}(X_i))(X_j-\mathbb{E}(X_j))\right) = \mathbb{E}(X_i X_j)-\mathbb{E}(X_i)\mathbb{E}(X_j)$

Dos propiedades importantes de la covarianza son:

\begin{enumerate}
\item Cov(X,X)= Var(X)
\item $Cov(X,Y)=Cov(Y,X)$
\end{enumerate}

\end{itemize}

\subsection{Esperanza, varianza y covarianza de vectores aleatorios}

Un vector aleatorio es un vector de variables aleatorias.

Notación: como durante el curso vamos a trabajar con vectores aleatorios, vamos a generalizar los símbolos que iremos usando:
\begin{itemize}
\item $X = (X_1, X_2,...,X_p)'$ será un vector de p variables aleatorias. Las variables aleatorias serán $X_1, X_2,...,X_p$. La comilla simple $'$ indica que $X$ es un vector columna.
\item $\mu$ será la esperanza del vector aleatorio X: $\mathbb{E}(X)$. Las esperanzas de cada variable aleatoria serán $\mu_1, \mu_2,...,\mu_p$.
\item Si A es una matriz, A' es su traspuesta
\end{itemize}

Por tanto, dado un vector de p variables aleatorias (vector aleatorio p-dimensional), tenemos un resultado parecido.

\begin{itemize}
\item Esperanza. Será un vector columna con las esperanzas de cada variable aleatoria.
\[
\mathbb{E}(X) = \mu = (\mu_1, \mu_2,..., \mu_p)'
\]

Donde cada $\mu_i = \mathbb{E}(X_i)$.

Ejemplo p=3:
\[
\mathbb{E}(X)=
\mathbb{E}\left[
\left(
\begin{array}{c}
X_1\\
X_2\\
X_3
\end{array}
\right)
\right]=
\left(
\begin{array}{c}
\mathbb{E}(X_1)\\
\mathbb{E}(X_2)\\
\mathbb{E}(X_3)
\end{array}
\right)=
\left(
\begin{array}{c}
\mu_1\\
\mu_2\\
\mu_3
\end{array}
\right)=
\mu
\]

Propiedades:
\begin{enumerate}
\item $\mathbb{E}(X+c) = \mathbb{E}(X)+c$. Como en el caso de variables aleatorias.
\item $\mathbb{E}(AX) = A\mathbb{E}(X)$. Donde A es una matriz de dimensión $pxp$ siendo p la dimensión de X.

Lo vemos para p=3:

\[
\mathbb{E}(AX)=
\mathbb{E}\left[
\left(
\begin{array}{ccc}
a_{1,1}& a_{1,2}& a_{1,3}\\
a_{2,1}& a_{2,2}& a_{2,3}\\
a_{3,1}& a_{3,2}& a_{3,3}
\end{array}
\right)
\left(
\begin{array}{c}
X_1\\
X_2\\
X_3
\end{array}
\right) \right]=
\mathbb{E}\left[
\left(
\begin{array}{c}
a_{1,1}X_1 - a_{1,2}X_2 - a_{1,3}X_3\\
a_{2,1}X_1 - a_{2,2}X_2 - a_{2,3}X_3\\
a_{3,1}X_1 - a_{3,2}X_2 - a_{3,3}X_3
\end{array}
\right)
\right]=
\]

\[
=\left(
\begin{array}{c}
a_{1,1}\mathbb{E}(X_1) - a_{1,2}\mathbb{E}(X_2) - a_{1,3}\mathbb{E}(X_3)\\
a_{2,1}\mathbb{E}(X_1) - a_{2,2}\mathbb{E}(X_2) - a_{2,3}\mathbb{E}(X_3)\\
a_{3,1}\mathbb{E}(X_1) - a_{3,2}\mathbb{E}(X_2) - a_{3,3}\mathbb{E}(X_3)
\end{array}
\right)=
\left(
\begin{array}{ccc}
a_{1,1}& a_{1,2}& a_{1,3}\\
a_{2,1}& a_{2,2}& a_{2,3}\\
a_{3,1}& a_{3,2}& a_{3,3}
\end{array}
\right)
\left(
\begin{array}{c}
\mathbb{E}(X_1)\\
\mathbb{E}(X_2)\\
\mathbb{E}(X_3)
\end{array}
\right)=
\]
\[
=A\mathbb{E}(X)
\]

\end{enumerate}

\item Varianza. La varianza va a ser una matriz, donde cada elemento va a ser la covarianza entre dos de las p variables aleatorias que conforman el vector. Será por tanto una matriz simétrica (ya que $\sigma_{i,j}=Cov(X_i,X_j)=Cov(X_j,X_i)=\sigma_{j,i}$). La matriz resultante será la llamada matriz de covarianzas $\Sigma$.
\[
Var(X)=\mathbb{E}\left((X-\mu)(X-\mu)'\right) = \mathbb{E}(XX')-\mu \mu'=\Sigma
\]

\begin{proof}
\[
Var(X)=\mathbb{E}\left((X-\mu)(X-\mu)'\right) = \mathbb{E}(XX'- \mu X' - X \mu'+\mu \mu')=
\]
\[
\mathbb{E}(XX')-\mathbb{E}(\mu X')-\mathbb{E}(X\mu')+\mathbb{E}(\mu \mu')= \mathbb{E}(XX')-\mu \mathbb{E}(X')-\mu' \mathbb{E}(X)+\mu\mu'=
\]
\[
 \mathbb{E}(XX')-\mu \mu'-\mu' \mu+\mu \mu' = \mathbb{E}(XX')-\mu \mu'=\Sigma
\]
\end{proof}

Ejemplo p=3:

\[
Var(X)=
\mathbb{E}\left[
\left(
\begin{array}{c}
X_1-\mu_1\\
X_2-\mu_2\\
X_3-\mu_3
\end{array}
\right)
(X_1-\mu_1, X_2-\mu_2, X_3-\mu_3)\right]=
\left(
\begin{array}{ccc}
\sigma_{1,1}& \sigma_{1,2}& \sigma_{1,3} \\
\sigma_{2,1}& \sigma_{2,2}& \sigma_{2,3} \\
\sigma_{3,1}& \sigma_{3,2}& \sigma_{3,3}
\end{array}
\right)=
\]

\[
=\left(
\begin{array}{ccc}
Var(X_1)& \sigma_{1,2}& \sigma_{1,3} \\
\sigma_{2,1}& Var(X_2)& \sigma_{2,3} \\
\sigma_{3,1}& \sigma_{3,2}& Var(X_3)
\end{array}
\right) = \Sigma
\]

Donde se cumple que $\sigma_{1,2}=\sigma_{2,1}$, $\sigma_{1,3}=\sigma_{3,1}$ y $\sigma_{3,2}=\sigma_{2,3}$. Y por tanto $\Sigma$ es simétrica.
\end{itemize}

Propiedades:
\begin{enumerate}
\item $Var(AX+b) = \mathbb{E}\left[ A(X-\mu)(X-\mu)'A' \right]=A \Sigma A'$.  Donde A es una matriz de dimensión $pxp$ siendo p la dimensión de X.
\begin{proof}
\[
Var(AX+b) = \mathbb{E}\left[ (AX+b-A\mu-b)(AX+b-A\mu-b)' \right] =
\]
\[
 =\mathbb{E}\left[ (AX-A\mu)(AX-A\mu)' \right] = \mathbb{E}\left[A(X-\mu)(X-\mu)'A'\right] = A\mathbb{E}\left[(X-\mu)(X-\mu)'\right]A' =
 \]
 \[
 =A \Sigma A'
\]

\end{proof}
\end{enumerate}

\textcolor{red}{Mirar si tiene importancia lo de $\Sigma$ semidefinida positiva y tal}

\subsection{Función característica}
La función característica de un vector aleatorio X es:
\[
\phi_X(t)=\mathbb{E}(\exp^{it'X})
\]

Siendo X y t p-dimensionales.

\begin{prop} Mecanismo de Cramer-Wold... \end{prop}

Esta función caracteriza la distribución de X:
\begin{prop} Sean X e Y dos vectores aleatorios:
\[
\phi_X(t)=\phi_Y(t) \Leftrightarrow X \stackrel{d}{=} Y
\]

\textcolor{red}{Completar este apartado consultando a Elena}
\end{prop}
\subsection{Matriz de covarianzas}
Como ya dijimos anteriormente la matriz de covarianzas $\Sigma$ define la varianza de un vector aleatorio y es simétrica. Por tanto podemos expresar $\Sigma$ de la siguiente forma:
\[
\Sigma = CDC^{-1}
\]

Siendo D una matriz diagonal.

\textcolor{red}{$C^{-1}=C'$ ya que las columnas de C son vectores ortogonales. OJO CUIDAO, que tienen que ser ortonormales...
Una matriz real A es ortogonal si y sólo si sus vectores filas o vectores columna son cada uno un conjunto ortonormal de vectores.}
Por tanto:
\[
\Sigma = CDC'  \text{ y } \Sigma^{-1} = CD^{-1}C'
\]

\textcolor{blue}{Caso particular:
\[
p=2 \text{ , }
\mu=\left(
\begin{array}{c}
0\\
0
\end{array}
\right)
\text{ , }
\left(
\begin{array}{cc}
\lambda_1& 0 \\
0 & \lambda_2
\end{array}
\right)
\]
Tenemos:
\[
(X_1, X_2)
\left(
\begin{array}{cc}
\lambda_1& 0 \\
0 & \lambda_2
\end{array}
\right)
\left(
\begin{array}{c}
X_1\\
X_2
\end{array}
\right) = cte
\Rightarrow
\frac{X_1^2}{\lambda_1}+\frac{X_2^2}{\lambda_2}=cte
\]
}


\subsection{Estandarización multivariante}
\begin{defn}
Sea un vector aleatorio X, es normal p-dimensional con vector de medias $\mu$ y matriz de covarianzas $\Sigma$ (notación: $X\equiv N_p(\mu, \Sigma)$) si tiene densidad dada por:

\[
f(x)=\abs{\Sigma}^{-1/2}(2\pi)^{-p/2} exp \left( -\frac{1}{2}(x-\mu)' \right)
\]
\end{defn}

\begin{prop} Si $X \equiv N_p(\mu, \Sigma)$ y definimos $Y = \Sigma^{-1/2}(X-\mu)$, entonces $Y_1,...,Y_p$ son i.i.d. N(0,1).\end{prop}

\begin{proof}
Sabemos por definición que:
\[
f_X(x)=\abs{\Sigma}^{-1/2}(2\pi)^{-p/2} exp \left( -\frac{1}{2}(x-\mu)' \right)
\]

Vamos a aplicar un cambio de variable en la fórmula de la densidad:

Despejando de $Y = h(X)= \Sigma^{-1/2}(X-\mu)$, obtenemos que $\Sigma^{1/2}Y+\mu=h^{-1}(Y)=X$.

Y ahora cogemos el Jacobiano de $h^{-1}(Y)=X$ que será $\Sigma^{1/2}$ ($\mu$ es una constante e Y es la variable).

También hay que considerar la exponencial de la fórmula de la densidad, ahi hacemos el cambió de variable de:

$$e^X \text{por} e^{h^{-1}(Y)}=e^{\Sigma^{1/2}Y+\mu}$$

Y el Jacobiano sería $e^{\Sigma^{1/2}Y}$:


Por tanto nos quedaría:
\[
f(X) = f(h^{-1}(Y))*\abs{Jh(x)} = \abs{\Sigma}^{-1/2}(2 \pi)^{-p/2} \exp\left(-\frac{1}{2}(\Sigma^{-1/2}Y+\mu-\mu)'  \right) \exp\left( \Sigma^{1/2}Y \right) \Sigma^{1/2}  =
\]
\[
= \abs{\Sigma}^{-1/2}(2 \pi)^{-p/2} \exp\left(-\frac{1}{2}(\Sigma^{-1/2}Y)' \right) \exp\left( \Sigma^{1/2}Y \right) \abs{\Sigma}^{1/2} =
\]
\[
\abs{\Sigma}^{-1/2}(2 \pi)^{-p/2} \exp\left(-\frac{1}{2}(Y'\Sigma^{-1/2}\Sigma^{1/2}Y \right) \abs{\Sigma^{1/2}} = (2 \pi)^{-p/2} \exp\left(-\frac{1}{2}(Y'Y) \right)
\]
\end{proof}


\subsection{Ejercicio 1}
Definimos el siguiente vector aleatorio: $X = (X_1,X_2,X_3)' \equiv N_3(\mu, \Sigma)$ con:

\[
\mu=
\left(
\begin{array}{c}
0\\
0\\
0
\end{array}
\right) \text{,       }
\Sigma=
\left(
\begin{array}{ccc}
7/2& 1/2& -1 \\
1/2& 1/2& 0 \\
-1& 0& 1/2
\end{array}
\right)
\]

\ppart Calcula las distribuciones marginales $X_i \equiv N(\mathbb{E}(X_i), Var(X_i))$:

$X_1\equiv N(0, 7/2)$

$X_2\equiv N(0, 1/2)$

$X_3\equiv N(0, 1/2)$

Para calcular estos valores solo hace falta mirar los datos que nos da el problema, el vector de medias $\mu$ y la matriz de covarianzas $\Sigma$:

\[
\Sigma=\left(
\begin{array}{ccc}
Var(X_1)& \sigma_{1,2}& \sigma_{1,3} \\
\sigma_{2,1}& Var(X_2)& \sigma_{2,3} \\
\sigma_{3,1}& \sigma_{3,2}& Var(X_3)
\end{array}
\right)
\]

\[
\mu=
\left(
\begin{array}{c}
\mathbb{E}(X_1)\\
\mathbb{E}(X_2)\\
\mathbb{E}(X_3)
\end{array}
\right)=
\left(
\begin{array}{c}
\mu_1\\
\mu_2\\
\mu_3
\end{array}
\right)
\]

\ppart Calcula la distribución del vector $(X_1,X_2)'$:

Este vector sigue una distribución normal que puede obtener de las matriz $\Sigma$ y el vector de medias $\mu$:
\[
\left(
\begin{array}{c}
X_1\\
X_2
\end{array}
\right)
\equiv N_2\left[
\left(
\begin{array}{c}
0\\
0
\end{array}
\right)
\text{, }
\left(
\begin{array}{cc}
7/2& 1/2 \\
1/2 & 1/2
\end{array}
\right)
\right]
\]

\ppart ¿Son $X_2$ y $X_3$ independientes?

Sí son independientes ya que la covarianza entre ambas variables es 0. La covarianza entre $X_2$ y $X_3$ es el elemento de la fila 3 y la columna 2 de la matriz de covarianzas $\Sigma$, (que al ser $\Sigma$ simétrica coincide con el elemento de la fila 2 y la columna 3).

\ppart ¿Es $X_3$ independiente del vector $(X_1, X_2)'$?
???

\ppart Calcula la  distribución de la variable aleatoria $(2X_1-X_2+3X_3)$.

Procedemos de la siguiente manera:

\[
(2X_1-X_2+3X_3)=(2,-1,3)\left(
\begin{array}{c}
X_1\\
X_2\\
X_3
\end{array}
\right)\equiv
N\left( 0,  \right)
\]



\subsection{Distribuciones condicionadas}

\begin{prop}

Sea $X=(X_1|X_2)$ con $X_1∈ℝ^p$ y $X_2∈ℝ^{p-q}$. Consideramos las particiones correspondientes de $µ$ y de $\Sigma$.

\end{prop}

\begin{proof}
Definimos $X_{2.1} = X_2 - Σ_{21}Σ_{11}^{-1}X_1$.

\[
\begin{pmatrix}
X_1\\
X_{2.1}
\end{pmatrix} =
\begin{pmatrix}
I &| &0\\
\hline
- Σ_{21}Σ_{11}^{-1}  &| &I
 \end{pmatrix}
\]

Como es una combinación lineal de $(X_1,X_{2.1})'$, entonces $X_{2.1}$ es normal multivariante.

Vamos a calcular la media y la matriz de covarianzas de $X_{2.1}$

$X_{2-1} = N\left( µ_2-Σ_{21}Σ_{11}^{-1}µ_1 , \begin{pmatrix} Σ_{11} &|&0\\\hline 0&|&Σ_{2.1} \end{pmatrix} \right)$

Donde las covarianzas se calculan: $AΣA'$, siendo $A$ la matriz de la combinación lineal, es decir:

\[
A=\begin{pmatrix}
I &| &0\\
\hline
- Σ_{21}Σ_{11}^{-1}  &| &I
 \end{pmatrix}
\]



\paragraph{Conclusiones:}

\begin{itemize}
	\item $X_1$ es independiende de $X_{2.1}$
	\item $X_{2.1}$ es normal, con media y varianza calculadas anteriormente.
	\subitem $X_{2.1}|X_1$, al ser independientes, también se distribuye normalmente, con los mismos parámetros.
	\item Dado $X_1$, los vectores $X_{2.1}$ y $X_2$  difieren en el vector constante $Σ_{21}Σ_{11}^{-1}X_1 \implies X_2|X_1 = N\left( µ_{2.1}, Σ_{2.1} \right)$
\end{itemize}

\end{proof}

\begin{example}
Vamos a considerar $X_1, X_2$ como escalares, para entender la proposición. Este ejemplo le surgió a un investigador que quería predecir la estatura de los hijos en función de la de los padres (que no padres y madres, sólo padres).


\[
\begin{pmatrix}
X\\Y
\end{pmatrix} \equiv N_2\left( \begin{pmatrix} µ_x \\ µ_y \end{pmatrix}, \begin{pmatrix}
σ_x^2&σ_{xy}\\σ_{xy}&σ_y^2
\end{pmatrix} \right)
\]
\label{form::EspVarCondicionada}
Definimos $\gor{Y} = E(Y|X) = µ_y + \frac{σ_{xy}}{σ_x^2}(x-µ_x)$. La esperanza de la altura del hijo condicionada a la altura del padre será la media de las alturas de los hijos corregida por un factor en el que influye la diferencia de altura del padre con respecto a su media. Es de esperar que si Yao Ming tiene un hijo, sea más alto que la media.

El factor de corrección $\frac{σ_{xy}}{σ_x^2}$ es importante y no me he enerado bien de dónde sale.

Ahora vamos a calcular $V(Y|X) = σ_{y}^2 - \frac{σ_{xy}^2}{σ_x^2} = σ_y^2 \left( 1- \rho^2\right)$ donde $\rho = \frac{σ_{xy}^2}{σ_x^2σ_y^2}$, el coeficiente de correlación.

Ha dicho algo así como \textbf{La única relación que puede existir entre 2 variables normales es una relación lineal.}


Este coeficiente de correlación aparece también en la expresión de la esperanza. Vamos a verlo:

 \[\gor{Y} = µ_y + \frac{σ_{xy}}{σ_x^2}(x-µ_x) \dimplies \frac{\gor{Y}-µ_y}{σ_y} = \frac{σ_{xy}}{σ_xσ_y}\frac{x-µ_x}{σ_x}\]

 Es decir:

 \[
\frac{\gor{Y}-µ_y}{σ_y} = \rho \frac{x-µ_x}{σ_x}
 \]

Aplicado a la estatura de los hijos respecto de los padres, se interpreta como: ``Si un padre es muy alto, su hijo será alto pero no destacará tanto como el padre''. Este fenómeno lo definió como \concept{Regresión a la mediocridad}.

\end{example}

\begin{defn}[Homocedástico]
$Σ_{2.1}$ no depende de $X_1$.

Esto se da cuando $\begin{pmatrix}X_1,X_2\end{pmatrix}$ es normal multivariante. Si no fueran normal multivariante, serían heterocedásticas. \footnote{Un ejemplo sería $X_1$ la renta de una familia y $X_2$ los ahorros de la misma. Los datos no se distribuyen conjuntamente normal, con lo que la $Σ_{2.1}$ si depende de $X_1$. Ya veremos más adelante este concepto con mayor detalle.}
\end{defn}


\begin{example}

Ahora vamos a ver un par de ejemplos numéricos:

Sea \[\begin{pmatrix}X,Y\end{pmatrix} \equiv N_2 \left( \begin{pmatrix}0,0\end{pmatrix}, \begin{pmatrix}10&3\\3&1\end{pmatrix} \right)\]

\paragraph{Distribución $Y|X$:}

\[E(Y|X) = \frac{3}{10}x\]
\[V(Y|X) = \frac{1}{10}\]

\paragraph{Distribución $X|Y$:}

\[E(X|Y) = 3y\]
\[V(X|Y) = 1\]

Ambas son normales unidimensionales ya que $(X Y)$ es normal multivariante.

Sea \[\begin{pmatrix}X,Y\end{pmatrix} \equiv N_2 ...\]

Sea $Z_1 = X+Y$ y $Z_2 = X-Y$.

\[
\begin{pmatrix}Z_1//Z_2\end{pmatrix} = \begin{pmatrix}1&1\\1&-1\end{pmatrix}\begin{pmatrix}X\\Y\end{pmatrix} \implies \begin{pmatrix}Z_1\\Z_2\end{pmatrix} = N_2\left(\begin{pmatrix}2\\0\end{pmatrix},\begin{pmatrix}7&1\\1&3\end{pmatrix}\right)
\]

Ahora vamos a calcular lo que nos piden: $E(Z_1|Z_2=1)$.

\[E(Z_1|Z_2=1) = 2 + \frac{1}{3}(1-0) = \frac{7}{3}\]

Es importante destacar que la distribución no depende del valor concreto por ser homocedásticas .

\end{example}

\section{Contrastes no paramétricos}
Hipótesis no paramétrica: hipótesis que no se formula en términos de un número finito de parámetros.

\begin{enumerate}
\item Bondad de ajuste: A partir de una muestra $X_1,...,X_n \equiv F$ de muestras (observaciones) \textcolor{red}{(son muestras o variables aleatorias o es simple notación?)} aleatorias independientes idénticamente distribuidas, contrastar:
\begin{itemize}
\item $H_0: F=F_0$ donde $F_0$ es una distribución prefijada.
\item $H_0: F \in \{F_O : O\in H\}$ H es el espacio paramétrico.
\end{itemize}
\item Homogeneidad: Dados $X_1,...,X_n \equiv F$ y $Y_1,...,Y_n \equiv G$ de muestras (observaciones) aleatorias independientes idénticamente distribuidas. Contrastar $H_0: F=G$.
\item Hipótesis de independencia: Dada $(X_1,Y_1),...,(X_n,Y_n) \equiv F$ de muestras (observaciones) aleatorias independientes idénticamente distribuidas. Contrastar $H_0: X$ e $Y$ son independientes.
\end{enumerate}

\subsection{Contraste $\chi^2$ de bondad de ajuste}
Consideramos una distribución totalmente especificada bajo $H_0: X_1,...,X_n \equiv F$ de muestras i.i.d.

$H_0: F=F_0$ es la hipótesis nula y queremos ver que F, que es la distribución obtenida con los datos verdaderos (las muestras $X_i$ obtenidas empíricamente) es igual a $F_0$ que es la distribución teórica.

\textbf{Notación: }$P_A(B)$ es la probabilidad de B condicionada a A.

Vamos a definir los pasos que tenemos que seguir para comprobar si $H_0$ es cierta:
\begin{enumerate}
\item Se definen k clases $A_1,...,A_k$. 

\item Se cuentan cuántos datos caen en cada clase (frecuencias observadas). Cada clase la llamaremos $O_i=\#\{j:X_j\in A_i\}$.

\item Se calculan las frecuencias esperadas si $H_0$ fuese cierta. A este dato lo llamaremos $\mathbb{E}_i$ o $\mathbb{E}_{H_0}(O_i)$: 
\[
\mathbb{E}_{H_0}(O_i) = np_i
\]

\obs Las $O_i$ son variables aleatorias que se distribuyen como una binomial $B(n, p_i=P_{H_0}(A_i))$. Siendo $n$ el número de intentos y $p_i$ la probabilidad de que una muestra pertenezca a la clase $A_i$ bajo la hipótesis nula. 

La notación puede resultar liosa, a grades rasgos:
\begin{itemize}
\item $O_i$ tendrá un valor que será la frecuencia observada de una clase i. Es decir, el número de observaciones que caen en una la clase i.
\item $\mathbb{E}_{H_0}(O_i)$ será el valor esperado de $O_i$ considerando la hipótesis nula como cierta. La esperanza de una $B(n,p)$ es igual a $np$.
\end{itemize}

\item Se comparan las frecuencias observadas y esperadas mediante el estadístico de Pearson:
\[
T = \sum_{i=1}^n \frac{(O_i-E_i)^2}{E_i}
\]

Se divide entre $E_i$ para darle más importancia a la diferencia si el valor es pequeño, Por ejemplo, si E=100 y O=101, no es lo mismo que si E=1 y O=2. Sin embargo, si no dividiéramos por $E_i$ nos daría el mismo resultado.

\item Se rechaza $H_0$ en la región crítica $R=\{T > c\}$ donde c es tal que $\alpha=P_{H_0}(T>c)$. Es decir, $\alpha$ (también llamado 'nivel de significación') es la probabilidad de rechazar la hipótesis nula siendo esta cierta. O dicho de otra forma, la probabilidad de entrar en la región de rechazo $'T>c'$ considerando que $H_0$ es cierta.
\end{enumerate}


Ahora vamos a ver qué podemos decir del estadístico de Pearson 'T':
\[
O_i=B(n,p_i) \simeq N(np_i, np_i(1-p_i))
\]

Imaginémonos por un momento que podríamos despreciar el término $np_i^2$ de la varianza de la normal. Nos quedaría:
\[
\simeq N(np_i, np_i(1-p_i)) \simeq N(np_i, np_i) \simeq N(E_i, E_i)
\]

Que por el Teorema Central del Límite (\url{https://es.wikipedia.org/wiki/Teorema_del_l%C3%ADmite_central}) nos queda:
\[
\frac{O_i-E_i}{\sqrt{E_i}} \simeq N(0,1)
\]

Y como deberíamos saber, una distribución $\chi^2_k$  no es más que una distribución de probabilidad continua con un parámetro k que representa los grados de libertad de la variable aleatoria $X = Z_1^2 + \cdots + Z_k^2$, donde $Z_i$ son variables aleatorias normales independientes de media cero y varianza uno. Por tanto:

\[
\frac{(O_i-E_i)^2}{E_i} \simeq \chi^2_1
\]

Por tanto, como tenemos k clases, podríamos tener $T\simeq \chi^2_k$. pero por otro lado sabemos que $O_1,O_2,...,O_k=n$, esta restricción hace que no haya una independencia entre todos los sumandos $O_i$, por tanto nos queda: $T\simeq \chi^2_{k-1}$.

Finalmente nos queda que la región de rechazo, dado un nivel de significación $\alpha$, se alcanza cuando el estadístico de Pearson T, obtenido a partir de los datos muestrales, vale más que $\chi^2_{k-1}, \alpha$. $\chi^2_{k-1}, \alpha$ es, dada la función de densidad de una $\chi^2_{k-1}$, el valor del eje de abscisas que hace que se quede un $\alpha*100\%$ del área encerrada bajo la curva a la derecha de ese valor:

%DIBUJOOOO

\obs
\begin{enumerate}
\item  Tal y como lo hemos definido tenemos que $\sum_{i=1}^k O_i = n$ y que $\sum_{i=1}^k E_i = \sum_{i=1}^k np_i= n(p_1+p_2+...+p_k) = n$, por tanto tenemos:
\[
T=\sum_{i=1}^k \frac{(O_i-E_i)^2}{E_i} = \sum_{i=1}^k \frac{O_i^2}{E_i}-n
\]

\item Por definición de $\chi^2_{k-1}$, su esperanza es:
\[
\mathbb{E}_{H_0}(T) \simeq k-1
\]

\item Por definición de $\chi^2_{k-1}$, su varianza es:
\[
Var_{H_0}(T) \simeq 2(k-1)
\]

\begin{defn}[p-valor]
valor de $\alpha$ mínimo con el que se empieza a rechazar la hipótesis nulo.
\end{defn}

\end{enumerate}

\begin{example}
Tiramos un dado 100 veces y obtenemos:

\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
Resultados & 1 & 2 & 3 & 4 & 5 & 6 \\
\hline
Frecuencia & 10 & 20 & 20 & 10 & 15 & 25\\
\hline
\end{tabular}

Y consideramos $H_0: p_i=1/6 \text{ } \forall i=1,...,6$. Es decir que el dado no está trucado y cada cara tiene la misma probabilidad ($p_i$) de salir.

Por otro lado consideramos $H_1: \exists i$ tal que $p_i\neq 1/6$. Es decir, que el dado está trucado y hay caras que salen mas que otras.

Seguimos los pasos:
\begin{enumerate}
\item En este caso cada clase será la cara del dado que sale, habrá por tanto 6 clases: k=6.
\item Se cuentan cuantos datos caen en cada clase: $O_1=10$, $O_2=20$, $O_3=20$, $O_4=10$, $O_5=15$, $O_6=25$
\item Se calculan las frecuencias esperadas si $H_0$ fuese cierta. En este caso $n=10+20+20+10+15+25=100$ y $p_i=1/6 \text{ } \forall i=1,...,6$. Nos queda: $\mathbb{E}_i=np_i = 100 \frac{1}{6}=100/6$.
\item Se obtiene el estadístico de Pearson:
\[
T=\sum_{i=1}^n \frac{O_i^2}{E_i}-n = \frac{6}{100}(10^2+20^2+20^2+10^2+15^2+25^2)-100=11
\]
\item Rechazamos $H_0$ si $T>c$. En este caso, consideramos un nivel de significación $\alpha = 0.05$. Sabemos que $\alpha = P_{H_0}(T>c)$ Como tenemos 6 clases, el estadístico de Pearson tendrá una distribución $\chi^2_5$. Buscamos en la tabla (mirar apéndice) y obtenemos que $\chi^2_5, 0.05 = 11.07$. Este será nuestro valor de c.

Puesto que nuestra región de rechazo es $R=(T>c)$, y tenemos que $11>11.07$, no podemos rechazar la hipótesis nula, y por tanto, no podemos concluir que el dado esta trucado.

Si consideramos un valor de significación $\alpha = 0.06$ si hubiéramos rechazado la hipótesis nula (aunque con un 6\% de opciones de equivocarnos) y hubiéramos concluido que el dado esta trucado.

%Meter dibujo de gráfico chi cuadrado con 5 grados de libertad
%Meter tabla de chi cuadrado
\end{enumerate}

\end{example}



\chapter{Prácticas}
Se incluyen las soluciones de las prácticas:
\includepdf[pages={1-5}]{pdf/_p1E2.pdf}

\appendix
\chapter{Ejercicios}
\input{tex/EstII_ejercicios.tex}



\end{document}
