% -*- root: ../InvestigacionOperativa.tex -*-

\section{Introducción}

En esta sección, vamos a repasar conceptos y algunas aplicaciones porque de aquí en adelante trabajaremos con estos conceptos.


\begin{itemize}
	\item Gradiente.
	\item Matriz Hessiana.
	\item Desarrollo de Taylor en varias variables:
		\[
			f(\vx) = f(\gor{\vx}) + \grad f(\gor{\vx})^\top (\vx-\gor{\vx}) + (\vx-\gor{\vx})\Hf{\gor{\vx}}(\vx-\gor{\vx}) + \norm{\vx-\gor{\vx}}R(\gor{\vx},\vx-\gor{\vx})
		\]
		con $\displaystyle \lim R(\vx,\gor{\vx}) = 0$ ($R$ es el resto).
\end{itemize}

\subsection{Condiciones para óptimos locales}

\paragraph{Condición necesaria de primer orden}
Si $\vx$ es un máximo o mínimo local, entonces $\grad f(\vx) = 0$.

Para ver que la condición es necesaria pero no suficiente, podemos pensar en $f(x_1,x_2) = x_1^2 - x_2^2$, cuyo gradiente se anula en $(0,0)$, pero no es un mínimo ni un máximo. Estamos ante un punto de silla (punto de inflexión).


\paragraph{Condiciones necesarias de segundo orden} Sea $f$ 2 veces diferenciable en $\gor{\vx}$. Si $\gor{\vx}$ es un mínimo local de $f$, encones:
\begin{itemize}
	\item $\grad f(\gor{\vx}) = 0$
	\item $\Hf{\gor{\vx}}$ semidefinida positiva.
\end{itemize}

Con el mismo ejemplo anterior podemos ver que es condición necesaria.

\paragraph{Condiciones suficientes} Sea $f$ 2 veces diferenciable en $\gor{\vx}$. Si $\gor{\vx}$ es un mínimo local de $f$, encones:
\begin{itemize}
	\item $\grad f(\gor{\vx}) = 0$
	\item $\Hf{\gor{\vx}}$ definida positiva.
\end{itemize}
\begin{proof}
	Si $\vx$ no es un mínimo local estrico, entonces existe una sucesión $\vx_k \to \gor{\vx}$ con $\gor{\vx_k} ≠ \vx$ tal que $f(\vx_k) ≤ f(\gor{\vx})$.

	Sea \[d_k = \frac{\vx_k - \gor{\vx}}{\norm{\vx_k - \gor{\vx}}}\]

	Las normas están acotadas, y estamos en $\real^n$ y algún argumento de compacidad, podemos tomar una subsucesión convergente de $d_k\to d$, que vamos a llamar $d_k$

	Tomamos el desarrollo de Taylor de $f(\vx_k)$:

	\[
		f(\vx_k) = f(\gor{\vx}) = f(\gor{\vx}) + \frac{1}{2}\Hf{f(\gor{\vx})}(\vx_k - \gor{\vx}) + \norm{\vx_k - \gvx}^2R(\gx,\gx_k)
	\]

	\[
		0≥\frac{f(\vx_k - f(\gx)}{\norm{\vx_k - \gvx}^2} = \frac{1}{2}d_k^t\Hf{f(\gvx)}d_k + R(\gvx,\vx_k)
	\]
	Hemos llegado a que $\Hf{f(\gvx)}$ tiene que ser semidefinida negativa.
\end{proof}

Estas condiciones están bien, pero no son del todo interesantes para este curso, ya que estamos buscando soluciones globales a problemas y todas estas condiciones son locales.
Vamos a necesitar alguna restricción más sobre $f$ para poder buscar soluciones globales.

\subsection{Funciones convexas}

\begin{defn}[Función\IS convexa]
Una función $\appl{f}{D}{ℝ}$ es convexa si su dominio $D\subset\real ℝ^n$ es covexa y  $∀\vx,\vy\in D$, $∀λ∈[0,1]$ se verifica:

\[
	f(λ\vx + (1-λ)\vy ≤ λf(\vx) + (1-λ)f(\vy)
\]
\end{defn}

\obs
\begin{itemize}
	\item \concept{Convexidad\IS estricta} requiere $<$ en lugar de $≤$.
	\item Una función es (estrictamente) cóncava si $-f$ es (estrictamente) convexa.
	\item $f$ es convexa $\dimplies ∀x,\vec{u}, g(t) = f(x+tu)$ es convexa (en su dominio, es decir $\{t : x+tu\in D\}$)
	\begin{proof}
		\[g(λt_1+(1-λ)t_2) = f(x+(λt_1+(1-λt_2)\vu) = ...
		\]
	\end{proof}
\end{itemize}

\begin{example}
\begin{itemize}
	\item $f(x) = x^2$ es convexa.

	Como todavía no hemos visto la definición de convexidad en términos de la segunda derivada, vamos a recurrir a la definición.
	\begin{proof}
		\[
			\left(λx_1 + (1-λ)x_2\right)^2 ≤ λx_1^2 +(1-λ)x_2^2)
		\]
	\end{proof}
	\item $f(\vx) = \max\{x_1,...,x_n\}$ es convexa.
	\begin{proof}
		\[f(λ\vx + (1-λ) \vy) = \max\set{λx_1 + (1-λ)y_1,...,λx_n + (1-λ)y_n} ≤ λ\max\{x_k\} + (1-λ)\max\{y_k\} = λf(\vx) + (1-λ)f(\vy)\]
	\end{proof}
	\item Las funciones afines $f(x) = A\vx + b$ son cóncavas y convexas.
	\item Cualquier norma $f(\vx) = \norm{\vx}$ es una función convexa.
	\item Si $S$ es convexo, la distancia a $S$:
	\[
		f(x) = d(x,S) = \inf_{s\in S}\norm{\vx-s}
	\]
	es una función convexa.

\end{itemize}

\begin{theorem}[Desigualdad\IS de Jensen]
Si $\appl{f}{D}{ℝ}$ es convexa, entonces $∀x_1,...,x_n\in D$ y $,λ_1,...,λ_n ≥0$ con $\sum^n λ_i = 1$ se cumple:

\[f\left(\sum^k x_iλ_iy\right) ≤ \sum^k λ_if(x_i)\]

\end{theorem}
\begin{proof}
Por inducción sobre $k$.
\end{proof}

\obs Podríamos interpretar $λ_i$ como probabilidades y tendríamos:

Si $X$ es una variable aleatoriao que toma los valores $x_i$ con probabilidad $λ_i$, entonces $f(\esp{X})≤\esp{f(X)}$

\end{example}

Vamos a ver más condiciones de convexidad de funciones, para no tener que recurrir a la definición.

\paragraph{Completar la teoría}


\paragraph{Ejemplos}
\begin{itemize}
	\item $f(x) = -\log(x)$ podemos derivar 2 veces y como es positiva, convexa.
	\item $f(x) = e^{ax}$ es convexa.
	\item $f(x) = x^a$
	\subitem $a≤0, a≥1 \to$ convexa.
	\subitem $0<a<1 \to $ cóncava.
	\item $f(x) = x^\top Ax + a^\top x+ c$ donde $A$ es simétrica. Queremos ver que $f$ convexa $\dimplies$ $A$ definida positiva.
	\item La más relevante: $X$ una matrix $n\times p$ y sean $β\in\real^p$ e $y\in\real^n$, entonces:
	$f(β) = \norm{y-Xβ}$ es convexa, siendo $\norm{·}$ cualquier norma.

	Esta función es convexa porque es la composición de 2 funciones. $Xβ$ es una función afín y la norma es convexa para cualquier norma. Entonces, como $f(β)$ es una combinación de funciones convexas, es convexa.
\end{itemize}



\section{Cuasiconvexidad}


Aquí faltan bastantes cosas.


\subsection{Utilidad}

La cuasiconvexidad es una propiedad más relajada que al convexidad pero que es condición suficiente para algunos resultados interesantes. Por ejemplo:

\begin{prop}
Mínimo local en función cuasiconvexa es un mínimo global.
\end{prop}

\begin{proof}
Se deja como ejercicio
\end{proof}




\section{Tras la reparación}

\subsection{Resumen}

\begin{table}[hbtp]
\centering
\begin{tabular}{c|c|c}
Primal $\equiv$ (P) & $\begin{array}{c} \min c'x\\Ax = b\\x≥ 0\end{array}$ & $\begin{array}{c} \max c'x\\Ax ≤ b\\x≥ 0\end{array}$\\
\hline
Dual $\equiv$ (D) & $\begin{array}{c} \max b'u\\A'u ≥ c\end{array}$ & $\begin{array}{c} \max b'u\\A^\top u ≤ c\\u≥ 0\end{array}$\\
\end{tabular}
\caption{Resumen de pasar del problema primal al dual.}
\end{table}


\subsection{Interpretación económica del problema dual}

La solución del problema dual del pastelero es $u = \left(\rfrac{1}{3},0,10\right)$, que son los valores que aparecían en el problema \ref{ejer:pastelero}. Además, vemos también que $\bar{p} = \bar{d} = 775$. Es decir, la dualidad es fuerte. 

Vamos a demostrarlo teóricamente.


\begin{defn}[Dualidad\IS fuerte]
$\bar{p} = \bar{d}$
\end{defn}


\begin{example} Escribe el dual de:

...

\begin{ioprob}
\goal{$\min -u_1 - u_2$}
\restrictions{$-u_2≥1$}{$u_2 ≤ 1-$}{$u_1-u_2≥-1$}{$u_1 ≥$}{$u_2≥0$}{}
\end{ioprob}

Aquí no tienen solución ni el factible ni el primal.
\end{example}

\begin{theorem}[Dualidad fuerte para problemas lineales]

Consideremos un problema lineal en forma estándar $(P)$ y el correspondiente problema dual $(D)$. 

Si $(P)$ tiene solución factible óptima finita, también la tiene $(D)$ y los correspondientes valores óptimos son iguales.
\end{theorem}

\begin{proof}
\[u' \equiv c_B'B^{-1}\]

\[
\begin{array}{c}
u'b = c'_BB^{-1}b
c'x = c'_BB^{-1}n]c_N'0
\end{array}
\]


$u$ es factible para $(D)$ $\dimplies$ 

\[u'A ≤ c' = c'_BB^{-1}\left(B | N \right) ≤ (c'_B,c'_N)\dimplies \underbrace{c_B'B^{-1}N}_{z_j} ≤ c'_N\]

Demostrar que $u$ es factible es lo mismo que demostrar que $z_j ≤ c_j$ en la tabla del simplex. Como $\bar{x}$ es el óptimo, en la última fila de simplex tenemos todos $≤0$, con lo que $z_j ≤ c_j$

\paragraph{Conclusión: } Encontrar un óptimo para el primal es encontrar un factible para el dual.

\end{proof}



\[
\dpa{\bar{z}}{b_i} = u_i
\]
Esto demuestra la observación de que la solución del dual del pastelero corresponde al beneficio marginal que gana de más al aumentar en 1 su cantidad de azúcar/mantequilla/harina. Esto es general para cualquier problema lineal.





\subsection{Algortimo simple-dual}

Tenemos un problema de programación lineal que resolvemos con simplex normal. Llegamos a una solución óptima que es factible del dual. Ahora, imaginemos que cambiamos $b$, con lo que las restricciones dejan de cumplirse. El estado de la tabla del simplex se mantiene, salvo la columna de la izquierda. 
Este es un punto de partida para este algoritmo. Otro caso ventajoso de este algoritmo se da al tener ¿coeficientes negativos?


\paragraph{Completar de las traspas}


\begin{example} Resolver el problema:

...

Lo primero es pasarlo a forma estándar:


\begin{ioprob}
\goal{$\min{3x_1 + 2x_2}$}
\restrictions{$3x_1-x_2+x_3 = -3$}{...}{...}{...}{...}{...}
\end{ioprob}


En este caso, no podemos empezar con el algoritmo del simplex, porque hay $b_i< 0$. Pero tendríamos una solución óptima si fuera factible. Al ser óptima en el primal, es factible en el dual y podemos aplicar el algoritmo del simplex-dual.
Vamos a ver la evolución de las tablas.


\paragraph{Vamos a verlo gráficamente paso a paso}

\begin{figure}[hbtp]
\centering
\begin{tikzpicture}[scale=1.5]
\draw[thick,->] (0,0) -- (3.5,0);
\draw[thick,->] (0,0) -- (0,3.5);
\draw[thick,-] (3,0) -- (0,3);
\draw[thick,-] (1,0) -- (0,3);
\draw[thick,-] (1.5,0) -- (0,2);
\filldraw[fill=blue, opacity = 0.4] (3/5,6/5) -- (1.5,0) -- (3,0) -- (0,3) -- cycle;
\node [label={below:$(1)$},nodepoint, inner sep=2pt,color=red] at (0,0) {};
\node [label={left:$(2)$},nodepoint, inner sep=2pt,color=blue] at (0,2) {};
\node [label={below left:$(3)$},nodepoint, inner sep=2pt,color=green] at (3/5,6/5) {};
\end{tikzpicture}
\caption{Evolución del algoritmo del simplex-dual.}
\end{figure}


\end{example}