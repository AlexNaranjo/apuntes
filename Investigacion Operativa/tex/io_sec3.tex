% -*- root: ../InvestigacionOperativa.tex -*-

\section{Introducción}

En esta sección, vamos a repasar conceptos y algunas aplicaciones porque de aquí en adelante trabajaremos con estos conceptos.


\begin{itemize}
	\item Gradiente.
	\item Matriz Hessiana.
	\item Desarrollo de Taylor en varias variables:
		\[
			f(\vx) = f(\gor{\vx}) + \grad f(\gor{\vx})^\top (\vx-\gor{\vx}) + (\vx-\gor{\vx})\Hf{\gor{\vx}}(\vx-\gor{\vx}) + \norm{\vx-\gor{\vx}}R(\gor{\vx},\vx-\gor{\vx})
		\]
		con $\displaystyle \lim R(\vx,\gor{\vx}) = 0$ ($R$ es el resto).
\end{itemize}

\subsection{Condiciones para óptimos locales}

\paragraph{Condición necesaria de primer orden} 
Si $\vx$ es un máximo o mínimo local, entonces $\grad f(\vx) = 0$.

Para ver que la condición es necesaria pero no suficiente, podemos pensar en $f(x_1,x_2) = x_1^2 - x_2^2$, cuyo gradiente se anula en $(0,0)$, pero no es un mínimo ni un máximo. Estamos ante un punto de silla (punto de inflexión).


\paragraph{Condiciones necesarias de segundo orden} Sea $f$ 2 veces diferenciable en $\gor{\vx}$. Si $\gor{\vx}$ es un mínimo local de $f$, encones:
\begin{itemize}
	\item $\grad f(\gor{\vx}) = 0$
	\item $\Hf{\gor{\vx}}$ semidefinida positiva.
\end{itemize}

Con el mismo ejemplo anterior podemos ver que es condición necesaria. 

\paragraph{Condiciones suficientes} Sea $f$ 2 veces diferenciable en $\gor{\vx}$. Si $\gor{\vx}$ es un mínimo local de $f$, encones:
\begin{itemize}
	\item $\grad f(\gor{\vx}) = 0$
	\item $\Hf{\gor{\vx}}$ definida positiva.
\end{itemize}
\begin{proof}
	Si $\vx$ no es un mínimo local estrico, entonces existe una sucesión $\vx_k \to \gor{\vx}$ con $\gor{\vx_k} ≠ \vx$ tal que $f(\vx_k) ≤ f(\gor{\vx})$.

	Sea \[d_k = \frac{\vx_k - \gor{\vx}}{\norm{\vx_k - \gor{\vx}}}\]

	Las normas están acotadas, y estamos en $\real^n$ y algún argumento de compacidad, podemos tomar una subsucesión convergente de $d_k\to d$, que vamos a llamar $d_k$

	Tomamos el desarrollo de Taylor de $f(\vx_k)$:

	\[
		f(\vx_k) = f(\gor{\vx}) = f(\gor{\vx}) + \frac{1}{2}\Hf{f(\gor{\vx})}(\vx_k - \gor{\vx}) + \norm{\vx_k - \gvx}^2R(\gx,\gx_k)
	\]

	\[
		0≥\frac{f(\vx_k - f(\gx)}{\norm{\vx_k - \gvx}^2} = \frac{1}{2}d_k^t\Hf{f(\gvx)}d_k + R(\gvx,\vx_k)
	\]
	Hemos llegado a que $\Hf{f(\gvx)}$ tiene que ser semidefinida negativa.
\end{proof}

Estas condiciones están bien, pero no son del todo interesantes para este curso, ya que estamos buscando soluciones globales a problemas y todas estas condiciones son locales.
Vamos a necesitar alguna restricción más sobre $f$ para poder buscar soluciones globales.

\subsection{Funciones convexas}

\begin{defn}[Función\IS convexa]
Una función $\appl{f}{D}{ℝ}$ es convexa si su dominio $D\subset\real ℝ^n$ es covexa y  $∀\vx,\vy\in D$, $∀λ∈[0,1]$ se verifica:

\[
	f(λ\vx + (1-λ)\vy ≤ λf(\vx) + (1-λ)f(\vy)
\]
\end{defn}

\obs 
\begin{itemize}
	\item \concept{Convexidad\IS estricta} requiere $<$ en lugar de $≤$.
	\item Una función es (estrictamente) cóncava si $-f$ es (estrictamente) convexa.
	\item $f$ es convexa $\dimplies ∀x,\vec{u}, g(t) = f(x+tu)$ es convexa (en su dominio, es decir $\{t : x+tu\in D\}$)
	\begin{proof}
		\[g(λt_1+(1-λ)t_2) = f(x+(λt_1+(1-λt_2)\vu) = ...
		\]
	\end{proof}
\end{itemize}

\begin{example}
\begin{itemize}
	\item $f(x) = x^2$ es convexa.

	Como todavía no hemos visto la definición de convexidad en términos de la segunda derivada, vamos a recurrir a la definición.
	\begin{proof}
		\[
			\left(λx_1 + (1-λ)x_2\right)^2 ≤ λx_1^2 +(1-λ)x_2^2)
		\]
	\end{proof}
	\item $f(\vx) = \max\{x_1,...,x_n\}$ es convexa.
	\begin{proof}
		\[f(λ\vx + (1-λ) \vy) = \max\set{λx_1 + (1-λ)y_1,...,λx_n + (1-λ)y_n} ≤ λ\max\{x_k\} + (1-λ)\max\{y_k\} = λf(\vx) + (1-λ)f(\vy)\]
	\end{proof}
	\item Las funciones afines $f(x) = A\vx + b$ son cóncavas y convexas.
	\item Cualquier norma $f(\vx) = \norm{\vx}$ es una función convexa.
	\item Si $S$ es convexo, la distancia a $S$:
	\[
		f(x) = d(x,S) = \inf_{s\in S}\norm{\vx-s}
	\]
	es una función convexa.

\end{itemize}

\begin{theorem}[Desigualdad\IS de Jensen]
Si $\appl{f}{D}{ℝ}$ es convexa, entonces $∀x_1,...,x_n\in D$ y $,λ_1,...,λ_n ≥0$ con $\sum^n λ_i = 1$ se cumple:

\[f\left(\sum^k x_iλ_iy\right) ≤ \sum^k λ_if(x_i)\]

\end{theorem}
\begin{proof}
Por inducción sobre $k$.
\end{proof}

\obs Podríamos interpretar $λ_i$ como probabilidades y tendríamos:

Si $X$ es una variable aleatoriao que toma los valores $x_i$ con probabilidad $λ_i$, entonces $f(\esp{X})≤\esp{f(X)}$

\end{example}

Vamos a ver más condiciones de convexidad de funciones, para no tener que recurrir a la definición.

\paragraph{Completar la teoría}


\paragraph{Ejemplos}
\begin{itemize}
	\item $f(x) = -\log(x)$ podemos derivar 2 veces y como es positiva, convexa.
	\item $f(x) = e^{ax}$ es convexa.
	\item $f(x) = x^a$ 
	\subitem $a≤0, a≥1 \to$ convexa.
	\subitem $0<a<1 \to $ cóncava.
	\item $f(x) = x^\top Ax + a^\top x+ c$ donde $A$ es simétrica. Queremos ver que $f$ convexa $\dimplies$ $A$ definida positiva.
	\item La más relevante: $X$ una matrix $n\times p$ y sean $β\in\real^p$ e $y\in\real^n$, entonces:
	$f(β) = \norm{y-Xβ}$ es convexa, siendo $\norm{·}$ cualquier norma.

	Esta función es convexa porque es la composición de 2 funciones. $Xβ$ es una función afín y la norma es convexa para cualquier norma. Entonces, como $f(β)$ es una combinación de funciones convexas, es convexa.
\end{itemize}

