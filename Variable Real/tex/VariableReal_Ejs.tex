% -*- root: ../VariableReal.tex -*-
\section{Hoja 1}

\begin{problem} Sea \meas un espacio de medida.

\ppart Demostrar que $\appl{f}{Ω ⊆ X}{ℝ}$ es medible si y sólo si el conjunto $\set{x ∈ Ω \tq f(x) > r}$ es medible $∀r ∈ ℝ$.
\ppart Demostrar que si la sucesión de funciones $\appl{f_n}{Ω}{ℝ}$ es medible para todo $n$ también lo son las funciones $\sup f_n,\ \inf f_n,\ \limsup f_n$ y $\liminf f_n$.
\ppart Demostrar que si $\appl{f_n}{Ω}{ℝ}$ son medibles y $f_n(x) \convs f(x)$ puntualmente, entonces $f$ es medible.
\solution

\spart

Si $f$ es medible, entonces $∀E ⊆ ℝ$ medible se tiene que $\inv{f}(E)$ es medible, y en particular esto valdrá para los conjuntos de la forma $(r, +∞)$.

Para demostrar la implicación al otro lado, vemos ciertas propiedades de la inversa: sabemos que $μ(\inv{f}(E^c)) = μ(X) - μ(\inv{f}(E))$, que $\inv{f}(E ∩ F) = μ(E)∩μ(F)$ y que $\inv{f}(E∪F) = \inv{f}(E) ∪ \inv{f}(F)$. Así, podemos construir cualquier subconjunto de $ℝ$ a partir de intervalos de la forma $(r, +∞)$, que sabemos que son medibles. Esas operaciones son compatibles con la inversa y entonces podremos descomponerlo todo en conjuntos medibles\footnote{Esto merece algo más de explicación pero bueno.}.

\spart\label{ej:H1:SupremosMedibles} \textit{Nota: esto es una demostración extendida de la \fref{prop:SupremoInfimoMedibles}.}

Para demostrarlo vamos a usar el apartado anterior.

En el caso del supremo, sabemos que $\appl{\sup f_n}{Ω}{ℝ}$ será medible si y sólo si el conjunto \[ \set{x∈Ω \tq \sup_{n∈ℕ} f_n (x) > r } = \bigcup_{n ∈ ℕ} \set{x ∈ Ω \tq f_n(x) > r} \] es medible para todo $r ∈ ℝ$, que lo es porque se puede descomponer como unión de conjuntos medibles. En el caso del ínfimo lo tenemos análogamente pero haciendo la intersección de esos conjuntos: \[ \set{x ∈ Ω \tq \inf_{n∈ℕ} f_n(x) > r} = \bigcap_{n∈ℕ} \set{x∈Ω \tq f_n(x) > r } \]

\spart

Si $f_n \convs f$ puntualmente, eso significa que $\limsup f_n = \liminf f_n = f$, luego por lo que hemos visto en el apartado anterior es efectivamente medible.


\end{problem}

\begin{problem} Probar que si $\appl{f}{ℝ}{ℂ}$ es continua y $f(x) = 0$ en casi todo punto con respecto a la medida de Lebesgue, entonces $f(x) = 0\; ∀x ∈ ℝ$.

\solution

Si $f$ es nula en casi todo punto con respecto a la medida de Lebegue, entonces sólo podemos tener puntos aislados que difieran de 0. Sin embargo, si esos puntos existen no se cumpliría la definición de continuidad.

Más formalmente y usando la definición topológica de continuidad\footnote{No me apetece pensar continuidad en los complejos.} supongamos que existe $a ∈ ℝ$ tal que $c = f(a) ≠ 0$. Dado que $f$ es nula en casi todo punto, $c$ tiene que ser punto aislado y por lo tanto existe un $r ∈ ℝ$ tal que $0 ∉ \bola_r(c) ⊆ ℂ$, es decir, existe un entorno alrededor de $c$ que no contiene a $0$. Sin embargo, si cogemos un entorno $E = (a - ε, a + ε)$, tenemos que $f(E)$ es nulo en casi todo punto, por lo que $f(E) \nsubseteq \bola_r(c)$ y por lo tanto $f$ no puede ser continua. Contradicción, así que $f(x) = 0\; ∀x∈ℝ$.

\end{problem}

\begin{problem} \label{ej:H1:ConvMonotonaSeries} Sea $\set{f_n}$ una sucesión de funciones medibles con $f_n ≥ 0$. Usar el \nlref{thm:ConvMonotona} para demostrar que \[ \int_Ω\left(\sum_{n=1}^∞ f_n\right) \dif μ = \sum_{n=1}^∞ \int_Ω f_n \dif μ \]
\solution

Lo cierto es que esto es el \nlref{thm:ConvMonotonaSeries}, así que no hay que complicarse mucho la vida. Simplemente hay que definir la sucesión \[ S_N = \sum_{n=1}^N f_n \] que es monótona creciente por ser $f_n ≥ 0$, luego podemos usar el \fref{thm:ConvMonotona}. Lo que tenemos es por lo tanto que  \begin{align*} \int_Ω\left(\sum_{n=1}^∞ f_n\right) \dif μ &= \int_Ω \lim_{N\to \infty} S_N \dif μ \eqexpl{\ref{thm:ConvMonotona}} \lim_{N\to ∞} \int_Ω S_N \dif μ = \\ &= \lim_{N\to ∞} \int_Ω \sum_{n=1}^N f_n \dif μ \eqreasonup{Suma finita} \lim_{N\to ∞} \sum_{n=1}^N \int_Ω f_n \dif μ = \sum_{n=1}^∞ \int_Ω f_n \dif μ \end{align*}

\end{problem}

\begin{problem}[5] Probar la siguiente generalización de la \nref{prop:DesigualdadMarkov}. Sea $\appl{g}{ℝ}{[0, +∞]}$ una función medible y creciente en la imagen de $\appl{f}{\meas}{ℝ}$. Probar que \[ μ\left(\set{x ∈ X \tq f(x) > t}\right) ≤ \frac{1}{g(t)} \int_X g(f(x)) \dif μ(x) \]

Como consecuencia, probar que si $f_n \convs f$ en $\espL$, entonces $f_n \convs f$ en medida.
\solution

La desigualdad de Markov nos dice que, dada una función medible $f$, entonces \[ μ\left(\set{x∈X \tq \abs{f(x)} > ε }\right) ≤ \frac{1}{ε} \int_X \abs{f} \dif μ \]

Sea $E = \set{x ∈ X \tq f(x) > t}$. Podemos usar esto para descomponer la integral: \[ \frac{1}{g(t)} \int_X g(f(x)) \dif μ = \frac{1}{g(t)} \int_{X \setminus E} g(f(x)) \dif μ + \frac{1}{g(t)} \int_E g(f(x)) \dif μ\]

Como $g$ es creciente en la imagen de $f$, tenemos que $∀a ∈ f(E),\, g(a) > g(t)$, así que podemos simplificar en la segunda integral y nos queda que \begin{multline*}\frac{1}{g(t)} \int_{X \setminus E} g(f(x)) \dif μ + \frac{1}{g(t)} \int_E g(f(x)) \dif μ ≥ \frac{1}{g(t)} \int_{X \setminus E} g(f(x)) \dif μ + \frac{1}{g(t)} \int_E g(t) \dif μ = \\ = \int_{X \setminus E} g(f(x)) \dif μ + μ(E) ≥ μ(E) \end{multline*} y queda probada la desigualdad.

Ahora tenemos que usar esto para demostrar que si $f_n \convs f$ en $\espL$, entonces $f_n \convs f$ en medida o, en otras palabras, que
\[ \int_X \abs{f_n - f} \dif μ \to 0 \implies ∀ε > 0 \quad μ\left(\set{x∈X \tq \abs{f_n(x) - f(x)} > ε}\right) \to 0 \]

% TODO: Acabar esto.

\end{problem}

\section{Hoja 2}

\begin{problem} Sea μ una medida con signo sobre el espacio medible \meds. Definimos \index{Variación!total de una medida}la variación total de una medida como $\abs{μ} = μ_+ + μ_-$, siendo $μ_+, μ_-$ las partes positiva y negativa de una medida respectivamente, dadas por la descomposición de Jordan. Decimos que μ es finita si y sólo si $\abs{μ}$ lo es.

\ppart Probar la siguiente caracterización de la variación total\footnote{En los ejercicios aparece como partición de $X$ pero no le veo mucho sentido a eso.}: \[ \abs{μ} (E) = \sup \set{ \sum_{n=1}^∞ \abs{μ(E_i)} \tq \set{E_n} \text{ es una partición de } E} \]

Demostrar además que \begin{align*} μ_+(E) &= \sup \set{μ(F) \tq F ⊆ E, F ∈ \algb{X}}  \\  μ_-(E) &= - \inf \set{μ(F) \tq F ⊆ E, F ∈ \algb{X}} \end{align*}

Verificar que $μ = μ_+ - μ_-$ y que $\abs{μ} = μ_+ + μ_-$.

\ppart Probar que $\abs{μ}$ es la medida minimal tal que $-\abs{μ} ≤ μ ≤ \abs{μ}$.
\ppart Probar que $E ∈ \algb{X}$ es μ-nulo si y sólo si es $\abs{μ}$-nulo.
\ppart Probar que $\abs{μ_1 + μ_2} ≤ \abs{μ_1} + \abs{μ_2}$.
\ppart Probar que μ es finita si y sólo si $μ_+, μ_-$ lo son.
\ppart Sean $μ_1, μ_2$ medidas con signo. Probar que \[ μ_1 \perp μ_2 \iff μ_1 \perp \abs{μ_2} \iff μ_1\perp μ_{2,+} \iff μ_1 \perp μ_{2,-} \]
\solution

\spart Según el \nref{thm:DescompHahn} y el \nref{thm:DescompJordan}, siempre tendremos una partición de $X = X_+ ∪ X_-$ con medidas asociadas $μ_+, μ_-$ singulares. Al considerar todas las posibles particiones de $E_i$, siempre habrá alguna tal que los $E_i$ sean subconjuntos o bien de $X_+$ o de $X_-$ (es decir, que o bien $E_i ∩ X_+ = ∅$ o $E_i ∩ X_- = ∅$).

Si nos encontramos con una partición de este tipo, tendremos que $\abs{μ(E_i)} = μ_+(E_i)$ o que $\abs{μ(E_i)} = μ_-$, y por lo tanto la suma sobre todos los elementos de la partición será igual a $\abs{μ}(E)$. Sólo nos faltaría demostrar que esta partición es la que tiene valor máximo.

Por suerte, eso lo podemos ver muy fácilmente. Sea $\set{E^*_n}$ otra partición de $E$ que no cumpla la ``exclusividad'', es decir, que existan conjuntos cuyas intersecciones con $X_+$ \textbf{y} con $X_-$ sean no vacías. En ese caso, podemos considerar que cada elemento de la partición se puede descomponer $E^*_i = E_j ∪ E_k$ con $E_j, E_k$ parte de la partición $\set{E_n}$ y con\footnote{Si esto no ocurre porque no tenemos conjuntos suficientemente pequeños, siempre podemos montarnos otra partición que tendrá la misma medida de la suma.} $E_j ⊆ X_+$, $E_k ⊆ X_-$, de tal forma que \[ \abs{μ(E_i^*)} = \abs{μ(E_j) + μ(E_k)} = \abs{μ_+(E_j) - μ_-(E_k)} ≤ μ_+(E_j) + μ_-(E_k) \] donde para algunos $E_i$ la desigualdad será estricta (los que intersequen con $X_+$ y con $X_-$) y para otros habrá igualdad (más que nada sólo intersecarán con $X_+$ ó con $X_-$ y $E_j$ o $E_k$ serán vacíos). En cualquier caso, la suma sobre la partición siempre será menor que la suma sobre la partición ``exclusiva'' $\set{E_n}$.

Con esta demostración, es fácil ver que las definiciones de $μ_+, μ_-$ en base a supremo e ínfimo de subconjuntos de $E$ son válidas. Sólo tenemos que darnos cuenta de que $μ(F)$ valdrá el máximo cuando $F = E ∩ X_+$, y el mínimo (negativo, por eso el cambio de signo) cuando $F = E ∩ X_-$.

\spart

\spart

No veo muy bien cómo hacerlo, más que nada porque si $μ_+(E) = μ_-(E)$ se nos puede estropear la cosa.

\spart

\spart Bastante trivial.

\spart Hay dos tipos de implicaciones: o las que se ven fácilmente o las que me da la sensación de que no funcionan, como lo de $μ_1 \perp μ_{2,+} \iff μ_1 \perp μ_{2,-}$.

\end{problem}

\begin{problem}[6] Sea $\appl{μ}{\mathcal{B}(ℝ)}{[-∞,+∞]}$ una medida con signo sobre los borelianos de $ℝ$. Definimos la función $g(x) = μ([0,x])$. Probar que

\ppart $μ$ es medida continua si y sólo si $g$ es continua.
\ppart $μ$ es medida absolutamente continua si y sólo si $g$ es una función absolutamente continua.
\solution

Vamos con la definición de función absolutamente continua primero.

\begin{defn}[Función\IS absolutamente continua] \label{def:FuncAbsCont} $\appl{f}{ℝ}{ℝ}$ lo es si y sólo si para todo $ε>0$ existe un $δ>0$ tal que \[ \sum_{i=1}^k \abs{f(y_i) - f(x_i)} < ε\] para cualquier familia de intervalos disjuntos $[x_1, y_1], \dotsc, [x_n, y_n]$ tales que \[ \sum_{i=1}^k \abs{y_i - x_i} < δ \]

La continuidad absoluta implica continuidad uniforme (donde la δ no depende del punto que se coja), que a su vez implica continuidad normal (donde δ depende del ε pero también del punto $x_0$ que se valore).

La continuidad absoluta es la clase de funciones más grande tal que el teorema fundamental del cálculo integral con la integral de Lebesgue y la derivada clásica es cierto.
\end{defn}

\spart

Si $g$ es continua en $x_0$, entonces para todo $ε > 0$ existe un $δ > 0$ tal que, si $\abs{x_0 - x} < δ$, entonces $\abs{g(x_0) - g(x)} < ε$. Transformando esa última ecuación y suponiendo que $x_0 < x$, tenemos que $\abs{μ([0, x_0]) - μ([0,x])} = \abs{μ([x_0,x)} < ε$. Igualmente, si tomamos $x_1 > x$ con $\abs{x_1 - x} < δ$, tendremos que $\abs{μ([x, x_1))} < ε$. Usando la desigualdad triangular y operando un poquillo nos quedará que $\abs{μ([x_0, x_1))} < 2ε$.

Podemos hacer ε tan pequeño como queramos, de tal forma que $μ([x_0, x_1)) \to 0$ con $x ∈ [x_0, x_1)$ siempre, y por convergencia dominada tendremos que $μ(\set{x}) = 0$.

Creo que además podemos recorrer esta demostración al revés para sacar la implicación al otro lado.

\spart


\end{problem}

\begin{problem}[8] Si $f ∈ \espL$, con $f \not\equiv 0$, probar que

\ppart Existen $c, R > 0$ tales que la función maximal $Mf(x) ≥ c \abs{x}^{-N}$ para $\abs{x} > R$.
\ppart Como consecuencia, cuando $t > 0$ es pequeño, existe $c'$ tal que \[ m\left(\set{x ∈ ℝ^N \tq Mf(x) ≥ t}\right) ≥ \frac{c'}{t} \]
\ppart Probar que, en general, es falsa la desigualdad \[ \norm{Mf}_{\espL} ≤ C \norm{f}_{\espL} \]

Indicación: describir la función maximal de $f(x) = \ind_{(-1, 1)} (x)$.
\solution

\spart Cogemos $α = \inf Mf(x)$, que está bien definido ($Mf(x) ≥ \abs{f(x)} ≥ 0$) y además no es 0 (es el supremo de los valores medios para $r > 0$, sólo puede ser $0$ si $f \equiv 0$ y ya hemos dicho por hipótesis que eso no ocurre). Por otra parte, $c \abs{x}^{-N} < c R^{-N}$ si $\abs{x} >R$, así que sólo tenemos que coger $c,R$ que cumplan que $α ≥ cR^{-N}$.

\spart Es cierto, sólo hay que aplicar el \nref{thm:Maximal}. No sé, eso sí, como hacerlo como consecuencia de lo otro.

\spart Cuando $x ∈ (-1, 1)$, el valor de la función maximal es $1$. Cuando está fuera, el valor máximo se alcanzará en el primer momento en el que la bola $\bola_r(x)$ cubra $(-1,1)$ (es decir, la bola de menor radio que cubre ese intervalo). Esa bola será el intervalo $(-1, x + (x + 1))$ (si $x < 0$ es análogo), esto es, la bola de radio $x + 1$. Así, tendremos que \[ Mf(x) = \fint\limits_{(-1, 2x + 1)}\ind_{(-1, 1)} (t) \dif t = \frac{m((-1, 1))}{m((-1, 2x + 1))} = \frac{1}{\abs{x}} \quad x ∉ (-1,1) \]

Si ahora hacemos la integral de $Mf$ nos quedará que \[ \norm{Mf}_{\espL} = \int_{-∞}^∞ \abs{Mf(x)} \dif x = 2 \int_{1}^∞ \frac{1}{x} \dif x + \int_{-1}^1 \dif x \] que no converge por el $\frac{1}{x}$, luego la desigualdad no se cumple.

\end{problem}

\begin{problem} A partir del \nref{thm:Maximal} y usando la \nlref{thm:DesigualdadJensen}, probar que si $p ∈ [1,∞)$ existe $C < ∞$ tal que para toda $f ∈ \espLp(ℝ)$ y para todo $λ>0$ se cumple que \[ m\left(\set{x ∈ ℝ \tq Mf(x) > λ}\right) ≤ \frac{C}{λ^p} \int_ℝ \abs{f(y)}^p \dif y \] donde $M$ es la \nlref{def:FuncMaximalHL}.

\solution
\end{problem}

\begin{problem}[13] Probar el siguiente teorema que relaciona la derivada de Radon-Nikodim con el \nref{thm:DifLebesgue}. Sea μ una medida de Borel Regular y sea $μ = μ_S + μ_{AC}$ con $μ_{AC} = f \dif m$.  Entonces, para casi todo punto $x ∈ ℝ^n$, se tiene que \[ \lim_{r \to 0} \frac{μ(E_r)}{m(E_r)} = f(x) \] para toda familia $\set{E_r}$ que tiende suavemente (\fref{def:EncogerBien}) a $x$.

\solution

Siguiendo con la descomposición, tenemos que $μ(E_r) = μ_S(E_r) + μ_{AC} (E_r)$. Tenemos que ver que la parte singular nos la podemos quitar.

Como $E_r$ es una familia que tiende suavemente a $x$, entonces $m(E_r) > α m(\bola_r(x))$ con $α > 0$. En concreto, esto nos dice que $m(E_r) ≠ 0$. Como $μ_S \perp m$, la intersección de sus soportes tiene medida $0$, luego $μ(E_r) = 0$.

Ahora sólo tenemos que ir a por la parte singular continua, que como se tiene que $μ_{AC} = f \dif m$ con $f$ integrable, entonces nos queda que \[ μ_{AC} (E_r)  = \int_{E_r} f \dif m \]

Dividiendo esto por $m(E_r)$, nos queda el valor medio de $f$, así que finalmente tenemos \[ \lim_{r \to 0} \frac{μ(E_r)}{m(E_r)} = \frac{1}{m(E_r)} \int_{E_r} f \dif m = f(x)\] por el \nlref{thm:DifLebesgue}.
\end{problem}

\begin{problem}
Probar que si $f ∈ \espLloc$ y es continua en $x_0 ∈ ℝ^N$, entonces $x_0 ∈ L_f$. Es decir, que cada punto de continuidad es un \nlref{def:PuntoLebesgue}.
\solution

Recordamos que $x_0$ es un punto de Lebesgue de $f$ si y sólo si \[ \lim_{r \to 0} \fint_{\bola_r(x_0)} f \dif m = f(x_0) \]

Si $f$ es continua en $x_0$, entonces $∀ε > 0$ existe un $δ > 0$ tal que, si $\abs{x-x_0} < δ$, entonces $\abs{f(x) - f(x_0)} < ε$. Lo que hacemos entonces es coger bolas de radio $δ \to 0$ e integrar $f(x) - f(x_0)$:
\[ \fint_{\bola_δ(x_0)} \abs{f(x) - f(x_0)} \dif x ≤ \fint_{\bola_δ(x_0)} ε \dif x = ε \]

Si esta integral se va a cero con el valor absoluto cuando $ε \to 0$ (y por lo tanto $δ \to 0$ igualmente), entonces está claro que $\fint_{\bola_δ(x_0)} f(x) - f(x_0) \dif x \to 0$, y podemos sacar el $f(x_0)$ de ahí para quedarnos con la igualdad que teníamos que demostrar.
\end{problem}

\section{Hoja 3}

\begin{problem}[2] Usar la \nref{thm:DesigualdadJensen} para demostrar las siguientes desigualdades:

\ppart Desigualdad entre media aritmética y geométrica. Para todo $1 ≤ n ∈ ℕ$ y todo $x_i ∈ [0, +∞)$, \[ (x_1 \dotsb x_n)^{\frac{1}{n}} ≤ \frac{x_1 + \dotsb + x_n}{n} \]

\ppart Desigualdad entre media aritmética y geométrica generalizada: para todo $1 ≤ n ∈ ℕ$ y todo $α_i,x_i ∈ [0, +∞)$ con $α_1 + \dots + α_n = 1$, se tiene que \[ \prod x_i^{α_i} ≤ \sum α_i x_i \]

\solution

\spart Una función que nos permite pasar de productos a sumas es el logaritmo, así que vamos a ver qué pasa si tomamos $φ = \log$ en un espacio discreto. Por ser formales, diremos que $X = ℤ_n$, con $f(z) = x_z$ y la medida dada como $μ(z) = 1\;∀z ∈ ℤ_n$, de tal forma que  $\int f = \sum x_i$ y $μ(ℤ_n) = n$.

En este caso, φ no es convexa sino cóncava. Lo bueno es que entonces $-φ$ es convexa, así que podemos multiplicar por $-1$ en ambos lados de la desigualdad y se nos invierte. Así, lo que tenemos es que \[ \log\left(\frac{1}{n} \sum x_i\right) ≥ \frac{1}{n} \sum \log x_i  = \frac{1}{n} \log \prod x_i = \log \left(\prod x_i\right)^{\frac{1}{n}} \] y quitando los logaritmos ya tenemos lo que nos pedían.

\spart Aquí tenemos que cambiar la medida, y decir que $μ(z) = α_z$, lo que nos queda que $μ(ℤ_n) = 1$, que en el fondo nos va a simplificar algo las cosas. Aplicando de nuevo la desigualdad de Jensen con $φ = \log$, tenemos que \begin{align*}
\log\left(\int f \dif μ\right) &≥ \int \log f \dif μ \\
\log\left(\sum α_i x_i\right) &≥ \sum α_i \log x_i = \log\left(\prod x_i^{α_i}\right)
\end{align*}, que de nuevo se nos queda en lo que nos pedían demostrar si quitamos logaritmos.

\end{problem}

\begin{problem} Probar la \nlref{prop:DesHolder} como consecuencia de la \nlref{thm:DesigualdadJensen}.

\solution

La desigualdad de Hölder nos dice que, siendo $1 ≤ p ≤ + ∞$ y $q = \frac{p}{p - 1} ≥ 1$ de tal forma que $\frac{1}{p} + \frac{1}{q} = 1$, tenemos que \[ \int_X \abs{fg} \dif μ ≤ \norm{f}_p \norm{g}_q \]

Para demostrarla, vamos a hacer un truco que consiste en montar una medida artificial. Sea \[ ν = \frac{f^p}{\norm{f}_p^p} \] una medida, cuya principal ventaja es que $ν(X) = \int_X \frac{f^p}{\norm{f}_p^p} \dif μ = 1$ y por lo tanto $\fint \dif ν\equiv \int \dif ν$.

Sea $φ(z) = z^q$, convexa. Podemos aplicar ahora la desigualdad de Jensen a una función $h$ que ya veremos luego cómo montamos. La desigualdad nos dice que \begin{align*}
\left(\int h \dif ν\right)^q &≤ \int h^q \dif ν \\
\left(\int \frac{hf^p}{\norm{f}_p^p} \dif μ\right)^q &≤ \int \frac{h^qf^p}{\norm{f}_p^p} \dif μ
\end{align*}

Querríamos ahora que $hf^p$ fuese $fg$, así que tomamos $h = \frac{g}{f^{p-1}}$, que además nos da que $h^q f^p = \frac{g^qf^p}{f^{q(p-1)}} = \frac{g^qf^p}{f^p} = g^q$. Sustituyendo, tenemos que
\begin{align*}
\left(\int \frac{hf^p}{\norm{f}_p^p} \dif μ\right)^q &≤ \int \frac{h^qf^p}{\norm{f}_p^p} \dif μ \\
\frac{1}{\norm{f}_p^{pq}} \left(\int fg \dif μ\right)^q &≤ \frac{1}{\norm{f}_p^p} \int g^q \dif μ \\
\int fg \dif μ &≤ \norm{f}_p^{\frac{p(q-1)}{q}} \norm{g}_q \\
\int fg \dif μ &≤ \norm{f}_p \norm{g}_q
\end{align*}, que es la desigualdad de Hölder ya que $p\frac{q - 1}{q} = p\frac{\frac{p}{p-1} - \frac{p - 1}{p-1}}{\frac{p}{p - 1}} = \frac{\frac{1}{p-1}}{\frac{1}{p-1}}$ = 1.
\end{problem}


\begin{problem} Probar que la desigualdad de Hölder es una igualdad si y sólo si $\abs{f}^p = λ\abs{g}^{q}$ CTP para alguna constante $λ > 0$, con $q$ el exponente conjugado de Hölder.

\solution

Si $\abs{f}^p = λ\abs{g}^{q}$, entonces \[ \norm{f}_p = \left(\int_X \abs{f}^p \right)^{\frac{1}{p}} = \left(\int_X λ \abs{g}^q \right)^{\frac{1}{p}} = λ^\frac{1}{p} \norm{g}_q^{\frac{q}{p}} \], y como $\frac{q}{p} + 1 = \frac{q + p}{p} = q$, entonces $\norm{fg}_q = λ^{\frac{1}{p}} \norm{g}_q^q$.

Por otra parte \[ \int_X \abs{fg} \dif μ = \int_X \abs{λ^\frac{1}{p} g^\frac{q}{p} g} \dif μ  = λ^{\frac{1}{p}} \int_X g^q \dif μ = λ^{\frac{1}{p}} \norm{g}_q^q \], así que en este sentido queda demostrado.

Supongamos ahora que $\norm{fg}_1 = \norm{f}_p \norm{g}_q$. Entonces, no sé cómo demostrarlo.

\end{problem}

\begin{problem}[5] Probar por inducción las siguientes generalizaciones de la \nlref{prop:DesHolder}.

\ppart Dados $1 < p_1, p_2, \dotsc, p_n < ∞$ con $f_i ∈ L^{p_i}$ para $i = 1,2, \dotsc, n$, entonces se cumple que \[ \abs{\int \prod f_i \dif μ} ≤ \prod \norm{f_i}_{p_i} \] cuando $\sum\frac{1}{p_i} = 1$.
\solution

\spart Sabemos que para $n = 1,2$ se cumple. Supongamos que se cumple para $n - 1$ y vamos a demostrar que se cumple para $n$. Consideramos $g = \prod_{i=1}^{n-1} f_i$, con $\sum_{i=1}^{n-1}\frac{1}{p_i} = 1$, que sabemos que cumple que $\norm{g}_1 ≤ \prod \norm{f_i}_{p_i}$, luego en concreto $g ∈ L^1$.

\end{problem}
