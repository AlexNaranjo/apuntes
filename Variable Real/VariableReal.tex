\documentclass[nochap,palatino]{apuntes}

\usepackage{fancysprefs}
\usepackage{natbib}
\usepackage{tikztools}

\bibliographystyle{plainnat}

\title{Variable Real}
\author{Guillermo Julián Moreno}
\date{15/16 C1}

\begin{document}
\pagestyle{plain}
\maketitle

\tableofcontents
\newpage
% Contenido.

\section{Teoría de la integral y la medida}

La teoría moderna de la integración sigue lo que se dice los tres principios de J.E. Littlewood (1885 - 1977), que dan de cierta forma la idea detrás de la integración según Lebesgue.

\begin{enumerate}
\item Cada conjunto (medible) es \textit{casi} una suma finita de intervalos.
\item Cada función absolutamente integrable es \textit{casi} continua.
\item Cada sucesión convergente puntualmente de funciones absolutamente integrable es \textit{casi} uniformemente convergente.
\end{enumerate}

Por ejemplo, uno de los problemas que se pretende resolver con la integral de Lebesgue es el de la convergencia de funciones. Con la integral de Riemann, para que se pueda intercambiar el límite con la integral (esto es, que $\int \lim f_n = \lim \int f_n$), la hipótesis necesaria es que $f_n$ converja uniformemente. La integral de Lebesgue si nos permitirá dar una noción más completa de convergencia (tercer principio) que nos permita intercambiar fácilmente la integral con el límite.

La teoría de Lebesgue también nos permite saltarnos problemas como el planteado por la \concept[Función\IS de Dirichlet]{función\IS de Dirichlet}, dada por \[ \mathbb{D}(x) = \begin{cases} 1 & x ∈ ℚ \\ 0 & x ∈ ℝ \setminus ℚ \end{cases} \]

$\mathbb{D}$ no es continua y por lo tanto no es integrable Riemann. La cuestión es que es \textit{casi} continua (segundo principio), así que con Lebesgue podemos quitarnos los problemas que aparecen en los puntos $x ∈ ℚ$ e integrarla sin problemas.

En resumen, en todos los casos lo que tendremos es que, para todo ε, existe un conjunto de puntos de medida ε (los ``puntos malos'') de tal forma que nos lo podemos quitar y tendremos una función convergente, continua o integrable.

Este curso consistirá en la búsqueda y manejo de esos ``puntos malos'': veremos si podremos integrar en ellos, derivarlos y qué ocurre con las funciones en esos puntos.

Para trabajar con estas ideas desarrollaremos el concepto de las medidas que, por así decirlo, serán ``funciones generalizadas''. Por ejemplo, la \concept[Delta\IS de Dirac]{delta\IS de Dirac}, una ``función'' que vale cero en todo punto menos en $x = 0$, donde tiene un valor no concreto, se define en realidad como una medida dada por \[ δ_0 (A) = \begin{cases} 1 & 0 ∈ A \\ 0 & 0 ∉ A \end{cases} \] de tal forma que $\int_{ℝ} f(x) \dif δ = f(0)$.

\subsection{Conjuntos, espacios y funciones medibles}

A lo largo del curso trabajaremos con varias convenciones. Trabajaremos sobre la tres-tupla $(X, \mathcal{X}, μ)$, lo que llamaremos un espacio de medida. $X$ será el conjunto sobre el que mediremos. Normalmente será un subconjunto de $ℝ^n$, pero para el caso nos da igual. $\mathcal{X}$ será la σ-álgebra, el conjunto de subconjuntos medibles de $X$\footnote{Ver apuntes de TIM de Pedro.}. Por último, μ será la medida que usemos para integrar, usando la integral de Lebesgue.

¿Cuál es la diferencia entre las integrales de Riemann y de Lebesgue? Lo que se suele decir es que el primero mide en vertical y el segundo en horizontal. Una analogía es verlo como contar billetes. Por ejemplo, tenemos un billete de 50, uno de 20, dos de 10 y dos de 5. Riemann los contaría según le salen de la cartera, mientras que Lebesgue los agruparía primero en conjuntos de billetes iguales y luego contaría.

Mirando funciones, lo que hace Riemann es montar rectángulos en el eje $X$, mientras que Lebesgue los monta en el eje $Y$. De esta forma, la integral de Lebesgue es más estable, mientras que la de Riemann ve mejor las oscilaciones, por así decirlo. Un ejemplo curioso es el de la siguiente integral, \( \int_0^∞ \frac{\sin x}{x} \dif x \label{eq:IntRiemmanNoLeb} \) que es Riemann integrable pero no Lebesgue porque el valor absoluto no es integrable.

Para seguir trabajando con las medidas y poder integrar, necesitaremos saber qué funciones podemos medir y cuáles no.

\begin{defn}[Función\IS medible] Dados dos espacios de medida $(X, \mathcal{X}, μ),\,(Y, \mathcal{Y},ν)$, se dice que una función \stdf es medible si y sólo si $\inv{f}(E) ∈ \mathcal{X}\quad ∀E ∈ \mathcal{Y}$.
\end{defn}

También necesitaremos saber en qué σ-álgebra estamos trabajando. Nosotros usaremos normalmente la σ-álgebra de Borel:

\begin{defn}[{σ}-álgebra\IS de Borel] Denotada por \borel, es la σ-álgebra más que pequeña tal que los abiertos son medibles.\end{defn}

Otra $\salgb$ importante es la \concept{{σ}-álgebra\IS de Lebesgue}, denotada por $\lebg (ℝ^n)$. La relación entre esta y la de Borel es que la de Borel está contenida en la de Lebesgue: $\borel \subsetneq \lebg $. De hecho, $\lebg  \setminus \borel = \mathcal{N}$, que es el conjunto de conjuntos de medida Lebesgue 0. Por ejemplo, las uniones numerables tienen medida cero, como los racionales. Este es el gran avance de Lebesgue: identificar los conjuntos de medida 0 para poder integrar más funciones.

Algo que nos interesará es ver la composición de funciones.

\begin{prop}

\begin{enumerate}
\item La composición de funciones \borel-medibles es \borel-medible.
\item La composición de funciones \lebg-medibles no es, en general, \lebg-medible.
\item La composición de funciones \lebg-medibles con una de ellas continua sí es \lebg-medible.
\end{enumerate}
\end{prop}

\subsection{Medidas positivas}

\begin{defn}[Medida\IS positiva]\footnote{Positivas implicará a partir de ahora mayor \textit{o igual} que cero.} Una medida positiva (no negativa) en un espacio medible \meds es una función
\begin{align*}
\appl{μ}{\mathcal{X}&}{[0,+∞)} \\
E &\longmapsto μ(E) ≥ 0
\end{align*} y tal que
\begin{enumerate}
\item $μ(∅) = 0$.
\item $μ\left(\bigcup_{n=1}^∞ E_n\right) = \sum_{n=1}^∞ μ(E_n)$ con $E_n$ disjuntos dos a dos.
\end{enumerate}
\end{defn}

Hay dos nombres cocretos que nos interesarán. Si $μ(X) < ∞$, entonces se dice que μ es una \concept[Medida\IS finita]{medida\IS finita}. Si $μ(X) = 1$, será una \concept[Medida\IS infinita]{medida\IS de probabilidad}.

Un ejemplo de una medida que no es una función es la delta de Dirac que habíamos visto antes, que de hecho es una medida de probabilidad.

\begin{prop}[Propiedades de la medida][Medida!propiedades de] Dado un espacio medible \meas:

\begin{enumerate}
\item \textbf{Monotonía}: Si $E⊂F$, entonces $μ(E) ≤ μ(F)$.
\item \textbf{Sub-aditividad}: $μ\left(\bigcup_{n=1}^∞ E_n\right) ≤ \sum_{n=1}^∞ μ(E_n)$ (sólo es igual si los conjuntos son disjuntos).
\item \textbf{Convergencia monótona}: Dada una sucesión de conjuntos $E_1 ⊂ E_2 ⊂ \dotsb$, entonces $μ\left(\bigcup_{n=1}^∞ E_n\right) = \lim μ(E_n)$.
\item \textbf{Convergencia dominada}: Dada una sucesión decreciente de conjuntos $E_1 ⊃ E_2 ⊃ \dotsb$, entonces $μ\left(\bigcap E_n\right) = \lim μ(E_n)$.
\end{enumerate}
\end{prop}

\subsection{Teorema de Littlewood}

Al principio de esta sección veíamos los principios de Littlewood. Estos se traducen en teoremas para los que ya tenemos suficiente teoría para enunciarlos. El primer teorema, obviamente, corresponde al primer principio de Littlewood.

\begin{theorem} Para todo $ε>0$ y para todo $E∈\mathcal{X}$ con $μ(E) < ∞$, existe un $F∈\algbA⊂ \mathcal{X}$ tal que \[ μ\left((E\setminus F) ∪ (F \setminus E)\right) < ε \], con $\algbA$ la familia de conjuntos simples (intervalos, cuadrados o cubos según la dimensión, que son los que sabemos medir).
\end{theorem}

Lo que este teorema nos dice es que cualquier conjunto medible se puede aproximar tan bien como queramos por conjuntos simples, que es la idea del primer principio de Littlewood.

\subsection{Integración}

Dado \meas un espacio de medida, definiremos la integral de la siguientes formas. Primero iremos a lo más simple, que son (atención al nombre) las funciones simples:

\begin{defn}[Función\IS simple] Se dice que $\appl{f}{X}{[0, +∞)}$ es simple si $f$ es de la forma \[ f(x) = \sum a_i \ind_{E_i}(x) \] para $\set{E_i} ⊆ \mathcal{X}$.\end{defn}

Para funciones simples $\appl{f}{X}{[0,+∞)}$ como \[ \int_X f\ \dif μ = \sum_{i=1}^n a_i μ\set{\inv{f}(a_i)} \] Como notación, hay que tener en cuenta que \[ \set{\inv{f}(a_i)} \equiv \set{x∈X\tq f(x)=a_i} \]

Para funciones más complicadas usaremos, como siempre, límites. Dada $\appl{f}{X}{[0, +∞)}$ medible, definiremos su integral como \[\int_X f \dif μ = \sup_{\substack{0≤g≤f \\ g \text{ simple}}} \int_X g \dif μ \]

Una observación que necesitaremos para poder usar esa definición es la siguiente proposición:

\begin{prop} Dada una familia de funciones $\appl{f_n}{X}{(-∞,+∞)}$ medibles, entonces $\sup f_n$ y $\inf f_n$ son medibles.\end{prop}

Con esto, ya podemos dar la definición de función integrable, que es bastante sencilla:

\begin{defn}[Función\IS integrable] Se dice que $f$ es integrable si y sólo si $\int f \dif μ < ∞$.\end{defn}

Ahora sólo nos falta aprender a integrar funciones negativas. Para eso definiremos las dos partes de una función: \begin{align*}
f_+ &= \max\set{f,0} \\
f_- &= \max\set{-f, 0}\end{align*}, de tal forma que $f = f_+ - f_-$, $\abs{f}= f_+ + f_-$ y entonces su integral es \[ \int_X f \dif μ = \int_X f_+ \dif μ - \int_X f_- \dif μ \]

Análogamente podemos hacer la integral de funciones complejas descomponiendo \[ f = (\Re f)_+ - (\Re f)_- + \imath (\Im f)_+ - \imath (\Im f)_- \] e integrando igual que antes.

Vamos a ver ahora las propiedades de las integrales.

\begin{prop} Dada $\appl{f}{X}{[-∞,+∞]}$, entonces

\begin{enumerate}
\item $f$ es integrable si y sólo si $\abs{f}$ es integrable\footnote{Esto en Riemann no es cierto, ver el contraejemplo de la \fref{eq:IntRiemmanNoLeb}.}, esto es, que $\int_X \abs{f} \dif μ < ∞$. En este caso decimos $f ∈ \lebg^1\meas \equiv \lebg^1 (X) \equiv \lebg_μ^1 (X) \equiv \lebg^1$, según lo que demos por supuesto en cada caso, donde $\lebg^1$ es el espacio vectorial de las funciones absolutamente integrable.
\item La aplicación que lleva cada función a su integral $f\longmapsto \int_X f \dif μ$ es lineal. Es fácil ver que $αf + βg = α \int_X f \dif μ + β \int_X g\dif μ$.
\item \concept{Desigualdad\IS triangular en $\lebg^1$}. Dada $\appl{f}{X}{ℂ}$ absolutamente integrable, entonces $\abs{\int f \dif μ} ≤ \int \abs{f}\dif μ$.
\item \textbf{Comparación}. Si $0 ≤ f ≤ g$, entonces $0 ≤ \int f \dif μ ≤ \int g \dif μ$. Además, si $f ≥ 0$ entonces $\int f = 0$ si y sólo si $f = 0$ en casi todo punto\footnote{$f = 0$ en casi todo punto si $∃N ⊂ X$ tal que $μ(N) = 0$ con $f ≠ 0$ en $N$.}. En el otro sentido, si $\int\abs{f} = 0$ entonces $f \equiv 0$ en casi todo punto.
\item Una función integrable se puede modificar todo lo que se quiera en un conjunto de medida nula mantieniendo su integral constante. Esta propiedad será un problema para definir los espacios de funciones integrables, como veremos más tarde.
\item $f ∈ \lebg^1_μ(X;ℂ)$ entonces $f$ es finita en casi todo punto.
\item La integral de Lebesgue ``generaliza'' la de Riemman, en el sentido de que todas las fórmulas como integración por partes, jacobianos y cambios de coordenadas son las mismas.
\end{enumerate}
\end{prop}

\begin{proof}
\begin{enumerate}
\item % TODO.
\end{enumerate}
\end{proof}

Ahora vamos con varios teoremas sobre integración.

\begin{theorem}[Teorema\IS de Fubini-Tonelli] Dada $\appl{f}{X×Y}{ℂ}$ medible, entonces \[ \iint\limits_{X×Y} f \dif μ \dif ν = \int_X \underbrace{\left(\int_Y f(x,y) \dif ν(y)\right)}_{F_1(x)} \dif μ(x) = \int_Y \underbrace{\left(\int_X f(x,y) \dif μ(x)\right)}_{F_2(y)} \dif ν(y) \] con $F_1$, $F_2$ medibles.

Concretamente, Tonelli nos da la medibilidad para funciones no negativas y Fubini la condición de integrabilidad.

\label{thm:FubiniTonelli}
\end{theorem}

Otro teorema, que se corresponde con el tercer principio de Littlewood.

\begin{theorem}[Teorema\IS de Egorov] Dada $\appl{f}{X}{ℂ}$ medible y $f_n \convs f$ en casi todo punto en $A⊆X$, entonces $f_n \convs f$ casi uniformemente en $A$. Esto es, converge uniformemente salvo en un conjunto de medida cero. Más formalmente, $∀ε>0$ existe un $N_ε⊆A$ tal que $f_n \convs f$ en $A\setminus N_ε$ y $0 ≤ μ(N_ε) < ε$.
\end{theorem}

La prueba de este teorema es sencilla y bonita:

\begin{proof} Podemos asumir que $f_n \convs f$ en todo punto\footnote{TODO: Entender por qué.}. Entonces, para $k,n ∈ ℕ$ definimos \[ E_n(k) = \sum_{m=n}^∞ \set{x∈X \tq \abs{f_m(x) - f(x)} > \frac{1}{k}} \], es decir, el conjunto de puntos en los que la función se aleja del límite.
Para todo $k$ fijo, es fácil ver que $E_n(k) ⊇ E_{n+1}(k) ⊇ \dotsb $ y además $\bigcap_{n=1}^∞ E_n(k) = ∅$\footnote{Esto también queda TODO.}. Entonces, por convergencia dominada de las medidas, sabemos que $\lim_{n\to \infty} μ(E_n(k)) = μ\left(\bigcap E_n\right) = μ(∅) = 0$.

Ahora la prueba se acaba de forma bastante sencilla: dado $ε > 0$ y $k∈ℕ$, elegimos $m_k$ tal que $μ(E_n(k)) ≤ \frac{ε}{2^k}$. Entonces, definimos $E = \bigcup_{k=1}^∞ E_{n_k}(k)$, y entonces $μ(E) ≤ \sum_{n≥1} μ(E_{n_k}) ≤ \sum_{k≥1} \frac{ε}{2^k} ≤ ε$.

Así, si $n > n_k$ y $x ∉ E$, tendremos que $\abs{f_n(x) - f(x)} < \frac{1}{k}$.

En definitiva, lo que hemos hecho ha sido coger todos los puntos donde va a converger y hacer algo más que me he perdido completamente.
\end{proof}

\begin{theorem}[Teorema\IS de Lusin] Dada $\appl{f}{X}{ℂ}$ medible con $μ(X) < ∞$, entonces $f$ es casi continua, que como viene siendo hasta ahora significa que $f$ es continua salvo en un conjunto de medida cero. Esto es, que $∀ε>0$ existe un $N_ε⊆X$ tal que $f$ es continua en $X\setminus N_ε$ y $μ(N_ε) < ε$.
\end{theorem}

\subsubsection{Integración en coordenadas polares}

Además de las coordenadas lineales, en los reales podemos usar (y lo haremos habitualmente) las coordenadas polares o esféricas, esto es, coordenadas basadas en el uso de una esfera o equivalente en $ℝ^n$ en lugar del cuadrado/cubo/cosa. Veamos cómo generalizar las coordenadas polares sin necesidad de usar cosenos y senos por todas partes.

\begin{defn}[Coordenadas\IS polares] Sea $\set{x ∈ ℝ^n \tq \md{x} = 1} = \bbs^{n-1}$ la esfera unidad en $ℝ^n$. Entonces, si $x ∈ ℝ^n \minuszero$ las coordenadas polares de $x$ son
\begin{align*}
r  &= \md{x} ∈ (0,∞) \\
θ &= \frac{x}{\md{x}} ∈ \bbs^{n-1} \\ % TODO: Esta notación es caca.
\end{align*}
con el cambio de coordenadas dado por la aplicación biyectiva
\begin{align*}
\appl{Φ}{ℝ^n\minuszero&}{(0,∞)×\bbs^{n-1}} \\
x &\longmapsto (r,θ)
\end{align*} con inversa continua $\inv{Φ}(r,θ) = rθ$.
\end{defn}

Dadas estas coordenadas, necesitamos definir una medida. Lo fácil será hacerlo usando $ℝ^n$, que para eso sabemos medirlo perfectamente. Denotaremos por $μ_\ast$ la medida de Borel en $(0,∞)×\bbs^{n-1}$ inducida por Φ desde la medida $μ$ de Lebesgue en $ℝ^n$. En otras palabras, \[ μ_\ast (E) = μ(\inv{Φ}(E)) \]

Pero podemos definir esa misma medida de forma constructiva, desde el espacio de las coordenadas polares.

\begin{theorem} \citep[Teorema 2.49]{folland99} Sea $ρ=ρ_n$ la medida en $(0,∞)$ dada por $ρ(E) = \int_E r^{n-1}\dif r$. Entonces existe una única medida de Borel $σ = σ_{n-1}$ en $\bbs^{n-1}$ tal que $μ_\ast = ρ × σ$. Si $f$ es Borel-medible en $ℝ^n$ y $f ≥ 0$ o $f ∈ \lebg^1(μ)$, entonces
\(
\int_{ℝ^n} f(x) \dif x = \int_0^{∞} \int_{\bbs^{n-1}} f(rθ) r^{n-1} \dif σ(θ) \dif r
\label{eq:MedidaPolares} \)
\end{theorem}

\begin{proof} Es fácil ver que, si $f$ es la función característica de un conjunto (esto es, $f \equiv \ind_E$), entonces \eqref{eq:MedidaPolares} es simplemente otra forma de decir que $μ_\ast = ρ × σ$, y se puede generalizar a $f$ sin mucha dificultad\footnote{TODO: Generalízalo, anda, so vago.}. Entonces, sólo tenemos que construir σ.

Si $E$ es un conjunto de Borel en $\bbs^{n-1}$ y $a > 0$, sea \[ E_a = \inv{Φ}\left((0,a] × E\right) = \set{rθ \tq 0 < r ≤ a, θ ∈ E }\]

Dado que \eqref{eq:MedidaPolares} se tiene que cumplir cuando en particular $f = \ind_{E_1}$; tenemos que tener que \[ μ(E_1) = \int_0^1\int_E r^{n-1} \dif σ(θ) \dif r = σ(E) \int_0^1 r^{n-1} \dif r = \frac{σ(E)}{n} \]

Definimos entonces $σ(E) = n · μ(E_1)$. Dado que la aplicación $E\mapsto E_1$ lleva conjuntos de Borel a conjuntos de Borel y conmuta con la unión, intersección y comlemento, está claro\footnote{TODO: Esto debería mirarlo bien.} que σ es una medida de Borel en $\bbs^{n-1}$. Además, dado que $E_a$ es la imagen de $E_a$ por la aplicación $x \mapsto ax$, se sigue de \citep[Teorema 2.44]{folland99} que $μ(E_a) = a^n μ(E_1)$, y por lo tanto si $0 < a < b$, entonces \[ μ_\ast\left((a,b] × E\right) = μ(E_b \setminus E_a) = \frac{b^n-a^n}{n}σ(E) = σ(E) \int_a^b r^{n-1}\dif r = (ρ × σ) \left((a,b]×E\right)\]

Fijado un conjunto de Borel $E$ de $\bbs^{n-1}$, sea $\algbA_E$ la colección de uniones finitas y disjuntas de conjutnos de la forma $(a,b] × E$, que es un álgebra en $(0,∞)×E$ que genera la σ-álgebra $\mathcal{M}_E = \set{A×E \tq A ∈ \borel_{\bbs^{n-1}}}$. Por el cálculo anterior tenemos que $μ_\ast = ρ × σ$ en $\algbA_E$, y por ser la medida única entonces esta ecuación también se cuple en $\mathcal{M}_E$. Pero\footnote{Se me acaba el tiempo y no me apetece seguir copiando. TODO.}

\end{proof}

\begin{corol} Si $f$ es una función medible en $ℝ^n$ tal que $f(x) = g(\abs{x})$ para alguna función $g$ en $(0,∞)$, entonces \[ f(x) \dif x = σ(S^{n-1}) \int_0^{∞} g(r) r^{n-1}\dif r \]
\label{crl:MediblePolares1}
\end{corol}

\begin{prop} \citep[Proposición 2.53]{folland99} Si $a > 0$, entonces \[ \int_{ℝ^n} e^{-a\md{x}^2} \dif x = \left(\frac{π}{a}\right)^\frac{n}{2} \]
\end{prop}

\begin{proof} Denotamos la integral por $I_n$. Para $n=2$, por el \fref{crl:MediblePolares1} tenemos que \[ I_2 = 2π\int_0^∞ re^{-ar^2} \dif r = \eval{- \left(\frac{π}{a}\right) e^{-ar^2}}_0^∞ = \frac{π}{a} \]

Dado que $e^{-a\md{x}^2} = \prod_{j=1}^n e^{-ax_j^2}$, por el teorema de Tonelli (\ref{thm:FubiniTonelli}) tenemos que $I_n=(I_1) ^ n$. En particular $I_1 = (I_2)^{\frac{1}{2}}$, y la proposición sigue naturalmente.
\end{proof}

Este resultado nos permitirá calcular $σ(\bbs^{n-1})$ para todo $n$ en términos de la función $Γ$.

\begin{prop} \citep[Proposición 2.54]{folland99} \label{prop:MedidaSn} \[ σ(\bbs^{n-1}) = \frac{2π^\frac{n}{2}}{Γ(n/2)} \]
\end{prop}

Podemos usar esta proposició para sacar la medida de la esfera unitaria en $n$ dimensiones.

\begin{corol} \label{crl:MedidaBn} Sea $\bola_n = \set{x ∈ ℝ^n \tq \md{x} < 1}$. Entonces \[ μ(\bola_n) = \frac{π^{n/2}}{Γ\left(\frac{n}{2} + 1\right)} \]
\end{corol}

Para practicar la integración en polares, podemos ver algunas integrales. Por ejemplo, veamos \[ \int_{ℝ^n}\frac{1}{\md{x}^α} \dif x \] para $α ∈ ℝ$. Si usamos el \fref{crl:MediblePolares1}, con $f(x) = g(\md{x})$ para $g(r) = \frac{1}{r}$ tenemos que \[ \int_{ℝ^n}\frac{1}{\md{x}^α} \dif x = \frac{2π^{n/2}}{Γ(n/2)} \int_0^∞ g(r) r^{n-1} \dif r = \frac{2π^{n/2}}{Γ(n/2)} \int_0^∞ r^{n-1 - α} \dif r\]

Comprobar que $\int_{ℝ^N} \frac{1}{(1 + \abs{x})^α} < ∞$ para cierto $α$ encontrarlo (pone algo de cambio a polares) y luego que $\int_{B_1(0)} \frac{1}{\abs{x}^{β}} < ∞$ y $\int_{B_1(0)} \frac{1}{(1-\abs{x})^α}$.

\subsection{Convergencia de funciones}

A lo largo del curso, nos interesará estudiar y entender la convergencia de funciones. Sin embargo, como hemos visto en otros cursos, hay varios tipos de convergencia y varios resultados sobre lo que les ocurre a las funciones y a la integral.

Antes de entrar en materia, una pequeña definición que no hemos visto, que es la del supremo esencial. Es análogo a todo lo que estamos haciendo hasta ahora, extendiendo conceptos no a todos los puntos sin a todos ``salvo un conjunto de medida cero''. En el caso del supremo es un poco más delicado, así que vamos a hacerlo bien.

\begin{defn}[Supremo\IS esencial] Sea $\appl{f}{X}{ℝ}$ una función y \meas nuestro espacio de medida habitual. Definimos entonces el supremo esencial como \[ \essup_{x∈X} f(x) = \inf \set{a ∈ ℝ \tq μ\left(\inv{f}((a, ∞))\right) = 0} \]

En otras palabras, es el mayor valor de $f$ tal que por encima sólo hay un conjunto de valores de medida cero.
\end{defn}

Por ejemplo, es fácil ver que $\sup_{x∈(0,1)} x^n = 1$; pero $\essup_{x∈(0,1)}x^n$ ya es más complicado. Lo podemos ver sin cosas esenciales: \[ x^n \convs l(x) = \begin{cases} 0 & x < 1 \\ 1 & x = 1 \end{cases} \], converge puntualmente en todo punto pero no uniformemente (el límite no es continuo). Además, es fácil ver así que $\essup_{x∈(0,1)} x^n = 0$, ya que la función sólo es mayor que ese valor en un conjunto de medida cero (un único punto).

Después de esta ligeramente útil introducción, vamos a ver los teoremas.

\subsubsection{Teoremas de convergencia de funciones}

\begin{theorem}[Teorema\IS de convergencia monótona] (o teorema de Beppo-Levi) \label{thm:ConvMonotona}
Dado un espacio de medida \meas y una sucesión monótona de funciones $0 ≤ f_1 ≤ \dotsb ≤ f_n ≤ \dotsb$ medibles, entonces  \[ \int_X \lim_n f_n \dif μ = \lim_n \int_X f_n \dif μ \]

Se puede extender a sucesiones decrecientes simplemente cambiado el signo.
\end{theorem}

Hay una variación sutil de este teorema: es prácticamente lo mismo pero aun así lo veremos.

\begin{theorem}[Teorema\IS de convergencia monótona para series] Dada una sucesión de funciones $\appl{f_n}{X}{[0,+∞]}$ medibles y no negativas, entonces \[ \int_X \sum_x f_n \dif μ = \sum_n \int f_n \dif μ\]
\end{theorem}

¿Por qué decimos que es casi lo mismo? Si definimos las sumas parciales $S_N = \sum_{n=1}^N f_n$ tendremos una sucesión monótona creciente y podremos aplicar el \fref{thm:ConvMonotona}.

\begin{theorem}[Lema\IS de Fatou] \label{thm:Fatou} Dada una sucesión de funciones $0 ≤ f_n$ no negativas y medibles, entonces \[ \int_X \liminf_n f_n \dif μ ≤ \liminf_n \int_X f_n \dif μ \]
\end{theorem}

Para saber el $\limsup$, vamos al siguiente lema:

\begin{theorem}[Lema\IS de Fatou-Lebesgue] \label{thm:FatouInverso} (también llamado de Fatou inverso) Dada una sucesión de funciones $f_n$ $0 ≤ f_n ≤ g$, con $g$ integrable, entonces \[ \limsup_n \int f_n \dif μ ≤ \int_X \limsup f_n \dif μ \]
\end{theorem}

Para probar este lema, simplemente hay que aplicar el lema de Fatou normal (\ref{thm:Fatou}) a $F_n = g - f_n ≥ 0$.

\begin{theorem}[Teorema\IS de convergencia dominada] \label{thm:ConvDominada} Dada una sucesión $f_n \convs f$ en casi todo punto, si existe una función $g$ integrable y no negativa tal que $\abs{f_n} ≤ g$ entonces \[ \int_X \lim_n f_n \dif μ = \lim_n \int f_n \dif μ\]
\end{theorem}

\begin{proof}[Esquema]
Para la prueba del teorema, simplemente usaríamos los dos teoremas \ref{thm:Fatou} y \ref{thm:FatouInverso}. Si existe el límite de las integrales, existen los límites superior e inferior y tenemos las desigualdades. % TODO: Se deja como ejercicio al lector.
\end{proof}

\begin{theorem}[Teorema\IS de convergencia dominada para series] Dada una sucesión de funciones $f_n$ medibles tal que $f_n \convs f$ en casi todo punto. Entonces si $\sum_n \int \abs{f_n}\dif μ < ∞$, entonces  \[ \int_X \sum_n f_n \dif μ = \sum_n \int f_n \dif μ\]
\end{theorem}

Además de los teoremas, nos interesará ver varios ejemplos de series especiales que nos ``fastidien'' los teoremas.

Uno sencillo para Fatou es \( f_n(x) = \begin{cases} \frac{1}{n} & x ∈ [0,n] \\ 0 & x ∉ [0,n] \end{cases} \label{eq:ContrajemploFatou1} \), que convege puntualmente a 0 uniformemente. Sin embargo, $\int_ℝ f_n \dif x = 1$ para todo $n$. Entonces tenemos un menor estricto en la ecuación del lema de Fatou (\ref{thm:Fatou}).

Como ejercicio, tendríamos que ver qué le ocurre a la función \[ f_n(x) = \begin{cases} \frac{-1}{n} & x ∈ [n, 2n] \\ 0 & x ∉ [n, 2n] \end{cases} \], que converge uniformemente a cero pero la integral es -1 para todo punto. % TODO

Después de todo esto, vamos con el primer ``teorema de verdad del curso'', que nos va a dar el término que falta en el lema de Fatou. A modo de curiosidad, es un resultado que se ha obtenido en los años 80, bastante después de los resultados de Lebesgue y Fatou. Es además un teorema con muchas aplicaciones y con una prueba muy bonita.

\begin{theorem}\label{thm:LiebLoss}\citep[Teorema 1.9]{liebLoss01}, publicado en \cite{brezis1983relation}. Dado un espacio de medida \meas y una sucesión de funciones $\appl{f_n}{X}{ℂ}$ medibles, con $f_n \convs f$ en casi todo punto y $f$ medible\footnote{Esto viene gratis por ser $f_n$ medibles.}, y además\footnote{Pedimos una versión más general de la función dominante de otros teoremas: simplemente queremos todas las funciones de la familia sean similares en su integral (o algo así).} $\abs{f_n}^p$ es integrable y $\int \abs{f_n}^p ≤ C$ para todo $n$. Entonces \[ \lim_{n\to ∞ } \int_X\abs{\abs{f_n}^p - \abs{f}^p - \abs{f_n - f}^p} \dif μ = 0\]
\end{theorem}

¿Qué tiene que ver ese churro con el lema de Fatou? Si recordamos la desigualdad triangular ($\abs{\abs{a} - \abs{b}} ≤ \abs{a-b}$), tomando $a = \abs{f_n}^p - \abs{f}^p$ y $b = \abs{f_n -f}^p$, tendremos que \[ \lim_f \int \abs{\abs{f_n}^p - \abs{f}^p} - \lim \int \abs{f_n - f}^p ≤ \lim \int \abs{\abs{f_n}^p - \abs{f}^p - \abs{f_n - f}^p} \dif μ = 0\], por lo que quitando también los límites nos quedará \[ \int \abs{\abs{f_n}^p - \abs{f}^p} = \int\abs{f_n -f}^p + o(n) \] con $o(n) \convs 0$. Esto nos quedará claro si lo vemos con el contraejemplo de Fatou de \eqref{eq:ContrajemploFatou1}, viendo cómo la diferencia entre los términos nosequé.

En la fórmula de Fatou, nos quedará un ``resto'' (igual que el $o$ de Taylor): \[ \liminf \int \abs{f_n}^p ≥ \int \liminf \abs{f_n}^p + \int \abs{f-f_n}^p + o(n) ≥ 0 \]

Ahora, un poquito de notación:

\begin{defn}[Función\IS absolutamente integrable de orden $p$] Decimos que $f$ es absolutamente integrable de orden $p$ con $p > 0$ si y sólo si $\abs{f}^p$ es absolutamente integrable, esto es, que \[ 0 ≤ \int_X \abs{f}^p \dif μ < ∞ \]. Por notación, diremos que $f ∈ \lebg^p\meas$
\end{defn}

También es interesante ver que los espacios $\lebg^p$ son espacios vectoriales. Si tomamos $α,β ∈ ℂ$ y $f,g ∈ \lebg^p$, entonces querremos ver si $αf + βg ∈ \lebg^p$. Operando, podemos descomponer \[ \int\abs{αf+βg}^p \dif μ ≤ C_p \int \left(\abs{αf}^p + \abs{βg}^p\right) ≤ C_p \left(\abs{α}^p \int \abs{f}^p + \abs{β}^p \int \abs{g}^p \right) < ∞ \] y ver que efectivamente la combinación lineal también es absolutamente integrable.

Se ha tirado mucho rato insistiendo así que yo lo escribo: cuidado con confundir entre la convergencia en \textit{casi} todo punto y la convergencia en \textit{todo} punto.

Y ahora un \textit{mindfuck}: ¿por qué necesitamos la integral de Lebesgue? ¿Por qué puñetas querríamos hacer integrales de monstruos como el de $\ind_{ℝ\setminus ℚ}$? Pues por lo siguiente: \[ \lim_{m,n\to \infty} \left(\cos (n!x)\right)^m = \ind_{ℝ\setminus ℚ}\]

Resulta que, cosas de las mates, algunos límites de funciones continuas no son continuos, sin convergencia uniforme, y Lebesgue nos soluciona estos problemas.

Ahora vamos con la prueba del \fref{thm:LiebLoss}. Antes, alguna desigualdad que necesitaremos:

\begin{prop}[Desigualdad\IS de Young] \label{prop:Young} Dados $p > 0$ y $a,b≥0$, entonces \[ (a+b)^p ≤ C_p (a^p + b^p)\]
\end{prop}

\begin{proof} Podemos simplificar la desigualdad dividiendo entre $b^p$ en ambos lados, y nos queda \[ \left(\frac{a}{b} + 1\right)^p ≤ C_p \left(\left(\frac{a}{b}\right)^p + 1\right) \]

Si hacemos un cambio de variable $t = \frac{a}{b}$, entonces podemos definir dos funciones \begin{align*}
f(t) &= (t+1)^p \\
g(t) &= C_p(t^p + 1)
\end{align*} y sólo nos queda demostrar que $f(t) ≤ g(t)$ para $t ≥ 0$. Para $p=1$ la desigualdad es trivial. Para $p ∈ (0,1)$ algo que me he perdido.

Cuando $p > 1$, vemos que para $t=0$, $f(0) = 1$ y $g(0) = C_p$, luego tiene que ser $C_p ≥ 1$. Si derivamos, tenemos que \[ p(t+1)^{p-1} ≤ pC_p t^{p-1} \]

Como la desigualdad de Young es simétrica, nos vale demostrarlo que $t ≥ 1$ (si $t<1$, cambiamos $a$ por $b$ a la hora de despejar). Un salto raro aquí, y ahora vemos que para $t = 1$ nos queda que $C_p ≥ 2^{p-1}$.
\end{proof}

\begin{prop} Dados $a,b∈ℂ$, para todo $ε> 0$ y $p > 0$, se tiene que
\( \abs{\abs{a+b}^p - \abs{b}^p} ≤ ε\abs{b}^p + C_{ε,p} \abs{a}^p \label{eq:ProofLiebLossDesNum} \) \label{prop:DesigualdadLiebLoss}
\end{prop}

\begin{proof} Al igual que en la desigualdad anterior, podemos llamar $t = \frac{a}{b}$ y suponer $t ≥ 0$. Entonces, tenemos que demostrar (podemos quitarnos un valor absoluto por ahí) que \begin{align*}
\abs{x+1}^p - 1 &≤ ε + C_{ε,p}\abs{x}^p \\
\abs{x+1}^p - C_{ε,p}\abs{x}^p - 1 &≤ ε
\end{align*}

Buscamos el máximo de la parte izquierda de la ecuación, así que derivamos con respecto a $x$ y buscamos la solución a \[ p(x+1)^{p-1} = p C_{ε,p} x^{p-1} \]

Ahora despejando tenemos que \[ x = \frac{1}{C_{ε,p}^{\frac{1}{p-1}} - 1} \] y sustituyendo sale.

Luego hay otra prueba, la forma rápida de hacerlo. Sabemos que una función convexa es una $f$ tal que $f\left((1-λ)A + λB\right) ≤ (1-λ) f(A) + λf(B)$ para todo $λ∈[0,1]$. Ponemos entonces \[ A = \frac{a}{1-λ} \quad B = \frac{b}{λ} \] y nos queda que \[ f(a+b) ≤ (1-λ) f\left(\frac{a}{1-λ}\right) + λ f\left(\frac{b}{λ}\right) \]

Ahora proponemos que $f(t) = \abs{t}^p$ es convexa, con $t ∈ ℂ$. En ese caso, sustituimos, nos sale $ε = λ^{1-p}$ y sale.
\end{proof}

Vamos ahora con la prueba del teorema de verdad.

\begin{proof}
Para el primer paso, recordamos la desigualdad numérica de la \fref{prop:DesigualdadLiebLoss}. Para el segundo paso necesitaremos una función especial para usar en ella el teorema de convergencia dominada (\fref{thm:ConvDominada}). Entonces escribiremos la serie como $f_n ≝ f + g_n$, por lo que $g_n \convs 0$.

Entonces, declaramos (hay que probarlo) que
\( \underbracket{G_{ε,n}}_{(A)} = \left[\underbracket{\abs{\abs{f+g_n}^p - \abs{g_n}^p - \abs{f}^p}}_{(B)} - ε\abs{g_n}^p\right]_+ \convs 0
\label{eq:ProofLiebLoss1} \)
para todo $ε > 0$, donde $[F]_+ = \max \set{F, 0}$.

Ahora, fijamos $ε > 0$, y entonces tendremos que \[ \lim_{n\to ∞} \int G_{n,ε} = 0\], más que nada porque como decíamos $g_n \to 0$ CTP\footnote{En casi todo punto.}.

Seguimos:
\(
0 ≤ \int \underbracket{\abs{\abs{f+g_n}^p - \abs{g_n}^p - \abs{f}^p}}_{(B)} ≤ ε \int \abs{g_n}^p + \underbracket{\int G_{ε,n}}_{(A)}
\label{eq:ProofLiebLoss2} \)
, donde lo único que hemos hecho ha sido reestructurar \eqref{eq:ProofLiebLoss1}.

Anteriormente hemos dicho que $(A) \to 0$, así que sólo nos queda saber qué pasa con $ε\int\abs{g_n}^p$. Para ello, usaremos la hipótesis que nos queda del teorema, y es que sabemos que
\(
\int_X \abs{f_n}^p \dif μ ≤ C < ∞
\label{eq:ProofLiebLossH2} \)
, así que podemos estimar
\(
\int\abs{g_n}^p = \int \abs{f_n -f}^p ≤ C_p \int \left(\abs{f_n}^p + \abs{f}^p \right) \eqreasonup[≤]{\eqref{eq:ProofLiebLossH2}} 2 C C_p = \bar{C}
\label{eq:ProofLiebLoss3} \)

Tomando límites en \eqref{eq:ProofLiebLoss2} y sustituyendo con \eqref{eq:ProofLiebLoss3}, tenemos \[ 0 ≤ \lim_n \int \abs{\abs{f+g_n}^p - \abs{g_n}^p - \abs{f}^p} ≤ ε\bar{C} + \underbracket{\lim_n \int G_{ε,n}}_{\to 0} \]

Ponemos ahora $ε \to 0^+$ y nos quedará (sándwich) \[ \lim_n \abs{\abs{f+g_n}^p - \abs{g_n}^p - \abs{f}^p} = 0 \]

Ya lo tenemos hecho salvo por los pasos intermedios. Tenemos que demostrar \eqref{eq:ProffLiebLoss2}. Usando la desigualdad triangular, sabemos que
\[ \abs{\abs{f+g_n}^p - \abs{g_n}^p - \abs{f}^p} ≤ \abs{\abs{f+g_n}^p - \abs{g_n}^p} + \abs{f}^p \]

Ahora podemos usar la desigualdad numérica de \eqref{eq:ProofLiebLossDesNum} con $a = f$, $b = g_n$ y nos quedará que \[ \abs{\abs{f+g_n}^p - \abs{g_n}^p} + \abs{f}^p  ≤ ε\abs{g_n}^p + C_ε \abs{f}^p + \abs{f}^p \]

Entonces, \[ 0 ≤ G_{ε,n} ≤ (1 + C_ε) \abs{f}^p \] y podemos usar el teorema de convergencia dominada (\fref{thm:ConvDominada}) para decir que $\int G_{ε,n} \convs 0$.
\end{proof}

\subsubsection{Modos de convergencia}

Vamos a ver ahora algunos tipos de convergencia de funciones que nos serán muy útiles para ver contraejemplos y hacer pruebas. Esto está inspirado en \citep[Sección 2.4]{folland99}. En todos los casos, vamos a ver definiciones para decir si una sucesión $f_n$ converge a $f$ en los diferentes modos.

\begin{defn}[Convergencia\IS en sentido integral $L^p$] Si y sólo si \[ \int_X \abs{f_n - f}^p \dif μ \to 0\] \label{def:ConvLp}
\end{defn}

\begin{defn}[Convergencia\IS en sentido uniforme $C^0$] Si y sólo si \[ \sup_{x∈X} \abs{f_n(x) - f(x)} \to 0 \]
\end{defn}

\begin{defn}[Convergencia\IS en sentido esencialmente uniforme $L^{∞}$] Si y sólo si \[ \essup_{x∈X} \abs{f_n(x) - f(x)} \to 0\]
\end{defn}

\begin{defn}[Convergencia\IS en sentido casi uniforme] Si y sólo si $f_n\to f$ uniformemente en $X\setminus N_ε$ con $μ(N_ε) < ε$ para todo $ε > 0$
\end{defn}

\begin{defn}[Convergencia\IS puntual] Si y sólo si $∀x∈X$ $f_n(x) \to f(x)$.
\end{defn}

\begin{defn}[Convergencia\IS en casi todo punto] \label{def:ConvCTP} Si y sólo si $f_n(x) \to f(x)$ $∀x∈X \setminus N$ y $μ(N) = 0$.
\end{defn}

\begin{defn}[Convergencia\IS en medida] \label{def:ConvMedida} Si y sólo si $∀ε>0$ se tiene que \[ μ\left(\set{x∈X \tq \abs{f_n(x) - f(x)} > ε}\right) \convs 0\], esto es, que la medida del conjunto donde no converge tiende a ser nula.
\end{defn}

\begin{defn}[Sucesión\IS de Cauchy en medida] Si y sólo si $∀ε>0$ se tiene que \[ μ\left(\set{x∈X \tq \abs{f_n(x) - f_m(x)} > ε}\right) \convs[][n,m] 0\]
\end{defn}

Normalmente, cada una de estas definiciones suele tener un lema, un teorema o una definición detrás. Por ejemplo, ¿es cierto que la convergencia en casi todo punto (\fref{def:ConvCTP}) implica la convergencia $L^1$ (\fref{def:ConvLp})? En general, esto es falso, pero si $\abs{f_n} ≤ g ∈ L^1$ podemos usar el \nref{thm:ConvDominada}. Si en cambio tenemos una sucesión monótona, podemos usar el \nref{thm:ConvMonotona}.

En el otro sentido, ¿es esto cierto? En general no, pero sí que existe una subsucesión $f_{n_k} \convcs[n_k] f$.

\begin{prop}[Desigualdad\IS de Markov] \[ μ\left(\set{x∈X \tq \abs{f_n(x) -f} > ε }\right) ≤ \frac{1}{ε} \int_X \abs{f_n -f} \dif μ \]
\end{prop}

Para ver esta desigualdad, podemos considerar $f_n - f = f$ y nos da lo mismo. Podemos definir entonces \[ f_ε(x) = \begin{cases} ε & f(x) ≥ ε \\ 0 & f(x) < ε \end{cases} \], y entonces la integral es \[ \int_X f(x) \dif μ ≥ \int_X f_ε(x) = \int_{f(x) < ε} 0 + ε \int_{f(x) ≥ ε} \dif μ \]. En otras palabras, lo que hacmos es simplificar la función y luego usar una desigualdad muy estúpida para quitarnos la parte de medida cero.

Antes de seguir, una caracterización de la convergencia en medida que está escrita en los ejercicios:

\begin{prop} Dada una sucesión $f_n$, $f_n \convs f$ en medida si y sólo si \[ \dst(f, f_n) = \int_X \frac{\abs{f_n -f}}{1 + \abs{f_n - f}} \dif μ \convs 0 \]
\end{prop}

\begin{proof} Vamos a empezar probando la implicación hacia la izquierda. Sea $M_n = \set{x ∈ X \tq \abs{f_n -f} > ε}$, el ``conjunto malo''. Entonces \[ \int_X \frac{\abs{f_n-f}}{1 + \abs{f_n -f}} \dif μ ≥ \int_{M_n} \frac{\abs{f_n-f}}{1 + \abs{f_n -f}} \eqreasonup[≥]{Por ser $\frac{x}{1+x}$ func. creciente} \int_{M_n} \frac{ε}{1 + ε} \dif μ = \frac{ε}{1 + ε} μ(M_n) \]

Sabiendo que la integral converge a 0 con $n$ para cada ε, tiene que ser $μ(M_n) \convs 0$ y por lo tanto se cumple la definición de \nref{def:ConvMedida}.

Para ir al otro lado, suponemos que $f_n \convs f$ en medida. En ese caso \[ \int_X \frac{\abs{f_n-f}}{1 + \abs{f_n -f}} \dif μ = \int_{M^c_n} \frac{\abs{f_n-f}}{1 + \abs{f_n -f}} \dif μ + \int_{M_n} \underbracket{\frac{\abs{f_n-f}}{1 + \abs{f_n -f}}}_{≤1} \dif μ ≤ \int_{M^c_n} \frac{\abs{f_n-f}}{1 + \abs{f_n -f}}\dif μ + μ(M_n) \]


\end{proof}

\begin{figure}[hbtp]
\inputtikz{MapaConvergencias}
\caption{Mapa de las diferentes convergencias y las relaciones entre ellas.}
\label{fig:MapaConvergencias}
\end{figure}


\paragraph{Algunos contraejemplos interesantes} Es posible buscar ciertos ``monstruos'' que nos van a aclarar ciertas cosas en cuanto a modos de convergencia:

\begin{align}
f_n &= \ind_{(0,n)} \label{eq:Conv:CE1} \\
f_n &= \ind_{(n, n+1)} \label{eq:Conv:CE2} \\
f_n &= n \ind_{[0, \frac{1}{n}]} \label{eq:Conv:CE3} \\
f_n &= \ind_{\left(\frac{j}{2^k}, \frac{j+1}{2^{k+1}}\right)}\quad 0≤j<2^k, \; n = j + 2^k \label{eq:Conv:CE4}
\end{align}

La primera ecuación, \eqref{eq:Conv:CE1}, converge uniformemente a 0 en $ℝ$. Ahora bien, es fácil ver que su integral es $1$ para todo $n$. En otras palabras, tenemos convergencia uniforme pero no en la integral. ¿Qué ocurre aquí?

La cuestión es que \eqref{eq:Conv:CE1} tiene un soporte que se dispara a infinito. En este caso, hemos visto los teoremas de convergencia dominada (\fref{thm:ConvDominada}) y monótona (\fref{thm:ConvMonotona}), pero, ¿por qué no podemos aplicarlos? El de convergencia monótona es imposible porque no tenemos monotonía, y el de dominada tampoco porque no hay ninguna función integrable que lo domine. Una función dominante sería $\frac{1}{1 + \abs{x}}$, pero nuestro problema es que no es integrable cuando el soporte es infinito.

Vamos con las otras ecuaciones. \eqref{eq:Conv:CE2} tiene convergencia puntual a $0$, pero con integral 1 (siempre tenemos el mismo escalón pero puesto en otro lugar del eje). Tenemos de nuevo el problema del soporte, que podríamos quitarnos si considerásemos un soporte finito: en ese caso, para $n$ suficientemente grande el intervalo se nos iría fuera del soporte y tendríamos integral cero.

La siguiente ecuación, \eqref{eq:Conv:CE3}, que converge a cero en casi todo punto, nos da un ejemplo de lo contrario: concentración del soporte.\footnote{TODO: Habría que darle alguna vuelta a esto porque creo que me he perdido algo.}

% TODO: Dibujar esta mierda de ecuación.
El caso de la última ecuación \eqref{eq:Conv:CE4}, que a veces se le llama la ``máquina de escribir'', es más interesante todavía. Empezamos por ver que es convergente a $0$ en $L^1$ (ver \fref{def:ConvLp}). Si integramos, \[ \int_{ℝ} \abs{f_n} \dif x = \int_0^1 \ind_{\left(\frac{j}{2^k}, \frac{j+1}{2^{k+1}}\right)} = \int_{\frac{j}{2^k}}^{\frac{j+1}{2^k}} \dif x = \frac{1}{2^k} \convs 0 \]

En ese caso, la teoría nos dice que existe una subsucesión $f_{n_k} \convs 0$ en CTP. Lo que no existe es convergencia puntual de \textit{toda} la sucesión. Si fijamos $x ∈ [0,1]$, por ejemplo, $x = 0$, tenemos que \[ f_n (0) = \begin{cases} 1 & \text{para ciertos } n\\ 0 &\text{para otros} \end{cases} \], esto es, la sucesión sería $f_n(0) = \{1,0,1,\underbracket{0,0,0}_{2^2 - 1},1,\underbracket{0,0,0,0,0,0,0}_{2^3 - 1},1,\dotsc\}$ y no va converger puntualmente. Ahora, es fácil ver que hay una subsucesión que sí converge a 0 (y otra que sí converge a $1$, que es la formada por los valores de las posiciones $2^k$).

\subsection{Medidas con signo}

Vamos a trabajar ahora con un espacio medible \meds. Consideramos $\mpos\meds$ como el espacio de medidas positivas (no negativas). Más formalmente, \[ \mpos\meds = \set{\appl{μ}{\algb{X}}{[0, +∞]} \tq μ(∅) = 0,\, μ \text{ es σ-aditiva}} \]

El primer ejemplo es el siguiente. Tomaremos $m ∈ \mpos\meds$, y construimos la \concept{Medida\IS asociada a una función medible} $\appl{f}{X}{[0,∞]}$ como $\appl{m_f}{\algb{X}}{[0,+∞)}$ dada por \( m_f(E) = \int_X \ind_E f \dif m = \int_E f \dif m \label{eq:MedidaAsocFuncion} \)

De una forma más tradicional, lo que hemos escrito es equivalente al $f \dif x$. Tendríamos además que $\dif m_f = f \dif m$, e incluso podríamos hacer una manipulación formal y definir una ``derivada'' un tanto extraña tal que \[ \deriv{m_f}{x} = f \]

Pero tranquilos que todavía no nos hemos convertido en físicos pervirtiendo derivadas. A eso le podremos dar una definición estricta que tenga sentido para medidas multidimensionales y que nos dé la densidad de medida de una medida.

Una pequeña definición antes de seguir:

\begin{defn}[Medida\IS σ-finita] Dado un espacio medible \meds y una medida μ, se dice que μ es σ-finita si y sólo si existe un conjunto numerable $\algb{U} ⊆ \algb{X}$ tal que su unión recubre todo $X$ y $μ(U) < ∞$ para todo $U ∈ \algb{U}$. \end{defn}

Ahora, un teorema-ejercicio.

\begin{theorem} \citep[Signed Measures]{terence10} Sea μ una medida σ-finita. Dadas dos funciones $\appl{f,g}{X}{[0,∞]}$, entonces $m_f = m_g \iff f = g$ en casi todo punto.\end{theorem}

\begin{proof}
% TODO
\end{proof}

Vamos a empezar con la definición esa de derivadas de medidas. De momento, una definición temporal:

\begin{defn}[Medida\IS derivable] Decimos que $μ∈\mpos$ es derivable con respecto a $m$ si y sólo si $\dif μ = f \dif μ$ con $\appl{f}{X}{[0,∞]}$ $m$-medible.

En ese caso, $f$ será la \concept{Derivada\IS de Radon-Nikodin} dada por \[ \deriv{μ}{m} = f \]
\end{defn}

\begin{prop}
Sea $μ$ una medida de Lebesgue en $X = [0,+∞] = ℝ_+$. Sea $μ ∈ \mpos\meas[X][m]$. Sea $\deriv{μ}{m} = f$ la derivada de Radon-Nikodin. Si $f$ es continua, entonces la función \[ F(x) = μ([0,x]) = \int_0^x \dif μ = \int_0^x f \dif x \] es derivable en sentido clásico y $\deriv{F}{x} = \deriv{}{x}μ([0,x]) = f(x)$.
\end{prop}

Un ejercicio: dado $x∈ℝ^n$, consideramos la \concept{Delta\IS de Dirac} $δ_x$ en $x$, dada por \[ δ_x(E) = \begin{cases} 1 & x ∈ E \\ 0 & x ∉ E \end{cases} \], entonces las únicas medidas derivables en el sentido de Radon-Nikodin respecto a $δ_x$ son las medidas $cδ_x$ para $c≥0$. La idea heurística es que si la medida μ que consideramos no vale $0$ en los mismos puntos que $δ_x$, valdrá infinito y no será derivable.

Ahora sí, vamos a ir a por las medidas con signo. Vamos a recordar una cosa: si $μ,ν ∈ \mpos\meas[X][m]$, entonces es fácil definir operaciones como la suma y la multiplicación por números $c ∈ ℝ$: \begin{align*} (μ+ν)(E) &= μ(E) + ν(E) \\ (cμ)(E) &= c μ(E) \end{align*} y, además, si $μ≤ν$, entonces $μ(E) ≤ ν(E) \, ∀E ∈ \algb{X}$. Esto se puede ver también en el caso concreto de medidas asociadas a funciones medibles.

Nos gustaría que este espacio de medidas fuese un espacio vectorial, pero tenemos un problema: con una combinación lineal de medidas positivas podemos sacar una medida ``negativa'' y salirnos del espacio. Tenemos que ampliarlo pues de alguna forma.

Podemos hacer una primera definición de $σ$ una medida con signo tal que \[ σ(E) ≝ μ(E) - ν(E) \] con $μ,ν$ medidas positivas. Ahora bien, tendríamos un problema y es ver qué pasa cuando $μ(E),ν(E)$ son infinitos. La idea, más bien barata, es obligar a que una de las dos sea finita. Vamos a definirlo bien.

\begin{defn}[Medida\IS con signo] Una medida con signo es una función \begin{align*}
	\appl{μ}{\algb{X}&}{[-∞,+∞]} \\
	E &\longmapsto μ(E)
\end{align*} cumpliendo las dos siguientes propiedades :
\begin{enumerate}
\item $μ(∅) = 0$.
\item $μ$ puede ser $+∞$ pero no $-∞$ (o viceversa).
\item $μ$ es σ-aditiva. Esto es, dada $\set{E_n}⊆X$ familia de conjuntos disjuntos dos a dos, entonces \[ \sum_{n=1}^∞ μ(E_n) = μ\left(\bigcup_{n=1}^∞ E_n\right) \]
\end{enumerate}
\end{defn}

El primer ejemplo de medidas con signo es la resta de dos medidas positivas siendo una finita. Pero podemos preguntarnos si todas las medidas son de esta forma. La respuesta es sí, pero es dificilillo de demostrar así que veremos sólo el teorema.

\begin{theorem}[Teorema\IS de descomposición de Hahn] Sea $μ∈ \msgn\meds$. Entonces existe una partición de $X = X_+ ∪ X_-$ tal que $\restr{μ}{X_+} ≥ 0$ y $\restr{μ}{X_-} ≤ 0$, y con $X_+$ conjutno totalmente positivo y $X_-$ conjunto totalmente negativo, únicos salvo conjuntos de medida cero.

La restricción de una medida la definimos como, dado $A ∈ \algb{X}$, definimos $\restr{\algb{X}}{A} ≝ \set{X ∈ \algb{X} \tq X ⊆ A}$ y $\restr{μ}{A}(E) = μ(E∩A)$.
\end{theorem}

\begin{defn}[Conjunto\IS totalmente positivo/negativo] $X_{+/-}$ es un conjunto totalmente positivo/negativo  respecto a $μ$ si $∀E ∈ \algb{X}$ con $E ⊆ X_{+/-}$ se tiene que $μ(E) ≥ 0$ (o $μ(E) ≤ 0$).\end{defn}

El teorema nos demuestra la pregunta de antes: siempre tendremos dos medidas $\restr{μ}{X_+} ≥ 0$ y $-\restr{μ}{X_-} ≥ 0$ que podremos combinar.

Ahora bien, a pesar de eso no debemos pensar que el espacio de medidas (que denotaremos por $\msgn\meds$) no es un espacio vectorial. Definamos por ejemplo $μ = m_1 - m_2$ y $\bar{μ} = m_2 - m_3$, con $m_1,m_3$ infinitas y $m_2$ finita. Si sumamos $μ + \bar{μ}$, la cosa se nos queda indefinida. ¿Qué podemos hacer si queremos tener un espacio vectorial? Pues usar sólo medidas finitas con signo. Este \concept{Espacio\IS vectorial de medidas finitas} se denotará por $\mfin\meds$.

\textit{TODO: aquí faltan dos clases.}

\begin{defn}[Medida\IS singular] Se dice que dos medidas $μ,ν$ son singulares una respecto de la otra (denotado por $μ\perp ν$) si y sólo si\footnote{Por si no nos acordamos, $\mop{sop} μ = \set{E ∈ \algb{X} \tq μ(E) ≠ 0}$.} $\mop{sop} μ ∩ \mop{sop} ν = ∅$. Equivalentemente, esto ocurre si y sólo si $\restr{μ}{\mop{sop} ν}(E) = 0$ para todo $E$.
\end{defn}

\begin{theorem}[Teorema\IS de Lebesgue-Radon-Nikodin] Sea $μ ∈ \msgn\meas[X][m]$ medida con signo. Entonces existe una \textbf{única} descomposición \[ μ = m_f + μ_s \] donde $f ∈ L^1\meas[X][m]$ (esto es, $f$ es absolutamente integrable respecto a $m$) y $m_s \perp m$. Si μ es positiva no negativa, entonces $m_f$ y $μ_s$ lo son.

Recordamos que $m_f$ es la medida asociada a una función medible y ya vimos en qué consiste en la \fref{eq:MedidaAsocFuncion}.
\end{theorem}

Por ejemplo, podemos tomar $μ = \lebg_{ℝ^n} + δ_0$, donde $\lebg_{ℝ^n}$ es la medida de Lebesgue (la habitual) en $ℝ^n$, que se puede expresar como la medida asociada a la función constante $f = 1$.

\begin{theorem}[Corolario\IS de Radon-Nikodin] \label{thm:CorolRadonNikodin} Sea \meas un espacio de medida, con $m ≥ 0$. Entonces las siguientes afirmaciones son equivalentes:

\begin{enumerate}
\item $μ = m_f$ para una $f ∈ L^1\meas$.
\item $μ(E) = 0$ cuando $m(E) = 0$.
\item $∀ε > 0\; ∃δ > 0$ tal que $μ(E) < ε$ si $m(E) < δ$.
\end{enumerate}
\end{theorem}

\begin{defn}[Medida\IS absolutamente continua] Cuando una de las tres condiciones del teorema anterior \eqref{thm:CorolRadonNikodin} se cumple, decimos que μ es absolutamente continua con respecto a $μ$, y se denota por $μ \ll m$.
\end{defn}

En definitiva, lo que nos dice ese teorema es que las medidas absolutamente continuas cambian la meidad de los conjuntos, pero de una forma regular. Por ejemplo, nos dice algo muy intuitivo que es que, si tenemos $μ$ una medida asociada a una función $f$, entonces si $m(E) = 0$ la medida asociada, que no es más que la integral, tendrá que ser cero igualmente: \[ μ(E) = m_f(E) = \int_E f\dif m = 0 \]

Un ejemplo es que la delta de Dirac no es absolutamente continua con respecto a la medida de Lebesgue $\lebg_{ℝ^n}$. Es obvio que $\lebg_{ℝ^n} (\set{x}) = 0\; ∀x ∈ ℝ^n$, pero $δ_0(\set{0}) = 1$, luego no se cumplen las condiciones.

Esta definición nos da un corolario que no es más que una reformulación de los resultados de Radon-Nikodin, pero que en realidad es un teorema que descubrió Lebesgue independientemente.

\begin{theorem}[Teorema\IS de descomposición de Lebesgue] \label{thm:DescLebesgue} Sea \meas espacio de medida, con $m ≥ 0$. Sea $μ ∈ \msgn\meas$ medida con signo. Entonces, existe una única descomposición \[ μ = μ_{AC} + μ_S \] donde $μ_{AC}$ es absolutamente continua con respecto a $m$ ($μ_{AC} \ll m$) y $μ_s \perp m$.
\end{theorem}

Aquí se cumple de nuevo que si $μ$ es positiva ($μ ≥ 0$) entonces $μ_{AC}$ y $μ_S$ también lo son. Sólo hay que ver que como $μ_S \perp μ_{AC}$, entonces tienen soportes disjuntos. Si existiese un conjunto $E$ tal que $μ_S(E) < 0$, entonces $μ_{AC}(E) = 0$ por tener soportes disjuntos, por lo que tendríamos $μ < 0$, contradicción.

Una cosa que hemos pasado por encima en ese ejemplo es que el teorema sólo nos dice que $μ_S \perp m$, no que $μ_S \perp μ_{AC}$. La cuestión es que si tenemos $E ∈ \algb{X}$, con $E ⊆ \mop{sop}(μ_{AC})$, entonces pasa algo. % TODO: Algo qué?

Si trabajamos con medidas continuas, podemos refinar la descomposición:

\begin{defn}[Medida\IS continua] Dada una medida $μ$ con cada punto $x ∈ X$ medible, se dice que μ es continua si y sólo si $μ(\set{x}) = 0$.
\end{defn}

\begin{prop} En las mismas condiciones del \nref{thm:DescLebesgue} y además con $μ$ continua, existe una única descomposición \[ μ = μ_{AC} + \underbracket{μ_{SC} + μ_{PP}}_{μ_S} \], donde $μ_{SC}$ es una medida singular continua y $μ_{PP}$ es una \concept{Medida\IS pure-point}, esto es, suma de deltas de Dirac.
\end{prop}

Para verlo bien, podríamos definir una medida \[ μ = \lebg_{ℝ^2} + \lebg_{ℝ} + δ_{0} \], de tal forma que tendríamos que la dimensión del soporte de $\lebg_{ℝ^2}$ es mayor que al de $\lebg_{ℝ}$.


\section{Espacios de Banach y Hilbert. Espacios $L^p$}

\section{SOC en espacio de Hilbert. Series de Fourier}

\section{Transformada de Fourier}

\section{Introducción a la teoría espectral}

\section{La ecuación del calor}

%% Apéndices (ejercicios, exámenes)
\appendix

\chapter{Ejercicios}
\input{tex/VariableReal_Ejs.tex}

\nocite{terence10,folland99}

\bibliography{../Apuntes}{}
\printindex
\end{document}
