\subsection{Tema 1 - Estadística descriptiva}

\begin{problem}[2] Demostrar que \[ \sum_{i=1}^n \left(x_i-\avg{x}\right)^2 = \min_{a\in \real} \sum_{i=1}^n(x_i-a)^2 \]

\solution

Definimos una función \[ g(a) = \sum_{i=1}^n(x_i-a)^2 \], buscamos su derivada \[ g'(a) = -2 \sum_{i=1}^n(x_i-a) \] e igualamos a cero:

\begin{gather*}
-2 \sum_{i=1}^n(x_i-a) = 0 \\
\sum_{i=1}^n x_i - \sum_{i=1}^n a = 0 \\
n \avg{x} = n a \\
\avg{x} = a 
\end{gather*}

Esto quiere decir que la media muestral es el valor que minimiza la distancia con cada uno de los datos de la muestra.
\end{problem}

\begin{problem}[5]Determina si es verdadero o falso:

\ppart Si añadimos 7 a todos los datos de un conjunto, el primer cuartil aumenta en 7 unidades y el rango intercuartílico no cambia.

\ppart Si todos los datos de un conjunto se multiplican por -2, la desviación típica se dobla.
\solution 

\spart Añadir siete a todos los datos es una traslación, así que la distribución de los datos no cambia.

\spart Teniendo en cuenta que si multiplicamos todos los datos del conjunto por $-2$ la media también se multiplica por $-2$, y sustituyendo en la fórmula de la varianza:

\[ \sigma' = \sqrt{\frac{1}{n} \sum_{i=1}n (-2x_i)^2 - (-2\avg{x})^2} = \sqrt{\frac{1}{n} \sum_{i=1}4\left(n x_i^2 - \avg{x}^2\right)} = \sqrt{4\sigma^2} = 2\sigma \]

Por lo tanto, la desviación típica sí se dobla.

\spart Usando los cálculos del apartado anterior vemos que la varianza se multiplica por cuatro.

\spart Efectivamente: cambiar el signo haría una reflexión de los datos sobre el eje Y y la asimetría estaría orientada hacia el lado contrario. 

\end{problem}

\subsection{Tema 2 - Muestreo aleatorio}

\begin{problem}[1] Se desea estimar el momento de orden 4, $\alpha_3 = \esp{X^3}$ en una v.a. $X$ con distirbución exponencial de parámetro 2, es decir, la función de distribución de $X$ es $F(t) = \prob{X ≤ t} = 1 - e^{-2t}$ para $t≥0$. Definir un estimador natural para $\alpha_3$ y calcular su error cuadrático medio.

\solution

Usando el criterio de \textit{plugin}, podríamos definir el estimador \[ \hat{\alpha}_3 = \int_\real x^3\,d\fd_n(x) \]. 

Calculamos ahora el error cuadrático medio:

\begin{gather*}
ECM(\hat{\alpha}_3) = \esp{\hat{\alpha}_3 - \alpha_3}^2 = \esp{(\hat{\alpha}_3 - \esp{\hat{\alpha}_3} + \esp{\hat{\alpha}_3} - \alpha_3) ^2} = \\
= \esp{(\hat{\alpha}_3 - \esp{\hat{\alpha_3}})^2 +  (\esp{\hat{\alpha_3}}- \alpha_3)^2 + 2(\hat{\alpha}_3 - \esp{\hat{\alpha_3}}) (\esp{\hat{\alpha_3}}- \alpha_3)} = \\
= \underbrace{\esp{(\hat{\alpha_3} - \esp{\hat{\alpha_3}})^2}}_{(a)}+ \underbrace{\left(\esp{\hat{\alpha_3}} - \alpha_3\right)^2}_{(b)} + \underbrace{2 \cdot \esp{ (\esp{\hat{\alpha_3}}- \alpha_3)^2 + 2(\hat{\alpha}_3 - \esp{\hat{\alpha_3}})}}_{(c)} 
\end{gather*}

Aquí ya hay cosas raras. (c) es cero por alguna razón, luego hay que calcular la varianza y el sesgo.

\[ \text{sesgo}(\hat{\alpha}_3) = \esp{\hat{\alpha}_3} - \alpha_3 = \alpha_3 - \alpha_3 = 0 \]

\[ \var{\hat{\alpha}_3} = \var{\frac{1}{n}\sum X_i^3 } = \frac{1}{n^2}\var{\sum X_i^3} = \frac{1}{n^2}\sum \var{X_i^3} = \frac{\var{X^3}}{n} \]

y, teniendo en cuenta el enunciado,

\[ \var{X^3} = \esp{X^6} - \esp{X^3}^2 = \frac{6!}{2^6} - \left(\frac{3!}{2^3}\right)^2 = \frac{171}{16} \]

y por lo tanto

\[ \text{ECM}(\hat{\alpha}_3) = \frac{171}{16n} = O(\frac{1}{n)} \convs 0 \]

donde lo que más nos importa es la convergencia a cero, que indica que cuanto más muestras tenemos mejor será el estimador.

\end{problem}

\begin{problem}[2] Supongamos que la muestra tiene tamaño $n=50$ y que la distribución de las $X_i$ es una $N(4,1)$. 

\ppart Obtener, utilizando la desigualdad de Chebichev, una cota superior para la probabilidad $\prob{\abs{\avg{X} - 4} > 0.3}$.

\ppart Calcula exactamente $\prob{\abs{\avg{X} - 4} > 0.3}$ utilizando la distribución de $X_i$. 

\solution
\spart

Como la media es cuatro, la desigualdad de Checbichev nos da una cota de 

\[ \frac{\var{\avg{x}}}{0.3^2} = \frac{\var{X}}{n \cdot 0.3^2} \simeq 0.22 \]

\spart

Normalizamos

\[ Z = \frac{\avg{X} - 4}{\frac{1}{\sqrt{50}}} ~ N(0,1) \]

y calculamos.

\[ \prob{\abs{\avg{X} - 4} > 0.3} = \prob{\abs{Z} > \frac{0.3}{\frac{1}{\sqrt{50}}}} = 2 \cdot \prob{Z > 2.12} = 0.038 \]

\end{problem}

\begin{problem}[4] Denotemos por 

\[ C_n = \int_\real \left(\fd_n(t) - F(t)\right)^2 \, dF(t) \]

la llamada discrepancia de Cramer-Von Mises entre $\fd_n$ y $F$. ¿Converge a cero casi seguro esta discrepancia?

Calcular la distribución asintótica de la sucesión $D_n = \sqrt{n}\left(\fd_n(t) - F(t)\right)$ para un valor fijo $t\in\real$.

\solution

\[ C_n = \int_\real \left(\fd_n(t) - F(t)\right)^2 \, dF(t) = \int_\real \left(\fd_n(t) - F(t)\right)^2 f(t) \, dt \]

Como por el teorema de Glivenko-Cantelli (\ref{thmGlivenko}) tenemos que 

\[ \fd_n(t) - F(t) ≤ \sup_t \abs{\fd_n(t) - F(t)} = \md{\fd_n - F}_\infty \]

entonces 

\[ \int_\real \left(\fd_n(t) - F(t)\right)^2 f(t) \, dt ≤  \md{\fd_n - F}_\infty^2 \int_\real f(t) \,dt = \md{\fd_n - F}_\infty^2 \]

Igualmente por Glivenko-Cantelli, 

\[ \md{\fd_n - F}_\infty^2 \convcs 0  \qed \]

\spart

Para calcular la distirbución asintótica de \[ D_n = \sqrt{n}\left(\fd_n(t) - F(t)\right) \] usamos el Teorema Central del Límite (\ref{thmCentral}). Necesitamos algo que se asemeje a una media muestral, y de hecho

\[ \fd_n(t) = \frac{1}{n} \sum_{i=1}^n \ind_{(-\infty, t]} (X_i) = \frac{1}{n} \sum_{i=1}^n Y_i = \avg{Y} \]

Por otra parte, $Y = \ind_{(-\infty, t]}(X)$ y por lo tanto \[ \esp{Y} = \esp{\ind_{(-\infty, t]}(X)} = \prob{X ≤ t} = F(t) \]

Ya podemos aplicar el TCL, pero nos falta saber cuál es la desviación típica de $Y$. Como es una distribución de Bernoulli 

\[ \mathbb{V}(Y) = p(1-p) = F(t)(1-F(t)) \]

y por lo tanto 

\[ D_n \convdist N\left(0, \sqrt{F(t)(1-F(t))}\right) \]
\end{problem}

\begin{problem}[5] Sea $X$ una v.a. cuya función de densidad depende de un parámetro desconocido $\theta \in \real$, concretamente \[ f(x;\theta) = \frac{1}{\pi}\frac{1}{1+(x-\theta)^2} \] para $x\in \real$. Comprobar que $\theta$ coincide con la mediana y la moda de $X$ pero que la media $\esp{X}$ no está definida.

Diseñoar un experimento de simulación en R, tomando algún valor concreto de $\theta$, orientado a comprobar cómo se comportan la mediana muestral y la media muestral como estimadores de $\theta$: mientras la mediana muestral se acerca al verdadero valor de $\theta$ al aumentar $N$, la media muestral oscila fuertemente y no se acerca a $\theta$ aunque se aumente el tamaño muestral $n$.

\solution Viendo la función, vemos que es simétrica con respecto al eje $x= \theta$. Por lo tanto, el punto que deja a izquierda y derecha la misma probablidad, la mediana, es precisamente $\theta$. 

De la misma forma, la moda es el valor máximo de la distribución, que se ve claramente que ocurre cuando $x=\theta$.
\end{problem}

\begin{problem}[7] Sea $X$ una v.a con distribución absolutamente continua. Sea $F$ la correspondiente función de distribución y $f = F'$ continua en todo punto la función de densidad. para $r\in \{1,\dotsc,n\}$, denotemos por $X_{(r)}$ el $r$-simo estadístico ordenado de una muestra de tamaño $n$ extraída de $X$. Calcular la función de distirbución y la de densidad de la v.a. $X_{(r)}$.

\solution

Por definición

\[ F_{X_{(r)}} (x) = \prob{X_{(r)} ≤ x }\]

que es la probabilidad que al menos $r$ elementos de la muestra sean menores o iguales que $x$. Luego la probabilidad es igual a

\begin{gather*}
\sum_{j=r}^n \prob{\text{exactamente j observaciones de la muestra son ≤ x}} =  \\
= \sum_{j=r}^n \prob{B(n, F(x)) = j} = \sum_{j=r}^n \comb{n}{j}F(x)^j \left(1 - F(x)\right)^{n-j}
\end{gather*}

Ahora sólo falta calcular la densidad de $X_{(r)}$, y la obtenemos derivando

\begin{gather*}
 f_{X_{(r)}} (x) = \\
 = \sum_{j=r}^n \left(\comb{n}{j}j(F(x)^{j-1}(1-F(x))^{n-j}f(x) - (F(x))^j(n-j)(1-F(x))^{n-j-1} f(x)\right) = \\
 = \sum_{j=r}^n \comb{n}{j} j(F(x)^{j-1}(1-F(x))^{n-j}f(x)  - \sum_{j=r}^n\comb{n}{j} (F(x))^j(n-j)(1-F(x))^{n-j-1} f(x) = \\
 = \comb{n}{r} r(F(x))^{r-1} (1-F(x))^{n-1}f(x) + \sum_{j=r+1}^n \comb{n}{j}j(F(x))^{j-1} f(x) (1-F(x))^{n-j} \\
 \quad \quad - \sum_{j=r}^n\comb{n}{j}(n-j)(F(x))^j (1-F(x))^{n-j-1}f(x) = \\
 n\comb{n-1}{r-1}(F(x))^{r-1} (1-F(x))^{n-r} f(x)    +   \sum_{l=r}^{n-1}n\comb{n-1}{l}(F(x))^l (1-F(x))^{n-l-1} f(x) \\
 \quad\quad -  \sum_{j=r}^{n-1}n\comb{n-1}{j}(F(x))^j (1-F(x))^{n-j-1} f(x)
\end{gather*} 

Los dos últimos términos se cancelan y nos queda que 

\[ f_{X_{(r)}} (x) = n\comb{n-1}{r-1}(F(x))^{r-1} (1-F(x))^{n-r} f(x) \]

Consideremos los dos casos particulares del mínimo y máximo de la muestra. Con el mínimo, $r=1$ y entonces

\[ F_{X_{(1)}} (x)= \prob{X_{(1)} ≤ x} = \sum_{j=1}^n\comb{n}{j}(F(x))^j(1-F(x))^{n-j} = 1 - (1-F(x))^n \]

En el caso del máximo:

\[ F_{X_{(n)}} (x) = \prob{X_{(n)} ≤ x } = (F(x))^n \]

\end{problem}

\begin{problem}[8] Sea $\hat{f}_n$ un estimador kernel de la densidad basado en un núcleo $K$ que es una función de densidad con media finita. Comprobar que, en general, $\hat{f}_n(t)$ es un estimador sesgado de $f(t)$ en el sentido de que \textbf{no} se tiene $\esp{\hat{f}_n(t)} = f(t)$ para todo $t$ y para toda densidad $f$.

\solution

\begin{gather*}
\esp{\hat{f}_n(t)} = \esp{\frac{1}{nh}\sum_{i=1}^n K \left(\frac{t-X_i}{h}\right)} = \\
= \frac{1}{nh}\sum_{i=1}^n \esp{K\left(\frac{t-X_i}{h}\right)} = \frac{1}{h} \esp{K\left(\frac{t-X}{h}\right)} = \\
= \frac{1}{h} \int_\real K \left(\frac{t-x}{h}\right) f(x) \,dx
\end{gather*}

Haciendo un cambio de variable $x = t-hz$, $dx = -h\,dz$,

\[ \frac{1}{h} \int_\real K \left(\frac{t-x}{h}\right) f(x) \,d(x)  = \frac{-1}{h} \int_\real Kz f(t-hz) h \,dz  = -\int_\real Kz f(t-hz)\,dz \]

Usando el desarrollo en serie de Taylor, la profesora se ha liado y yo desisto de copiar todo ese chorizo. 

Al final no se van las integrales en los que haya una potencia par de $z$. Habrá un montón de términos que se harán muy pequeños porque haremos que $h\to 0$ cuando $n\to\infty$ y por lo tanto el resto se hacen más pequeños y sólo queda el primer término que es $f(t)$.
\end{problem}