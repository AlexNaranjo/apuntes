%
% Soluciones a los ejercicios de Estadística I.
%
% Curso 2013-2014
%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Tema 1 - Estadística descriptiva}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[1]
\solution
\centerline{\includegraphics[page=1,scale=0.745]{pdf/_Solucion_T1P1.pdf}} % scale obtenido empíricamente para que quepa en la página
\includepdf[pages=2-]{pdf/_Solucion_T1P1.pdf}
\end{problem}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[2] Demostrar que \[ \sum_{i=1}^n \left(x_i-\avg{x}\right)^2 = \min_{a\in \real} \sum_{i=1}^n(x_i-a)^2 \]

\solution

Definimos una función \[ g(a) = \sum_{i=1}^n(x_i-a)^2 \] buscamos su derivada \[ g'(a) = -2 \sum_{i=1}^n(x_i-a) \] e igualamos a cero:

\begin{gather*}
-2 \sum_{i=1}^n(x_i-a) = 0 \\
\sum_{i=1}^n x_i - \sum_{i=1}^n a = 0 \\
n \avg{x} = n a \\
\avg{x} = a 
\end{gather*}

Esto quiere decir que la media muestral es el valor que minimiza la distancia con cada uno de los datos de la muestra.
\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\begin{problem}[3]
\solution
\centerline{\includegraphics[page=1,scale=0.8328]{pdf/_Solucion_T1P3.pdf}} % scale obtenido empíricamente para que quepa en la página
\end{problem}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[4]
\solution
\centerline{\includegraphics[page=1,scale=0.77]{pdf/_Solucion_T1P4.pdf}} % scale obtenido empíricamente para que quepa en la página
\includepdf[pages=2-]{pdf/_Solucion_T1P4.pdf}
\end{problem}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[5]Determina si es verdadero o falso:

\ppart Si añadimos 7 a todos los datos de un conjunto, el primer cuartil aumenta en 7 unidades y el rango intercuartílico no cambia.

\ppart Si todos los datos de un conjunto se multiplican por -2, la desviación típica se dobla.

\ppart Si todos los datos de un conjunto se multiplican por 2, la varianza se dobla.

\ppart Al multiplicar por tres todos los datos de un conjunto, el coeficiente de asimetría no varía

\ppart Si el coeficiente de correlación entre dos variables vale -0.8, los valores por debajo del promedio de una variable están asociados con valores por debajo del promedio de la otra.

\ppart Si $\forall i\,y_i<x_i$ entonces el coeficiente de correlación es negativo.


\ppart Si cambiamos el signo de todos los datos de un conjunto, el coeficiente de asimetría también cambia de signo.

\ppart Al restar una unidad a cada dato de un conjunto, la desviación típica siempre disminuye.

\ppart Si a un conjunto de datos con media $\gx$ se le añade un nuevo dato que coincide con $\gx$, la
media no cambia y la desviación típica disminuye.

\solution 

\spart Falso. Añadir siete a todos los datos es una traslación, así que la distribución de los datos no cambia. El rango intercuartílico se mantiene y el cuantil también.

\spart Teniendo en cuenta que si multiplicamos todos los datos del conjunto por $-2$ la media también se multiplica por $-2$, y sustituyendo en la fórmula de la varianza:

\[ \sigma' = \sqrt{\frac{1}{n} \sum_{i=1}n (-2x_i)^2 - (-2\avg{x})^2} = \sqrt{\frac{1}{n} \sum_{i=1}4\left(n x_i^2 - \avg{x}^2\right)} = \sqrt{4\sigma^2} = 2\sigma \]

Por lo tanto, la desviación típica sí se dobla.

\spart Usando los cálculos del apartado anterior vemos que la varianza se multiplica por cuatro.

\spart Efectivamente: cambiar el signo haría una reflexión de los datos sobre el eje Y y la asimetría estaría orientada hacia el lado contrario. 

\spart  Teniendo en cuenta que si multiplicamos todos los datos del conjunto por $3$ la media también se multiplica por $3$

El coeficiente de asimetría se calcula:

\[\frac{1}{n} \sum_{i=1}^n (x_i-\gx)^3\]

Sustituyendo en la fórmula del coeficiente de asimetría

\[\frac{1}{n} \sum_{i=1}^n (3x_i - 3\gx)^3 = \frac{1}{n} \sum_{i=1}^n 3^3 (x_i-\gx)^3 = 27 \cdot \frac{1}{n} \sum_{i=1}^n (x-\gx)^3\]

Por lo tanto el coeficiente de asimetría sí varía.

\spart Falso. \[ \hat\sigma^2 = \frac{1}{n} \sum_{j=1}^n (y_j - \avg{y})^2 = \begin{cases} y_j = x_j - 1 \\ \avg{y} = \frac{1}{n} \sum_{j=1}^n (x_j - 1) = \frac{1}{n} ( \sum_{j=1}^n x_j ) - 1 = \avg{x} - 1 \end{cases} \]
\[ = \frac{1}{n} \sum_{j=1}^n (x_j - 1 - ( \avg{x} - 1))^2 = \frac{1}{n} \sum_{j=1}^n (x_j - \avg{x})^2  = \sigma^2 \]

\spart Falso. 2 variables pueden tener una correlación creciente aunque $y_i<x_i$.

\spart Falso. La desviación típica se mantiene (los datos siguen estando ``igual de separados'').

\spart Verdadero. Al hacer el cálculo de la media no varía (en la fórmula del ejercicio 2 se puede comprobar que si añadimos un $x_i=\gx$ el sumatorio de la derecha queda igual) y la desviación típica disminuye.

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\begin{problem}[6]
\solution
\centerline{\includegraphics[page=1,scale=0.783]{pdf/_Solucion_T1P6.pdf}} % scale obtenido empíricamente para que quepa en la página
\includepdf[pages=2-]{pdf/_Solucion_T1P6.pdf}
\end{problem}
\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[7]
Relaciona los histogramas con los boxplot
\solution
Fijándose en los intervalos entre los que se mueven los datos es la forma más fácil.

\[\begin{array}{cc}
1 \to 2\\
2 \to 1\\
3 \to 3
\end{array}\]

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[8]
Del diagrama de dispersión presentado se pregunta:

\ppart ¿Existe alguna relación?

\ppart ¿Hay algún dato atípico?

\ppart De los 3 valores siguientes: $0.01, 0.83, -0,73$ ¿cuál crees que podría corresponder al coeficiente de correlación?

\solution

\spart Parece que sí.

\spart Bastante obvio que sí

\spart 0.83. Como la nube de puntos parece que se aproxima a una recta con pendiente positiva, la correlación debe ser positiva. Además, como ``se parece bastante'' a una recta, la correlación debe ser cercana a 1.

\end{problem}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\begin{problem}[9]
\solution
\centerline{\includegraphics[page=1,scale=0.78]{pdf/_Solucion_T1P9.pdf}} % scale obtenido empíricamente para que quepa en la página
\end{problem}
\newpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[10]
¿Qué valor tiene que tomar $x$ para que el coeficiente de correlación sea 1?

\ppart $A = \{(1,1),(2,3),(2,3),(4,x)\}$
\ppart $B = \{(1,1),(2,3),(3,4),(4,x)\}$

\solution

Para que el coeficiente de correlación sea exactamente 1, los puntos tienen que estar en la misma recta. Buscamos el $x$ que cumpla eso.

\spart $x=6$

\spart Imposible (porque los 3 puntos dados no están alineados)

\end{problem}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Tema 2 - Muestreo aleatorio}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[1] Se desea estimar el momento de orden 4, $\alpha_3 = \esp{X^3}$ en una v.a. $X$ con distribución exponencial de parámetro 2, es decir, la función de distribución de $X$ es $F(t) = \prob{X \leq t} = 1 - e^{-2t}$ para $t\geq0$. Definir un estimador natural para $\alpha_3$ y calcular su error cuadrático medio.

\solution

Usando el criterio de \textit{plugin}, podríamos definir el estimador \[ \hat{\alpha}_3 = \int_\real x^3\,d\fd_n(x) \]

Calculamos ahora el error cuadrático medio:

\begin{gather*}
\ECM(\hat{\alpha}_3) =
\esp{(\hat{\alpha}_3 - \alpha_3)^2} =
\esp{(\hat{\alpha}_3 - \esp{\hat{\alpha}_3} + \esp{\hat{\alpha}_3} - \alpha_3 )^2} = \\
\esp{\underbrace{(\hat{\alpha_3} - \esp{\hat{\alpha_3}})^2}_{(a)} +
     \underbrace{(\esp{\hat{\alpha_3}} - \alpha_3)^2}_{(b)} +
     \underbrace{2 \cdot (\hat{\alpha}_3 - \esp{\hat{\alpha_3}}) \cdot
                    (\esp{\hat{\alpha_3}} - \alpha_3)
    }_{(c)}}
\end{gather*}

Calculamos (b) que es el $ \sesgo^2(\hat{\alpha_3}) $ :

\[ \sesgo (\hat{\alpha}_3) = \esp{\hat{\alpha}_3} - \alpha_3 = \alpha_3 - \alpha_3 = 0 \]

Como el sesgo es $0$, tenemos que (c) es también $0$. \\
Solo nos queda calcular $ \esp{(a)} $, que es la varianza:

\[
\var{\hat{\alpha}_3} =
\var{\frac{1}{n}\sum X_i^3 } =
\frac{1}{n^2}\var{\sum X_i^3} =
\frac{1}{n^2}\sum \var{X_i^3} =
\frac{\var{X^3}}{n}
\]

y, teniendo en cuenta el enunciado,

\[ \var{X^3} = \esp{X^6} - \esp{X^3}^2 = \frac{6!}{2^6} - \left(\frac{3!}{2^3}\right)^2 = \frac{171}{16} \]

y por lo tanto

\[ \ECM (\hat{\alpha}_3) = \frac{171}{16n} = O\left(\frac{1}{n}\right) \convs 0 \]

donde lo que más nos importa es la convergencia a cero, que indica que cuanto más muestras tenemos mejor será el estimador.

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[2] Supongamos que la muestra tiene tamaño $n=50$ y que la distribución de las $X_i$ es una $N(4,1)$. 

\ppart Obtener, utilizando la desigualdad de Chebichev, una cota superior para la probabilidad $\prob{\abs{\avg{X} - 4} > 0.3}$.

\ppart Calcula exactamente $\prob{\abs{\avg{X} - 4} > 0.3}$ utilizando la distribución de $X_i$. 

\solution
\spart

Como $\mu = 4$, la desigualdad de Chebichev nos da una cota de 

\[
\prob{\abs{\avg{X} - 4} > 0.3} \leq
\frac{\var{\avg{X}}}{0.3^2} =
\frac{\var{X}}{n \cdot 0.3^2} \simeq 0.22
\]

\spart

Normalizamos

\[ Z = \frac{\avg{X} - 4}{\frac{1}{\sqrt{50}}} \sim N(0,1) \]

y calculamos.

\[ \prob{\abs{\avg{X} - 4} > 0.3} = \prob{\abs{Z} > \frac{0.3}{\frac{1}{\sqrt{50}}}} = 2 \cdot \prob{Z > 2.12} = 0.034 \]

\end{problem}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[3]
Utilizando $R$ dibuja la función de densidad y la función de distribución de una v.a. con distribución beta de parámetros $a = 3$, $b = 6$.\\
A continuación dibuja, sobrepuestas en cada uno de los gráficos, las aproximaciones a F y f obtenidas respectivamente mediante la función empírica y un estimador kernel.\\
Verificar empíricamente el grado de aproximación, en las estimaciones de F y f, que se obtiene mediante un experimento de simulación basado en 200 muestras de tamaño 20. Es decir, considerando, por ejemplo, la estimación de F, se trata de simular 200 muestras de tamaño 20; para cada una de ellas evaluar el error (medido en la norma del supremo) que se comete al aproximar F por $\mathbb{P}_n$. Por último, calcular el promedio de los 200 errores obtenidos. Análogamente para la estimación de f.

\solution
\begin{verbatim}
# Dibuja la función de densidad y la funcion de distribución de una v.a. con 
# distribución beta de parametros a=3 y b=6.

a = 3
b = 6
n = 20
t = seq(0,1,0.01)
densidad = dbeta(t, a, b, ncp = 0, log = FALSE)
fndistrib = pbeta(t, a, b, ncp = 0, log = FALSE)
X = rbeta(n,a, b, ncp = 0)
kernelest = density(X,kernel="gaussian")
M = max(max(densidad),max(kernelest$y))
distremp = ecdf(X)

layout(matrix(1:2,2,1))
layout.show(2)

plot(t,densidad,type="l",lwd=2,col="tomato3",xlab="",ylab="",ylim=c(0,M),
     main="Densidad y estimador kernel",font.main=1,cex.main=1)
lines(kernelest,type="l",lwd=2,col="navyblue")
mtext("Distribución beta(3,6)",side=3,line=3,cex=1.5)
plot(t,fndistrib,type="l",lwd=2,col="tomato3",xlab="",ylab="",ylim=c(0,1),
     main="Función de distribución poblacional y empírica",font.main=1,cex.main=1)
lines(distremp,do.points=FALSE,lwd=2,col="navyblue")

# Verificar empiricamente el grado de aproximacion:
nMC = 200
Supremo1 = rep(0,nMC) ; Supremo2 = rep(0,nMC)
for (i in 1:nMC){
  XMC = rbeta(n,a, b, ncp = 0)
  kernelMC = density(XMC,kernel="gaussian")
  densidadMC = dbeta(kernelMC$x, a, b, ncp = 0, log = FALSE)
  Supremo1[i] = max(abs(kernelMC$y - densidadMC))
  distempMC = ecdf(XMC)
  Supremo2[i] = max(abs(distempMC(t) - fndistrib))
}
Error1 = mean(Supremo1)
Error2 = mean(Supremo2)
\end{verbatim}
\end{problem}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[4] Denotemos por 

\[ C_n = \int_\real \left(\fd_n(t) - F(t)\right)^2 \, dF(t) \]

la llamada discrepancia de Cramer-Von Mises entre $\fd_n$ y $F$. \ppart ¿Converge a cero casi seguro esta discrepancia?

\ppart Calcular la distribución asintótica de la sucesión $D_n = \sqrt{n}\left(\fd_n(t) - F(t)\right)$ para un valor fijo $t\in\real$.

\solution
\spart 
\[ C_n = \int_\real \left(\fd_n(t) - F(t)\right)^2 \, dF(t) = \int_\real \left(\fd_n(t) - F(t)\right)^2 f(t) \, dt \]

Tenemos que 

\[ \fd_n(t) - F(t) \leq \sup_t \abs{\fd_n(t) - F(t)} = \md{\fd_n - F}_\infty \]

entonces 

\[ \int_\real \left(\fd_n(t) - F(t)\right)^2 f(t) \, dt \leq \md{\fd_n - F}_\infty^2 \int_\real f(t) \,dt = \md{\fd_n - F}_\infty^2 \]

Finalmente, por el teorema de Glivenko-Cantelli (\ref{thmGlivenko}) tenemos que 

\[ \md{\fd_n - F}_\infty^2 \convcs 0  \qed \]

\spart

Para calcular la distribución asintótica de

\[ D_n = \sqrt{n}\left(\fd_n(t) - F(t)\right) \]

usamos el Teorema Central del Límite (\ref{thmCentral}). Necesitamos algo que se asemeje a una media muestral, y de hecho

\[
\fd_n(t) =
\frac{1}{n} \sum_{i=1}^n \ind_{(-\infty, t]} (X_i) =
\frac{1}{n} \sum_{i=1}^n Y_i =
\avg{Y}
\]

Por otra parte, $Y = \ind_{(-\infty, t]}(X)$ y por lo tanto \[ \esp{Y} = \esp{\ind_{(-\infty, t]}(X)} = \prob{X \leq t} = F(t) \]

Ya podemos aplicar el TCL, pero nos falta saber cuál es la desviación típica de $Y$. Como es una distribución de Bernoulli 

\[ \mathbb{V}(Y) = p(1-p) = F(t)(1-F(t)) \]

y por lo tanto aplicando el TCL

\[ D_n \convdist N\left(0, \sqrt{F(t)(1-F(t))}\right) \]
\end{problem}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[5] Sea $X$ una v.a. cuya función de densidad depende de un parámetro desconocido $\theta \in \real$, concretamente
\[ f(x;\theta) = \frac{1}{\pi}\frac{1}{1+(x-\theta)^2} \]
para $x\in \real$. Comprobar que $\theta$ coincide con la mediana y la moda de $X$ pero que la media $\esp{X}$ no está definida.

Diseñar un experimento de simulación en R, tomando algún valor concreto de $\theta$, orientado a comprobar cómo se comportan la mediana muestral y la media muestral como estimadores de $\theta$: mientras la mediana muestral se acerca al verdadero valor de $\theta$ al aumentar $N$, la media muestral oscila fuertemente y no se acerca a $\theta$ aunque se aumente el tamaño muestral $n$.

\solution Viendo la función, vemos que es simétrica con respecto al eje $x= \theta$. Por lo tanto, el punto que deja a izquierda y derecha la misma probablidad, la mediana, es precisamente $\theta$. 

La moda es el valor máximo de la distribución,
\[ f'(x;\theta) = \frac{1}{\pi} \frac{-2(x-\theta)}{(1+(x-\theta)^2)^2} = 0 \dimplies x = \theta \]
Y se ve que es un máximo porque es el punto en el que el signo de la derivada pasa de positivo a negativo.
\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[6]
Se extrae una muestra aleatoria de tamaño $n = 600$ de una v.a. cuya desviación típica es $\sigma = 3$.
Calcular aproximadamente la probabilidad
\[\prob{\abs{\avg{X} - \mu} < 0.1}\]

\solution
Tenemos 2 posibilidades: Tipificar o con Chebichev.

Según Chebichev, tenemos que 

\[ \prob{\abs{X - \esp{X}} > \epsilon} \leq \frac{\var{X}}{\epsilon^2} \]

Tenemos que $\mu = \esp{\avg{X}}$, tenemos que hallar $\var{\avg{X}}$:

\[ \var{\avg{X}} = \frac{\var{X}}{n} = \frac{\sigma^2}{n} = 0.015 \]

Y por lo tanto,

\[
\prob{\abs{\avg{X} - \mu} < 0.1} =
1 - \underbrace{\prob{\abs{\avg{X} - \mu} > 0.1}}_{ \leq 1.5} \geq 
-0.5
\]

Que no es una aproximación muy buena. Así que pasamos a tipificar:

\[ Z = \frac{\avg{X} - \mu}{\frac{\sigma}{\sqrt{n}}} \sim N(0,1) \]

Entonces:
\begin{gather*}
\prob{\abs{\avg{X} - \mu} < 0.1} =
1 - \prob{\abs{\avg{X} - \mu} > 0.1} =\\
= 1 - \prob{\abs{Z} > \frac{0.1\sqrt{n}}{\sigma}} =
1 - 2 \cdot \prob{Z > \frac{0.1\sqrt{n}}{\sigma}} =
0.582
\end{gather*}

(Recordemos que $\var{\avg{X}} = \frac{\var{X}}{n}$ y que $\esp{\avg{X}} = \mu = \esp{X}$)

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[7] Sea $X$ una v.a con distribución absolutamente continua. Sea $F$ la correspondiente función de distribución y $f = F'$ continua en todo punto la función de densidad. Para $r\in \{1,\dotsc,n\}$, denotemos por $X_{(r)}$ el $r$-simo estadístico ordenado de una muestra de tamaño $n$ extraída de $X$. Calcular la función de distribución y la de densidad de la v.a. $X_{(r)}$.

\solution

Por definición, la función de distribución es:

\[ F_{X_{(r)}} (x) = \prob{X_{(r)} \leq x }\]

que es la probabilidad que al menos $r$ elementos de la muestra sean menores o iguales que $x$. Luego la probabilidad es igual a

\begin{gather*}
\sum_{j=r}^n \prob{\text{exactamente j observaciones de la muestra\footnotemark son $\leq$ x}} =  \\
= \sum_{j=r}^n \prob{B(n, F(x)) = j} = \sum_{j=r}^n \comb{n}{j}F(x)^j \left(1 - F(x)\right)^{n-j}
\end{gather*}
\footnotetext{que una observación sea exactemente $\leq x$ es una Bernouilli, y la suma de Bernouillis es la Binomial}

Ahora sólo falta calcular la densidad de $X_{(r)}$, y la obtenemos derivando la función de distribución:

\newpage
\begin{gather*}
 f_{X_{(r)}} (x) = \\
 = \sum_{j=r}^n \left(\comb{n}{j}j(F(x)^{j-1}(1-F(x))^{n-j}f(x) - (F(x))^j(n-j)(1-F(x))^{n-j-1} f(x)\right) = \\
 = \sum_{j=r}^n \comb{n}{j} j(F(x)^{j-1}(1-F(x))^{n-j}f(x)  - \sum_{j=r}^n\comb{n}{j} (F(x))^j(n-j)(1-F(x))^{n-j-1} f(x) = \\
 = \comb{n}{r} r(F(x))^{r-1} (1-F(x))^{n-1}f(x) + \sum_{j=r+1}^n \comb{n}{j}j(F(x))^{j-1} f(x) (1-F(x))^{n-j} \\
 \quad \quad - \sum_{j=r}^n\comb{n}{j}(n-j)(F(x))^j (1-F(x))^{n-j-1}f(x) = \\
 n\comb{n-1}{r-1}(F(x))^{r-1} (1-F(x))^{n-r} f(x) + \sum_{l=r}^{n-1}n\comb{n-1}{l}(F(x))^l (1-F(x))^{n-l-1} f(x) \\
 \quad\quad -  \sum_{j=r}^{n-1}n\comb{n-1}{j}(F(x))^j (1-F(x))^{n-j-1} f(x)
\end{gather*} 

Los dos últimos términos se cancelan y nos queda que:

\[ f_{X_{(r)}} (x) = n\comb{n-1}{r-1}(F(x))^{r-1} (1-F(x))^{n-r} f(x) \]

Consideremos los dos casos particulares del mínimo y máximo de la muestra. Con el mínimo, $r=1$ y entonces:
\vspace{-10pt} % demasiado espacio en blanco entre el texto y esta ecuación.
\begin{gather*}
F_{X_{(1)}} (x) =
\prob{X_{(1)} \leq x} =
\sum_{j=1}^n\comb{n}{j}(F(x))^j(1-F(x))^{n-j} =\footnotemark
1 - (1-F(x))^n
\end{gather*}
\footnotetext{$1 = 1^{n} = (1 - F(x) + F(x))^n = \sum\limits_{j=0}^{n}\comb{n}{j}(F(x))^j(1-F(x))^{n-j} $}

En el caso del máximo:
\begin{gather*}
F_{X_{(n)}} (x) =
\prob{X_{(n)} \leq x } =
\prod\limits_{j=1}^{n}\prob{X_{(j)} \leq x } =
\footnotemark (F(x))^n
\end{gather*}
\footnotetext{$\forall j$ $X_{(j)} \sim X \implies \prob{X \leq x} = F(x)$}
\end{problem}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[8] Sea $\hat{f}_n$ un estimador kernel de la densidad basado en un núcleo $K$ que es una función de densidad con media finita. Comprobar que, en general, $\hat{f}_n(t)$ es un estimador sesgado de $f(t)$ en el sentido de que \textbf{no} se tiene $\esp{\hat{f}_n(t)} = f(t)$ para todo $t$ y para toda densidad $f$.

\solution

Lo que buscamos es calcular el sesgo:

\begin{equation} \label{eq8H2}
\text{sesgo}\,(\hat{f}_n(t)) = \esp{\hat{f}_n(t)} - f(t)
\end{equation}

\begin{gather*}
\esp{\hat{f}_n(t)} =
\esp{\frac{1}{nh}\sum_{i=1}^n K \left(\frac{t-X_i}{h}\right)} =
\frac{1}{nh}\sum_{i=1}^n \esp{K\left(\frac{t-X_i}{h}\right)} =\\
= \frac{1}{h} \esp{K\left(\frac{t-X}{h}\right)} =
\frac{1}{h} \int_\real K \left(\frac{t-x}{h}\right) f(x) \,dx = ...
\end{gather*}

Haciendo un cambio de variable $x = t-hz$, $dx = -h\,dz$, los límites se invierten,

\[
... = \frac{1}{h} \int_{-\infty}^\infty K \left(\frac{t-x}{h}\right) f(x) \,d(x) =
\frac{1}{h} \int_\infty^{-\infty} K(z) f(t-hz) (- h) \,dz =
\int_{-\infty}^\infty K(z) f(t-hz)\,dz
\]

Usando que $K$ es función de densidad $\implies \int K = 1$, (\ref{eq8H2}) nos queda

\begin{gather*}
... = \int_{-\infty}^\infty K(z) f(t-hz)\,dz - \int_{-\infty}^\infty K(z) f(t)\, dz =
\int_{-\infty}^\infty K(z) \left[f(t-hz)-f(t)\right]\,dz =\\
= hf'(t)\int_{-\infty}^\infty zK(z)\,dz + \frac{1}{2} h^2 f''(t) \int_{-\infty}^\infty z^2K(z)\,dz + \frac{1}{6}h^3 f'''(t) \int_{-\infty}^\infty z^3K(z)\,dz + \dotsb  
\end{gather*}

Al hacer el desarrollo de Taylor, como $K$ es una función simétrica, las integrales con índice impar (con $z=1, 3,\dotsc$) se anulan. Sin embargo, el segundo término no lo hace. Por lo tanto, el sesgo de un estimador kernel \textbf{no es nunca cero}. 

El sesgo del estimador kernel depende de $h$ (el parámetro de suavizado o \textit{bandwith}) en potencias pares. Por eso, se toma de manera tal que $h\convs 0$ y entonces $\text{sesgo}\,\hat{f}_n(t) \convs 0$ pero manteniendo un equilibrio para que la varianza también sea pequeña y no tengamos picos en el histograma (ver sección \ref{secEst}).

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Tema 3 - Estimación puntual paramétrica}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[1]
Sea X una v.a. con distribución exponencial de parámetro $\theta$.\\
Calcula la función de distribución, la de densidad y la función cuantílica de la v.a.\\
$ Y = \theta X^{1/3} $.
\solution
\begin{gather*}
\footnotemark\text{Como X }\sim exp(\theta) \,(\theta > 0) \y Y = \theta X^{1/3} \dimplies X = (\frac{Y}{\theta})^{3}\\
f(x) = f((\frac{y}{\theta})^3) =
\theta e^{-\theta (\frac{y}{\theta})^3} =
\theta e^{-\frac{y^3}{\theta^2}} =
f(y) \\
F(X) = F((\frac{Y}{\theta})^{3}) =
\int f((\frac{y}{\theta})^{3}) (3\frac{y^2}{\theta^3}) dy =\\
= \int 3\frac{y^2}{\theta^2} e^{-\frac{y^3}{\theta^2}} dy =
- e^{-\frac{y^3}{\theta^2}} + C = F(Y),  C \in \real
\end{gather*}
\footnotetext{Esta solución puede estar mal. Edu}
Finalmente, como $ e^{-\frac{y^3}{\theta^2}} $ es creciente con valor máximo 0 y mínimo -1 $ \implies $ C = 1.

La función cuantílica por definición es: $ F^{-1}(p) = inf\{y \tq F(y) \geq p\} $, luego
\[
F^{-1}(p) = \inf_{p \in [0,1]}\{ y \tq 1 - e^{-\frac{y^3}{\theta^2}} \geq p \}
\]

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[2]
Supongamos que $X$ mide el error cometido en la medición de una magnitud. $X$ es una v.a. normal de media 0 y varianza $\theta$. 	
\[X\leadsto N(0,\sqrt{\theta}), \theta>0, \Theta = (0,\infty)\]

Se desea estimar $\theta$ a partir de una muestra.

\ppart Calcular el estimador de máxima verosimilitud $T_n$.

\ppart Probar que $T_n$ es insesgado y eficiente.

\ppart Estudiar la distribución asintótica de $T_n$.

\solution
\spart Buscamos el máximo de la función de verosimilitud
\[
L_n(\theta;X_1,...,X_n) =
\prod_{i=1}^n f(x_i;\theta) =
\frac{1}{(\sqrt{2\pi} \, \theta)^{\frac{n}{2}}} e ^ {-\frac{1}{2\theta} \sum x_i^2}
\]

El máximo de la función de verosimilitud será también el máximo de la logverosimilitud

\[logL_n(\theta) = \frac{n}{2}\cdot log(2\pi) - \frac{n}{2}log(\theta) - \frac{1}{2\theta} \sum x_i^2\]

Para ello derivamos e igualamos a 0.

\[\dpa{}{\theta} logL_n(\theta) = -\frac{n}{2\theta} + \frac{1}{2\theta^2}\sum x_i^2  = 0\]

\[\frac{1}{2}\left( - \frac{n}{\theta} + \frac{\sum x_i^2}{\theta^2}\right) = 0 \implies T_n = e.m.v.(\theta) = \frac{1}{n}\sum x_i^2\]


\spart $\esp[\theta]{T_n} = \esp[\theta]{\frac{1}{n}\sum x_i^2} = \esp[\theta]{X^2} = \theta$

Nos tenemos que dar cuenta de que $\var{X} = \esp{X^2} - \esp{X}^2$. En este caso $\esp{X} = \mu = 0$ por lo que $\esp{X^2} = \theta$ por hipótesis.
Vamos a calcular la información de fisher para comprobar si el estimador es eficiente o no.

\[ log f(x;\theta) = \frac{-1}{2}log(2\pi)-\frac{1}{2}log(\theta) - \frac{1}{2\theta}X^2\]
Derivamos:
\[\dpa{}{\theta} log f(x;\theta) = -\frac{1}{2\theta} + \frac{1}{2\theta^2}X^2\]
Elegimos derivar otra vez o elevar al cuadrado (2 alternativas para calcularlo).

En este caso vamos a elevar al cuadrado:

\[\dpa{}{\theta}logf(X;\theta) = \frac{1}{4\theta^2} \left( 1+\frac{X^4}{\theta^2} - 2\frac{X^2}{\theta}\right)\]

Entonces la información de fisher será:

\[I(\theta) = \esp[\theta]{\frac{1}{4\theta^2} \left( 1+\frac{X^4}{\theta^2} - 2\frac{X^2}{\theta}\right)} = \frac{1}{4\theta^2} \left( 1+\frac{\esp[\theta]{X^4}}{\theta^2} - 2\frac{\esp[\theta]{X^2}}{\theta}\right)\]

Aplicamos por hipótesis: $\esp[\theta]{X^4} = 3\theta^2$

\[I(\theta) = \frac{1}{4\theta^2} \left(1+\frac{3\theta^2}{\theta^2} - 2 \frac{\theta}{\theta}\right) = \frac{1}{2\theta^2}\]

Vamos a calcular \[\var[\theta]{T_n} = \var[\theta]{\frac{1}{n}\sum x_i^2} = \frac{1}{n^2}\sum \var[\theta]{x_i^2} = \frac{n}{n^2} \var[\theta]{X^2} =\]
\[ \frac{1}{n}\left(\esp[\theta]{X^4} - \esp[\theta]{X^2}\right) = \frac{1}{n}(3\theta^2-\theta^2) = \frac{2\theta^2}{n} = \frac{1}{nI(\theta)}\]

Como la varianza coincide con la cota de Frécher-Cramer-Rao entonces podemos decir que es un estimador eficiente.

Los siguientes pasos para comprobar lo bueno que es el estimador son: \begin{itemize}
\item $T_n$ asintóticamente normal.
\item $T_n$ es consistente casi seguro.
\end{itemize}

\spart Vamos a estudiar la distribución asintótica:

\[\sqrt{n}(T_n-\theta) \convs[d] N(0,\sigma(\theta))\]

Llamando $Y_i = X_i^2 \implies \esp[\theta]{Y} = \esp[\theta]{X^2} = \theta$

Entonces por el TCL (Teorema Central del Límite): \[\displaystyle \sqrt{n}(\hat{Y} - \esp[\theta]{Y}) \convs[d] N(0,\sqrt{\var{Y}})\]

Donde $\var{Y} = \var[\theta]{X^2} = \esp{(X^2)^2} - \esp{X^2}^2 = 3\theta^2 - \theta^2 = 2\theta^2$
\end{problem}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[3] Se dispone de un gran lote de piezas producidas en una cadena de montaje. Denotemos por $p$ la proporción de piezas defectuosas en ese lote. Supongamos que se seleccionan al azar sucesivamente (con reemplazamiento) piezas del lote hasta que se encuentra una defectuosa. Sea $X$ la variable aleatoria que indica el número de la extracción en la que aparece la primera pieza defectuosa.

\ppart Calcular $\prob{X=k}$ para $k=1,2,\dotsc$ Obtener el estimador de $p$ por el método de los momentos, a partir de una muestra $X_1,\dotsc , X_n$.

\ppart Obtener el estimador de $p$ por el método de máxima verosimilitud. Calcular su distribución asintótica.
\solution
\spart
La probabilidad sigue una distribución geométrica de parámetro $p$:

\[ \prob{X=k} = (1-p)^{k-1}p \]

\spart Calculamos la función de verosimilitud:

\[ L_n(p;x_1,\dotsc,x_n) = \prod_{i=1}^n f(x_i;p) = \prod_{i=1}^n (1-p)^{x_i - 1}p = (1-p)^{\sum\limits_{i=1}^n x_i - 1} p^n \]

Tomamos logaritmos

\[ \log L_n(p) = \log(1 - p) \sum_{i=1}^n (x_i - 1) + n \log{p} \]

y derivando
\begin{gather*}
\dpa{}{p} \log L_n(p) = \frac{-1}{1-p} \sum_{i=1}^n (x_i - 1) + \frac{n}{p} = 0 \dimplies \\
\frac{1 - \hat{p}}{\hat{p}} = \frac{1}{n} \sum_{i=1}^n (x_i - 1) \dimplies
1 - \hat{p} = \hat{p} (\avg{x} - 1) \dimplies
\text{emv}(p) = \hat{p} = \frac{1}{\avg{x}}
\end{gather*}

Vamos a calcular su distribución asintótica, aplicando el método delta.

Para ello observamos que tomando $g(x) = \frac{1}{x}$, tenemos que $g(\avg{x}) = \hat{p}$.

Comprobamos que $g(\esp{X}) = \frac{1}{\esp{X}} = \frac{1}{\frac{1}{p}} = p $

Luego por el método delta y aplicando el TCL:

\[ \sqrt{n} (g(\avg{X}) - g(\esp{X})) = \sqrt{n} (\hat{p} - p) \convs[d] N\left(0, \abs{g'(\esp{X})}\sqrt{\var{X}}\right) \]

Como $g'(x) = \frac{-1}{x^2}$, y $\var{X} = \frac{1 - p}{p^2} $, entonces

\[
N\left(0, \abs{g'(\esp{X})}\sqrt{\var{X}}\right) =
N\left(0, \frac{1}{\frac{1}{p^2}} \frac{\sqrt{1-p}}{p} \right) =
N\left(0, p \sqrt{1-p} \right)
\]

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[4]
Estudiar si es eficiente el estimador de máxima verosimilitud de una poisson.
\solution

\[ P(X = x) = e^{-\lambda} \frac{\lambda^{x}}{x!} \]

El cálculo del estimador de máxima verosimilitud se hizo en clase llegando a $\lambda = \gor{x}$ (\ref{ejEmvPoisson}).

Para ver si es eficiente vemos si es su varianza es igual a la cota de FCR. Necesitamos la información de Fisher para comprobar eso.

Para calcular la información de fisher derivamos el logaritmo de la densidad

\[ \log {f(\lambda; x)} = -\lambda + x \log{\lambda} - \log{x!} \]

\[\dpa{}{\lambda} \log{f (\lambda; x)} = - 1 + \frac{x}{\lambda} + 0 \]

Para calcular la información de Fisher podemos volver a derivar o elevar al cuadrado. Elegimos volver a derivar

\[\dpa{^2}{^2\lambda} \log{f (\lambda; x)} = -\frac{x}{\lambda^2}\]

Entonces tenemos que \[ I(\lambda) = \esp{-\dpa{^2}{^2\lambda} \log{f (\lambda; x)}} = \esp{\frac{x}{\lambda^2}} = \frac{1}{\lambda^2}\esp{X} = \frac{1}{\lambda} \]

La cota de FCR será entonces $\displaystyle\frac{1}{n \frac{1}{\lambda}} = \frac{\lambda}{n}$.

Calculamos la varianza:

\[\var{\lambda} = \var{\gx} = \frac{\var{x}}{n} = \frac{\lambda}{n}\]

Como tenemos la igualdad podemos afirmar que \textbf{si} es un estimador eficiente.
\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[5]
Distribución de Rayleigh, cuya función de densidad es:
\[f(x;\theta) = \frac{x}{\theta^2} e^{\frac{-x^2}{2\theta^2}} \mathbb{I}_{[0,\infty)} (x), \theta > 0\]

\ppart Calcular el estimador de máxima verosimilitud (e.m.v.) y por el método de los momentos

\ppart Calcular la consistencia del e.m.v.

\ppart ¿Son asintóticamente normales ambos estimadores?

\solution

\spart

\[L_n(\theta;x_1,...,x_n) = \frac{x_1 \cdot ... \cdot x_n}{\theta^2} e^{\frac{-1}{2\theta^2} \sum_{i=1}^n x_i^2}\]
\[log L_n(\theta) = \sum log x_i - 2nlog\theta -\frac{1}{2\theta^2}\sum x_i^2\]
\[\dpa log L_n(\theta) = \frac{1}{\theta} \left(-2n+\frac{1}{\theta^2}\sum x_i^2\right) = 0\]
\[\implies \hat{\theta}^2 = \frac{\sum x_i^2}{2n} \implies \hat{\theta} =  emv(\theta) =\left(\frac{\sum x_i^2}{2n}\right)^{\frac{1}{2}}\]

Estimador razonable porque $\esp{x^2} = \var{x} + \esp{x}^2 = \left(\sqrt{\frac{\pi}{2}}\theta\right)^2 + \frac{4-\pi}{2} \theta^2 = 2\theta^2 \dimplies \theta^2 = \frac{1}{2} E(x^2)$

Buscamos ahora el estimador $\tilde\theta$ por el \textbf{método de los momentos}

\[ \esp[\theta]{X}= \theta\sqrt{\frac{\pi}{2}} = \avg{X} \] 

y entonces el estimador es \[\tilde{\theta} = \avg{X}\sqrt{\frac{2}{\pi}} \]

\spart

\textbf{Consistencia:} $\hat{\theta}^2 = \frac{1}{2} \gor{Y}, Y_i = X_i^2$

Por la ley fuerte de los grandes números (\ref{thmGrandes}) sabemos que:
\begin{gather*}
\gor{Y} \convs[cs] E_{\theta}(Y) = E_{\theta}(X^2) = 2\theta^2
\end{gather*}

Vamos a aplicar el teorema de Slutsky.

Sea $g(x) = \sqrt{\frac{1}{2}x}$ definida sobre $[0,\infty)$.

Teorema de Slutsky (\ref{thmSlutsky}) $\implies g\left(\gor{Y}\right) = \sqrt{\frac{1}{2} \frac{\sum x_i^2}{n}} \convcs g(E_{\theta}) = \sqrt{\frac{1}{2}\theta^2} = \theta \implies $ El e.m.v. de $\theta$, $\hat{\theta}$ es consistente c.s.

\spart

Queremos aplicar el método delta:

\[\sqrt{n}(\hat{\theta} - \theta) = \sqrt{n}\left(g\left(\gor{Y}\right) - g\left(E(Y)\right)\right) \convs[d]N(0,\abs{g'(E(Y))}\sqrt{V(Y)}\]

\[E_{\theta}(Y) = E_{\theta} (X^2) = 2\theta^2\]
\[V_{\theta}(Y) = E(X^4) - E^2(X^2) = 8\theta^4-4\theta^4 = 4\theta^4\]

Entonces tenemos que $g'(E(Y)) = \displaystyle \frac{1}{2\sqrt{2E(Y)}} = \frac{1}{4\theta}$.

Con esta información completamos:  

\[\sqrt{n}(\hat{\theta} - \theta) \convs[d] N\left(0,\sqrt{\frac{1}{2\theta}}\right)\]

Buscamos ahora la convergencia asintótica del estimador por el método de los momentos:

\[ \sqrt{n}(\tilde\theta-\theta) = \sqrt{n}\left(\avg{X}\frac{2}{\pi}  - \esp{X}\frac{2}{\pi}\right) = \sqrt{\frac{2}{\pi}}\sqrt{n}(\avg{X}-\esp{X}) \]

que, por el TCL (\ref{thmCentral})

\[ \sqrt{\frac{2}{\pi}}\sqrt{n}(\avg{X}-\esp{X})  \convdist  \sqrt{\frac{2}{\pi}}N\left(0,\theta\sqrt{\frac{4-\pi}{2}}\right) = N\left(0,\theta\sqrt{\frac{4-\pi}{\pi}}\right) \]

y por lo tanto es efectivamente asintóticamente normal.

\end{problem}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[6]
Se dice que una v.a. $X$ tiene distribución Beta de parámetros a > 0 y\\
b > 0 (y se denota $X \sim Beta(a, b)$) si su función de densidad es
\[ f(x; a, b) = \frac{\Gamma(a + b)}{\Gamma(a) \Gamma(b)} x^{a-1} (1-x)^{b-1} \ind_{\left[ 0, 1 \right]}(x). \]
siendo $\Gamma$ la función gamma que aparece en la definición de la distribución del mismo nombre.
Calcular el valor de $\esp{X}$ y $\var{X}$.
\solution

Vamos a utilizar la siguiente propiedad de la gamma: $\Gamma(n + 1) = n \cdot \Gamma(n)$.\\
Empecemos con $\esp{X}$:
\begin{gather}\label{espBeta}
\esp{X} =
\int\limits_{0}^{1} x f(x) dx =
\int\limits_{0}^{1} x \cdot \frac{\Gamma(a + b)}{\Gamma(a) \Gamma(b)} x^{a-1} (1-x)^{b-1} dx = \\
= \frac{\Gamma(a + b)}{\Gamma(a) \Gamma(b)} \frac{\Gamma(a + 1) \Gamma(b)}{\Gamma(a + 1 + b)} \underbrace{\int\limits_{0}^{1} \frac{\Gamma(a + 1 + b)}{\Gamma(a + 1) \Gamma(b)} x^{(a + 1) - 1} (1 - x)^{b - 1} dx}_{= 1 \text{ porque es la función de densidad de una Beta(a + 1, b)}} = \nonumber\\
= \frac{\Gamma(a + b)}{\Gamma(a) \Gamma(b)} \frac{a \cdot \Gamma(a) \Gamma(b)}{(a + b)\cdot\Gamma(a + b)} =
\frac{a}{a + b}\nonumber
\end{gather}
Y ahora calcularemos la varianza:
\begin{gather}\label{varBeta}
\var{X} = \esp{X^2} - \esp{X}^2
\end{gather}
\begin{gather}\label{espX2Beta}
\esp{X^2} =
\int\limits_{0}^{1} x^2 f(x) dx =
\int\limits_{0}^{1} x^2 \cdot \frac{\Gamma(a + b)}{\Gamma(a) \Gamma(b)} x^{a-1} (1-x)^{b-1} dx = \\
= \frac{\Gamma(a + b)}{\Gamma(a) \Gamma(b)} \frac{\Gamma(a + 2) \Gamma(b)}{\Gamma(a + 2 + b)} \underbrace{\int\limits_{0}^{1} \frac{\Gamma(a + 2 + b)}{\Gamma(a + 2) \Gamma(b)} x^{(a + 2) - 1} (1 - x)^{b - 1} dx}_{= 1 \text{ porque es la función de densidad de una Beta(a + 2, b)}} =\nonumber\\
= \frac{(a + 1) a}{(a + b + 1) (a + b)} \frac{\Gamma(a + b)}{\Gamma(a) \Gamma(b)} \frac{\Gamma(a) \Gamma(b)}{\Gamma(a + b)} =
\frac{(a + 1) a}{(a + b + 1) (a + b)} \nonumber
\end{gather}
Sustituimos en (\ref{varBeta}) lo obtenido en (\ref{espBeta}) y (\ref{espX2Beta}):
\begin{gather*}
\var{X} =
\frac{(a + 1) a}{(a + b + 1) (a + b)} - \left( \frac{a}{a + b} \right)^2 =
\frac{(a + 1) \cdot a \cdot (a + b) - a^2 \cdot (a + b + 1)}{(a + b + 1) (a + b)^2} =\\
= \frac{a^3 + a^2 b + a^2 + ab - a^3 -a^2 b - a^2}{(a + b + 1) (a + b)^2} =
\frac{ab}{(a + b + 1) (a + b)^2}
\end{gather*}
\end{problem}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[7]
\solution
Ver transparencias 36 y 37 del tema 3.
\end{problem}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[8]
Sea $X \sim N(\mu,\sqrt{\theta})$. Estamos interesados en la estimación de $\theta$ basados en muestras $X_1,\dotsc,X_n$ de tamaño $n$. Calcular la cota de Fréchet-Cramer-Rao (\ref{thmCotaFCR}) para estimadores insesgados.

\solution

La cota FCR es \[ \frac{1}{n I(\theta)} \]

Podíamos calcular la información de Fisher como

\[ I(\theta) = \esp{\left(\dpa{}{\theta}\log f(X;\theta)\right)^2} = - \esp{\frac{∂^2}{∂\theta^2}\log f(X;\theta)} \]

Usaremos la segunda expresión. Calculamos primero el logaritmo:

\[ \log f(X;\theta) = \frac{-1}{2}\log 2\pi - \frac{1}{2}\log \theta - \frac{1}{2\theta}(x-\mu)^2 \]

y derivamos dos veces

\begin{gather*}
 \dpa{}{\theta} \log f(X;\theta) = \log f(X;\theta) = -\frac{1}{2\theta} + \frac{1}{2\theta^2}(x-\mu)^2 \\
 \frac{∂^2}{∂\theta^2} \log f(X;\theta) = \frac{1}{2\theta^2} - \frac{2}{2\theta^3} (x-\mu)^2 = \frac{1}{\theta^2} \left(\frac{1}{2} - \frac{1}{\theta}(x-\mu)^2\right) 
 \end{gather*}
 
 Calculamos ahora la esperanza:
 
 \[ \esp{\frac{1}{\theta^2} \left(\frac{1}{2} - \frac{1}{\theta}(x-\mu)^2\right) } = -\frac{1}{\theta^2}\left(\frac{1}{2} - \frac{1}{\theta} \underbrace{\esp{X-\mu}^2}_{\theta}\right) = \frac{1}{2\theta^2} \]
 
 y por lo tanto la cota FCR vale $\dfrac{2\theta^2}{n}$, el valor mínimo.

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[9]
Sea $\sample[X]$ una muestra de una v.a. con función de densidad 

\[ f(x;\theta) = \theta x^{\theta - 1} \]

Sea  \[ T_n(X_1,\dotsc,X_n) = \frac{-1}{n}\sum_{i=1}^n\log X_i \]

\ppart Probar que \[\esp[\theta]{T_n} = \frac{1}{\theta};\; \var[\theta]{T_n} = \frac{1}{n\theta^2} \]
\ppart ¿Es eficiente $T_n$ como estimador de $\frac{1}{\theta}$?

\solution

\spart
Aplicamos que la esperanza de la media muestral de una variable es la esperanza de la variable. En este caso nuestra variable es $log X$.

\[ \esp[\theta]{T_n} = -\esp[\theta]{\log X} = - \int_0^1 \log x \theta x ^{\theta-1}\,dx = \frac{1}{\theta} \]

Calculamos ahora la varianza (aplicando $\displaystyle \var{\gor{X}} = \frac{\var{X}}{n}$).

\begin{gather*}
\var[\theta]{T_n} = \frac{\var[\theta]{\log X}}{n} = \\
= \esp[\theta]{\log^2 X} - \esp[\theta]{\log X}^2 = \frac{1}{\theta^2}
\end{gather*}

\end{problem}


\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[10]
El número de fallos que se producen anualmente en cierto mecanismo es una v.a. con distribución de Poisson de parámetro $\theta$. El valor de $\theta$ no es conocido exactamente, pero se tiene cierta información a priori que permite considerarlo como una v.a. con distribución $\gamma(a, p)$ (a y p son conocidos). Si \sample[x] son observaciones observaciones independientes de la variable aleatoria ``número de fallos'', calcular la distribución a posteriori y obtener, a partir de ella, un estimador puntual para $\theta$.
\solution

Sea $X \equiv$ número de fallos anuales $\sim Poisson(\theta)$, $\theta > 0$.\\
Su función de densidad es
\[
f(x \vert \theta) =
\prob[\theta]{X = x} =
e^{- \theta} \frac{\theta^x}{x!} \text{ , } \; x = 1, 2, 3, ...
\]
Y el prior es
\[
\pi(\theta) =
\frac{a^p}{\Gamma(p)} e^{-a \cdot \theta} \theta ^{p - 1} \; \text{ con } \theta > 0, a > 0, p > 0
\]
Entonces
\begin{gather*}
\pi(\theta \vert \sample[x]) \approx
f(\sample[x] \vert) \pi(\theta) =
\left( \prod\limits^n_{i = 1} e^{-\theta} \frac{\theta^{x_i}}{x_i !} \right) \frac{a^p}{\Gamma(p)} e^{-a \cdot \theta} \theta ^{p - 1} \sim\\
\sim e^{- \theta (n + a)} \theta^{(\sum\limits^n x_i + p) - 1} \sim
\Gamma(a + n, \sum\limits^n x_i + p)
\end{gather*}
Luego el estimador Bayes de $\theta$ es
\[
\esp{\pi(\theta \vert \sample[x])} =
\frac{\sum\limits^n x_i + p}{a + n} =
\frac{\sum\limits^n x_i}{a + n} + \frac{p}{a + n} =
\avg{x} \cdot \underbrace{\frac{n}{a + n}}_{\xrightarrow[n \to \infty]{} 1} + \frac{p}{a} \cdot \underbrace{\frac{a}{a + n}}_{\xrightarrow[n \to \infty]{} 0}
\Rightarrow \hat\theta = \avg{x}
\]

\end{problem}


\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[11]
(Este ejercicio es del parcial del año pasado)

\solution

$X \leadsto Unif[0,\theta]$
Con \[ f(x) = \displaystyle\left\{\begin{array}{cc}
\frac{1}{\theta} & 0\leq x \leq \theta\\
0 & x \notin [0,\theta]
\end{array}\right.\]

Vamos a calcular la función de distribución:

\[F_{\theta} (x) = \mathbb{P}_{\theta}\{X\leq x\} = \int_{-infty}^x f_{\theta}(t)dt = \int_0^x \frac{1}{\theta} dt = \frac{x}{\theta} \ si 0\leq x \leq \theta\]

\[F_{\theta} = \left\{\begin{array}{cc}
\frac{x}{\theta} & 0\leq x \leq \theta\\
0 & x \notin [0,\theta]
\end{array}\right.\]

Nos piden dibujar las funciones. 


Vamos a calcular \[L_n(\theta;x_i) = \prod_{i=1}^n f_{\theta} (x_i) = \left\{\begin{array}{cc}
\left(\frac{1}{\theta}\right)^n & \forall x_i \in [0,\theta]\\
0 & \exists x_i\notin [0,\theta]
\end{array}\right.\]

Calculamos la $logL_n$ que nos piden dibujarla:

\[logL_n(\theta) = \left\{\begin{array}{cc}
-nlog(\theta) & si \ max(\{x_i\})\leq \theta\\
0 & si \ no
\end{array}\right.\]
Dibujoo!

\[\hat{\theta_n} = e.m.v.(\theta) = max\left(L_n(\theta)\right)\]

También vale tomando el logaritmo:

\[\hat{\theta}_n = e.m.v. (\theta) = arg\ mas logL_n(\theta) = max\{x_i\}\]
porque \[ logLn(\theta) = \displaystyle\left\{\begin{array}{cc}
-nlog(\theta) & max\{x_i\} \leq \theta\\
-\infty & si \ no
\end{array}\right.\]
\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Tema 4 - Intervalos de confianza}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[1 y 2]

\ppart Representa un estimador de la función de densidad de la v.a. X = cantidad de contaminación por mercurio (en p.p.m.) en los peces capturados en los ríos norteamericanos Lumber y Wacamaw (ver fichero Datos-mercurio.txt). Comparar esta densidad estimada con la densidad normal de igual media y desviación típica (representada en la misma gráfica). En vista de las dos funciones dirías que la función de densidad de X es aproximadamente normal?

\ppart Obtener un intervalo de confianza de nivel 0.95 para la media de X.

\ppart Se puede considerar fiable este intervalo a pesar de la posible no-normalidad de X?

\ppart Qué tamaño muestral habrá que tomar para estimar la contaminación media con un error máximo de 0.06?
\solution
Solucionado por Amparo, descargable \href{http://www.uam.es/personal_pdi/ciencias/abaillo/MatEstI/T4DatosMercurio.pdf}{aquí}.

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[3]

\ppart Representa en un mismo gráfico las densidades de las distribuciones $\chi^2_k $ con k = 4,8,20,30.

\ppart $X \sim \gamma(5,10)$. Calcular $\mathbb{P}\{X\leq 3\}$

\ppart Sea $Y \sim \chi_{200}^2$. Calcular $\mathbb{P}\{Y\leq 3\}$

\solution
\spart
El código R utilizado para generar las gráficas es:

\begin{verbatim}
> x = seq(0,20,length.out=1000)
> d1=dchisq(x,df=4)
> d2=dchisq(x,df=8)
> d3=dchisq(x,df=10)
> d4=dchisq(x,df=20)
> plot(x,d1,type='l')
> lines(x,d2,type='l',col='blue')
> lines(x,d3,type='l',col='green')
> lines(x,d4,type='l',col='red')
\end{verbatim}

\begin{center}
\includegraphics[width=1\textwidth]{img/Chicuadrado.png}
\label{Ejercicio 4}
\end{center}

\spart
Vamos a usar el resultado visto en clase:
Si $X\sim \gamma(a,p)$ entonces tenemos que 
\begin{gather*}
X \sim c \cdot \gamma(\frac{a}{c}, p) \implies
c \cdot X \sim \gamma(\frac{a}{c}, p)\\
\text{Como } \gamma \left( \frac{a}{c}, p \right) \sim \chi \left( \frac{1}{2}, \frac{k}{2} \right) \text{ y } a = 5, p = 10
\end{gather*}

Tenemos que $c = 10$, luego:

\[ \prob{10X \leq 30} = \prob{ \chi^2_{20 }\leq 30}  \]

Tenemos varias opciontes. Una de ellas es ir a R y calcularlo con el comando \[ {pchisq(30,20)} = 0.93 \]

Y la otra es irse a las tablas y vemos que $\mathbb{P}\{\chi^2_{20} \leq 30\} \simeq 1 - \frac{0.1 + 0.05}{2} = 0.93 $, ya que en las tablas estamos entre $28.4$ y $31.4$.

\spart Sea $Y \sim \chi_{200}^2$

Podemos hacerlo en R directamente y nos da $\prob{Y \leq 3} = 10 ^{-141}$

A mano, aplicamos el T.C.L, que dice:
\[ \sqrt{n} (\avg{X} - \mu) \convs[d] N(0,\sigma) \]

Entonces tenemos: $\gor{X} \sim N\left(\esp{X}, \displaystyle \sqrt{\frac{\var{X}}{n}}\right)$

Donde\footnote{Recuerda: $\var{\chi^2_k} = 2 * k$} $\esp{X} = \esp{Z^2} = \var{Z} = 1$ y $\var{X} = \var{Z^2} = \var{\chi^2_1} = 2 $

Con lo que:
\[
\avg{X} \sim N \left( 1, \sqrt{\frac{2}{200}} \right) = N \left( 1,\frac{1}{10} \right)
\]

Sustituyendo y estandarizando:

\[
\prob{\avg{X} \leq \frac{3}{20} } \simeq \prob{Z \leq \frac{\frac{3}{200} - 1}{\frac{1}{10}} } = \prob{Z \leq -9.85} = 3 \cdot 10^{-23}
\]

Una diferencia bastante distinta a lo que decía R. Tras un debate entre Miguel y Amparo de 10 minutos no se ha llegado a ninguna conclusión.
\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[4]
\ppart Utilizando el fichero Datos-lipidos.txt, estima, mediante un intervalo de confianza de nivel
0.95, la proporción de pacientes que tienen una concentración de colesterol superior o igual a
220 mg/dl. ¿Qué tamaço muestral habrá que usar para tener una probabilidad aproximada de 0.95 de no cometer un error mayor que 0.01 en la estimación de esta proporción?

\ppart
\solution
Solucionado por Amparo, descargable 
\href{http://www.uam.es/personal_pdi/ciencias/abaillo/MatEstI/T4DatosLipidos.pdf}{aqui}
\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[5] Sea una v.a. con función de densidad $f(x;\theta) = \theta x^{-(\theta + 1)}\ind_{[1,\infty)} $

\ppart Obtener el e.m.v.

\ppart Obtener su distribución asintótica

\ppart Calcular la cantidad pivotal aproximada y, a partir de ella, un intervalo de confianza de nivel aproximada $1-\alpha$ para $\theta$
\solution
\spart \[\dpa{logL(\theta)}{\theta} = 0 \implies e.m.v.(\theta) = \frac{1}{\gor{Y}}\]
donde $Y = log X_i$

\spart Posibles caminos:

a) $ \hat{\theta} \convs[d] $¿?

b) $ \sqrt{n}(\hat{\theta} - \theta) \convs[d] N\left(0,?\right) $

\pagebreak
La primera opción es algo difusa y la segunda es mucho más concreta y mejor.

Tenemos que examinar la expresión $\sqrt{n}(\hat{\theta} - \theta)$
Tenemos 2 posibilidades con las que calcular este tipo de cosas (T.C.L.) y método delta (que es el que emplearemos a continuación)

\[ \mu = \esp{X}; \sigma = \var{X} \]
\[ \sqrt{n}\left(g(\gor{X}) - g(u)\right) \convs N(0,\abs{g'(u)} \sigma) \]

Aplicando el método delta:

\[
\sqrt{n}(\hat{\theta} - \theta) = \sqrt{ n}\left(g(\gor{y})-g(\esp{Y})\right)\convs[d] N\left(0,\underbrace{\abs{g'\left(\frac{1}{\theta}\right)}}_{\theta^2} \sqrt{\var{Y}}\right) = N(0,\theta)
\]

Peeero... hay que tener cuidado con que $\theta = g(\esp{Y})$ porque sino no podemos aplicar el método delta.

\[
\var{Y} = \esp{Y^2} - \esp{^2 Y} = \underbrace{\int_1^{\infty} (log\,x)^2 \theta x^{-(\theta + 1)}dx}_{\displaystyle\frac{2}{\theta^2}} - \frac{1}{\theta^2} = \frac{1}{\theta^2}
\]

\spart
La cantidad pivotal es un estadístico que depende de la muestra y del parámetro desconocido (del que estamos calculando el intervalo) y cuya distribución, al menos asintóticamente) es totalmente conocida.

En el apartado b) hemos encontrado la distribución asintótica para poder construir la cantidad pivotal.

Tipificamos el resultado anterior para evitar que la distribución dependa del parámetro desconocido.

\[
\frac{1}{\theta} \sqrt{n}(\hat{\theta} - \theta)  = 
\sqrt{n} \left(\frac{\hat{\theta}}{\theta} - 1 \right) = \mathbb{Q}(\theta;X_1,...,X_N)
\]

Esta es nuestra cantidad pivotal, que depende de la muestra (por el $\hat{\theta}$) y depende del parámetro.
\[1-\alpha  = \mathbb{P} = \{q_1(\alpha) \leq \mathbb{Q}(\theta;X_1,...,X_N) \leq q_2 (\alpha)\}\]
Tras despejar obtenemos
\[
IC_{1-\alpha}(\theta) =
(\frac{\hat\theta}{1 + \frac{1}{\sqrt{n}} z_{\alpha/2}}, \frac{\hat\theta}{1 - \frac{1}{\sqrt{n}} z_{\alpha/2}})
\]
\end{problem}

\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[6]
Sea $\sample$ una muestra de una v.a. uniforme en el interalo $[0,\theta]$ con $0 < \theta < 1$. Obtener una cantidad pivotal para $\theta$ a partir del emv. Usando esta cantidad pivotal construye un intervalo de confianza para $\theta$ de nivel prefijado $1-\alpha$.

\solution

El e.m.v es \[ emv (\theta) = \hat{\theta} = \max X_i \] La cantidad pivotal para $\theta = Q(\theta; \sample)$

\[ F_{X_{(n)}} (x) = \prob{\hat{\theta}_n \leq x} = \prob{X_{(n)} \leq x} = \prod_{i=1}^{n} \prob{X_i \leq x} = \begin{cases}
0& x<0 \\
\left( \frac{x}{\theta} \right)^n & 0\leq x \leq \theta \\
1 & x > 1
\end{cases}\]

Tomo $Q(\theta; \sample ) = \dfrac{X_{(n)}}{\theta} = \dfrac{\hat{\theta}}{n}$, que es válido como cantidad pivotal porque \[ \prob{Q \leq x} = \prob{\frac{X_{(n)}}{\theta} \leq x} = \begin{cases}
0 & x<0 \\
x^n & 0\leq x \leq \theta \\
1 & x > 1
\end{cases} \]

Tenemos que elegir dos valores $q_1, q_2$ de tal forma que 

\[ 1- \alpha = \prob{q_1(\alpha) \leq Q(\theta;\sample) \leq q_2(\alpha)} \]

¿Cómo elegirlos? Queremos buscar que la longitud del intervalo de confianza $IC_{1-\alpha}(\theta) = \left(\dfrac{\hat{\theta}_n}{q_2},\dfrac{\hat{\theta}_n}{q_1}\right)$ sea mínima. Calculamos esa longitud:

\[ \text{len IC} = \hat{\theta}_n\left(\frac{1}{q_1}-\frac{1}{q_2}\right)=\hat{\theta}_n \left(\frac{q_2-q_1}{q_1q_2}\right) \]

Es decir, tenemos que buscar que $q_1-q_2$ sea más pequeño y además tienen que ser lo mayores posible. Por lo tanto, la elección óptima es 

\[ q_2 = 1,\;q_1=\alpha^{1/n} \]

\end{problem}


\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[7]
Construye tres intervalos de confianza asintóticos diferentes para el parámetro $\lambda$ de una distribución de Poisson usando los tres métodos siguientes:

\ppart Utiliza el comportamiento asintótico de la media muestral, estima de forma consistente la varianza y aplica el teorema de Slutsky.

\ppart Igual que el anterior, pero sin estimar la varianza

\ppart Aplicando el método delta para \textit{estabilizar la varianza}, es decir, buscando una función $g$ tal que $\sqrt{n}(g(\avg{X}) - g(\lambda))\convdist N(0,1)$.

\solution

\spart El TCL (\ref{thmCentral}) nos dice que

\[ \sqrt{n}\frac{\avg{X} - \lambda}{\sqrt{\lambda}} \convdist N(0,1) \]

Entonces tenemos que 
\begin{equation}
 1-\alpha = \prob{-z_{\alpha/2}\leq\sqrt{n}\frac{\avg{X} - \lambda}{\sqrt{\lambda}} \leq z_{\alpha/2}} \label{eqEj7}
 \end{equation}

Sustituyo $\lambda$ en el denominador por una estimación consistente $\hat{\lambda}\convs[P, c.s]\lambda$:

\[ \sqrt{n}\frac{\avg{X} - \lambda}{\sqrt{\hat{\lambda}}} \convdist N(0,1) \]

Como sabemos que $\lambda = \esp{X}$, tomamos la media muestral como el estimador: $\hat{\lambda} = \avg{X}$. La convergencia nos queda entonces como


\[ \sqrt{n}\frac{\avg{X} - \lambda}{\sqrt{\avg{X}}} \convdist N(0,1) \]

y por lo tanto tomamos $ \sqrt{n}\dfrac{\avg{X} - \lambda}{\sqrt{\avg{X}}}$ como nuestra cantidad pivotal. Despejamos ahora en (\ref{eqEj7}):

\[ \prob{\avg{X} - z_{\alpha/2} \sqrt{\frac{\avg{X}}{n}} 
	\leq \lambda
	\leq \avg{X} + z_{\alpha/2} \sqrt{\frac{\avg{X}}{n}}}
	\]

\spart
\noindent Partimos de nuevo de (\ref{eqEj7}), pero no tenemos que estimar $\lambda$. Esta ecuación es equivalente a 

\[ \prob{n\frac{(\avg{X}-\lambda)^2}{\lambda} \leq z_{\alpha/2}^2} \]

De ahí sólo tenemos que despejar $\lambda$ para hallar nuestro intervalo de confianza.

\spart Tenemos que buscar que se satisfaga la ecuación \[ \sqrt{n}(g(\avg{X}) - g(\lambda))\convdist N(0,1) \]

Sin embargo, el método delta (\ref{defMetDelta}) nos dice algo distinto:

\[ \sqrt{n}(g(\avg{X}) - g(\lambda))\convdist N(0,\abs{g'(\mu)}\sqrt{\var{X}}) \]

Entonces tenemos que 

\[ \abs{g'(\lambda)}\sqrt{\lambda} = 1 \implies g'(\lambda) = \frac{1}{\sqrt{\lambda}} \]

e integrando vemos que $g(\lambda) = 2\sqrt{\lambda} $.
\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[8]
\ppart Se desea evaluar aproximadamente, por el \textit{método de Montecarlo}, la integral 
\[ p = \int_0^1f(x)\,dx \] 
de una función continua $\appl{f}{[0,1]}{[0,1]}$. Para ello se generan 500 observaciones independientes $(X_i,Y_i)$ con $i=1,\dotsc,500$ con distribución uniforme en el cuadrado $[0,1]×[0,1]$ y se estima $p$ mediante
\[ \hat{p} = \sum_{i=1}^{500} \frac{Z_i}{500} \]
donde la v.a. $Z_i$ vale 1 si $Y_i \leq f(X_i)$ y $0$ en caso contrario. ¿Qué distribución tienen las $Z_i$? Suponiendo que, en una muestra concreta hemos obtenido $\sum_{i=1}^{500} z_i = 255$, obtener un intervalo de confianza de nivel $0.99$ para la correspondiente estimación de $p$.

\solution
\spart La v.a. sigue una distribución de Bernoulli, de tal forma que

\begin{equation} \prob{Z=1}=\prob{Y \leq f(X)} \label{eqEj8} \end{equation}

\pagebreak
La distribución de densidad de la v.a. $(X_i, Y_i)$ es 
\[
f(x,y) = \begin{cases}
1 & (x,y) \in [0,1] \x [0,1] \\
0 & \text{en otro caso}
\end{cases}
\]

Aplicando esto en $(\ref{eqEj8})$

\[ \prob{Z=1} = \prob{(X,Y) \in \{(x,y)\tq y \leq f(x) \}} = \int_0^1\int_0^{f(x)} \,dy\,dx = \int_0^1f(x)\,dx = p \]

y llegamos a la forma de estimar la integral que queríamos. 

Vamos a contruir el intervalo de confianza de nuvel $0.99$.

\[IC_{0.99} (p) = \left(\gor{z} \pm Z_{0.005}\sqrt{\frac{\gor{z}(1-\gor{\gz})}{500}}\right) = \left(\hat{p} \pm 2575 \sqrt{\frac{\hat{p}(1-\hat{p})}{500}}) \right) = (0.45\pm 0.057)\]


\spart En este caso sabemos el valor de \[ p = \int_0^1 x^2dx = \frac{1}{3} \]
Buscamos un $n$ que cumpla: \[ z_{0.005} \sqrt{\frac{\frac{1}{3}\cdot\frac{2}{3}}{n}} \implies n > 14734.72 \]

\end{problem}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[9]
Sea X una v.a. con distribución normal de media $\mu$ y varianza $\theta$. Estamos interesados en la estimación de $\theta$ basados en muestras $X_1,...,X_n$. Si $s^2$ denota la cuasivarianza muestral, calcular $\var{s^2}$ y compararla con la cota de Fréchet-Cramer-Rao obtenida en la relación 3 de problemas.
\solution

Comentarios previos: Sabemos que $s^2$ es un estimador insesgado de
\[ \var{X} = \frac{1}{n - 1} \sum_{i=1}^n (X_i - \gor{X})^2 \]

Vamos a calcular $\var{s^2}$

Posibilidades:
\begin{itemize}
\item Aunque es un poco largo
\[
\var{s^2} = \esp{s^4} - \left[\esp{s^2}\right]^2
\]

\pagebreak

\item Si $X \sim N(\mu,\sigma)$ entonces \[\frac{(n-1)s^2}{\sigma^2} \sim \chi_{n-1}^2 \]
\end{itemize}

Vamos a utilizar la segunda opción\footnote{ver (\ref{ChiSquared})} y que $\var{\chi^2_{n - 1}} = 2 (n - 1)$:

\begin{gather*}
\var{s^2} =
\var{\frac{n-1}{\sigma^2}s^2\cdot\frac{\sigma^2}{n-1}} =
\frac{\sigma^4}{(n-1)^2} \var{\frac{n-1}{\sigma^2}s^2} =\\
= \frac{\sigma^4}{(n-1)^2} \var{\chi^2_{n-1}} \stackrel{\sigma^2 = \theta}{=}
\frac{\theta^2}{(n-1)^2}2(n-1) =
\frac{2\theta^2}{n-1}
\end{gather*}

$s^2$ por lo tanto no es eficiente $\left( \text{porque la Cota de FCR es: } \displaystyle\frac{2\theta}{n}\right)$.
Por ser $\theta$ la varianza de una $N(\mu,\sigma)$, cuya cota de FCR se calcula en el problema 8H3.

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Tema 5 - Contraste de hipótesis}
\subsection{Hoja 5A}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[1]
En octubre de 2007 el periódico \textit{The New York Times} realizó un muestreo en 20 restaurantes y tiendas de Nueva York con objeto de analizar la variable $X$, que representa el contenido en ppm de metilmercurio en el sushi de atún que se pone a la venta. La media y la cuasi-desviación típica muestrales obtenidas con estas 20 observaciones de $X$ fueron $\avg{x} = 0.794,\, s=0.2953$. Supongamos que $X$ tiene distribución aproximadamente normal.

\ppart ¿Proporcionan estos datos suficiente evidencia estadística a nivel $0.05$ a favor de la hipótesis de que la concentración media de metilmercurio en las raciones de sushi de atún en la población considerada es superior a 0.6 ppm? El p-valor, ¿es menor o mayor que 0.01?

\ppart Obtener, a partir de estos datos, un intervalo de confianza de nivel 0.95 para la concentración media de metilmercurio $\mu$ en toda la población. Calcular el mínimo tamaño muestral mínimo que habría que utilizar para, con una probabilidad de 0.95, estimar la concentración media de metilmercurio con un error máximo de 0.06 ppm.

\solution

\spart Empezamos definiendo la hipótesis nula, que será que $\mu \leq 0.6$ ya que queremos una evidencia muy fuerte para rechazar que la concentración suba del nivel mínimo.

Tenemos el siguiente contraste a nivel $\alpha = 0.05$:
\begin{gather*}
H_0 : \;\;\; \mu \leq 0.6\\
H_1 : \;\;\; \mu > 0.6
\end{gather*}
La región de rechazo en este caso es 

\[ R = \{ T > t_{19;\alpha} \}\]

donde \[ T = \frac{\avg{x} - 0.6}{0.2953/\sqrt{20}} = 2.938 \]

Por otra parte, $t_{19;\alpha} = 1.729$. Se cumple la condición de la región de rechazo, por lo tanto rechazamos $H_0$. El p-valor del contraste tendrá que ser menor entonces que $0.05$.

Para saber si el p-valor es menor que $0.01$ calculamos $t_{19;0.01}=2.53$. Como sigue siendo menor que $T$, seguimos rechazando $H_0$ y por lo tanto el p-valor del contraste será menor que $0.01$.

Si quisiésemos obtener el p-valor concreto del contraste, buscaríamos el valor de $\alpha$ tal que $ t_{19;\alpha} = 2.938$. En R, obtendríamos este valor con la orden

\begin{verbatim}
> pt(2.938, 19, lower.tail=FALSE)
[1] 0.004221168
\end{verbatim}

El p-valor es por lo tanto $0.004$. Esto quiere decir que la probabilidad de obtener la muestra que hemos conseguido suponiendo que $H_0$ sea cierta (esto es, suponiendo que la media de ppm de metilmercurio en el atún es menor que $0.6$) es extremadamente baja, y o bien hemos obtenido una muestra muy, muy extraña o $H_0$ es falsa. Por lo tanto, lo razonable sería rechazar la hipótesis nula y decir que, de media, la concentración de metilmercurio es mayor que $0.6$.

\spart El intervalo de confianza sería 

\[ IC_{0.95} (\mu) = \left(\avg{x} \mp t_{n-1;\frac{\alpha}{2}}\frac{s}{\sqrt{n}} \right) = (0.656, 0.932) \]

Como además $0.6\notin IC_{0.95}(\mu)$, rechazaríamos $H_0:\,\mu=0.06$ a nivel $\alpha=0.05$.

Para hallar el tamaño muestral mínimo buscamos que 

\[ IC_{0.95}(\mu) = (\avg{x} \mp 0.06)\]

Despejando, tenemos que resolver

\[ t_{n-1;0.025}\frac{s}{\sqrt{n}} < 0.06\]

Como no conocemos $s$, lo sustituimos por una aproximación, la cuasivarianza muestral de los 20 restaurantes que teníamos al principio. Además, intuimos que $n$ va a ser grande y por lo tanto $t$ se aproximaría a una distribución normal $Z = N(0,1)$, y por lo tanto

\[ t_{n-1;0.025} \approx z_{0.025} = 1.96 \]

y entonces $n > 93$.

Otra forma de aproximar el $t_{n-1; 0.025}$ sería sustituirlo por $t_1$ ya que a menos grados de libertad, menor peso tienen las colas, luego $t_{n-1; 0.025} < t_{1; 0.025}$.

Despejando obtenemos que n > 3910.

Finalmente, otra forma de aproximarlo sería tomar $n - 1 = 20$, ya que sabemos que el n va a ser mayor que 20. Con esta aproximación obtenemos que n > 105.
\end{problem}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\begin{problem}[2]
\solution
\centerline{\includegraphics[page=1,scale=0.8328]{pdf/_Solucion_T5_a_P2.pdf}} % scale obtenido empíricamente para que quepa en la página
\includepdf[pages=2-]{pdf/_Solucion_T5_a_P2.pdf}
\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\begin{problem}[3]
\solution
\centerline{\includegraphics[page=1,scale=0.8328]{pdf/_Solucion_T5_a_P3.pdf}} % scale copiado del anterior
\end{problem}

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[4] Los niveles en sangre de una hormona denominada FSH están asociados con la fertilidad femenina. Las mujeres que tienen un nivel de FSH ``alto'' (superior a 10 IU/L) tienen en general más dificultad para concebir que aquellas que tienen niveles bajos de FSH. En un estudio realizado recientemente, se analizó la posible relación entre el grupo sanguíneo y la fertilidad. Para ello se midieron los niveles de FSH en una muestra de 254 mujeres en edad fértil con grupo sanguíneo ``O'' y resultó que 43 de ellas tenían niveles altos de FSH y, por tanto, podrían tener dificultades para concebir. En otra muestra, independiente de la anterior, de 309 mujeres cuyo grupo sanguíneo no es O, resultó que 27 tenían niveles altos de FSH. 

\ppart ¿Proporcionan estos datos suficiente evidencia estadística, al nivel 0.05, a favor de la hipótesis de que las mujeres con grupo sanguíneo 0 tienen más dificultades para concebir que las que tienen otro grupo sanguíneo?

\ppart Calcular el tamaño muestral necesario para, con probabilidad 0.95, estimar en la población de mujeres del grupo 0 el porcentaje de las que tienen un nivel alto de FSH, con un error máximo de 2 puntos.

\solution

Consideramos la v.a. $X$ que vale $1$ si una mujer del grupo 0 tiene nivel alto de FSH y 0 si no, y que sigue una distribución de Bernoulli con probabilidad $p_1$. Análogamente, definimos la v.a. $Y$ que vale $1$ si una mujer del grupo no 0 tiene nivel alto de FSH y 0 si no, y que sigue una distribución de Bernoulli con probabilidad $p_2$.

Tenemos que 

\begin{gather*}
\sum_{i=1}^{254} x_i = 43 \\
\sum_{i=1}^{309} y_i = 27 
\end{gather*}

\spart Primero tenemos que definir la hipótesis nula:

\[ H_0:\: p_1 \leq p_2 \]

es decir, que las mujeres con grupo 0 no tienen más dificultad para concebir. Tomamos esto como la hipótesis nula porque es la que aceptamos por defecto, y queremos una evidencia muy fuerte para poder decir que es falsa.

Para construir la región de rechazo, usamos la región del formulario para comparación de proporciones. Usando el TCL, tenemos que si $p_1=p_2=p$ entonces tanto $\avg{X}$ como $\avg{Y}$ van a seguir una distribución normal con $n_i = n_1$ o $n_2$ según sea $X$ ó $Y$

\[ N\left(p, \sqrt{\frac{p(1-p)}{n_i}}\right) \]

y por lo tanto el estadístico del contraste es

\[ Z = \frac{\avg{X} - \avg{Y}}{\sqrt{\avg{p}(1-\avg{p})\left(\frac{1}{n_1}+\frac{1}{n_2}\right)}} \]

siendo $\avg{p}$ un estimador puntual de $p$, y que se calcula como 

\[ \avg{p} = \frac{\sum x_i + \sum y_i}{n_1 + n_2} = \frac{n_1\avg{x} + n_2\avg{y}}{n_1 + n_2} \]

La región de rechazo es

\[ R = \left\{ \avg{x} - \avg{y} > z_{0.05}\sqrt{\avg{p}(1-\avg{p})\left(\frac{1}{n_1}+\frac{1}{n_2}\right)} \right\} \equiv \{ 0.0819 > 0.0460 \} \]

y por lo tanto rechazamos la hipótesis nula al nivel $\alpha=0.05$.

Calculamos ahora el p-valor para tener más datos sobre la hipótesis:

\[ \text{p-valor}\, = \prob{N(0,1) > z} = \dotsb \]

\spart Necesitamos un intervalo de confianza

\[ IC_{0.95}(p_1) = \left(\avg{x} \pm z_0.025\sqrt{\frac{\avg{x}(1-\avg{x}}{n_1}}\right) \]

donde $z_0.025\sqrt{\frac{\avg{x}(1-\avg{x}}{n_1}}$ es el error cometido al estimar $p_1$ con el IC, y que tiene que ser menor que $0.02$. Como no tenemos el valor de $\avg{x}$, lo sustituimos por el valor de la media muestral obtenido en la anterior medición, de tal forma que tenemos que $n_1\geq1351$ para obtener la confianza requerida. 

Si quisiésemos ser más conservadores, sustituiríamos $\avg{x}$ por el valor máximo que podemos obtener, aunque en este caso saldría un tamaño muestral mucho más grande.

\end{problem}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[5]
El gasto telefónico medio bimensual en una muestra de 10 usuarios elegidos al azar en una ciudad ha resultado ser 90 euros y la cuasidesviación típica 11 euros. En otra ciudad se ha tomado, de modo independiente, otra muestra de 12 usuarios y los valores obtenidos para la media y la cuasidesviación típica muestrales han sido, respectivamente, 80 y 10.

\ppart ¿Proporcionan estos datos suficiente evidencia estadística, al nivel 0.05, a favor de la hipótesis  de que el gasto medio en la primera ciudad es más alto que el gasto medio en la segunda? Suponer que las varianzas de las variables que indican los gastos telefónicos en ambas ciudades son iguales. Indicar claramente las restantes suposiciones necesarias para garantizar la validez del procedimiento empleado.

\ppart El p-valor ¿es mayor o menor que 0.01? Razonar la respuesta.

\solution

\spart Definimos las dos variables aleatorias que tenemos: $X$ es el gasto medio bimensual en la primera ciudad, y $Y$ el gasto en la segunda. Tomamos las esperanzas y varianzas:

\begin{gather*}
\esp{X} = \mu_1,\;\var{X} = \sigma_1^2 \\
\esp{Y} = \mu_2,\;\var{Y} = \sigma_2^2 
\end{gather*}

 Definimos la hipótesis nula: $H_0:\, \mu_1\leq\mu_2$, es decir, que el gasto medio en la primera ciudad no es mayor que en la segunda.
 
 Tenemos que suponer que $X$ e $Y$ son normales para poder definir bien el estadístico del contraste. Si usásemos cualquier otra distribución el estadístico del contraste toma una distribución mucho más complicada que no podríamos determinar correctamente. También suponemos que son independientes.                       

La región de rechazo es 

\[ R = \left\{ \avg{x} - \avg{y} > t_{n_1+n_2-2, \alpha} s_p\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}\right\} \] 

Calculando, tenemos que 
\begin{gather*}
\avg{x}-\avg{y} = 10 \text{ y } s_p^2 = 109.45 \\
R = \{10 > 7.73 \}
\end{gather*}
y por lo tanto rechazamos la hipótesis nula.

\spart Calculamos la región de rechazo para $\alpha=0.01$:

\[ R= \{10 > 11.32 \} \]

y por lo tanto para nivel $0.01$ no hay evidencia para rechazar $H_0$. Entonces, el p-valor es mayor que $0.01$.

\end{problem}


\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[6]
Se realiza un experimento para comparar los incrementos en los niveles plasmáticos de insulina producidos por la ingesta de carne y de pescado. Para ello se midieron los incrementos (medido esn picomoles por litro) producidos en la concentración de insulina en la sangre de 6 voluntarios, 90 minutos después de comer un bistec de 250 gramos. Dos días más tarde se realizó de nuevo el experimento con las mismas 6 personas, después de consumir un filete de pescado. En la tabla se observan los resultados:

\begin{tabular} {|l|c|c|c|c|c|c|}
\hline
Persona & 1 & 2 & 3 & 4 & 5 & 6\\
\hline
Resultados con la carne: & 109& 106 & 111& 105 & 110 & 108\\
\hline
Resultados con el pescado: & 100& 95& 105& 106& 80& 88\\
\hline
\end{tabular}

\ppart Proporcionan estos datos suficiente estadística a nivel significación 0.05 para afirmar que el incremento medio...?
\solution

\spart 

\paragraph{1)} Definir las variables:
\begin{itemize}
\item $X$ nivel de insulina en 1 voluntario tras la ingesta de carne. Llamamos a $\esp{X} = \mu_1$
\item $Y$ nivel de insulina en \textbf{el mismo} voluntario tras la ingesta de carne. $\esp{Y} = \mu_2$
\end{itemize}

Tenemos que las variables no son independientes (porque son muestras tomadas de los mismo voluntarios). A este tipo de datos le llamamos datos emparejados \index{Datos \IS emparejados}

\paragraph{2) Definir las hipótesis}
\begin{itemize}
\item $H_0: \mu_1 \leq \mu_2$
\item $H_1 : \mu_1>\mu_2$
\end{itemize}

\paragraph{3)} Como tenemos datos emparejados, podemos trabajar más facilmente con la diferencia, es decir, definimos $D=X-Y$ y definimos el contraste (siendo $\esp{D}=\mu$)
\begin{itemize}
\item $\Huge_0 : \mu \leq 0$
\item $H_1: \mu >0$
\end{itemize}

Que es un contraste equivalente.

Además tenemos que $D \sim N(\mu,\sigma)$ 

Suponer que la diferencia es una normal es el procedimiento estándar para datos emparejados. (nos la jugamos, es una hipótesis del problema, que puede ser más o menos razonable. En este caso, lo único que de momento sabemos hacer es suponer que es normal (si no fuera normal, tendríamos que aplciar el TCL (para lo que necesitamos n grande) y con este tamaño muestral (6) no podríamos aplicarlo)

Mirando en la tabla de regiones de rechazo tenemos:
\[R = \left\{\gor{d}> t_{n-1;\alpha} \frac{s_d}{\sqrt{n}}\right\}\]
Donde $\displaystyle \frac{\gor{d}}{s_d/\sqrt{n}}$ es el estadístico del contraste, que sigue una $t_{n-1}$.

De los datos extraemos $\gor{d} = 12.5;s_d=10.97$.

Para $\alpha = 0.05$ calculamos el cuantil correspondiente de la $t$ de Student. Para $\alpha = 0.05$ es 9.02.

De aquí deducimos que sí hay evidencia para rechazar la hipótesis nula (porque  $\displaystyle \frac{\gor{d}}{s_d/\sqrt{n}} > 9.02$).

\spart Tomando $\alpha = 0.01$ no se cumple la condición de rechazo, no pudiendo negar entonces la hipótesis nula.

\spart Es el típico ejercicio mecánico de extraer el tamaño muestral. % TODO: serás vago
\end{problem}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[7]
Se ha comprobado que la probabilidad de curación espontánea (sin medicación alguna) de cierta enfermedad es de 0.4. Un laboratorio ha obtenido un nuevo medicamento para tratar la enfermedad y quiere demostrar que es eficaz. Para ello, se aplica el tratamiento a 100 pacientes que sufren la enfermedad en silencio y se observa cuántos de ellos se leen este texto.

\ppart Si se han curado 50 personas de las 100. ¿puede afirmarse que el medicamento es eficaz a nivel $\alpha = 0.05$? Calcula el p-valor del contraste.

\ppart ¿Cuántas personas de las 100 deberían curarse como mínimo para poder afirmar al nivel $\alpha = 0.001$ que el tratamiento es eficaz?

\ppart Supongamos que la probabilidad de curación con el tratamiento fuese realmente de 0.5 y que se realiza el test de nivel 0.05 con 100 personas. ¿Cuál sería la probabilidad de error, es decir, la probabilidad de rechazar el medicamento como inútil?

\solution
\spart
Sea $X \sim Bernouilli(p)$, luego $\sum\limits^{100}_i x_i = $ ``número de pacientes que se curan''. Tenemos el siguiente contraste a nivel $\alpha = 0.05$:
\begin{gather*}
H_0 : \;\;\; p \leq 0.4\\
H_1 : \;\;\; p > 0.4
\end{gather*}
La región de rechazo es

\[ R = \{ z = \frac{\avg{x} - 0.4}{\sqrt{\frac{0.4 \cdot 0.6}{100}}} > z_{0.05} \} \]

Como $\frac{\avg{x} - 0.4}{\sqrt{\frac{0.4 \cdot 0.6}{100}}} = 2.041 > 1.645 \implies$ hay evidencia muestral para afirmar que el medicamento es eficaz a nivel $\alpha = 0.05$ (rechazo $H_0$).

El pvalor se calcula así
\begin{verbatim}
> pnorm(2.041, lower.tail=FALSE)
[1] 0.02062541
\end{verbatim} 
Luego a nivel $\alpha = 0.01 < $ p-valor, no habría suficiente evidencia muestral para rechazar la hipótesis nula.

\spart
\[
\frac{\avg{x} - 0.4}{\sqrt{\frac{0.4 \cdot 0.6}{100}}} > z_{0.001}
\implies \sum\limits^{100} x_i > 55.1
\]

\spart
Como $p = 0.5 \rightarrow H_1$ es cierta $\implies$ solo puede cometerse el error de tipo II. Luego

\begin{gather*}
\prob[p = 0.5]{\text{error tipo II}} =
\prob[p = 0.5]{\text{aceptar } H_0} =
1 - \prob[p = 0.5]{R} =
1 - \beta_n(0.5) =\\
= 1 - \prob{\frac{\avg{X} - 0.4}{\sqrt{\frac{0.4 \cdot 0.6}{100}}} > z_{0.05}} =
1 - \prob{\avg{X} > 0.4 + z_{0.05} \sqrt{\frac{0.4 \cdot 0.6} {100}}}
\stackrel{X \approx N(p, \sqrt{\frac{p \cdot(1-p)}{n}}) = N(0.5, 0.05)}{=}\\
= 1 - \prob{Z = \frac{\avg{X} - 0.5}{0.05} > \frac{0.4 + z_{0.05} \cdot \sqrt{\frac{0.4 \cdot 0.6}{100}} - 0.5}{0.05}} =\\
= 1 - \prob{Z > -0.388} =
1 - (1 - \prob{Z > 0.388}) = 0.35
\end{gather*}

\end{problem}


\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[8] 

\ppart Supongamos que en una determinada población de referencia, formada por adultos sanos, el nivel en sangre de la enzima hepática GGT (gamma-glutamil-transpeptidasa) sigue aproximadamente una distribución normal con media poblacional $42 IU/L$ y desviación típica poblacional 13. Calcular aproximadamente el porcentaje de personas en la población que tienen un nivel de GGT superior a 80.

\ppart Supongamos ahora que se selecciona una muestra de 61 personas en otra población formada por bebedores habituales no diagnosticados de alcoholismo y se obtiene una media muestra de 58 IU/L con una desviación típica de 21. ¿Hay suficiente evidencia estadística, al nivel 0.05, para afirmar que la concentración media de GGT en la población de bebedores es mayor que 42?

\solution

\spart
Sea $X \sim N(42, 13)$,

\[
\prob{X > 80} =
\prob{\frac{X-42}{13} > \frac{80 -42}{13}} =
0.0017
\]

\spart
Sea $Y$ el nivel de GGT en sangre

Tenemos el siguiente contraste a nivel $\alpha = 0.05$:
\begin{gather*}
H_0 : \;\;\; \mu \leq 42\\
H_1 : \;\;\; \mu > 42
\end{gather*}
La región de rechazo es

\[ R = \{ z = \frac{\avg{y} - 42}{s/\sqrt{61}} > Z_{0.05} \} \]

Y por lo tanto rechazamos $H_0$ ya que $5.95 > 1.645$. Podemos calcular el p-valor de la siguiente manera

\[
\text{p-valor} =
\prob{N(0, 1) > 5.95} = 7*10^{-8}
\]

Con lo cual, es muy razonable rechazar la hipótesis nula.

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsection{Hoja 5B}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[1]
Tenemos una $X \sim \mop{exp}(\theta)$. Queremos contrastar para $\alpha = 0.01$ las dos siguientes hipótesis: $H_0: \theta = 5$ frente a $H_1: \theta = \theta_1$, siendo $\theta_1 > 5$ un valor prefijado.

\ppart Obtener la región crítica del test UMP.

\ppart Calcular la probabilidad de error de tipo II en este test.

\ppart Supongamos que para una determinada muestra, se obtiene $\sum_{i=1}^5 x_i= 5$. ¿Qué decisión habría que adoptar si se utiliza el test construido en a)?

\solution

\spart Primero comprobamos la propiedad de CVM\footnote{Ejemplo típico de aplicar el lema de Neyman-Pearson (\ref{thmNeymanPearson}).}:

\[ \frac{f_n(\sample[x];\theta_1)}{f_n(\sample[x];5)} = \left(\frac{\theta_1}{5}\right)^n e^{-(\theta_1 - 5)\sum\limits^n x_i} \]

Efectivamente, la función es monótona. % yo no lo tengo tan claro

Por tanto, la región de rechazo del test UMP es, por el lema de Neyman-Pearson (\ref{thmNeymanPearson}), la siguiente:

\[
R^{\ast} =
\left\{ \left(\frac{\theta_1}{5}\right)^n e^{(-\theta_1-5)\sum\limits^n x_i > k_\alpha}\right\}
\]

Ya que una vez fijado $\theta_1$ lo que determina la cota superior es el sumatorio, tenemos que

\[
R^{\ast} =
\left\{\sum x_i < c_{\alpha}\right\}
\text{ tal que }
\prob[\theta = 5]{ R^{\ast} } = \alpha
\]

Como $X \sim \exp(\theta) = \gamma(\theta, 1)$ y las $X_i$ son v.a.i., tenemos que 

\[ \sum X_i \sim \gamma(\theta, n) \]

y entonces

\[ \prob[\theta=5]{R^{\ast}} = \alpha = \prob{\gamma(5, n) < c_\alpha} \]

De esta forma, $c_\alpha$ es el cuantil $\alpha$ de la distribución $\gamma(5, n)$:

\[ c_\alpha = q_{5; n}(\alpha) \]

Finalmente, como $\alpha = 0.01 $, entonces

\[
R^{\ast} = \left\{ \sum\limits^n x_i < q_{5; n}(0.01) \right\}
\]

\spart Calculamos el error de tipo II, (\ref{errorTipoII})

\[
\prob[\theta_1]{R^c} =
1 - \prob[\theta_1]{R} =
1 - \prob[\theta_1]{\sum x_i < q_{5; n}(0,01)} =
1 - \prob{\gamma(\theta_1, n) < q_{5; n}(0,01)}
\]

Usando las propiedades de la distribución gamma, tenemos que

\[
\gamma(\theta_1, n) =
\gamma\left(\frac{\theta_1}{5}5, n\right) =
\frac{5}{\theta_1}\gamma(5, n)
\]

y entonces

\[
\prob[\theta_1]{R^c} =
1 - \prob{\gamma(5, n) < \frac{\theta_1}{5}q_{5; n}(0,01)} =
\prob{\gamma(5, n)\geq \frac{\theta_1}{5}q_{5; n} (0,01)}
\xrightarrow[\theta_1 \to \infty]{} 0
\]

Concretamente tenemos que

\[
\prob[\theta_1]{R^c} =
1 - 0.01 - \prob{q_{5; n}(0.01) \leq \gamma(5, n) < \frac{\theta_1}{5}q_{5; n} (0,01)} =
0.99 - O(1 - \frac{\theta_1}{5})
\]

Lo que quiere decir que la probabilidad de error de tipo II se hace arbitrariamente cercana a 1 - $\alpha$ cerca de $\theta_1$.

\spart Nuestra muestra nos da una estimación puntual de $\avg{x} = 1$.

\noindent Bajo la hipótesis nula, la media de la población debería ser $\frac{1}{5}$, ya que $\esp{X} = \frac{1}{\theta}$.

\noindent Bajo la hipótesis alternativa, la media debería ser $ < \frac{1}{5}$.

Intuitivamente, no tenemos evidencia muestral en contra de $H_0$. Comprobémoslo ahora calculando la región de rechazo:
tenemos que calcular el cuantil de la distribución Gamma:

\[ q_{5, 5}(0.01) = 0.2558 \rightarrow 5 \nless 0.2558 \]

Luego no hay evidencia muestral para rechazar la hipótesis nula, tal y como habíamos intuido.

\end{problem}


\pagebreak
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[2]

En una piscifactoría se desea contrastar la hipótesis nula de que el porcentaje de peces adultos que miden menos de 20 cm es como máximo del 10\% . Para ello, se toma una muestra de 6 peces y se rechaza $H_0$ si se encuentra más de uno con longitud inferior a 20 cm.

\ppart ¿Cuál es el nivel de significación de este contraste?

\ppart Calcula la potencia del contraste si en realidad hay un 20\% de peces que miden menos de 20 cm.

\solution

Sea $X \sim Bernouilli(p)$ tal que
\[
X = \begin{cases}
1 & \text{si un pez adulto de la piscifactoría mide menos de 20cm} \\
0 & \text{en otro caso}
\end{cases}
\]

Tenemos pues el siguiente contraste a nivel $\alpha$:
\begin{gather*}
H_0: \; p \leq 0.1\\
H_1: \; p > 0.1
\end{gather*}

Nos dicen que
\begin{gather*}
R = \left\{ \sum\limits^6 x_i > 1 \right\} =
\left\{ \sum\limits^6 x_i \geq 2 \right\}
\end{gather*}
Nótese que $\sum\limits^6 x_i$ es una binomial (6, p).

\spart
Tamaño del test = $\max \prob{ \text{error tipo I} } = \max_{p \leq 0.1} \prob[p]{R} \leq \alpha$.

Tenemos que maximizar la siguiente expresión:

\begin{gather*}
\beta(p) =
\prob[p]{R} =
\prob[p]{\sum\limits^6 X_i \geq 2} =
1 - \prob[p]{\sum\limits^6 X_i = 0} - \prob[p]{\sum\limits^6 X_i = 1} =\\
= 1 - (1 - p)^6 - 6*(1 - p)^5 p =
1 - (1 - p)^5 (1 + 5p)
\end{gather*}

Notese que hay 6 formas de obtener un 1 y cinco 0s. Bien, derivemos:
\begin{gather*}
\beta'(p) = -5 (1 - p)^4 * (-1) * (1 + 5p) - (1 - p)^5 * (5) =\\
= (1 - p)^4 (5 + 25p - 5 - 5p) =
(1 - p)^4 (20p) > 0 \; \forall p \in (0, 1)
\Rightarrow \beta \text{ es creciente en (0, 1)}
\end{gather*}

Luego
\begin{gather*}
\max_{p \leq 0.1} \prob[p]{R} = \max_{p \leq 0.1} \beta(p) = \beta(0.1) = 1 - 0.9^5 * (1 + 5*0.1) = 0.1143
\end{gather*}

Nótese que como $\beta$ es monótona creciente, alcanza su máximo en el extremo del intervalo.

\spart
Simplemente debemos calcular el valor de la función de potencia. Es decir, sustituir:

\[
\beta(0.2) = 1 - (1 - 0.2)^5 (1 + 5*0.2) = 0.3446
\]

\end{problem}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[3]
El error que se comete en la medición de una magnitud es una v.a. $X$ cuya función de densidad es 

\[ f(x;\theta) = \frac{1}{\sqrt{2\pi\theta}}e^{-\frac{x^2}{2\theta}} \]

siendo $\theta > 0$ un parámetro que se desea estimar. Obtener el test uniformemente más potente de nivel $\alpha$ para contrastar $H_0:\,\theta\leq\theta_0$ frente a $H_1:\,\theta > \theta_0$

\solution
OBSERVACIÓN: los errores de los aparatos de medición se suponen que siguen una N(0, $\sigma$).\\

Sea $X \sim N(0, \sqrt{\theta})$, $\; \theta > 0$, donde X es ``el error cometido por el aparato de medición''. 

Tenemos que comprobar primero que el cociente de verosimilitudes es monótono. Para ello tomamos $\theta_1 < \theta_2$ y calculamos la razón de verosimilitudes:

\[
\frac{f(\sample[x];\theta_2)}{f(\sample[x];\theta_1)} =
\left(\frac{\theta_1}{\theta_2}\right)^{\frac{n}{2}} e^{-\frac{1}{2}\left(\frac{1}{\theta_2} - \frac{1}{\theta_1}\right)\sum x_i^2}
\]

que sí es una función creciente\footnote{Porque el exponente de la exponencial es siempre positivo} de $T_n=\sum x_i^2$. Por lo tanto esta es una familia paramétrica CVM (ver definición \ref{defFamCVM}). Aplicando el teorema (\ref{thmNeymanPearson2})

\[ R = \{ T_n > k_\alpha \} \tq \prob[\theta_0]{R} = \alpha = \prob[\theta_0]{\sum\limits^n X_i^2 > k_\alpha} \] 

¿Cómo resolvemos la expresión de $k_\alpha$? Tomamos

\[ k_\alpha = \theta_0 \chi_{n;\alpha}^2 \]

Por lo que

\[ R = \prob[\theta_0]{\sum\limits^n X_i^2 > \theta_0 \chi_{n;\alpha}^2} \]

\end{problem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\begin{problem}[4]
\solution
\centerline{\includegraphics[page=1,scale=0.7835]{pdf/_Solucion_T5_b_P4.pdf}} % scale obtenido empíricamente para que quepa en la página
\includepdf[pages=2-]{pdf/_Solucion_T5_b_P4.pdf}
\end{problem}

\newpage

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{problem}[5]
Sea $X_1,\dotsc, X_{16}$ una muestra de tamaño 16 de una población normal de esperanza $\mu$ y varianza $\sigma^2 = 1$. Se desea contrastar $H_0:\,\mu = 0$ frente a $H_1:\,\mu \neq 0$.

\ppart Calcula la región crítica del contraste de razón de verosimilitudes de nivel $\alpha = 0.05$. ¿Qué decisión se toma a nivel $\alpha = 0.05$ si con 16 datos se ha obtenido una media muestral $\avg{x} = 1$?

\ppart Para el contraste anterior, ¿cuál es el valor de la función de potencia evaluada en $\mu = 0.75$?
\solution

\spart
Sea X $\sim N(\mu, 1)$, calculamos la función de verosimilitud:

\[ f(\sample[x];\mu) = \frac{1}{(2\pi)^{n/2}} e^{-\frac{1}{2}\sum(x_i-\mu)^2} \]

Nuestro espacio paramétrico es 
\begin{gather*}
\Theta_0 = \{ \mu = 0 \} \\
\Theta = \real
\end{gather*}

Entonces el cociente es\footnote{recuerda que el EMV($\mu$) = $\avg{x}$}

\[
\Lambda_n =
\frac{f(\sample[x];0)}{f(\sample[x];\avg{x})} =
e^{-\frac{1}{2} \left(\sum x_i^2 - \sum(x_i - \avg{x})^2 \right)} = 
e^{-\frac{1}{2}n \avg{x}^2}
\]

Y la región de rechazo es

\[ R = \{ \Lambda_n < k_\alpha \} \text{ donde } k_\alpha \text{ es tal que } \prob[\mu = 0]{R} = \alpha \] 

La región de rechazo se puede expresar (utilizando (\ref{bondadDeAjuste})) de forma equivalente

\[ R=\{ -2\log \Lambda_n > c_\alpha\} = \{ n\avg{x}^2> c_\alpha \} \]

con $c_\alpha$ cumpliendo la misma condición que $k_\alpha$. Es decir

\[ \alpha = \prob[\mu=0]{n\avg{X}^2 > c_\alpha} \]

Sabemos que la distribución de una media de normales es también una normal, luego bajo $H_0: \mu = 0$, $\avg{X} \sim N(0,\frac{1}{\sqrt{n}})$.\\
De la misma forma $\sqrt{n} \, \avg{X}^2\sim N(0,1)$ y finalmente $n\avg{x}^2\sim \chi_1^2$. Entonces

\[ R = \{ n \avg{x}^2 > \chi_{1;\alpha}^2 \} \]

A nivel $\alpha = 0.05$, como n = 16 y $\avg{x} = 1$, tenemos que
\[ R = \{ 16 > 3.84 \} \]
y por lo tanto, hay evidencia muestral para rechazar la hipótesis nula.

\spart Tenemos que

\[
\beta_n(\mu = 0.75) =
\prob[\mu = 0.75]{\mathrm{rechazar}\,H_0} =
\prob[\mu = 0.75]{R} =
\prob[\mu = 0.75]{n\avg{X}^2 > \chi_{1;\alpha}^2}
\] 

Evaluando

\[
n\avg{X}^2 =
n(\avg{X} - 0.75 + 0.75)^2 =
\underbrace{n(\avg{X} - 0.75)^2}_\text{$\chi^2_1$} +
0.75^2 +
\underbrace{2(\avg{X} - 0.75)\cdot 0.75}_\text{nueva v.a. $\rightarrow$ problemón}
\]

Amparo observa que esto nos complica la vida, así que toma otro camino:

\begin{gather*}
\prob[\mu = 0.75]{n\avg{X}^2 > \chi_{1;\alpha}^2} =
1 - \prob[\mu = 0.75]{n\avg{X}^2 \leq \chi_{1;\alpha}^2} =
1 - \prob[\mu = 0.75]{\abs{\frac{\avg{X}}{1/\sqrt{n}}} \leq (\chi_{1;\alpha}^2)^{1/2}} =\\
\stackrel{\text{$\avg{X} \sim N(0.75, \frac{1}{\sqrt{n}})$}}{=}
1 - \prob[\mu = 0.75]{-(\chi_{1;\alpha}^2)^{1/2} \leq \frac{\avg{X}}{1/\sqrt{n}} \leq (\chi_{1;\alpha}^2)^{1/2}} = ...
\end{gather*}

Con lo que solo nos queda estandarizar y resolver

\begin{gather*}
... =
1 - \prob[\mu = 0.75]{-(3.84)^{1/2} \leq \frac{\avg{X}}{1/\sqrt{n}} \leq (3.84)^{1/2}}
\stackrel{\text{Z $\sim \frac{\avg{X} - \mu}{\sigma}$, $\sigma = \frac{1}{\sqrt{n}}$, $\mu = 0.75$}}{=}\\
= 1 - \prob[\mu = 0.75]{\frac{-1.96\frac{1}{\sqrt{n}} - 0.75}{1/\sqrt{n}} \leq Z \leq \frac{1.96\frac{1}{\sqrt{n}} - 0.75}{1/\sqrt{n}}}
\stackrel{\text{n = 16}}{=}\\
= 1 - \prob{-4.96 \leq Z \leq -1.04}
\stackrel{\text{*}}{=}
1 - (\underbrace{\prob{Z > 1.04}}_\text{$\approx$ 0.15} - \underbrace{\prob{Z > 4.96}}_\text{$\approx$ 0})
\approx 0.85
\end{gather*}

(*) Aquí utilizo que la normal es simétrica para poder calcular esa probabilidad con las tablas que tenemos.

\end{problem}
