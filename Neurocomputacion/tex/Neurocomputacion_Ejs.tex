% -*- root: ../Neurocomputacion.tex -*-
\section{Redes neuronales sencillas para clasificación de patrones}
\subsection{Redes Hebb}

\begin{problem}[1]
Aplica la regla de Hebb a los patrones de entrenamiento que definen la función XOR.

\solution

\end{problem}

\begin{problem}[2]
There are 16 different logic functions (with two inputs and one output), of which 14
are linearly separable. Show that the Hebb rule can find the weights for all problems
for which weights exist, as long as bipolar representation is used and a bias is included.

\solution

\end{problem}

\begin{problem}[3]
 Consider character recognition using the Hebb rule. In Example 2.8, the "X" and
"0" used for training differed in all but four components. Show that the net will
respond correctly to an input vector formed from either the "X" or the "0" with
up to 20 components missing. (Whether it responds correctly, of course, is based on
your knowing which pattern you started with-you might prefer an "I don't know"
response. However, since the net input to the output unit is smaller the more com-
ponents are missing, the "degree of certainty" of the response can also be judged.)
Mistakes involve one or more pixels switching from the value in the original
training pattern to the opposite value. Show that at approximately 10 mistakes, the
net will be seriously confused. (The exact number depends on whether any of the
mistakes occur in the pixels where the training patterns differ.)
Example 2.8 could be rephrased as an example of distinguishing the pattern X
from not-X (rather than specifically detecting X versus 0). Another pattern (for not-
X) that might be added to the net is:

Find the new weights to store this pattern together with the two patterns used in
Example 2.8. (You can work from the weights in the example.) What does the bias
value tell you? How does the ability of the net to respond to noisy (missing or mis-
taken) data change as you add more patterns to the net? (You will probably need to
try more patterns of your own choosing in order to form a good conjecture to answer
this question.)

\solution

\end{problem}

\begin{problem}[4]
Create more letters, or different versions of X's and D's, for more training or testing
of the Hebb net.


\solution

\end{problem}

\begin{problem}[5]
\ppart Using the Hebb rule, find the weights required to perform the following classi-
fications: Vectors (I, I, I, 1) and ( - 1, 1, - 1, -1) are members of the class (and
therefore have target value 1); vectors (I, 1, 1, - 1) and (I, - 1, - 1, 1) are not
members of the class (and have target value -1).
\ppart Using each of the training x vectors as input, test the response of the net.


\solution

\end{problem}

\begin{problem}[6]
\ppart The Hebb rule is sometimes used by converting the binary training patterns (inputs
and targets) to bipolar form to find the weight matrix. Apply this procedure to
find the weights to store the following classifications:
\begin{center}
s(l) = (1,0, 1), t(1) = 1\\
s(2) = (1, 1,0), t(2) = 0
\end{center}
\ppart Using the binary step function (with threshold 0) as the output unit's activation
function, test the response of your network on each of the binary training patterns.
\ppart Using the bipolar step function (with threshold 0) as the output unit's activation
function, convert the training patterns to bipolar form and test the network re-
sponse again.
\ppart Test the response of your network on each of the following noisy versions of the
bipolar form of the training patterns:
\begin{center}
(0,-1,1) (0, 1, -1) (0, 0, 1) (0,0, -1) (0, 1,0) (0, -1,0)\\
(1, 0, 1) (1,0, -1) (1, -1,0) (1,0, 0) (1, 1,0) (1, 1, 1)
\end{center}
Which of the responses are correct, which are incorrect, and which are indefinite
(undetermined)?

\solution

\end{problem}

\subsection{Perceptrón}

\begin{problem}[7]
Graph the changes in separating lines as they occur in Example 2.12.
\solution

\end{problem}

\begin{problem}[8]
Explore the influence of the value of the learning rate on the speed of convergence
of perceptron learning:
\ppart Consider different values of a in Example 2.12; explain your results.
\ppart Modify the proof of the perceptron learning rule convergence theorem to include
an arbitrary learning rate a.
\solution

\end{problem}

\begin{problem}[9]
 Show that the use of a bias is essential in Example 2.11. That is, show that it is
impossible to find weights W I and W2 for which the points (1,1), (1,0), (0,1), and (0,0)
are classified correctly. First, show that (0,0) will never be classified correctly, and
in fact, no learning will ever occur for that point. Then, neglecting (0,0), consider
whether (1,1), (1,0), and (0,1) can be classified correctly. That is, do weights WI and
W2 exist such that
\begin{center}
(1)$w_1$ + (1)$w_2$ > $\theta $ > 0,\\
(1)$w_1$ + (0)$w_2$ < $-\theta $ < 0,\\
(0)$w_1$ + (1)$w_2$ < $-\theta $ - 0
\end{center}
\solution

\end{problem}

\begin{problem}[10]
Show that small initial weights still allow for any position of the initial decision line
for the perceptron.

\solution

\end{problem}

\begin{problem}[11]
 Repeat Example 2.11, and show that there is no change in the training process if
9 = O. Show that the separating line is
\[x_2=-\frac{2}{3}x_1+\frac{4}{3}\]
\solution

\end{problem}

\begin{problem}[12]
 Consider carefully the difference in what can be solved using the following activation
functions:
\[f=\left\{ \begin{array}{ll}1 & \text{ if net } \geq \theta \\ 0 & \text{otherwise} \end{array}\right.\]
or
\[f=\left\{ \begin{array}{ll}1 & \text{ if net } \geq \theta \\ -1 & \text{otherwise} \end{array}\right.\]
or
\[f=\left\{ \begin{array}{ll}1 & \text{ if net } \geq \theta \\ 0 & \text{if} -\theta < \text{net} < \theta \\ -1 & \text{ if net } < -\theta\end{array}\right.\]
\solution

\end{problem}

\begin{problem}[13]
 Even for 9 == 0, the perceptron learning rule prevents the correct classification of a
point on the dividing line (which is better than assigning it arbitrarily to either side
of the line). If 9 < a (the learning rate), does the exact value of 9 matter? Does it
matter if 9 > a? Does it make a difference whether we start with all initial weights
equal to 0, as in Examples 2.11-2.13, or with other values (small random numbers,
for instance)?

\solution

\end{problem}

\begin{problem}[14]
 A variation of the perceptron learning rule allows active input units to increase their
weights and inactive units to decrease their weights in such manner that the total
weights are constant [see Block, 1962, p. 144, footnote 50]. Consider the effect this
would have on the binary representation of the AND function in Example 2.11.

\solution

\end{problem}

\begin{problem}[15]
 Using the perceptron learning rule, find the weights required to perform the following
classifications: Vectors (I, I, 1, 1) and ( - 1, 1, - I, - 1) are members of the class
(and therefore have target value 1); vectors (l , I, I, -1) and (l, - 1, - 1, 1) are not
members of the class (and have target value -1) . Use a learning rate of 1 and starting
weights of O. Using each of the training x vectors as input, test the response of the
net.

\solution

\end{problem}


